
R version 4.5.0 (2025-04-11) -- "How About a Twenty-Six"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "pan"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('pan')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("ecme")
> ### * ecme
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ecme
> ### Title: ECME algorithm for general linear mixed model
> ### Aliases: ecme
> ### Keywords: models
> 
> ### ** Examples
> 
> ########################################################################
> # A simple linear model to these data using ecme(). This will be a
> # traditional repeated-measures style additive model with a fixed effect
> # for each column (occasion) and a random intercept for each subject.
> #
> # The data to be used is contained the object marijuana. Since the six
> # measurements per subject were not clearly ordered in time, we consider
> # a model that has an intercept and five dummy codes to allow the
> # population means for the six occasions to be estimated freely together
> # with an intercept randomly varied by subject. For a subject i with no
> # missing values, the covariate matrices will be
> #
> #                   1 1 0 0 0 0              1
> #                   1 0 1 0 0 0              1
> #           Xi =    1 0 0 1 0 0       Zi =   1
> #                   1 0 0 0 1 0              1
> #                   1 0 0 0 0 1              1
> #                   1 0 0 0 0 0              1
> #
> # When using ecme(), these are combined into a single matrix called
> # pred. The pred matrix has length(y) rows. Each column of Xi and Zi
> # must be represented in pred. Because Zi is merely the first column
> # of Xi, we do not need to enter that column twice. So pred is simply
> # the matrices Xi (i=1,...,9), stacked upon each other.
> #
> data(marijuana)
> # we only use the complete data to illustrate
> complete <- subset(marijuana,!is.na(y))
> attach(complete)
> pred <- with(complete,cbind(int,dummy1,dummy2,dummy3,dummy4,dummy5))
> xcol <- 1:6
> zcol <- 1
> # Now we can fit the model.
> result <- ecme(y,subj,occ,pred,xcol,zcol)
Performing ECME...
> result
$beta
[1] -3.209807 12.000000 20.098696 21.487458  4.173955 10.765363

$sigma2
[1] 97.61031

$psi
         [,1]
[1,] 1.649322

$converged
[1] TRUE

$iter
[1] 555

$loglik
  [1] -137.7208 -137.5883 -137.4643 -137.3870 -137.3358 -137.2998 -137.2732
  [8] -137.2529 -137.2369 -137.2241 -137.2136 -137.2049 -137.1976 -137.1913
 [15] -137.1860 -137.1813 -137.1773 -137.1737 -137.1705 -137.1677 -137.1651
 [22] -137.1628 -137.1608 -137.1589 -137.1572 -137.1557 -137.1542 -137.1529
 [29] -137.1517 -137.1506 -137.1496 -137.1486 -137.1477 -137.1469 -137.1461
 [36] -137.1454 -137.1447 -137.1441 -137.1435 -137.1430 -137.1424 -137.1419
 [43] -137.1415 -137.1410 -137.1406 -137.1402 -137.1398 -137.1395 -137.1391
 [50] -137.1388 -137.1385 -137.1382 -137.1379 -137.1377 -137.1374 -137.1372
 [57] -137.1369 -137.1367 -137.1365 -137.1363 -137.1361 -137.1359 -137.1357
 [64] -137.1355 -137.1354 -137.1352 -137.1351 -137.1349 -137.1348 -137.1346
 [71] -137.1345 -137.1344 -137.1343 -137.1341 -137.1340 -137.1339 -137.1338
 [78] -137.1337 -137.1336 -137.1335 -137.1334 -137.1333 -137.1332 -137.1331
 [85] -137.1331 -137.1330 -137.1329 -137.1328 -137.1328 -137.1327 -137.1326
 [92] -137.1326 -137.1325 -137.1324 -137.1324 -137.1323 -137.1323 -137.1322
 [99] -137.1322 -137.1321 -137.1320 -137.1320 -137.1320 -137.1319 -137.1319
[106] -137.1318 -137.1318 -137.1317 -137.1317 -137.1317 -137.1316 -137.1316
[113] -137.1315 -137.1315 -137.1315 -137.1314 -137.1314 -137.1314 -137.1313
[120] -137.1313 -137.1313 -137.1312 -137.1312 -137.1312 -137.1312 -137.1311
[127] -137.1311 -137.1311 -137.1311 -137.1310 -137.1310 -137.1310 -137.1310
[134] -137.1309 -137.1309 -137.1309 -137.1309 -137.1309 -137.1308 -137.1308
[141] -137.1308 -137.1308 -137.1308 -137.1307 -137.1307 -137.1307 -137.1307
[148] -137.1307 -137.1307 -137.1306 -137.1306 -137.1306 -137.1306 -137.1306
[155] -137.1306 -137.1306 -137.1305 -137.1305 -137.1305 -137.1305 -137.1305
[162] -137.1305 -137.1305 -137.1305 -137.1304 -137.1304 -137.1304 -137.1304
[169] -137.1304 -137.1304 -137.1304 -137.1304 -137.1304 -137.1304 -137.1303
[176] -137.1303 -137.1303 -137.1303 -137.1303 -137.1303 -137.1303 -137.1303
[183] -137.1303 -137.1303 -137.1303 -137.1303 -137.1302 -137.1302 -137.1302
[190] -137.1302 -137.1302 -137.1302 -137.1302 -137.1302 -137.1302 -137.1302
[197] -137.1302 -137.1302 -137.1302 -137.1302 -137.1302 -137.1301 -137.1301
[204] -137.1301 -137.1301 -137.1301 -137.1301 -137.1301 -137.1301 -137.1301
[211] -137.1301 -137.1301 -137.1301 -137.1301 -137.1301 -137.1301 -137.1301
[218] -137.1301 -137.1301 -137.1301 -137.1301 -137.1300 -137.1300 -137.1300
[225] -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300
[232] -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300
[239] -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300
[246] -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300 -137.1300
[253] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[260] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[267] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[274] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[281] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[288] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[295] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[302] -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299 -137.1299
[309] -137.1299 -137.1299 -137.1299 -137.1299 -137.1298 -137.1298 -137.1298
[316] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[323] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[330] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[337] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[344] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[351] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[358] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[365] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[372] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[379] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[386] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[393] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[400] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[407] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[414] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[421] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[428] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[435] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[442] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[449] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[456] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[463] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[470] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[477] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[484] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[491] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[498] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[505] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[512] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[519] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[526] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[533] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[540] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[547] -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298 -137.1298
[554] -137.1298 -137.1298

$cov.beta
          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
[1,]  12.40604 -24.60733 -24.62881 -24.63157 -24.63508 -24.62881
[2,] -36.60387  24.40258 -12.20129 -12.20129 -12.20129 -12.20129
[3,] -35.29112 -10.86707  23.06836 -10.84282 -10.83932 -10.84559
[4,] -36.67659 -12.24977 -12.22553  24.45106 -12.19777 -12.22553
[5,] -38.45735 -14.02702 -13.99928 -13.97502  26.22831 -13.99928
[6,] -35.29112 -10.86707 -10.84559 -10.84282 -10.83932  23.06836

$bhat
NULL

$cov.b
NULL

> 
> # Now we compare to lmer
> if(require(lme4)) {
+ result <- lmer(y~-1+pred+(1|subj))
+ result
+ vcov(result)
+ detach(complete)
+ }
Loading required package: lme4
Loading required package: Matrix
> ########################################################################
> 
> 
> 
> cleanEx()

detaching ‘package:lme4’, ‘package:Matrix’

> nameEx("pan")
> ### * pan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pan
> ### Title: Imputation of multivariate panel or cluster data
> ### Aliases: pan
> ### Keywords: models
> 
> ### ** Examples
> 
> ########################################################################
> # This example is somewhat atypical because the data consist of a
> # single response variable (change in heart rate) measured repeatedly;
> # most uses of pan() will involve r > 1 response variables. If we had
> # r response variables rather than one, the only difference would be
> # that the vector y below would become a matrix with r columns, one
> # for each response variable. The dimensions of Sigma (the residual
> # covariance matrix for the response) and Psi (the covariance matrix
> # for the random effects) would also change to (r x r) and (r*q x r*q),
> # respectively, where q is the number of random coefficients in the
> # model (in this case q=1 because we have only random intercepts). The
> # new dimensions for Sigma and Psi will be reflected in the prior
> # distribution, as Dinv and Binv become (r x r) and (r*q x r*q).  
> #
> # The pred matrix has the same number of rows as y, the number of
> # subject-occasions. Each column of Xi and Zi must be represented in
> # pred. Because Zi is merely the first column of Xi, we do not need to
> # enter that column twice. So pred is simply the matrix Xi, stacked
> # upon itself nine times.
> #
> data(marijuana)
> attach(marijuana)
> pred <- with(marijuana,cbind(int,dummy1,dummy2,dummy3,dummy4,dummy5))
> #
> # Now we must tell pan that all six columns of pred are to be used in
> # Xi, but only the first column of pred appears in Zi.
> #
> xcol <- 1:6
> zcol <- 1
> ########################################################################
> # The model specification is now complete. The only task that remains
> # is to specify the prior distributions for the covariance matrices
> # Sigma and Psi.
> #
> # Recall that the dimension of Sigma is (r x r) where r
> # is the number of response variables (in this case, r=1). The prior
> # distribution for Sigma is inverted Wishart with hyperparameters a 
> # (scalar) and Binv (r x r), where a is the imaginary degrees of freedom
> # and Binv/a is the prior guesstimate of Sigma. The value of a must be
> # greater than or equal to r. The "least informative" prior possible
> # would have a=r, so here we will take a=1. As a prior guesstimate of 
> # Sigma we will use the (r x r) identity matrix, so Binv = 1*1 = 1.
> #
> # By similar reasoning we choose the prior distribution for Psi. The
> # dimension of Psi is (r*q x r*q) where q is the number of random
> # effects in the model (i.e. the length of zcol, which in this case is
> # one). The hyperparameters for Psi are c and Dinv, where c is the
> # imaginary degrees of freedom (which must be greater than or equal to
> # r*q) and Dinv/c is the prior guesstimate of Psi. We will take c=1
> # and Dinv=1*1 = 1.
> #
> # The prior is specified as a list with four components named a, Binv,
> # c, and Dinv, respectively.
> #
> prior <- list(a=1,Binv=1,c=1,Dinv=1)
> ########################################################################
> # Now we are ready to run pan(). Let's assume that the pan function
> # and the object code have already been loaded into R. First we
> # do a preliminary run of 1000 iterations. 
> #
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=13579,iter=1000)
> #
> # Check the convergence behavior by making time-series plots and acfs
> # for the model parameters. Variances will be plotted on a log
> # scale. We'll assume that a graphics device has already been opened.
> #
> plot(1:1000,log(result$sigma[1,1,]),type="l")
> acf(log(result$sigma[1,1,]))
> plot(1:1000,log(result$psi[1,1,]),type="l")
> acf(log(result$psi[1,1,]))
> par(mfrow=c(3,2))
> for(i in 1:6) plot(1:1000,result$beta[i,1,],type="l")
> for(i in 1:6) acf(result$beta[i,1,])
> #
> # This example appears to converge very rapidly; the only appreciable
> # autocorrelations are found in Psi, and even those die down by lag
> # 10. With a sample this small we can afford to be cautious, so let's
> # impute the missing data m=10 times taking 100 steps between
> # imputations. We'll use the current simulated value of y as the first
> # imputation, then restart the chain where we left off to produce
> # the second through the tenth.
> #
> y1 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=9565,iter=100,start=result$last)
> y2 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=6047,iter=100,start=result$last)
> y3 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=3955,iter=100,start=result$last)
> y4 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=4761,iter=100,start=result$last)
> y5 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=9188,iter=100,start=result$last)
> y6 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=9029,iter=100,start=result$last)
> y7 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=4343,iter=100,start=result$last)
> y8 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=2372,iter=100,start=result$last)
> y9 <- result$y
> result <- pan(y,subj,pred,xcol,zcol,prior,seed=7081,iter=100,start=result$last)
> y10 <- result$y
> ########################################################################
> # Now we combine the imputation results according to mitools
> ########################################################################
> # First, we build data frames from above,
> d1 <- data.frame(y=y1,subj,pred)
> d2 <- data.frame(y=y2,subj,pred)
> d3 <- data.frame(y=y3,subj,pred)
> d4 <- data.frame(y=y4,subj,pred)
> d5 <- data.frame(y=y5,subj,pred)
> d6 <- data.frame(y=y6,subj,pred)
> d7 <- data.frame(y=y7,subj,pred)
> d8 <- data.frame(y=y8,subj,pred)
> d9 <- data.frame(y=y9,subj,pred)
> d10 <- data.frame(y=y10,subj,pred)
> # Second, we establish a S3 object as needed for the function MIcombine
> # nevertheless we start with an ordinary least squares regression
> require(mitools)
Loading required package: mitools
> d <- imputationList(list(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10))
> w <- with(d,lm(y~-1+pred))
> MIcombine(w)
Multiple imputation results:
      with(d, lm(y ~ -1 + pred))
      MIcombine.default(w)
             results       se
predint    -3.367055 4.126646
preddummy1 11.446035 5.439205
preddummy2 20.255944 5.495544
preddummy3 20.305059 5.633694
preddummy4  4.546684 5.933009
preddummy5 10.922610 5.495544
> # Now, we can turn to lmer as in lme4 package but in this case it is the
> # same.
> if(require(lme4)) {
+ w2 <- with(d,lmer(y~-1+pred+(1|subj)))
+ b <- MIextract(w2,fun=fixef)
+ Var <- function(obj) unlist(lapply(diag(vcov(obj)),function(m) m))
+ v <- MIextract(w2,fun=Var)
+ MIcombine(b,v)
+ detach(marijuana)
+ }
Loading required package: lme4
Loading required package: Matrix
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
boundary (singular) fit: see help('isSingular')
> ### bivariate example
> 
> data(bitest)
> attach(bitest)
The following objects are masked _by_ .GlobalEnv:

    y1, y2

> y <- with(bitest,cbind(y1,y2))
> 
> subj <- c(clusterid)
> pred <- cbind (int, x1, x2, x3)
> xcol <- 1:4
> zcol <- 1
> a <- 2
> c <- 2
> id2 <- matrix(c(1,0,0,1),ncol=2,nrow=2)
> Binv <- a*id2
> Dinv <- c*id2
> prior <- list(a=a, Binv=Binv, c=c, Dinv=Dinv)
> result <- pan(y, subj, pred, xcol, zcol, prior, seed=12345, iter=1000)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> ### * <FOOTER>
> ###
> cleanEx()

detaching ‘bitest’, ‘package:lme4’, ‘package:Matrix’, ‘package:mitools’

> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  1.305 0.077 1.383 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
