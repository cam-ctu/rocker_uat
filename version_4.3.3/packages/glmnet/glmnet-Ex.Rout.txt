
R version 4.3.3 (2024-02-29) -- "Angel Food Cake"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "glmnet"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('glmnet')
Loading required package: Matrix
Loaded glmnet 4.1-8
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Cindex")
> ### * Cindex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Cindex
> ### Title: compute C index for a Cox model
> ### Aliases: Cindex
> ### Keywords: Cox cross-validation models
> 
> ### ** Examples
> 
> 
> set.seed(10101)
> N = 1000
> p = 30
> nzc = p/3
> x = matrix(rnorm(N * p), N, p)
> beta = rnorm(nzc)
> fx = x[, seq(nzc)] %*% beta/3
> hx = exp(fx)
> ty = rexp(N, hx)
> tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator
> y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)
> fit = glmnet(x, y, family = "cox")
> pred = predict(fit, newx = x)
> apply(pred, 2, Cindex, y=y)
       s0        s1        s2        s3        s4        s5        s6        s7 
0.5000000 0.6203213 0.6723115 0.6850065 0.6890813 0.6981203 0.7017850 0.7038614 
       s8        s9       s10       s11       s12       s13       s14       s15 
0.7058944 0.7104428 0.7145407 0.7189389 0.7214889 0.7235161 0.7256127 0.7277238 
      s16       s17       s18       s19       s20       s21       s22       s23 
0.7286103 0.7295835 0.7300860 0.7305568 0.7308282 0.7311026 0.7313509 0.7317379 
      s24       s25       s26       s27       s28       s29       s30       s31 
0.7320700 0.7325061 0.7327457 0.7329104 0.7330403 0.7331241 0.7332540 0.7333435 
      s32       s33       s34       s35       s36       s37       s38       s39 
0.7335255 0.7337652 0.7337998 0.7339355 0.7338922 0.7339991 0.7340164 0.7340742 
      s40       s41       s42       s43       s44       s45       s46       s47 
0.7341319 0.7341926 0.7342157 0.7342792 0.7343283 0.7342648 0.7342879 0.7342417 
      s48 
0.7342243 
> cv.glmnet(x, y, family = "cox", type.measure = "C")

Call:  cv.glmnet(x = x, y = y, type.measure = "C", family = "cox") 

Measure: C-index 

     Lambda Index Measure      SE Nonzero
min 0.01920    28  0.7269 0.01170      14
1se 0.08509    12  0.7154 0.01095       8
> 
> 
> 
> 
> cleanEx()
> nameEx("assess.glmnet")
> ### * assess.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: assess.glmnet
> ### Title: assess performance of a 'glmnet' object using test data.
> ### Aliases: assess.glmnet confusion.glmnet roc.glmnet
> ### Keywords: classification models
> 
> ### ** Examples
> 
> 
> data(QuickStartExample)
> x <- QuickStartExample$x; y <- QuickStartExample$y
> set.seed(11)
> train = sample(seq(length(y)),70,replace=FALSE)
> fit1 = glmnet(x[train,], y[train])
> assess.glmnet(fit1, newx = x[-train,], newy = y[-train])
$mse
        s0         s1         s2         s3         s4         s5         s6 
10.5356884 10.0473027  9.5757375  8.7448060  8.0414304  7.4451552  6.8302096 
        s7         s8         s9        s10        s11        s12        s13 
 6.2099274  5.5979797  4.8918587  4.2620453  3.7367187  3.2983155  2.9322791 
       s14        s15        s16        s17        s18        s19        s20 
 2.6265064  2.3709330  2.1242795  1.9219451  1.7587268  1.6226452  1.5091624 
       s21        s22        s23        s24        s25        s26        s27 
 1.4144863  1.3354649  1.2694776  1.2143453  1.1682560  1.1297026  1.0974313 
       s28        s29        s30        s31        s32        s33        s34 
 1.0652936  1.0249928  0.9930414  0.9659219  0.9435578  0.9249772  0.9096485 
       s35        s36        s37        s38        s39        s40        s41 
 0.8970118  0.8866020  0.8773197  0.8703946  0.8648273  0.8609494  0.8597018 
       s42        s43        s44        s45        s46        s47        s48 
 0.8588511  0.8629691  0.8674411  0.8722799  0.8776263  0.8827885  0.8870380 
       s49        s50        s51        s52        s53        s54        s55 
 0.8911607  0.8951440  0.8989787  0.9030727  0.9070581  0.9109051  0.9145902 
       s56        s57        s58        s59        s60        s61        s62 
 0.9180974  0.9211897  0.9242868  0.9272307  0.9299921  0.9325693  0.9349674 
       s63        s64        s65        s66        s67        s68 
 0.9371937  0.9392563  0.9411642  0.9426805  0.9445011  0.9458195 
attr(,"measure")
[1] "Mean-Squared Error"

$mae
       s0        s1        s2        s3        s4        s5        s6        s7 
2.6946299 2.6328780 2.5673994 2.4405206 2.3249133 2.2195763 2.1151049 2.0115127 
       s8        s9       s10       s11       s12       s13       s14       s15 
1.9030069 1.7718821 1.6518698 1.5453805 1.4483515 1.3599423 1.2793871 1.2066589 
      s16       s17       s18       s19       s20       s21       s22       s23 
1.1339882 1.0695296 1.0150683 0.9781225 0.9498621 0.9241123 0.9006501 0.8811996 
      s24       s25       s26       s27       s28       s29       s30       s31 
0.8651666 0.8551227 0.8495833 0.8445360 0.8380969 0.8279767 0.8194392 0.8094367 
      s32       s33       s34       s35       s36       s37       s38       s39 
0.7996659 0.7907266 0.7838971 0.7794754 0.7754465 0.7704058 0.7660117 0.7621631 
      s40       s41       s42       s43       s44       s45       s46       s47 
0.7588402 0.7581804 0.7579210 0.7600092 0.7619297 0.7637101 0.7657734 0.7688329 
      s48       s49       s50       s51       s52       s53       s54       s55 
0.7717302 0.7744494 0.7769264 0.7797273 0.7827940 0.7855762 0.7881095 0.7904177 
      s56       s57       s58       s59       s60       s61       s62       s63 
0.7925209 0.7943038 0.7960365 0.7976354 0.7990963 0.8004283 0.8016421 0.8027481 
      s64       s65       s66       s67       s68 
0.8037558 0.8046740 0.8053809 0.8062495 0.8068503 
attr(,"measure")
[1] "Mean Absolute Error"

> preds = predict(fit1, newx = x[-train, ], s = c(1, 0.25))
> assess.glmnet(preds, newy = y[-train], family = "gaussian")
$mse
      s1       s2 
7.312414 1.498532 
attr(,"measure")
[1] "Mean-Squared Error"

$mae
       s1        s2 
2.1976089 0.9470799 
attr(,"measure")
[1] "Mean Absolute Error"

> fit1c = cv.glmnet(x, y, keep = TRUE)
> fit1a = assess.glmnet(fit1c$fit.preval, newy=y,family="gaussian")
> plot(fit1c$lambda, log="x",fit1a$mae,xlab="Log Lambda",ylab="Mean Absolute Error")
> abline(v=fit1c$lambda.min, lty=2, col="red")
> data(BinomialExample)
> x <- BinomialExample$x; y <- BinomialExample$y
> fit2 = glmnet(x[train,], y[train], family = "binomial")
> assess.glmnet(fit2,newx = x[-train,], newy=y[-train], s=0.1)
$deviance
      s1 
1.037535 
attr(,"measure")
[1] "Binomial Deviance"

$class
       s1 
0.1333333 
attr(,"measure")
[1] "Misclassification Error"

$auc
[1] 0.9351852
attr(,"measure")
[1] "AUC"

$mse
      s1 
0.333371 
attr(,"measure")
[1] "Mean-Squared Error"

$mae
       s1 
0.7909957 
attr(,"measure")
[1] "Mean Absolute Error"

> plot(roc.glmnet(fit2, newx = x[-train,], newy=y[-train])[[10]])
> fit2c = cv.glmnet(x, y, family = "binomial", keep=TRUE)
> idmin = match(fit2c$lambda.min, fit2c$lambda)
> plot(roc.glmnet(fit2c$fit.preval, newy = y)[[idmin]])
> data(MultinomialExample)
> x <- MultinomialExample$x; y <- MultinomialExample$y
> set.seed(103)
> train = sample(seq(length(y)),100,replace=FALSE)
> fit3 = glmnet(x[train,], y[train], family = "multinomial")
> confusion.glmnet(fit3, newx = x[-train, ], newy = y[-train], s = 0.01)
         True
Predicted   1   2   3 Total
    1      55  44  23   122
    2      29  82  17   128
    3      25  23 102   150
    Total 109 149 142   400

 Percent Correct:  0.5975 
> fit3c = cv.glmnet(x, y, family = "multinomial", type.measure="class", keep=TRUE)
> idmin = match(fit3c$lambda.min, fit3c$lambda)
> confusion.glmnet(fit3c$fit.preval, newy = y, family="multinomial")[[idmin]]
         True
Predicted   1   2   3 Total
    1      71  22  13   106
    2      39 127  26   192
    3      32  25 145   202
    Total 142 174 184   500

 Percent Correct:  0.686 
> 
> 
> 
> 
> cleanEx()
> nameEx("beta_CVX")
> ### * beta_CVX
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: beta_CVX
> ### Title: Simulated data for the glmnet vignette
> ### Aliases: beta_CVX x y
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
> data(QuickStartExample)
> x <- QuickStartExample$x; y <- QuickStartExample$y
> glmnet(x, y)

Call:  glmnet(x = x, y = y) 

   Df  %Dev  Lambda
1   0  0.00 1.63100
2   2  5.53 1.48600
3   2 14.59 1.35400
4   2 22.11 1.23400
5   2 28.36 1.12400
6   2 33.54 1.02400
7   4 39.04 0.93320
8   5 45.60 0.85030
9   5 51.54 0.77470
10  6 57.35 0.70590
11  6 62.55 0.64320
12  6 66.87 0.58610
13  6 70.46 0.53400
14  6 73.44 0.48660
15  7 76.21 0.44330
16  7 78.57 0.40400
17  7 80.53 0.36810
18  7 82.15 0.33540
19  7 83.50 0.30560
20  7 84.62 0.27840
21  7 85.55 0.25370
22  7 86.33 0.23120
23  8 87.06 0.21060
24  8 87.69 0.19190
25  8 88.21 0.17490
26  8 88.65 0.15930
27  8 89.01 0.14520
28  8 89.31 0.13230
29  8 89.56 0.12050
30  8 89.76 0.10980
31  9 89.94 0.10010
32  9 90.10 0.09117
33  9 90.23 0.08307
34  9 90.34 0.07569
35 10 90.43 0.06897
36 11 90.53 0.06284
37 11 90.62 0.05726
38 12 90.70 0.05217
39 15 90.78 0.04754
40 16 90.86 0.04331
41 16 90.93 0.03947
42 16 90.98 0.03596
43 17 91.03 0.03277
44 17 91.07 0.02985
45 18 91.11 0.02720
46 18 91.14 0.02479
47 19 91.17 0.02258
48 19 91.20 0.02058
49 19 91.22 0.01875
50 19 91.24 0.01708
51 19 91.25 0.01557
52 19 91.26 0.01418
53 19 91.27 0.01292
54 19 91.28 0.01178
55 19 91.29 0.01073
56 19 91.29 0.00978
57 19 91.30 0.00891
58 19 91.30 0.00812
59 19 91.31 0.00739
60 19 91.31 0.00674
61 19 91.31 0.00614
62 20 91.31 0.00559
63 20 91.31 0.00510
64 20 91.31 0.00464
65 20 91.32 0.00423
66 20 91.32 0.00386
67 20 91.32 0.00351
> 
> 
> 
> 
> cleanEx()
> nameEx("bigGlm")
> ### * bigGlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bigGlm
> ### Title: fit a glm with all the options in 'glmnet'
> ### Aliases: bigGlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> # Gaussian
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> fit1 = bigGlm(x, y)
> print(fit1)

Call:  bigGlm(x = x, y = y) 

  Df  %Dev Lambda
1 20 14.51      0
> 
> fit2=bigGlm(x,y>0,family="binomial")
> print(fit2)

Call:  bigGlm(x = x, y = y > 0, family = "binomial") 

  Df  %Dev Lambda
1 20 15.16      0
> fit2p=bigGlm(x,y>0,family="binomial",path=TRUE)
> print(fit2p)

Call:  bigGlm(x = x, y = y > 0, family = "binomial", path = TRUE) 

  Df  %Dev Lambda
1 20 15.16      0
> 
> 
> 
> 
> cleanEx()
> nameEx("cox.path")
> ### * cox.path
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cox.path
> ### Title: Fit a Cox regression model with elastic net regularization for a
> ###   path of lambda values
> ### Aliases: cox.path
> 
> ### ** Examples
> 
> set.seed(2)
> nobs <- 100; nvars <- 15
> xvec <- rnorm(nobs * nvars)
> xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0
> x <- matrix(xvec, nrow = nobs)
> beta <- rnorm(nvars / 3)
> fx <- x[, seq(nvars / 3)] %*% beta / 3
> ty <- rexp(nobs, exp(fx))
> tcens <- rbinom(n = nobs, prob = 0.3, size = 1)
> jsurv <- survival::Surv(ty, tcens)
> fit1 <- glmnet:::cox.path(x, jsurv)
> 
> # works with sparse x matrix
> x_sparse <- Matrix::Matrix(x, sparse = TRUE)
> fit2 <- glmnet:::cox.path(x_sparse, jsurv)
> 
> # example with (start, stop] data
> set.seed(2)
> start_time <- runif(100, min = 0, max = 5)
> stop_time <- start_time + runif(100, min = 0.1, max = 3)
> status <- rbinom(n = nobs, prob = 0.3, size = 1)
> jsurv_ss <- survival::Surv(start_time, stop_time, status)
> fit3 <- glmnet:::cox.path(x, jsurv_ss)
> 
> # example with strata
> jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))
> fit4 <- glmnet:::cox.path(x, jsurv_ss2)
> 
> 
> 
> cleanEx()
> nameEx("coxgrad")
> ### * coxgrad
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coxgrad
> ### Title: Compute gradient for Cox model
> ### Aliases: coxgrad
> ### Keywords: Cox model
> 
> ### ** Examples
> 
> set.seed(1)
> eta <- rnorm(10)
> time <- runif(10, min = 1, max = 10)
> d <- ifelse(rnorm(10) > 0, 1, 0)
> y <- survival::Surv(time, d)
> coxgrad(eta, y)
 [1] -0.06992882 -0.01679888  0.04327001  0.03107979  0.06179160  0.05843140
 [7]  0.08926450 -0.03192038 -0.23269137  0.06750215
> 
> # return diagonal of Hessian as well
> coxgrad(eta, y, diag.hessian = TRUE)
 [1] -0.06992882 -0.01679888  0.04327001  0.03107979  0.06179160  0.05843140
 [7]  0.08926450 -0.03192038 -0.23269137  0.06750215
attr(,"diag_hessian")
 [1] -0.060713941 -0.015383325 -0.050665386 -0.045093820 -0.032789674
 [6] -0.037886260 -0.009582991 -0.091840192 -0.130659065 -0.029475710
> 
> # example with (start, stop] data
> y2 <- survival::Surv(time, time + runif(10), d)
> coxgrad(eta, y2)
 [1]  0.000000e+00 -4.635984e-02  0.000000e+00  1.110223e-17  4.635984e-02
 [6]  8.261801e-02  9.992007e-17 -1.565746e-01  0.000000e+00  7.395658e-02
> 
> # example with strata
> y2 <- stratifySurv(y, rep(1:2, length.out = 10))
> coxgrad(eta, y2)
 [1] -0.04165091 -0.01278177  0.06621056  0.04756065  0.04227714 -0.03552950
 [7]  0.07175852 -0.06887459 -0.13859532  0.06962520
> 
> 
> 
> 
> cleanEx()
> nameEx("coxnet.deviance")
> ### * coxnet.deviance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coxnet.deviance
> ### Title: Compute deviance for Cox model
> ### Aliases: coxnet.deviance
> ### Keywords: Cox model
> 
> ### ** Examples
> 
> set.seed(1)
> eta <- rnorm(10)
> time <- runif(10, min = 1, max = 10)
> d <- ifelse(rnorm(10) > 0, 1, 0)
> y <- survival::Surv(time, d)
> coxnet.deviance(pred = eta, y = y)
[1] 23.53084
> 
> # if pred not provided, it is set to zero vector
> coxnet.deviance(y = y)
[1] 24.66365
> 
> # example with x and beta
> x <- matrix(rnorm(10 * 3), nrow = 10)
> beta <- matrix(1:3, ncol = 1)
> coxnet.deviance(y = y, x = x, beta = beta)
[1] 27.84795
> 
> # example with (start, stop] data
> y2 <- survival::Surv(time, time + runif(10), d)
> coxnet.deviance(pred = eta, y = y2)
[1] 5.859333
> 
> # example with strata
> y2 <- stratifySurv(y, rep(1:2, length.out = 10))
> coxnet.deviance(pred = eta, y = y2)
[1] 13.05457
> 
> 
> 
> 
> cleanEx()
> nameEx("cv.glmnet")
> ### * cv.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.glmnet
> ### Title: Cross-validation for glmnet
> ### Aliases: cv.glmnet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> set.seed(1010)
> n = 1000
> p = 100
> nzc = trunc(p/10)
> x = matrix(rnorm(n * p), n, p)
> beta = rnorm(nzc)
> fx = x[, seq(nzc)] %*% beta
> eps = rnorm(n) * 5
> y = drop(fx + eps)
> px = exp(fx)
> px = px/(1 + px)
> ly = rbinom(n = length(px), prob = px, size = 1)
> set.seed(1011)
> cvob1 = cv.glmnet(x, y)
> plot(cvob1)
> coef(cvob1)
101 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) -0.1162737
V1          -0.2171531
V2           0.3237422
V3           .        
V4          -0.2190339
V5          -0.1856601
V6           0.2530652
V7           0.1874832
V8          -1.3574323
V9           1.0162046
V10          0.1558299
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          .        
V21          .        
V22          .        
V23          .        
V24          .        
V25          .        
V26          .        
V27          .        
V28          .        
V29          .        
V30          .        
V31          .        
V32          .        
V33          .        
V34          .        
V35          .        
V36          .        
V37          .        
V38          .        
V39          .        
V40          .        
V41          .        
V42          .        
V43          .        
V44          .        
V45          .        
V46          .        
V47          .        
V48          .        
V49          .        
V50          .        
V51          .        
V52          .        
V53          .        
V54          .        
V55          .        
V56          .        
V57          .        
V58          .        
V59          .        
V60          .        
V61          .        
V62          .        
V63          .        
V64          .        
V65          .        
V66          .        
V67          .        
V68          .        
V69          .        
V70          .        
V71          .        
V72          .        
V73          .        
V74          .        
V75         -0.1420966
V76          .        
V77          .        
V78          .        
V79          .        
V80          .        
V81          .        
V82          .        
V83          .        
V84          .        
V85          .        
V86          .        
V87          .        
V88          .        
V89          .        
V90          .        
V91          .        
V92          .        
V93          .        
V94          .        
V95          .        
V96          .        
V97          .        
V98          .        
V99          .        
V100         .        
> predict(cvob1, newx = x[1:5, ], s = "lambda.min")
     lambda.min
[1,] -1.3447658
[2,]  0.9443441
[3,]  0.6989746
[4,]  1.8698290
[5,] -4.7372693
> title("Gaussian Family", line = 2.5)
> set.seed(1011)
> cvob1a = cv.glmnet(x, y, type.measure = "mae")
> plot(cvob1a)
> title("Gaussian Family", line = 2.5)
> set.seed(1011)
> par(mfrow = c(2, 2), mar = c(4.5, 4.5, 4, 1))
> cvob2 = cv.glmnet(x, ly, family = "binomial")
> plot(cvob2)
> title("Binomial Family", line = 2.5)
> frame()
> set.seed(1011)
> cvob3 = cv.glmnet(x, ly, family = "binomial", type.measure = "class")
> plot(cvob3)
> title("Binomial Family", line = 2.5)
> ## Not run: 
> ##D cvob1r = cv.glmnet(x, y, relax = TRUE)
> ##D plot(cvob1r)
> ##D predict(cvob1r, newx = x[, 1:5])
> ##D set.seed(1011)
> ##D cvob3a = cv.glmnet(x, ly, family = "binomial", type.measure = "auc")
> ##D plot(cvob3a)
> ##D title("Binomial Family", line = 2.5)
> ##D set.seed(1011)
> ##D mu = exp(fx/10)
> ##D y = rpois(n, mu)
> ##D cvob4 = cv.glmnet(x, y, family = "poisson")
> ##D plot(cvob4)
> ##D title("Poisson Family", line = 2.5)
> ##D 
> ##D # Multinomial
> ##D n = 500
> ##D p = 30
> ##D nzc = trunc(p/10)
> ##D x = matrix(rnorm(n * p), n, p)
> ##D beta3 = matrix(rnorm(30), 10, 3)
> ##D beta3 = rbind(beta3, matrix(0, p - 10, 3))
> ##D f3 = x %*% beta3
> ##D p3 = exp(f3)
> ##D p3 = p3/apply(p3, 1, sum)
> ##D g3 = glmnet:::rmult(p3)
> ##D set.seed(10101)
> ##D cvfit = cv.glmnet(x, g3, family = "multinomial")
> ##D plot(cvfit)
> ##D title("Multinomial Family", line = 2.5)
> ##D # Cox
> ##D beta = rnorm(nzc)
> ##D fx = x[, seq(nzc)] %*% beta/3
> ##D hx = exp(fx)
> ##D ty = rexp(n, hx)
> ##D tcens = rbinom(n = n, prob = 0.3, size = 1)  # censoring indicator
> ##D y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)
> ##D foldid = sample(rep(seq(10), length = n))
> ##D fit1_cv = cv.glmnet(x, y, family = "cox", foldid = foldid)
> ##D plot(fit1_cv)
> ##D title("Cox Family", line = 2.5)
> ##D # Parallel
> ##D require(doMC)
> ##D registerDoMC(cores = 4)
> ##D x = matrix(rnorm(1e+05 * 100), 1e+05, 100)
> ##D y = rnorm(1e+05)
> ##D system.time(cv.glmnet(x, y))
> ##D system.time(cv.glmnet(x, y, parallel = TRUE))
> ## End(Not run)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("deviance.glmnet")
> ### * deviance.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deviance.glmnet
> ### Title: Extract the deviance from a glmnet object
> ### Aliases: deviance.glmnet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> fit1 = glmnet(x, y)
> deviance(fit1)
 [1] 109.99040 109.32750 108.77716 108.29306 106.91964 105.52615 104.36924
 [8] 103.40876 102.53615 101.72335 100.97541 100.19525  99.54756  98.91262
[15]  98.32659  97.76365  97.26771  96.82114  96.45032  96.14107  95.82850
[22]  95.55062  95.30423  95.09969  94.92988  94.78890  94.67186  94.57469
[29]  94.49421  94.42720  94.37140  94.32177  94.27979  94.24481  94.21547
[36]  94.19133  94.16774  94.14816  94.13190  94.11804  94.10331  94.09140
[43]  94.08182  94.07361  94.06676  94.06106  94.05634  94.05241  94.04916
[50]  94.04645  94.04421  94.04234  94.04080  94.03962  94.03845  94.03764
[57]  94.03690  94.03628  94.03577  94.03534  94.03498  94.03468  94.03443
[64]  94.03423  94.03406  94.03392
> 
> 
> 
> cleanEx()
> nameEx("fid")
> ### * fid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fid
> ### Title: Helper function for Cox deviance and gradient
> ### Aliases: fid
> 
> ### ** Examples
> 
> # Example with no ties
> glmnet:::fid(c(1, 4, 5, 6), 1:5)
$index_first
[1] 1 2 3 4 5

$index_ties
NULL

> 
> # Example with ties
> glmnet:::fid(c(1, 1, 1, 2, 3, 3, 4, 4, 4), 1:9)
$index_first
[1] 1 4 5 7

$index_ties
$index_ties$`1`
[1] 1 2 3

$index_ties$`3`
[1] 5 6

$index_ties$`4`
[1] 7 8 9


> 
> 
> 
> cleanEx()
> nameEx("glmnet-package")
> ### * glmnet-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmnet-package
> ### Title: Elastic net model paths for some generalized linear models
> ### Aliases: glmnet-package
> ### Keywords: models package regression
> 
> ### ** Examples
> 
> 
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> g2 = sample(1:2, 100, replace = TRUE)
> g4 = sample(1:4, 100, replace = TRUE)
> fit1 = glmnet(x, y)
> predict(fit1, newx = x[1:5, ], s = c(0.01, 0.005))
             s1         s2
[1,]  0.1623437  0.1923152
[2,] -0.5447409 -0.5540136
[3,]  0.3089815  0.3238559
[4,]  0.3441108  0.3593363
[5,] -0.4261315 -0.4431612
> predict(fit1, type = "coef")
21 x 66 sparse Matrix of class "dgCMatrix"
  [[ suppressing 66 column names ‘s0’, ‘s1’, ‘s2’ ... ]]
                                                                        
(Intercept) -0.1417928 -0.1417887 -0.14178501 -0.1417478737 -0.138172119
V1           .          .          .           .             .          
V2           .          .          .           0.0008919145  0.017974756
V3           .          .          .           .             0.004490984
V4           .          .          .           .             .          
V5           .          .          .           .             .          
V6           .          .          .           .             .          
V7           .          .          .           .             .          
V8           .         -0.0160778 -0.03072729 -0.0441276998 -0.056837237
V9           .          .          .           .             .          
V10          .          .          .           .             .          
V11          .          .          .           .             .          
V12          .          .          .           .             .          
V13          .          .          .           .            -0.012614957
V14          .          .          .           .             .          
V15          .          .          .           .             .          
V16          .          .          .           .             .          
V17          .          .          .           .             .          
V18          .          .          .           .             .          
V19          .          .          .           .             .          
V20          .          .          .           .             .          
                                                                        
(Intercept) -0.13456053 -0.13126978 -0.12827136 -0.126024382 -0.12457388
V1           .           .           .           0.004378485  0.01375550
V2           0.03420188  0.04898754  0.06245968  0.074737991  0.08592887
V3           0.01584973  0.02619921  0.03562927  0.044128743  0.05175894
V4           .           .           .           .            .         
V5           .           .           .           .            .         
V6           .           .           .           .            .         
V7           .           .           .           .            .         
V8          -0.06749967 -0.07721492 -0.08606709 -0.094273527 -0.10192396
V9           .           .           .           .            .         
V10          .           .           .           .            .         
V11          .           .           .           .            .         
V12          .           .           .           .            .         
V13         -0.02635333 -0.03887123 -0.05027708 -0.060623473 -0.06999385
V14          .           .           .           .            .         
V15          .           .           .           .            .         
V16          .           .           .           .            .         
V17          .           .           .           .            .         
V18          .           .           .           .            .         
V19          .           .           .           .            .         
V20          .           .           .           .            .         
                                                                          
(Intercept) -0.123092260 -0.12137748 -0.11981509 -0.119577235 -0.119702217
V1           0.021960097  0.02865998  0.03476503  0.040771967  0.046651303
V2           0.096321083  0.10623755  0.11527286  0.123933216  0.132184522
V3           0.059313151  0.06757379  0.07509966  0.081077849  0.086371361
V4           .            .           .           .            .          
V5           .            .           .           .            .          
V6           .            .           .           .            .          
V7           .            .           .           .            .          
V8          -0.109049143 -0.11589276 -0.12212845 -0.127596066 -0.132682865
V9           .            .           .           .            .          
V10          0.003153469  0.01321132  0.02237508  0.031222991  0.039319686
V11          .            .           .           .            0.002308117
V12          .            .           .           .            .          
V13         -0.079134826 -0.08883813 -0.09767926 -0.105620022 -0.112892265
V14          .            .           .          -0.002335272 -0.005610950
V15          .            .           .           .            .          
V16          .            .           .           .            .          
V17          .            .           .           .            .          
V18          .            .           .           .            .          
V19          .            .           .           0.005560349  0.011742684
V20          .            .           .           .            .          
                                                                              
(Intercept) -0.119850090 -0.1199868655 -0.119715815 -0.119468192 -0.1192272147
V1           0.052801292  0.0586827820  0.064280981  0.069379972  0.0740259709
V2           0.140333921  0.1476821797  0.154347055  0.160422725  0.1659586694
V3           0.091215440  0.0957861307  0.100150855  0.104131143  0.1077578535
V4           0.000717184  0.0019027959  0.002684843  0.003390248  0.0040329530
V5           .            .             .            .            .           
V6           .            .             .            .            .           
V7           .            .             .            .            .           
V8          -0.137696326 -0.1427242438 -0.147335354 -0.151537376 -0.1553661027
V9           .            .             .            .            .           
V10          0.046449191  0.0522711059  0.057435932  0.062146503  0.0664386329
V11          0.009274153  0.0154379581  0.020655093  0.025409743  0.0297420108
V12          .            0.0003596194  0.004115996  0.007538125  0.0106562395
V13         -0.119669170 -0.1255045072 -0.130597548 -0.135239685 -0.1394694410
V14         -0.008903354 -0.0117027938 -0.014606102 -0.017252092 -0.0196630233
V15          .            .             .            .            .           
V16          .            0.0027002708  0.006100211  0.009199773  0.0120239818
V17          .            .             .            .            .           
V18          .            .             .            .            .           
V19          0.017140319  0.0220285600  0.026530605  0.030633776  0.0343724390
V20          .            .             .            .            0.0001943677
                                                                            
(Intercept) -0.118594121 -0.118101454 -0.117716309 -0.117365415 -0.117045694
V1           0.077392961  0.080468100  0.083247052  0.085778639  0.088085327
V2           0.171311368  0.176486131  0.181472369  0.186015234  0.190154524
V3           0.111995300  0.116318789  0.120697191  0.124686028  0.128320506
V4           0.004965750  0.005494256  0.005719535  0.005922562  0.006107554
V5           .            .            .            .            .          
V6           .            .            .            .            .          
V7           .            .            .            .            .          
V8          -0.160029084 -0.164757650 -0.169510407 -0.173838068 -0.177781273
V9           .            .            .            .            .          
V10          0.070685070  0.075055599  0.079480248  0.083513450  0.087188352
V11          0.033348320  0.036478248  0.039196967  0.041675838  0.043934494
V12          0.014006830  0.017192160  0.020215555  0.022970403  0.025480519
V13         -0.143052671 -0.146314298 -0.149283842 -0.151990048 -0.154455842
V14         -0.021813177 -0.023489092 -0.024767674 -0.025934431 -0.026997537
V15          .            0.002387192  0.006560286  0.010361270  0.013824585
V16          0.015008120  0.017832074  0.020497741  0.022926287  0.025139088
V17          .            .            .            .            .          
V18          .            .            .            .            .          
V19          0.038320080  0.041608083  0.044352667  0.046853984  0.049133090
V20          0.005471067  0.010028068  0.013998837  0.017616098  0.020912011
                                                                            
(Intercept) -0.116754375 -0.116488937 -0.116247079 -0.116030669 -0.115829571
V1           0.090187095  0.092102149  0.093847074  0.095447494  0.096895354
V2           0.193926090  0.197362601  0.200493822  0.203333221  0.205933830
V3           0.131632108  0.134649515  0.137398865  0.139878028  0.142162598
V4           0.006276112  0.006429695  0.006569635  0.006722119  0.006836386
V5           .            .            .            .            .          
V6           .            .            .            .            .          
V7           .            .            .            .            .          
V8          -0.181374173 -0.184647891 -0.187630780 -0.190334643 -0.192812215
V9           .            .            .            .            .          
V10          0.090536787  0.093587755  0.096367684  0.098880477  0.101189913
V11          0.045992496  0.047867671  0.049576261  0.051136806  0.052554973
V12          0.027767643  0.029851585  0.031750395  0.033481818  0.035058135
V13         -0.156702581 -0.158749726 -0.160615009 -0.162307971 -0.163857049
V14         -0.027966199 -0.028848808 -0.029653009 -0.030391994 -0.031059144
V15          0.016980229  0.019855534  0.022475404  0.024846242  0.027022584
V16          0.027155309  0.028992415  0.030666318  0.032183857  0.033574173
V17          .            .            .            .            .          
V18          .            .            .            .            .          
V19          0.051209727  0.053101881  0.054825942  0.056398003  0.057829233
V20          0.023915124  0.026651449  0.029144687  0.031411613  0.033481921
                                                                               
(Intercept) -0.1156442603 -0.1154728036 -0.115428980 -0.115377054 -0.1153141739
V1           0.0982144658  0.0992693064  0.100309383  0.101244996  0.1020971618
V2           0.2083036039  0.2103594724  0.212190893  0.213873721  0.2154076147
V3           0.1442445027  0.1461104449  0.147727717  0.149226282  0.1505924286
V4           0.0069402001  0.0069911034  0.007165004  0.007284834  0.0073927784
V5           0.0000434915  0.0008976257  0.001655467  0.002354820  0.0029923439
V6           .             .             .            .            .           
V7           .            -0.0002734381 -0.000931219 -0.001508970 -0.0020347529
V8          -0.1950744810 -0.1970933106 -0.198784613 -0.200350890 -0.2017786646
V9           .             .             .            .            .           
V10          0.1032993418  0.1053102835  0.107013364  0.108596765  0.1100406165
V11          0.0538472279  0.0550756829  0.056275262  0.057356806  0.0583420986
V12          0.0364974848  0.0378876938  0.039201485  0.040390616  0.0414738838
V13         -0.1652717845 -0.1664264592 -0.167253596 -0.168036837 -0.1687514167
V14         -0.0316694566 -0.0323789602 -0.033130892 -0.033802786 -0.0344146885
V15          0.0290035616  0.0306960536  0.032145083  0.033489125  0.0347144201
V16          0.0348409838  0.0359343250  0.036827313  0.037655218  0.0384099518
V17          .             .             .            .            0.0001728693
V18          .             0.0004831618  0.001271401  0.001971039  0.0025910116
V19          0.0591363268  0.0602807332  0.061259512  0.062154882  0.0629799525
V20          0.0353654271  0.0370884408  0.038709060  0.040184630  0.0415471548
                                                                               
(Intercept) -0.1152801350 -0.1151553146 -0.115037998 -0.114930997 -0.1148705036
V1           0.1027671732  0.1035332622  0.104227453  0.104859971  0.1054509215
V2           0.2167168630  0.2179902490  0.219156122  0.220218650  0.2211671502
V3           0.1517965809  0.1530497744  0.154194364  0.155237460  0.1561612895
V4           0.0073959149  0.0075202512  0.007624600  0.007719499  0.0078283316
V5           0.0035616422  0.0041515396  0.004690228  0.005181133  0.0056186561
V6           0.0000345520  0.0014280435  0.002693182  0.003845990  0.0048929626
V7          -0.0026358434 -0.0031599790 -0.003628768 -0.004055662 -0.0044615343
V8          -0.2029882272 -0.2040223037 -0.204972901 -0.205839217 -0.2065863970
V9           .             .             .            .           -0.0002936383
V10          0.1112468619  0.1124995495  0.113649227  0.114697084  0.1156851114
V11          0.0592434758  0.0601137461  0.060902296  0.061620783  0.0622524720
V12          0.0424062089  0.0433413660  0.044192493  0.044967974  0.0456208344
V13         -0.1693605100 -0.1700667101 -0.170716710 -0.171309211 -0.1718531231
V14         -0.0350451081 -0.0355980361 -0.036097289 -0.036552076 -0.0370307719
V15          0.0358444956  0.0369155982  0.037896193  0.038789813  0.0396233865
V16          0.0391585235  0.0398451139  0.040473215  0.041045559  0.0415352802
V17          0.0005447025  0.0008361926  0.001098305  0.001337002  0.0015729396
V18          0.0031200944  0.0035191185  0.003878926  0.004206693  0.0044768899
V19          0.0637460991  0.0644908331  0.065170148  0.065789135  0.0664377783
V20          0.0427993583  0.0441739953  0.045425288  0.046565405  0.0475985868
                                                                            
(Intercept) -0.114877357 -0.114879507 -0.114889415 -0.114892363 -0.114893876
V1           0.106032174  0.106550971  0.107041806  0.107473234  0.107864694
V2           0.222070148  0.222889708  0.223617550  0.224295702  0.224916063
V3           0.157026739  0.157813393  0.158502387  0.159153292  0.159749440
V4           0.008163893  0.008402939  0.008646522  0.008849467  0.009029738
V5           0.006219175  0.006735062  0.007175716  0.007601120  0.007992369
V6           0.006277064  0.007466245  0.008504581  0.009487596  0.010388890
V7          -0.004813221 -0.005132325 -0.005435072 -0.005702608 -0.005944147
V8          -0.207430355 -0.208170623 -0.208792751 -0.209404308 -0.209966411
V9          -0.001711621 -0.002932283 -0.003953084 -0.004958016 -0.005884796
V10          0.116987954  0.118153827  0.119139968  0.120099008  0.120983187
V11          0.062639398  0.063006737  0.063376865  0.063683834  0.063960166
V12          0.046104648  0.046538527  0.046936956  0.047299268  0.047627790
V13         -0.172487076 -0.173071253 -0.173563264 -0.174041671 -0.174484248
V14         -0.037607311 -0.038130321 -0.038605319 -0.039039413 -0.039435283
V15          0.040530686  0.041354442  0.042075675  0.042755983  0.043379913
V16          0.041917935  0.042272968  0.042583505  0.042877398  0.043146541
V17          0.001806325  0.002024416  0.002217229  0.002398310  0.002563809
V18          0.004470020  0.004477531  0.004527974  0.004539394  0.004543723
V19          0.067436110  0.068321334  0.069079451  0.069810074  0.070481568
V20          0.048678418  0.049635706  0.050486011  0.051280494  0.052005887
                                                                            
(Intercept) -0.114895045 -0.114896076 -0.114897009 -0.114897859 -0.114898633
V1           0.108221132  0.108545868  0.108841750  0.109111345  0.109356989
V2           0.225481768  0.225997294  0.226467034  0.226895046  0.227285034
V3           0.160293152  0.160788647  0.161240138  0.161651522  0.162026359
V4           0.009193232  0.009342080  0.009477686  0.009601241  0.009713821
V5           0.008349443  0.008674887  0.008971435  0.009241640  0.009487842
V6           0.011211000  0.011960215  0.012642894  0.013264929  0.013831705
V7          -0.006163810 -0.006363886 -0.006546177 -0.006712272 -0.006863611
V8          -0.210479380 -0.210946907 -0.211372921 -0.211761093 -0.212114780
V9          -0.006730985 -0.007502275 -0.008205090 -0.008845475 -0.009428972
V10          0.121790512  0.122526387  0.123196933  0.123807916  0.124364622
V11          0.064211433  0.064440297  0.064648817  0.064838811  0.065011925
V12          0.047926865  0.048199330  0.048447583  0.048673781  0.048879884
V13         -0.174888607 -0.175257220 -0.175593115 -0.175899175 -0.176178046
V14         -0.039796009 -0.040124689 -0.040424171 -0.040697047 -0.040945682
V15          0.043949087  0.044467805  0.044940459  0.045371126  0.045763534
V16          0.043391988  0.043615665  0.043819476  0.044005182  0.044174390
V17          0.002714641  0.002852075  0.002977301  0.003091401  0.003195366
V18          0.004546711  0.004549283  0.004551603  0.004553713  0.004555635
V19          0.071094311  0.071652762  0.072161624  0.072625284  0.073047754
V20          0.052667061  0.053269534  0.053818490  0.054318679  0.054774433
                                                                            
(Intercept) -0.114899338 -0.114899981 -0.114900566 -0.114901839 -0.114902389
V1           0.109580812  0.109784750  0.109970572  0.110156079  0.110295996
V2           0.227640378  0.227964153  0.228259165  0.228518079  0.228771308
V3           0.162367898  0.162679095  0.162962646  0.163193266  0.163453865
V4           0.009816398  0.009909863  0.009995025  0.010085563  0.010147576
V5           0.009712172  0.009916573  0.010102815  0.010249361  0.010423680
V6           0.014348130  0.014818678  0.015247423  0.015589794  0.015988400
V7          -0.007001505 -0.007127149 -0.007241632 -0.007338371 -0.007442148
V8          -0.212437048 -0.212730686 -0.212998238 -0.213198601 -0.213459741
V9          -0.009960632 -0.010445061 -0.010886455 -0.011211205 -0.011643983
V10          0.124871872  0.125334060  0.125755187  0.126089672  0.126479155
V11          0.065169661  0.065313384  0.065444338  0.065587624  0.065675830
V12          0.049067678  0.049238788  0.049394697  0.049528483  0.049667562
V13         -0.176432142 -0.176663666 -0.176874621 -0.177054930 -0.177236177
V14         -0.041172229 -0.041378650 -0.041566733 -0.041728459 -0.041893452
V15          0.046121082  0.046446866  0.046743708  0.046996072  0.047257044
V16          0.044328567  0.044469046  0.044597046  0.044702022  0.044818600
V17          0.003290094  0.003376407  0.003455052  0.003514304  0.003590866
V18          0.004557386  0.004558981  0.004560435  0.004588665  0.004568906
V19          0.073432693  0.073783435  0.074103018  0.074353079  0.074653706
V20          0.055189699  0.055568073  0.055912834  0.056197965  0.056511490
                                                                            
(Intercept) -0.114902919 -0.114903611 -0.114904101 -0.114904425 -0.114904657
V1           0.110448830  0.110579396  0.110695786  0.110801061  0.110896727
V2           0.228987253  0.229190534  0.229376140  0.229545503  0.229699946
V3           0.163647145  0.163841922  0.164021295  0.164185275  0.164334899
V4           0.010220039  0.010283246  0.010337557  0.010385564  0.010428761
V5           0.010547536  0.010672255  0.010789454  0.010897477  0.010996342
V6           0.016276335  0.016565666  0.016835993  0.017084437  0.017311525
V7          -0.007521763 -0.007600515 -0.007673043 -0.007739142 -0.007799312
V8          -0.213629065 -0.213810992 -0.213980625 -0.214136430 -0.214278849
V9          -0.011920841 -0.012212452 -0.012490566 -0.012748098 -0.012984157
V10          0.126763079  0.127044467  0.127309044  0.127553084  0.127776505
V11          0.065792337  0.065885808  0.065967497  0.066040882  0.066107394
V12          0.049777770  0.049887284  0.049986320  0.050076089  0.050157708
V13         -0.177389723 -0.177529376 -0.177660708 -0.177782102 -0.177893348
V14         -0.042028128 -0.042155479 -0.042273611 -0.042381897 -0.042480760
V15          0.047468980  0.047670900  0.047857383  0.048028327  0.048184467
V16          0.044906639  0.044993736  0.045074416  0.045148387  0.045215950
V17          0.003640750  0.003692283  0.003741755  0.003787606  0.003829614
V18          0.004588406  0.004596246  0.004597586  0.004596715  0.004595199
V19          0.074865102  0.075078614  0.075279796  0.075465309  0.075635087
V20          0.056749379  0.056985578  0.057203447  0.057402615  0.057584297
                                                                            
(Intercept) -0.114904842 -0.114905001 -0.114905143 -0.114905270 -0.114905386
V1           0.110983808  0.111063123  0.111135383  0.111201220  0.111261206
V2           0.229840723  0.229969015  0.230085916  0.230192436  0.230289493
V3           0.164471310  0.164595630  0.164708916  0.164812142  0.164906198
V4           0.010467927  0.010503548  0.010535981  0.010565524  0.010592440
V5           0.011086578  0.011168852  0.011243835  0.011312164  0.011374425
V6           0.017518684  0.017707524  0.017879617  0.018036431  0.018179318
V7          -0.007854103 -0.007904013 -0.007949484 -0.007990914 -0.008028662
V8          -0.214408778 -0.214527222 -0.214635164 -0.214733523 -0.214823147
V9          -0.013199729 -0.013396318 -0.013575499 -0.013738783 -0.013887568
V10          0.127980452  0.128166412  0.128335897  0.128490342  0.128631072
V11          0.066167875  0.066222942  0.066273102  0.066318801  0.066360438
V12          0.050232016  0.050299701  0.050361366  0.050417550  0.050468742
V13         -0.177994939 -0.178087585 -0.178172029 -0.178248980 -0.178319099
V14         -0.042570904 -0.042653060 -0.042727924 -0.042796140 -0.042858297
V15          0.048326873  0.048456675  0.048574963  0.048682748  0.048780960
V16          0.045277567  0.045333730  0.045384910  0.045431546  0.045474040
V17          0.003867963  0.003902928  0.003934794  0.003963833  0.003990292
V18          0.004593567  0.004591994  0.004590531  0.004589187  0.004587959
V19          0.075790040  0.075931315  0.076060071  0.076177398  0.076284306
V20          0.057749906  0.057900827  0.058038349  0.058163656  0.058277832
                        
(Intercept) -0.114905491
V1           0.111315864
V2           0.230377928
V3           0.164991899
V4           0.010616965
V5           0.011431155
V6           0.018309513
V7          -0.008063057
V8          -0.214904810
V9          -0.014023138
V10          0.128759301
V11          0.066398376
V12          0.050515386
V13         -0.178382990
V14         -0.042914932
V15          0.048870448
V16          0.045512759
V17          0.004014401
V18          0.004586839
V19          0.076381718
V20          0.058381866
> plot(fit1, xvar = "lambda")
> fit2 = glmnet(x, g2, family = "binomial")
> predict(fit2, type = "response", newx = x[2:5, ])
       s0        s1        s2        s3        s4        s5        s6        s7
[1,] 0.55 0.5508197 0.5518644 0.5533468 0.5547358 0.5560362 0.5530104 0.5500710
[2,] 0.55 0.5441855 0.5377967 0.5300108 0.5228922 0.5163753 0.5177444 0.5231151
[3,] 0.55 0.5532864 0.5543438 0.5518438 0.5495867 0.5475430 0.5449393 0.5351263
[4,] 0.55 0.5493979 0.5472388 0.5423754 0.5379449 0.5339009 0.5315446 0.5280984
            s8        s9       s10       s11       s12       s13       s14
[1,] 0.5506972 0.5549247 0.5560892 0.5551073 0.5542355 0.5534602 0.5527697
[2,] 0.5239921 0.5201356 0.5167277 0.5136881 0.5108756 0.5082745 0.5058697
[3,] 0.5209633 0.5026612 0.4865167 0.4722117 0.4591213 0.4471514 0.4362124
[4,] 0.5256700 0.5243328 0.5236953 0.5235604 0.5234473 0.5233521 0.5232718
           s15       s16       s17       s18       s19       s20       s21
[1,] 0.5520446 0.5476933 0.5429052 0.5351065 0.5256012 0.5168382 0.5087816
[2,] 0.5037082 0.5037826 0.5031562 0.5004985 0.5028605 0.5054817 0.5078852
[3,] 0.4262733 0.4189535 0.4117641 0.4033109 0.3944733 0.3864167 0.3790885
[4,] 0.5230602 0.5179649 0.5131133 0.5080968 0.5043744 0.5009893 0.4978692
           s22       s23       s24       s25       s26       s27       s28
[1,] 0.5015316 0.4961426 0.4921169 0.4882804 0.4845014 0.4809080 0.4769170
[2,] 0.5079019 0.5061642 0.5048326 0.5041309 0.5041843 0.5039495 0.5029335
[3,] 0.3728682 0.3677146 0.3626090 0.3575042 0.3529293 0.3495202 0.3471112
[4,] 0.4920870 0.4840031 0.4763335 0.4694453 0.4630971 0.4568882 0.4506195
           s29       s30       s31       s32       s33       s34       s35
[1,] 0.4732485 0.4698817 0.4667930 0.4639608 0.4613649 0.4590033 0.4568242
[2,] 0.5011965 0.4996095 0.4981603 0.4968369 0.4956284 0.4945403 0.4935319
[3,] 0.3454630 0.3439591 0.3425864 0.3413335 0.3401899 0.3391569 0.3382040
[4,] 0.4446391 0.4391721 0.4341758 0.4296110 0.4254415 0.4216490 0.4181717
           s36       s37       s38       s39       s40       s41       s42
[1,] 0.4548291 0.4530032 0.4513329 0.4498053 0.4484303 0.4475250 0.4467215
[2,] 0.4926110 0.4917703 0.4910029 0.4903025 0.4896901 0.4895263 0.4894036
[3,] 0.3373344 0.3365410 0.3358171 0.3351566 0.3345434 0.3338232 0.3331815
[4,] 0.4149972 0.4120999 0.4094559 0.4070435 0.4048143 0.4023211 0.4000618
           s43       s44       s45       s46       s47       s48       s49
[1,] 0.4459649 0.4451742 0.4444341 0.4437947 0.4431768 0.4426093 0.4420908
[2,] 0.4892679 0.4892601 0.4892519 0.4892729 0.4892655 0.4892562 0.4892474
[3,] 0.3325821 0.3320033 0.3314721 0.3310163 0.3305736 0.3301688 0.3297996
[4,] 0.3979895 0.3958482 0.3938780 0.3921137 0.3904768 0.3889835 0.3876226
           s50       s51       s52       s53
[1,] 0.4416175 0.4411855 0.4407912 0.4404315
[2,] 0.4892392 0.4892318 0.4892249 0.4892186
[3,] 0.3294631 0.3291564 0.3288768 0.3286219
[4,] 0.3863825 0.3852526 0.3842231 0.3832850
> predict(fit2, type = "nonzero")
$s0
NULL

$s1
[1] 17

$s2
[1] 15 17

$s3
[1] 15 17

$s4
[1] 15 17

$s5
[1] 15 17

$s6
[1] 12 15 17

$s7
[1]  1 12 15 17

$s8
[1]  1 12 13 15 17

$s9
[1]  1 12 13 15 17

$s10
[1]  1  5 12 13 15 17

$s11
[1]  1  5 12 13 15 17

$s12
[1]  1  5 12 13 15 17

$s13
[1]  1  5 12 13 15 17

$s14
[1]  1  5 12 13 15 17

$s15
[1]  1  5 12 13 14 15 17

$s16
[1]  1  5 12 13 14 15 17

$s17
[1]  1  5  7 12 13 14 15 17

$s18
 [1]  1  5  7  9 12 13 14 15 16 17

$s19
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s20
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s21
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s22
 [1]  1  4  5  7  9 12 13 14 15 16 17 20

$s23
 [1]  1  4  5  7  8  9 12 13 14 15 16 17 20

$s24
 [1]  1  2  4  5  7  8  9 12 13 14 15 16 17 20

$s25
 [1]  1  2  4  5  7  8  9 11 12 13 14 15 16 17 20

$s26
 [1]  1  2  4  5  7  8  9 11 12 13 14 15 16 17 19 20

$s27
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 19 20

$s28
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s29
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s30
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s31
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s32
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s33
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s34
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s35
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s36
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s37
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s38
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s39
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s40
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s41
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s42
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s43
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s44
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s45
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s46
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s47
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s48
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s49
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s50
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s51
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s52
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s53
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

> fit3 = glmnet(x, g4, family = "multinomial")
> predict(fit3, newx = x[1:3, ], type = "response", s = 0.01)
, , 1

             1          2          3          4
[1,] 0.2084099 0.06094591 0.14285087 0.58779333
[2,] 0.1117824 0.09771133 0.47042226 0.32008405
[3,] 0.1956970 0.67741511 0.05326403 0.07362382

> 
> 
> 
> 
> cleanEx()
> nameEx("glmnet")
> ### * glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmnet
> ### Title: fit a GLM with lasso or elasticnet regularization
> ### Aliases: glmnet relax.glmnet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> # Gaussian
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> fit1 = glmnet(x, y)
> print(fit1)

Call:  glmnet(x = x, y = y) 

   Df  %Dev   Lambda
1   0  0.00 0.197600
2   1  0.60 0.180000
3   1  1.10 0.164000
4   2  1.54 0.149500
5   4  2.79 0.136200
6   4  4.06 0.124100
7   4  5.11 0.113100
8   4  5.98 0.103000
9   5  6.78 0.093870
10  5  7.52 0.085530
11  6  8.20 0.077940
12  6  8.91 0.071010
13  6  9.49 0.064700
14  8 10.07 0.058960
15  9 10.60 0.053720
16 10 11.12 0.048950
17 12 11.57 0.044600
18 12 11.97 0.040640
19 12 12.31 0.037030
20 13 12.59 0.033740
21 13 12.88 0.030740
22 14 13.13 0.028010
23 14 13.35 0.025520
24 14 13.54 0.023250
25 14 13.69 0.021190
26 14 13.82 0.019310
27 14 13.93 0.017590
28 14 14.02 0.016030
29 14 14.09 0.014600
30 14 14.15 0.013310
31 15 14.20 0.012120
32 17 14.25 0.011050
33 17 14.28 0.010070
34 17 14.32 0.009172
35 18 14.34 0.008357
36 19 14.36 0.007614
37 19 14.39 0.006938
38 19 14.40 0.006322
39 19 14.42 0.005760
40 20 14.43 0.005248
41 20 14.44 0.004782
42 20 14.45 0.004357
43 20 14.46 0.003970
44 20 14.47 0.003617
45 20 14.48 0.003296
46 20 14.48 0.003003
47 20 14.49 0.002736
48 20 14.49 0.002493
49 20 14.49 0.002272
50 20 14.50 0.002070
51 20 14.50 0.001886
52 20 14.50 0.001719
53 20 14.50 0.001566
54 20 14.50 0.001427
55 20 14.50 0.001300
56 20 14.50 0.001185
57 20 14.50 0.001079
58 20 14.51 0.000983
59 20 14.51 0.000896
60 20 14.51 0.000816
61 20 14.51 0.000744
62 20 14.51 0.000678
63 20 14.51 0.000618
64 20 14.51 0.000563
65 20 14.51 0.000513
66 20 14.51 0.000467
> coef(fit1, s = 0.01)  # extract coefficients at a single value of lambda
21 x 1 sparse Matrix of class "dgCMatrix"
                       s1
(Intercept) -0.1154251627
V1           0.1003781671
V2           0.2123146104
V3           0.1478378877
V4           0.0071738140
V5           0.0017068814
V6           .           
V7          -0.0009736938
V8          -0.1988997621
V9           .           
V10          0.1071297719
V11          0.0563547748
V12          0.0392889072
V13         -0.1673111781
V14         -0.0331802879
V15          0.0322438934
V16          0.0368881782
V17          .           
V18          0.0013228366
V19          0.0613253373
V20          0.0388175403
> predict(fit1, newx = x[1:10, ], s = c(0.01, 0.005))  # make predictions
              s1         s2
 [1,]  0.1623437  0.1923152
 [2,] -0.5447409 -0.5540136
 [3,]  0.3089815  0.3238559
 [4,]  0.3441108  0.3593363
 [5,] -0.4261315 -0.4431612
 [6,]  0.4629769  0.4778225
 [7,]  0.5536940  0.5777430
 [8,]  0.3760726  0.3938430
 [9,] -0.5032892 -0.5408060
[10,] -0.3201688 -0.3367624
> 
> # Relaxed
> fit1r = glmnet(x, y, relax = TRUE)  # can be used with any model
> 
> # multivariate gaussian
> y = matrix(rnorm(100 * 3), 100, 3)
> fit1m = glmnet(x, y, family = "mgaussian")
> plot(fit1m, type.coef = "2norm")
> 
> # binomial
> g2 = sample(c(0,1), 100, replace = TRUE)
> fit2 = glmnet(x, g2, family = "binomial")
> fit2n = glmnet(x, g2, family = binomial(link=cloglog))
> fit2r = glmnet(x,g2, family = "binomial", relax=TRUE)
> fit2rp = glmnet(x,g2, family = "binomial", relax=TRUE, path=TRUE)
> 
> # multinomial
> g4 = sample(1:4, 100, replace = TRUE)
> fit3 = glmnet(x, g4, family = "multinomial")
> fit3a = glmnet(x, g4, family = "multinomial", type.multinomial = "grouped")
> # poisson
> N = 500
> p = 20
> nzc = 5
> x = matrix(rnorm(N * p), N, p)
> beta = rnorm(nzc)
> f = x[, seq(nzc)] %*% beta
> mu = exp(f)
> y = rpois(N, mu)
> fit = glmnet(x, y, family = "poisson")
> plot(fit)
> pfit = predict(fit, x, s = 0.001, type = "response")
> plot(pfit, y)
> 
> # Cox
> set.seed(10101)
> N = 1000
> p = 30
> nzc = p/3
> x = matrix(rnorm(N * p), N, p)
> beta = rnorm(nzc)
> fx = x[, seq(nzc)] %*% beta/3
> hx = exp(fx)
> ty = rexp(N, hx)
> tcens = rbinom(n = N, prob = 0.3, size = 1)  # censoring indicator
> y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)
> fit = glmnet(x, y, family = "cox")
> plot(fit)
> 
> # Cox example with (start, stop] data
> set.seed(2)
> nobs <- 100; nvars <- 15
> xvec <- rnorm(nobs * nvars)
> xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0
> x <- matrix(xvec, nrow = nobs)
> start_time <- runif(100, min = 0, max = 5)
> stop_time <- start_time + runif(100, min = 0.1, max = 3)
> status <- rbinom(n = nobs, prob = 0.3, size = 1)
> jsurv_ss <- survival::Surv(start_time, stop_time, status)
> fit <- glmnet(x, jsurv_ss, family = "cox")
> 
> # Cox example with strata
> jsurv_ss2 <- stratifySurv(jsurv_ss, rep(1:2, each = 50))
> fit <- glmnet(x, jsurv_ss2, family = "cox")
> 
> # Sparse
> n = 10000
> p = 200
> nzc = trunc(p/10)
> x = matrix(rnorm(n * p), n, p)
> iz = sample(1:(n * p), size = n * p * 0.85, replace = FALSE)
> x[iz] = 0
> sx = Matrix(x, sparse = TRUE)
> inherits(sx, "sparseMatrix")  #confirm that it is sparse
[1] TRUE
> beta = rnorm(nzc)
> fx = x[, seq(nzc)] %*% beta
> eps = rnorm(n)
> y = fx + eps
> px = exp(fx)
> px = px/(1 + px)
> ly = rbinom(n = length(px), prob = px, size = 1)
> system.time(fit1 <- glmnet(sx, y))
   user  system elapsed 
  0.288   0.000   0.288 
> system.time(fit2n <- glmnet(x, y))
   user  system elapsed 
  0.065   0.000   0.067 
> 
> 
> 
> 
> cleanEx()
> nameEx("glmnet.control")
> ### * glmnet.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmnet.control
> ### Title: internal glmnet parameters
> ### Aliases: glmnet.control
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> glmnet.control(fdev = 0)  #continue along path even though not much changes
> glmnet.control()  # view current settings
$fdev
[1] 0

$eps
[1] 1e-06

$big
[1] 9.9e+35

$mnlam
[1] 5

$devmax
[1] 0.999

$pmin
[1] 1e-09

$exmx
[1] 250

$itrace
[1] 0

$prec
[1] 1e-10

$mxit
[1] 100

$epsnr
[1] 1e-06

$mxitnr
[1] 25

> glmnet.control(factory = TRUE)  # reset all the parameters to their default
> 
> 
> 
> 
> cleanEx()
> nameEx("glmnet.path")
> ### * glmnet.path
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmnet.path
> ### Title: Fit a GLM with elastic net regularization for a path of lambda
> ###   values
> ### Aliases: glmnet.path
> 
> ### ** Examples
> 
> set.seed(1)
> x <- matrix(rnorm(100 * 20), nrow = 100)
> y <- ifelse(rnorm(100) > 0, 1, 0)
> 
> # binomial with probit link
> fit1 <- glmnet:::glmnet.path(x, y, family = binomial(link = "probit"))
> 
> 
> 
> cleanEx()
> nameEx("makeX")
> ### * makeX
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeX
> ### Title: convert a data frame to a data matrix with one-hot encoding
> ### Aliases: makeX
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> set.seed(101)
> ### Single data frame
> X = matrix(rnorm(20), 10, 2)
> X3 = sample(letters[1:3], 10, replace = TRUE)
> X4 = sample(LETTERS[1:3], 10, replace = TRUE)
> df = data.frame(X, X3, X4)
> makeX(df)
           X1         X2 X3a X3b X3c X4A X4B X4C
1  -0.3260365  0.5264481   0   1   0   0   0   1
2   0.5524619 -0.7948444   0   0   1   0   1   0
3  -0.6749438  1.4277555   1   0   0   0   1   0
4   0.2143595 -1.4668197   1   0   0   1   0   0
5   0.3107692 -0.2366834   1   0   0   0   1   0
6   1.1739663 -0.1933380   0   1   0   1   0   0
7   0.6187899 -0.8497547   1   0   0   1   0   0
8  -0.1127343  0.0584655   0   1   0   1   0   0
9   0.9170283 -0.8176704   0   1   0   0   0   1
10 -0.2232594 -2.0503078   0   0   1   0   0   1
> makeX(df, sparse = TRUE)
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3a X3b X3c X4A X4B X4C
1  -0.3260365  0.5264481   .   1   .   .   .   1
2   0.5524619 -0.7948444   .   .   1   .   1   .
3  -0.6749438  1.4277555   1   .   .   .   1   .
4   0.2143595 -1.4668197   1   .   .   1   .   .
5   0.3107692 -0.2366834   1   .   .   .   1   .
6   1.1739663 -0.1933380   .   1   .   1   .   .
7   0.6187899 -0.8497547   1   .   .   1   .   .
8  -0.1127343  0.0584655   .   1   .   1   .   .
9   0.9170283 -0.8176704   .   1   .   .   .   1
10 -0.2232594 -2.0503078   .   .   1   .   .   1
> 
> ### Single data freame with missing values
> Xn = X
> Xn[3, 1] = NA
> Xn[5, 2] = NA
> X3n = X3
> X3n[6] = NA
> X4n = X4
> X4n[9] = NA
> dfn = data.frame(Xn, X3n, X4n)
> 
> makeX(dfn)
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
1  -0.3260365  0.5264481    0    1    0    0    0    1
2   0.5524619 -0.7948444    0    0    1    0    1    0
3          NA  1.4277555    1    0    0    0    1    0
4   0.2143595 -1.4668197    1    0    0    1    0    0
5   0.3107692         NA    1    0    0    0    1    0
6   1.1739663 -0.1933380   NA   NA   NA    1    0    0
7   0.6187899 -0.8497547    1    0    0    1    0    0
8  -0.1127343  0.0584655    0    1    0    1    0    0
9   0.9170283 -0.8176704    0    1    0   NA   NA   NA
10 -0.2232594 -2.0503078    0    0    1    0    0    1
> makeX(dfn, sparse = TRUE)
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
1  -0.3260365  0.5264481    .    1    .    .    .    1
2   0.5524619 -0.7948444    .    .    1    .    1    .
3          NA  1.4277555    1    .    .    .    1    .
4   0.2143595 -1.4668197    1    .    .    1    .    .
5   0.3107692         NA    1    .    .    .    1    .
6   1.1739663 -0.1933380   NA   NA   NA    1    .    .
7   0.6187899 -0.8497547    1    .    .    1    .    .
8  -0.1127343  0.0584655    .    1    .    1    .    .
9   0.9170283 -0.8176704    .    1    .   NA   NA   NA
10 -0.2232594 -2.0503078    .    .    1    .    .    1
> makeX(dfn, na.impute = TRUE)
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
1  -0.3260365  0.5264481 0.0000000 1.0000000 0.0000000 0.0000000 0.0000000
2   0.5524619 -0.7948444 0.0000000 0.0000000 1.0000000 0.0000000 1.0000000
3   0.3472605  1.4277555 1.0000000 0.0000000 0.0000000 0.0000000 1.0000000
4   0.2143595 -1.4668197 1.0000000 0.0000000 0.0000000 1.0000000 0.0000000
5   0.3107692 -0.4622295 1.0000000 0.0000000 0.0000000 0.0000000 1.0000000
6   1.1739663 -0.1933380 0.4444444 0.3333333 0.2222222 1.0000000 0.0000000
7   0.6187899 -0.8497547 1.0000000 0.0000000 0.0000000 1.0000000 0.0000000
8  -0.1127343  0.0584655 0.0000000 1.0000000 0.0000000 1.0000000 0.0000000
9   0.9170283 -0.8176704 0.0000000 1.0000000 0.0000000 0.4444444 0.3333333
10 -0.2232594 -2.0503078 0.0000000 0.0000000 1.0000000 0.0000000 0.0000000
        X4nC
1  1.0000000
2  0.0000000
3  0.0000000
4  0.0000000
5  0.0000000
6  0.0000000
7  0.0000000
8  0.0000000
9  0.2222222
10 1.0000000
attr(,"means")
        X1         X2       X3na       X3nb       X3nc       X4nA       X4nB 
 0.3472605 -0.4622295  0.4444444  0.3333333  0.2222222  0.4444444  0.3333333 
      X4nC 
 0.2222222 
> makeX(dfn, na.impute = TRUE, sparse = TRUE)
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
1  -0.3260365  0.5264481 .         1.0000000 .         .         .        
2   0.5524619 -0.7948444 .         .         1.0000000 .         1.0000000
3   0.3472605  1.4277555 1.0000000 .         .         .         1.0000000
4   0.2143595 -1.4668197 1.0000000 .         .         1.0000000 .        
5   0.3107692 -0.4622295 1.0000000 .         .         .         1.0000000
6   1.1739663 -0.1933380 0.4444444 0.3333333 0.2222222 1.0000000 .        
7   0.6187899 -0.8497547 1.0000000 .         .         1.0000000 .        
8  -0.1127343  0.0584655 .         1.0000000 .         1.0000000 .        
9   0.9170283 -0.8176704 .         1.0000000 .         0.4444444 0.3333333
10 -0.2232594 -2.0503078 .         .         1.0000000 .         .        
        X4nC
1  1.0000000
2  .        
3  .        
4  .        
5  .        
6  .        
7  .        
8  .        
9  0.2222222
10 1.0000000
> 
> ### Test data as well
> X = matrix(rnorm(10), 5, 2)
> X3 = sample(letters[1:3], 5, replace = TRUE)
> X4 = sample(LETTERS[1:3], 5, replace = TRUE)
> dft = data.frame(X, X3, X4)
> 
> makeX(df, dft)
$x
           X1         X2 X3a X3b X3c X4A X4B X4C
1  -0.3260365  0.5264481   0   1   0   0   0   1
2   0.5524619 -0.7948444   0   0   1   0   1   0
3  -0.6749438  1.4277555   1   0   0   0   1   0
4   0.2143595 -1.4668197   1   0   0   1   0   0
5   0.3107692 -0.2366834   1   0   0   0   1   0
6   1.1739663 -0.1933380   0   1   0   1   0   0
7   0.6187899 -0.8497547   1   0   0   1   0   0
8  -0.1127343  0.0584655   0   1   0   1   0   0
9   0.9170283 -0.8176704   0   1   0   0   0   1
10 -0.2232594 -2.0503078   0   0   1   0   0   1

$xtest
           X1         X2 X3a X3b X3c X4A X4B X4C
11 -0.5098443 -0.7556130   0   1   0   0   1   0
12  1.5661805  1.7384118   0   0   1   0   1   0
13  1.5273728  0.7580952   0   1   0   0   1   0
14  1.0059925  2.1152294   1   0   0   1   0   0
15 -0.5829222  1.6704604   0   1   0   0   1   0

> makeX(df, dft, sparse = TRUE)
$x
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3a X3b X3c X4A X4B X4C
1  -0.3260365  0.5264481   .   1   .   .   .   1
2   0.5524619 -0.7948444   .   .   1   .   1   .
3  -0.6749438  1.4277555   1   .   .   .   1   .
4   0.2143595 -1.4668197   1   .   .   1   .   .
5   0.3107692 -0.2366834   1   .   .   .   1   .
6   1.1739663 -0.1933380   .   1   .   1   .   .
7   0.6187899 -0.8497547   1   .   .   1   .   .
8  -0.1127343  0.0584655   .   1   .   1   .   .
9   0.9170283 -0.8176704   .   1   .   .   .   1
10 -0.2232594 -2.0503078   .   .   1   .   .   1

$xtest
5 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3a X3b X3c X4A X4B X4C
11 -0.5098443 -0.7556130   .   1   .   .   1   .
12  1.5661805  1.7384118   .   .   1   .   1   .
13  1.5273728  0.7580952   .   1   .   .   1   .
14  1.0059925  2.1152294   1   .   .   1   .   .
15 -0.5829222  1.6704604   .   1   .   .   1   .

> 
> ### Missing data in test as well
> Xn = X
> Xn[3, 1] = NA
> Xn[5, 2] = NA
> X3n = X3
> X3n[1] = NA
> X4n = X4
> X4n[2] = NA
> dftn = data.frame(Xn, X3n, X4n)
> 
> makeX(dfn, dftn)
$x
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
1  -0.3260365  0.5264481    0    1    0    0    0    1
2   0.5524619 -0.7948444    0    0    1    0    1    0
3          NA  1.4277555    1    0    0    0    1    0
4   0.2143595 -1.4668197    1    0    0    1    0    0
5   0.3107692         NA    1    0    0    0    1    0
6   1.1739663 -0.1933380   NA   NA   NA    1    0    0
7   0.6187899 -0.8497547    1    0    0    1    0    0
8  -0.1127343  0.0584655    0    1    0    1    0    0
9   0.9170283 -0.8176704    0    1    0   NA   NA   NA
10 -0.2232594 -2.0503078    0    0    1    0    0    1

$xtest
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
11 -0.5098443 -0.7556130   NA   NA   NA    0    1    0
12  1.5661805  1.7384118    0    0    1   NA   NA   NA
13         NA  0.7580952    0    1    0    0    1    0
14  1.0059925  2.1152294    1    0    0    1    0    0
15 -0.5829222         NA    0    1    0    0    1    0

> makeX(dfn, dftn, sparse = TRUE)
$x
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
1  -0.3260365  0.5264481    .    1    .    .    .    1
2   0.5524619 -0.7948444    .    .    1    .    1    .
3          NA  1.4277555    1    .    .    .    1    .
4   0.2143595 -1.4668197    1    .    .    1    .    .
5   0.3107692         NA    1    .    .    .    1    .
6   1.1739663 -0.1933380   NA   NA   NA    1    .    .
7   0.6187899 -0.8497547    1    .    .    1    .    .
8  -0.1127343  0.0584655    .    1    .    1    .    .
9   0.9170283 -0.8176704    .    1    .   NA   NA   NA
10 -0.2232594 -2.0503078    .    .    1    .    .    1

$xtest
5 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2 X3na X3nb X3nc X4nA X4nB X4nC
11 -0.5098443 -0.7556130   NA   NA   NA    .    1    .
12  1.5661805  1.7384118    .    .    1   NA   NA   NA
13         NA  0.7580952    .    1    .    .    1    .
14  1.0059925  2.1152294    1    .    .    1    .    .
15 -0.5829222         NA    .    1    .    .    1    .

> makeX(dfn, dftn, na.impute = TRUE)
$x
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
1  -0.3260365  0.5264481 0.0000000 1.0000000 0.0000000 0.0000000 0.0000000
2   0.5524619 -0.7948444 0.0000000 0.0000000 1.0000000 0.0000000 1.0000000
3   0.3472605  1.4277555 1.0000000 0.0000000 0.0000000 0.0000000 1.0000000
4   0.2143595 -1.4668197 1.0000000 0.0000000 0.0000000 1.0000000 0.0000000
5   0.3107692 -0.4622295 1.0000000 0.0000000 0.0000000 0.0000000 1.0000000
6   1.1739663 -0.1933380 0.4444444 0.3333333 0.2222222 1.0000000 0.0000000
7   0.6187899 -0.8497547 1.0000000 0.0000000 0.0000000 1.0000000 0.0000000
8  -0.1127343  0.0584655 0.0000000 1.0000000 0.0000000 1.0000000 0.0000000
9   0.9170283 -0.8176704 0.0000000 1.0000000 0.0000000 0.4444444 0.3333333
10 -0.2232594 -2.0503078 0.0000000 0.0000000 1.0000000 0.0000000 0.0000000
        X4nC
1  1.0000000
2  0.0000000
3  0.0000000
4  0.0000000
5  0.0000000
6  0.0000000
7  0.0000000
8  0.0000000
9  0.2222222
10 1.0000000
attr(,"means")
        X1         X2       X3na       X3nb       X3nc       X4nA       X4nB 
 0.3472605 -0.4622295  0.4444444  0.3333333  0.2222222  0.4444444  0.3333333 
      X4nC 
 0.2222222 

$xtest
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
11 -0.5098443 -0.7556130 0.4444444 0.3333333 0.2222222 0.0000000 1.0000000
12  1.5661805  1.7384118 0.0000000 0.0000000 1.0000000 0.4444444 0.3333333
13  0.3472605  0.7580952 0.0000000 1.0000000 0.0000000 0.0000000 1.0000000
14  1.0059925  2.1152294 1.0000000 0.0000000 0.0000000 1.0000000 0.0000000
15 -0.5829222 -0.4622295 0.0000000 1.0000000 0.0000000 0.0000000 1.0000000
        X4nC
11 0.0000000
12 0.2222222
13 0.0000000
14 0.0000000
15 0.0000000

> makeX(dfn, dftn, sparse = TRUE, na.impute = TRUE)
$x
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
1  -0.3260365  0.5264481 .         1.0000000 .         .         .        
2   0.5524619 -0.7948444 .         .         1.0000000 .         1.0000000
3   0.3472605  1.4277555 1.0000000 .         .         .         1.0000000
4   0.2143595 -1.4668197 1.0000000 .         .         1.0000000 .        
5   0.3107692 -0.4622295 1.0000000 .         .         .         1.0000000
6   1.1739663 -0.1933380 0.4444444 0.3333333 0.2222222 1.0000000 .        
7   0.6187899 -0.8497547 1.0000000 .         .         1.0000000 .        
8  -0.1127343  0.0584655 .         1.0000000 .         1.0000000 .        
9   0.9170283 -0.8176704 .         1.0000000 .         0.4444444 0.3333333
10 -0.2232594 -2.0503078 .         .         1.0000000 .         .        
        X4nC
1  1.0000000
2  .        
3  .        
4  .        
5  .        
6  .        
7  .        
8  .        
9  0.2222222
10 1.0000000

$xtest
5 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2      X3na      X3nb      X3nc      X4nA      X4nB
11 -0.5098443 -0.7556130 0.4444444 0.3333333 0.2222222 .         1.0000000
12  1.5661805  1.7384118 .         .         1.0000000 0.4444444 0.3333333
13  0.3472605  0.7580952 .         1.0000000 .         .         1.0000000
14  1.0059925  2.1152294 1.0000000 .         .         1.0000000 .        
15 -0.5829222 -0.4622295 .         1.0000000 .         .         1.0000000
        X4nC
11 .        
12 0.2222222
13 .        
14 .        
15 .        

> 
> 
> 
> 
> cleanEx()
> nameEx("na.replace")
> ### * na.replace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.replace
> ### Title: Replace the missing entries in a matrix columnwise with the
> ###   entries in a supplied vector
> ### Aliases: na.replace
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> set.seed(101)
> ### Single data frame
> X = matrix(rnorm(20), 10, 2)
> X[3, 1] = NA
> X[5, 2] = NA
> X3 = sample(letters[1:3], 10, replace = TRUE)
> X3[6] = NA
> X4 = sample(LETTERS[1:3], 10, replace = TRUE)
> X4[9] = NA
> dfn = data.frame(X, X3, X4)
> 
> x = makeX(dfn)
> m = rowSums(x, na.rm = TRUE)
> na.replace(x, m)
           X1         X2      X3a       X3b      X3c      X4A      X4B      X4C
1  -0.3260365  0.5264481 0.000000 1.0000000 0.000000 0.000000 0.000000 1.000000
2   0.5524619 -0.7948444 0.000000 0.0000000 1.000000 0.000000 1.000000 0.000000
3   2.2004116  1.4277555 1.000000 0.0000000 0.000000 0.000000 1.000000 0.000000
4   0.2143595 -1.4668197 1.000000 0.0000000 0.000000 1.000000 0.000000 0.000000
5   0.3107692  1.7576174 1.000000 0.0000000 0.000000 0.000000 1.000000 0.000000
6   1.1739663 -0.1933380 3.427756 0.7475398 2.310769 1.000000 0.000000 0.000000
7   0.6187899 -0.8497547 1.000000 0.0000000 0.000000 1.000000 0.000000 0.000000
8  -0.1127343  0.0584655 0.000000 1.0000000 0.000000 1.000000 0.000000 0.000000
9   0.9170283 -0.8176704 0.000000 1.0000000 0.000000 1.980628 1.769035 1.945731
10 -0.2232594 -2.0503078 0.000000 0.0000000 1.000000 0.000000 0.000000 1.000000
> 
> x = makeX(dfn, sparse = TRUE)
> na.replace(x, m)
10 x 8 sparse Matrix of class "dgCMatrix"
           X1         X2      X3a       X3b      X3c      X4A      X4B      X4C
1  -0.3260365  0.5264481 .        1.0000000 .        .        .        1.000000
2   0.5524619 -0.7948444 .        .         1.000000 .        1.000000 .       
3   2.2004116  1.4277555 1.000000 .         .        .        1.000000 .       
4   0.2143595 -1.4668197 1.000000 .         .        1.000000 .        .       
5   0.3107692  1.7576174 1.000000 .         .        .        1.000000 .       
6   1.1739663 -0.1933380 3.427756 0.7475398 2.310769 1.000000 .        .       
7   0.6187899 -0.8497547 1.000000 .         .        1.000000 .        .       
8  -0.1127343  0.0584655 .        1.0000000 .        1.000000 .        .       
9   0.9170283 -0.8176704 .        1.0000000 .        1.980628 1.769035 1.945731
10 -0.2232594 -2.0503078 .        .         1.000000 .        .        1.000000
> 
> 
> 
> 
> cleanEx()
> nameEx("plot.cv.glmnet")
> ### * plot.cv.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.cv.glmnet
> ### Title: plot the cross-validation curve produced by cv.glmnet
> ### Aliases: plot.cv.glmnet plot.cv.relaxed
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> set.seed(1010)
> n = 1000
> p = 100
> nzc = trunc(p/10)
> x = matrix(rnorm(n * p), n, p)
> beta = rnorm(nzc)
> fx = (x[, seq(nzc)] %*% beta)
> eps = rnorm(n) * 5
> y = drop(fx + eps)
> px = exp(fx)
> px = px/(1 + px)
> ly = rbinom(n = length(px), prob = px, size = 1)
> cvob1 = cv.glmnet(x, y)
> plot(cvob1)
> title("Gaussian Family", line = 2.5)
> cvob1r = cv.glmnet(x, y, relax = TRUE)
> plot(cvob1r)
> frame()
> set.seed(1011)
> par(mfrow = c(2, 2), mar = c(4.5, 4.5, 4, 1))
> cvob2 = cv.glmnet(x, ly, family = "binomial")
> plot(cvob2)
> title("Binomial Family", line = 2.5)
> ## set.seed(1011)
> ## cvob3 = cv.glmnet(x, ly, family = "binomial", type = "class")
> ## plot(cvob3)
> ## title("Binomial Family", line = 2.5)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("plot.glmnet")
> ### * plot.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.glmnet
> ### Title: plot coefficients from a "glmnet" object
> ### Aliases: plot.glmnet plot.multnet plot.mrelnet plot.relaxed
> ### Keywords: models regression
> 
> ### ** Examples
> 
> x=matrix(rnorm(100*20),100,20)
> y=rnorm(100)
> g2=sample(1:2,100,replace=TRUE)
> g4=sample(1:4,100,replace=TRUE)
> fit1=glmnet(x,y)
> plot(fit1)
> plot(fit1,xvar="lambda",label=TRUE)
> fit3=glmnet(x,g4,family="multinomial")
> plot(fit3,pch=19)
> 
> 
> 
> cleanEx()
> nameEx("predict.cv.glmnet")
> ### * predict.cv.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.cv.glmnet
> ### Title: make predictions from a "cv.glmnet" object.
> ### Aliases: predict.cv.glmnet coef.cv.glmnet coef.cv.relaxed
> ###   predict.cv.relaxed
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> cv.fit = cv.glmnet(x, y)
> predict(cv.fit, newx = x[1:5, ])
     lambda.1se
[1,] -0.1417928
[2,] -0.1417928
[3,] -0.1417928
[4,] -0.1417928
[5,] -0.1417928
> coef(cv.fit)
21 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) -0.1417928
V1           .        
V2           .        
V3           .        
V4           .        
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          .        
> coef(cv.fit, s = "lambda.min")
21 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) -0.1417887
V1           .        
V2           .        
V3           .        
V4           .        
V5           .        
V6           .        
V7           .        
V8          -0.0160778
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          .        
> predict(cv.fit, newx = x[1:5, ], s = c(0.001, 0.002))
             s1         s2
[1,]  0.2253033  0.2170970
[2,] -0.5442100 -0.5466183
[3,]  0.3291358  0.3278139
[4,]  0.3724632  0.3691489
[5,] -0.4436348 -0.4434621
> cv.fitr = cv.glmnet(x, y, relax = TRUE)
> predict(cv.fit, newx = x[1:5, ])
     lambda.1se
[1,] -0.1417928
[2,] -0.1417928
[3,] -0.1417928
[4,] -0.1417928
[5,] -0.1417928
> coef(cv.fit)
21 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) -0.1417928
V1           .        
V2           .        
V3           .        
V4           .        
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          .        
> coef(cv.fit, s = "lambda.min", gamma = "gamma.min")
21 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept) -0.1417887
V1           .        
V2           .        
V3           .        
V4           .        
V5           .        
V6           .        
V7           .        
V8          -0.0160778
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          .        
> predict(cv.fit, newx = x[1:5, ], s = c(0.001, 0.002), gamma = "gamma.min")
             s1         s2
[1,]  0.2253033  0.2170970
[2,] -0.5442100 -0.5466183
[3,]  0.3291358  0.3278139
[4,]  0.3724632  0.3691489
[5,] -0.4436348 -0.4434621
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.glmnet")
> ### * predict.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coef.glmnet
> ### Title: Extract coefficients from a glmnet object
> ### Aliases: coef.glmnet predict.glmnet coef.relaxed predict.relaxed
> ###   predict.elnet predict.lognet predict.multnet predict.mrelnet
> ###   predict.fishnet predict.coxnet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> x=matrix(rnorm(100*20),100,20)
> y=rnorm(100)
> g2=sample(1:2,100,replace=TRUE)
> g4=sample(1:4,100,replace=TRUE)
> fit1=glmnet(x,y)
> predict(fit1,newx=x[1:5,],s=c(0.01,0.005))
             s1         s2
[1,]  0.1623437  0.1923152
[2,] -0.5447409 -0.5540136
[3,]  0.3089815  0.3238559
[4,]  0.3441108  0.3593363
[5,] -0.4261315 -0.4431612
> predict(fit1,type="coef")
21 x 66 sparse Matrix of class "dgCMatrix"
  [[ suppressing 66 column names ‘s0’, ‘s1’, ‘s2’ ... ]]
                                                                        
(Intercept) -0.1417928 -0.1417887 -0.14178501 -0.1417478737 -0.138172119
V1           .          .          .           .             .          
V2           .          .          .           0.0008919145  0.017974756
V3           .          .          .           .             0.004490984
V4           .          .          .           .             .          
V5           .          .          .           .             .          
V6           .          .          .           .             .          
V7           .          .          .           .             .          
V8           .         -0.0160778 -0.03072729 -0.0441276998 -0.056837237
V9           .          .          .           .             .          
V10          .          .          .           .             .          
V11          .          .          .           .             .          
V12          .          .          .           .             .          
V13          .          .          .           .            -0.012614957
V14          .          .          .           .             .          
V15          .          .          .           .             .          
V16          .          .          .           .             .          
V17          .          .          .           .             .          
V18          .          .          .           .             .          
V19          .          .          .           .             .          
V20          .          .          .           .             .          
                                                                        
(Intercept) -0.13456053 -0.13126978 -0.12827136 -0.126024382 -0.12457388
V1           .           .           .           0.004378485  0.01375550
V2           0.03420188  0.04898754  0.06245968  0.074737991  0.08592887
V3           0.01584973  0.02619921  0.03562927  0.044128743  0.05175894
V4           .           .           .           .            .         
V5           .           .           .           .            .         
V6           .           .           .           .            .         
V7           .           .           .           .            .         
V8          -0.06749967 -0.07721492 -0.08606709 -0.094273527 -0.10192396
V9           .           .           .           .            .         
V10          .           .           .           .            .         
V11          .           .           .           .            .         
V12          .           .           .           .            .         
V13         -0.02635333 -0.03887123 -0.05027708 -0.060623473 -0.06999385
V14          .           .           .           .            .         
V15          .           .           .           .            .         
V16          .           .           .           .            .         
V17          .           .           .           .            .         
V18          .           .           .           .            .         
V19          .           .           .           .            .         
V20          .           .           .           .            .         
                                                                          
(Intercept) -0.123092260 -0.12137748 -0.11981509 -0.119577235 -0.119702217
V1           0.021960097  0.02865998  0.03476503  0.040771967  0.046651303
V2           0.096321083  0.10623755  0.11527286  0.123933216  0.132184522
V3           0.059313151  0.06757379  0.07509966  0.081077849  0.086371361
V4           .            .           .           .            .          
V5           .            .           .           .            .          
V6           .            .           .           .            .          
V7           .            .           .           .            .          
V8          -0.109049143 -0.11589276 -0.12212845 -0.127596066 -0.132682865
V9           .            .           .           .            .          
V10          0.003153469  0.01321132  0.02237508  0.031222991  0.039319686
V11          .            .           .           .            0.002308117
V12          .            .           .           .            .          
V13         -0.079134826 -0.08883813 -0.09767926 -0.105620022 -0.112892265
V14          .            .           .          -0.002335272 -0.005610950
V15          .            .           .           .            .          
V16          .            .           .           .            .          
V17          .            .           .           .            .          
V18          .            .           .           .            .          
V19          .            .           .           0.005560349  0.011742684
V20          .            .           .           .            .          
                                                                              
(Intercept) -0.119850090 -0.1199868655 -0.119715815 -0.119468192 -0.1192272147
V1           0.052801292  0.0586827820  0.064280981  0.069379972  0.0740259709
V2           0.140333921  0.1476821797  0.154347055  0.160422725  0.1659586694
V3           0.091215440  0.0957861307  0.100150855  0.104131143  0.1077578535
V4           0.000717184  0.0019027959  0.002684843  0.003390248  0.0040329530
V5           .            .             .            .            .           
V6           .            .             .            .            .           
V7           .            .             .            .            .           
V8          -0.137696326 -0.1427242438 -0.147335354 -0.151537376 -0.1553661027
V9           .            .             .            .            .           
V10          0.046449191  0.0522711059  0.057435932  0.062146503  0.0664386329
V11          0.009274153  0.0154379581  0.020655093  0.025409743  0.0297420108
V12          .            0.0003596194  0.004115996  0.007538125  0.0106562395
V13         -0.119669170 -0.1255045072 -0.130597548 -0.135239685 -0.1394694410
V14         -0.008903354 -0.0117027938 -0.014606102 -0.017252092 -0.0196630233
V15          .            .             .            .            .           
V16          .            0.0027002708  0.006100211  0.009199773  0.0120239818
V17          .            .             .            .            .           
V18          .            .             .            .            .           
V19          0.017140319  0.0220285600  0.026530605  0.030633776  0.0343724390
V20          .            .             .            .            0.0001943677
                                                                            
(Intercept) -0.118594121 -0.118101454 -0.117716309 -0.117365415 -0.117045694
V1           0.077392961  0.080468100  0.083247052  0.085778639  0.088085327
V2           0.171311368  0.176486131  0.181472369  0.186015234  0.190154524
V3           0.111995300  0.116318789  0.120697191  0.124686028  0.128320506
V4           0.004965750  0.005494256  0.005719535  0.005922562  0.006107554
V5           .            .            .            .            .          
V6           .            .            .            .            .          
V7           .            .            .            .            .          
V8          -0.160029084 -0.164757650 -0.169510407 -0.173838068 -0.177781273
V9           .            .            .            .            .          
V10          0.070685070  0.075055599  0.079480248  0.083513450  0.087188352
V11          0.033348320  0.036478248  0.039196967  0.041675838  0.043934494
V12          0.014006830  0.017192160  0.020215555  0.022970403  0.025480519
V13         -0.143052671 -0.146314298 -0.149283842 -0.151990048 -0.154455842
V14         -0.021813177 -0.023489092 -0.024767674 -0.025934431 -0.026997537
V15          .            0.002387192  0.006560286  0.010361270  0.013824585
V16          0.015008120  0.017832074  0.020497741  0.022926287  0.025139088
V17          .            .            .            .            .          
V18          .            .            .            .            .          
V19          0.038320080  0.041608083  0.044352667  0.046853984  0.049133090
V20          0.005471067  0.010028068  0.013998837  0.017616098  0.020912011
                                                                            
(Intercept) -0.116754375 -0.116488937 -0.116247079 -0.116030669 -0.115829571
V1           0.090187095  0.092102149  0.093847074  0.095447494  0.096895354
V2           0.193926090  0.197362601  0.200493822  0.203333221  0.205933830
V3           0.131632108  0.134649515  0.137398865  0.139878028  0.142162598
V4           0.006276112  0.006429695  0.006569635  0.006722119  0.006836386
V5           .            .            .            .            .          
V6           .            .            .            .            .          
V7           .            .            .            .            .          
V8          -0.181374173 -0.184647891 -0.187630780 -0.190334643 -0.192812215
V9           .            .            .            .            .          
V10          0.090536787  0.093587755  0.096367684  0.098880477  0.101189913
V11          0.045992496  0.047867671  0.049576261  0.051136806  0.052554973
V12          0.027767643  0.029851585  0.031750395  0.033481818  0.035058135
V13         -0.156702581 -0.158749726 -0.160615009 -0.162307971 -0.163857049
V14         -0.027966199 -0.028848808 -0.029653009 -0.030391994 -0.031059144
V15          0.016980229  0.019855534  0.022475404  0.024846242  0.027022584
V16          0.027155309  0.028992415  0.030666318  0.032183857  0.033574173
V17          .            .            .            .            .          
V18          .            .            .            .            .          
V19          0.051209727  0.053101881  0.054825942  0.056398003  0.057829233
V20          0.023915124  0.026651449  0.029144687  0.031411613  0.033481921
                                                                               
(Intercept) -0.1156442603 -0.1154728036 -0.115428980 -0.115377054 -0.1153141739
V1           0.0982144658  0.0992693064  0.100309383  0.101244996  0.1020971618
V2           0.2083036039  0.2103594724  0.212190893  0.213873721  0.2154076147
V3           0.1442445027  0.1461104449  0.147727717  0.149226282  0.1505924286
V4           0.0069402001  0.0069911034  0.007165004  0.007284834  0.0073927784
V5           0.0000434915  0.0008976257  0.001655467  0.002354820  0.0029923439
V6           .             .             .            .            .           
V7           .            -0.0002734381 -0.000931219 -0.001508970 -0.0020347529
V8          -0.1950744810 -0.1970933106 -0.198784613 -0.200350890 -0.2017786646
V9           .             .             .            .            .           
V10          0.1032993418  0.1053102835  0.107013364  0.108596765  0.1100406165
V11          0.0538472279  0.0550756829  0.056275262  0.057356806  0.0583420986
V12          0.0364974848  0.0378876938  0.039201485  0.040390616  0.0414738838
V13         -0.1652717845 -0.1664264592 -0.167253596 -0.168036837 -0.1687514167
V14         -0.0316694566 -0.0323789602 -0.033130892 -0.033802786 -0.0344146885
V15          0.0290035616  0.0306960536  0.032145083  0.033489125  0.0347144201
V16          0.0348409838  0.0359343250  0.036827313  0.037655218  0.0384099518
V17          .             .             .            .            0.0001728693
V18          .             0.0004831618  0.001271401  0.001971039  0.0025910116
V19          0.0591363268  0.0602807332  0.061259512  0.062154882  0.0629799525
V20          0.0353654271  0.0370884408  0.038709060  0.040184630  0.0415471548
                                                                               
(Intercept) -0.1152801350 -0.1151553146 -0.115037998 -0.114930997 -0.1148705036
V1           0.1027671732  0.1035332622  0.104227453  0.104859971  0.1054509215
V2           0.2167168630  0.2179902490  0.219156122  0.220218650  0.2211671502
V3           0.1517965809  0.1530497744  0.154194364  0.155237460  0.1561612895
V4           0.0073959149  0.0075202512  0.007624600  0.007719499  0.0078283316
V5           0.0035616422  0.0041515396  0.004690228  0.005181133  0.0056186561
V6           0.0000345520  0.0014280435  0.002693182  0.003845990  0.0048929626
V7          -0.0026358434 -0.0031599790 -0.003628768 -0.004055662 -0.0044615343
V8          -0.2029882272 -0.2040223037 -0.204972901 -0.205839217 -0.2065863970
V9           .             .             .            .           -0.0002936383
V10          0.1112468619  0.1124995495  0.113649227  0.114697084  0.1156851114
V11          0.0592434758  0.0601137461  0.060902296  0.061620783  0.0622524720
V12          0.0424062089  0.0433413660  0.044192493  0.044967974  0.0456208344
V13         -0.1693605100 -0.1700667101 -0.170716710 -0.171309211 -0.1718531231
V14         -0.0350451081 -0.0355980361 -0.036097289 -0.036552076 -0.0370307719
V15          0.0358444956  0.0369155982  0.037896193  0.038789813  0.0396233865
V16          0.0391585235  0.0398451139  0.040473215  0.041045559  0.0415352802
V17          0.0005447025  0.0008361926  0.001098305  0.001337002  0.0015729396
V18          0.0031200944  0.0035191185  0.003878926  0.004206693  0.0044768899
V19          0.0637460991  0.0644908331  0.065170148  0.065789135  0.0664377783
V20          0.0427993583  0.0441739953  0.045425288  0.046565405  0.0475985868
                                                                            
(Intercept) -0.114877357 -0.114879507 -0.114889415 -0.114892363 -0.114893876
V1           0.106032174  0.106550971  0.107041806  0.107473234  0.107864694
V2           0.222070148  0.222889708  0.223617550  0.224295702  0.224916063
V3           0.157026739  0.157813393  0.158502387  0.159153292  0.159749440
V4           0.008163893  0.008402939  0.008646522  0.008849467  0.009029738
V5           0.006219175  0.006735062  0.007175716  0.007601120  0.007992369
V6           0.006277064  0.007466245  0.008504581  0.009487596  0.010388890
V7          -0.004813221 -0.005132325 -0.005435072 -0.005702608 -0.005944147
V8          -0.207430355 -0.208170623 -0.208792751 -0.209404308 -0.209966411
V9          -0.001711621 -0.002932283 -0.003953084 -0.004958016 -0.005884796
V10          0.116987954  0.118153827  0.119139968  0.120099008  0.120983187
V11          0.062639398  0.063006737  0.063376865  0.063683834  0.063960166
V12          0.046104648  0.046538527  0.046936956  0.047299268  0.047627790
V13         -0.172487076 -0.173071253 -0.173563264 -0.174041671 -0.174484248
V14         -0.037607311 -0.038130321 -0.038605319 -0.039039413 -0.039435283
V15          0.040530686  0.041354442  0.042075675  0.042755983  0.043379913
V16          0.041917935  0.042272968  0.042583505  0.042877398  0.043146541
V17          0.001806325  0.002024416  0.002217229  0.002398310  0.002563809
V18          0.004470020  0.004477531  0.004527974  0.004539394  0.004543723
V19          0.067436110  0.068321334  0.069079451  0.069810074  0.070481568
V20          0.048678418  0.049635706  0.050486011  0.051280494  0.052005887
                                                                            
(Intercept) -0.114895045 -0.114896076 -0.114897009 -0.114897859 -0.114898633
V1           0.108221132  0.108545868  0.108841750  0.109111345  0.109356989
V2           0.225481768  0.225997294  0.226467034  0.226895046  0.227285034
V3           0.160293152  0.160788647  0.161240138  0.161651522  0.162026359
V4           0.009193232  0.009342080  0.009477686  0.009601241  0.009713821
V5           0.008349443  0.008674887  0.008971435  0.009241640  0.009487842
V6           0.011211000  0.011960215  0.012642894  0.013264929  0.013831705
V7          -0.006163810 -0.006363886 -0.006546177 -0.006712272 -0.006863611
V8          -0.210479380 -0.210946907 -0.211372921 -0.211761093 -0.212114780
V9          -0.006730985 -0.007502275 -0.008205090 -0.008845475 -0.009428972
V10          0.121790512  0.122526387  0.123196933  0.123807916  0.124364622
V11          0.064211433  0.064440297  0.064648817  0.064838811  0.065011925
V12          0.047926865  0.048199330  0.048447583  0.048673781  0.048879884
V13         -0.174888607 -0.175257220 -0.175593115 -0.175899175 -0.176178046
V14         -0.039796009 -0.040124689 -0.040424171 -0.040697047 -0.040945682
V15          0.043949087  0.044467805  0.044940459  0.045371126  0.045763534
V16          0.043391988  0.043615665  0.043819476  0.044005182  0.044174390
V17          0.002714641  0.002852075  0.002977301  0.003091401  0.003195366
V18          0.004546711  0.004549283  0.004551603  0.004553713  0.004555635
V19          0.071094311  0.071652762  0.072161624  0.072625284  0.073047754
V20          0.052667061  0.053269534  0.053818490  0.054318679  0.054774433
                                                                            
(Intercept) -0.114899338 -0.114899981 -0.114900566 -0.114901839 -0.114902389
V1           0.109580812  0.109784750  0.109970572  0.110156079  0.110295996
V2           0.227640378  0.227964153  0.228259165  0.228518079  0.228771308
V3           0.162367898  0.162679095  0.162962646  0.163193266  0.163453865
V4           0.009816398  0.009909863  0.009995025  0.010085563  0.010147576
V5           0.009712172  0.009916573  0.010102815  0.010249361  0.010423680
V6           0.014348130  0.014818678  0.015247423  0.015589794  0.015988400
V7          -0.007001505 -0.007127149 -0.007241632 -0.007338371 -0.007442148
V8          -0.212437048 -0.212730686 -0.212998238 -0.213198601 -0.213459741
V9          -0.009960632 -0.010445061 -0.010886455 -0.011211205 -0.011643983
V10          0.124871872  0.125334060  0.125755187  0.126089672  0.126479155
V11          0.065169661  0.065313384  0.065444338  0.065587624  0.065675830
V12          0.049067678  0.049238788  0.049394697  0.049528483  0.049667562
V13         -0.176432142 -0.176663666 -0.176874621 -0.177054930 -0.177236177
V14         -0.041172229 -0.041378650 -0.041566733 -0.041728459 -0.041893452
V15          0.046121082  0.046446866  0.046743708  0.046996072  0.047257044
V16          0.044328567  0.044469046  0.044597046  0.044702022  0.044818600
V17          0.003290094  0.003376407  0.003455052  0.003514304  0.003590866
V18          0.004557386  0.004558981  0.004560435  0.004588665  0.004568906
V19          0.073432693  0.073783435  0.074103018  0.074353079  0.074653706
V20          0.055189699  0.055568073  0.055912834  0.056197965  0.056511490
                                                                            
(Intercept) -0.114902919 -0.114903611 -0.114904101 -0.114904425 -0.114904657
V1           0.110448830  0.110579396  0.110695786  0.110801061  0.110896727
V2           0.228987253  0.229190534  0.229376140  0.229545503  0.229699946
V3           0.163647145  0.163841922  0.164021295  0.164185275  0.164334899
V4           0.010220039  0.010283246  0.010337557  0.010385564  0.010428761
V5           0.010547536  0.010672255  0.010789454  0.010897477  0.010996342
V6           0.016276335  0.016565666  0.016835993  0.017084437  0.017311525
V7          -0.007521763 -0.007600515 -0.007673043 -0.007739142 -0.007799312
V8          -0.213629065 -0.213810992 -0.213980625 -0.214136430 -0.214278849
V9          -0.011920841 -0.012212452 -0.012490566 -0.012748098 -0.012984157
V10          0.126763079  0.127044467  0.127309044  0.127553084  0.127776505
V11          0.065792337  0.065885808  0.065967497  0.066040882  0.066107394
V12          0.049777770  0.049887284  0.049986320  0.050076089  0.050157708
V13         -0.177389723 -0.177529376 -0.177660708 -0.177782102 -0.177893348
V14         -0.042028128 -0.042155479 -0.042273611 -0.042381897 -0.042480760
V15          0.047468980  0.047670900  0.047857383  0.048028327  0.048184467
V16          0.044906639  0.044993736  0.045074416  0.045148387  0.045215950
V17          0.003640750  0.003692283  0.003741755  0.003787606  0.003829614
V18          0.004588406  0.004596246  0.004597586  0.004596715  0.004595199
V19          0.074865102  0.075078614  0.075279796  0.075465309  0.075635087
V20          0.056749379  0.056985578  0.057203447  0.057402615  0.057584297
                                                                            
(Intercept) -0.114904842 -0.114905001 -0.114905143 -0.114905270 -0.114905386
V1           0.110983808  0.111063123  0.111135383  0.111201220  0.111261206
V2           0.229840723  0.229969015  0.230085916  0.230192436  0.230289493
V3           0.164471310  0.164595630  0.164708916  0.164812142  0.164906198
V4           0.010467927  0.010503548  0.010535981  0.010565524  0.010592440
V5           0.011086578  0.011168852  0.011243835  0.011312164  0.011374425
V6           0.017518684  0.017707524  0.017879617  0.018036431  0.018179318
V7          -0.007854103 -0.007904013 -0.007949484 -0.007990914 -0.008028662
V8          -0.214408778 -0.214527222 -0.214635164 -0.214733523 -0.214823147
V9          -0.013199729 -0.013396318 -0.013575499 -0.013738783 -0.013887568
V10          0.127980452  0.128166412  0.128335897  0.128490342  0.128631072
V11          0.066167875  0.066222942  0.066273102  0.066318801  0.066360438
V12          0.050232016  0.050299701  0.050361366  0.050417550  0.050468742
V13         -0.177994939 -0.178087585 -0.178172029 -0.178248980 -0.178319099
V14         -0.042570904 -0.042653060 -0.042727924 -0.042796140 -0.042858297
V15          0.048326873  0.048456675  0.048574963  0.048682748  0.048780960
V16          0.045277567  0.045333730  0.045384910  0.045431546  0.045474040
V17          0.003867963  0.003902928  0.003934794  0.003963833  0.003990292
V18          0.004593567  0.004591994  0.004590531  0.004589187  0.004587959
V19          0.075790040  0.075931315  0.076060071  0.076177398  0.076284306
V20          0.057749906  0.057900827  0.058038349  0.058163656  0.058277832
                        
(Intercept) -0.114905491
V1           0.111315864
V2           0.230377928
V3           0.164991899
V4           0.010616965
V5           0.011431155
V6           0.018309513
V7          -0.008063057
V8          -0.214904810
V9          -0.014023138
V10          0.128759301
V11          0.066398376
V12          0.050515386
V13         -0.178382990
V14         -0.042914932
V15          0.048870448
V16          0.045512759
V17          0.004014401
V18          0.004586839
V19          0.076381718
V20          0.058381866
> fit2=glmnet(x,g2,family="binomial")
> predict(fit2,type="response",newx=x[2:5,])
       s0        s1        s2        s3        s4        s5        s6        s7
[1,] 0.55 0.5508197 0.5518644 0.5533468 0.5547358 0.5560362 0.5530104 0.5500710
[2,] 0.55 0.5441855 0.5377967 0.5300108 0.5228922 0.5163753 0.5177444 0.5231151
[3,] 0.55 0.5532864 0.5543438 0.5518438 0.5495867 0.5475430 0.5449393 0.5351263
[4,] 0.55 0.5493979 0.5472388 0.5423754 0.5379449 0.5339009 0.5315446 0.5280984
            s8        s9       s10       s11       s12       s13       s14
[1,] 0.5506972 0.5549247 0.5560892 0.5551073 0.5542355 0.5534602 0.5527697
[2,] 0.5239921 0.5201356 0.5167277 0.5136881 0.5108756 0.5082745 0.5058697
[3,] 0.5209633 0.5026612 0.4865167 0.4722117 0.4591213 0.4471514 0.4362124
[4,] 0.5256700 0.5243328 0.5236953 0.5235604 0.5234473 0.5233521 0.5232718
           s15       s16       s17       s18       s19       s20       s21
[1,] 0.5520446 0.5476933 0.5429052 0.5351065 0.5256012 0.5168382 0.5087816
[2,] 0.5037082 0.5037826 0.5031562 0.5004985 0.5028605 0.5054817 0.5078852
[3,] 0.4262733 0.4189535 0.4117641 0.4033109 0.3944733 0.3864167 0.3790885
[4,] 0.5230602 0.5179649 0.5131133 0.5080968 0.5043744 0.5009893 0.4978692
           s22       s23       s24       s25       s26       s27       s28
[1,] 0.5015316 0.4961426 0.4921169 0.4882804 0.4845014 0.4809080 0.4769170
[2,] 0.5079019 0.5061642 0.5048326 0.5041309 0.5041843 0.5039495 0.5029335
[3,] 0.3728682 0.3677146 0.3626090 0.3575042 0.3529293 0.3495202 0.3471112
[4,] 0.4920870 0.4840031 0.4763335 0.4694453 0.4630971 0.4568882 0.4506195
           s29       s30       s31       s32       s33       s34       s35
[1,] 0.4732485 0.4698817 0.4667930 0.4639608 0.4613649 0.4590033 0.4568242
[2,] 0.5011965 0.4996095 0.4981603 0.4968369 0.4956284 0.4945403 0.4935319
[3,] 0.3454630 0.3439591 0.3425864 0.3413335 0.3401899 0.3391569 0.3382040
[4,] 0.4446391 0.4391721 0.4341758 0.4296110 0.4254415 0.4216490 0.4181717
           s36       s37       s38       s39       s40       s41       s42
[1,] 0.4548291 0.4530032 0.4513329 0.4498053 0.4484303 0.4475250 0.4467215
[2,] 0.4926110 0.4917703 0.4910029 0.4903025 0.4896901 0.4895263 0.4894036
[3,] 0.3373344 0.3365410 0.3358171 0.3351566 0.3345434 0.3338232 0.3331815
[4,] 0.4149972 0.4120999 0.4094559 0.4070435 0.4048143 0.4023211 0.4000618
           s43       s44       s45       s46       s47       s48       s49
[1,] 0.4459649 0.4451742 0.4444341 0.4437947 0.4431768 0.4426093 0.4420908
[2,] 0.4892679 0.4892601 0.4892519 0.4892729 0.4892655 0.4892562 0.4892474
[3,] 0.3325821 0.3320033 0.3314721 0.3310163 0.3305736 0.3301688 0.3297996
[4,] 0.3979895 0.3958482 0.3938780 0.3921137 0.3904768 0.3889835 0.3876226
           s50       s51       s52       s53
[1,] 0.4416175 0.4411855 0.4407912 0.4404315
[2,] 0.4892392 0.4892318 0.4892249 0.4892186
[3,] 0.3294631 0.3291564 0.3288768 0.3286219
[4,] 0.3863825 0.3852526 0.3842231 0.3832850
> predict(fit2,type="nonzero")
$s0
NULL

$s1
[1] 17

$s2
[1] 15 17

$s3
[1] 15 17

$s4
[1] 15 17

$s5
[1] 15 17

$s6
[1] 12 15 17

$s7
[1]  1 12 15 17

$s8
[1]  1 12 13 15 17

$s9
[1]  1 12 13 15 17

$s10
[1]  1  5 12 13 15 17

$s11
[1]  1  5 12 13 15 17

$s12
[1]  1  5 12 13 15 17

$s13
[1]  1  5 12 13 15 17

$s14
[1]  1  5 12 13 15 17

$s15
[1]  1  5 12 13 14 15 17

$s16
[1]  1  5 12 13 14 15 17

$s17
[1]  1  5  7 12 13 14 15 17

$s18
 [1]  1  5  7  9 12 13 14 15 16 17

$s19
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s20
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s21
 [1]  1  5  7  9 12 13 14 15 16 17 20

$s22
 [1]  1  4  5  7  9 12 13 14 15 16 17 20

$s23
 [1]  1  4  5  7  8  9 12 13 14 15 16 17 20

$s24
 [1]  1  2  4  5  7  8  9 12 13 14 15 16 17 20

$s25
 [1]  1  2  4  5  7  8  9 11 12 13 14 15 16 17 20

$s26
 [1]  1  2  4  5  7  8  9 11 12 13 14 15 16 17 19 20

$s27
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 19 20

$s28
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s29
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s30
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s31
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s32
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s33
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s34
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s35
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s36
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s37
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s38
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s39
 [1]  1  2  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s40
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s41
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s42
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s43
 [1]  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s44
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s45
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s46
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s47
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s48
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s49
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s50
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s51
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s52
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

$s53
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

> fit3=glmnet(x,g4,family="multinomial")
> predict(fit3,newx=x[1:3,],type="response",s=0.01)
, , 1

             1          2          3          4
[1,] 0.2084099 0.06094591 0.14285087 0.58779333
[2,] 0.1117824 0.09771133 0.47042226 0.32008405
[3,] 0.1956970 0.67741511 0.05326403 0.07362382

> 
> 
> 
> cleanEx()
> nameEx("print.cv.glmnet")
> ### * print.cv.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.cv.glmnet
> ### Title: print a cross-validated glmnet object
> ### Aliases: print.cv.glmnet print.cv.relaxed
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> fit1 = cv.glmnet(x, y)
> print(fit1)

Call:  cv.glmnet(x = x, y = y) 

Measure: Mean-Squared Error 

    Lambda Index Measure     SE Nonzero
min 0.1800     2   1.146 0.1846       1
1se 0.1976     1   1.146 0.1845       0
> fit1r = cv.glmnet(x, y, relax = TRUE)
> print(fit1r)

Call:  cv.glmnet(x = x, y = y, relax = TRUE) 

Measure: Mean-Squared Error 

    Gamma Index Lambda Index Measure      SE Nonzero
min     1     5 0.1976     1   1.148 0.09457       0
1se     1     5 0.1976     1   1.148 0.09457       0
> ## print.cv.glmnet(fit1r)  ## CHECK WITH TREVOR
> 
> 
> 
> cleanEx()
> nameEx("print.glmnet")
> ### * print.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.glmnet
> ### Title: print a glmnet object
> ### Aliases: print.glmnet print.relaxed print.bigGlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> x = matrix(rnorm(100 * 20), 100, 20)
> y = rnorm(100)
> fit1 = glmnet(x, y)
> print(fit1)

Call:  glmnet(x = x, y = y) 

   Df  %Dev   Lambda
1   0  0.00 0.197600
2   1  0.60 0.180000
3   1  1.10 0.164000
4   2  1.54 0.149500
5   4  2.79 0.136200
6   4  4.06 0.124100
7   4  5.11 0.113100
8   4  5.98 0.103000
9   5  6.78 0.093870
10  5  7.52 0.085530
11  6  8.20 0.077940
12  6  8.91 0.071010
13  6  9.49 0.064700
14  8 10.07 0.058960
15  9 10.60 0.053720
16 10 11.12 0.048950
17 12 11.57 0.044600
18 12 11.97 0.040640
19 12 12.31 0.037030
20 13 12.59 0.033740
21 13 12.88 0.030740
22 14 13.13 0.028010
23 14 13.35 0.025520
24 14 13.54 0.023250
25 14 13.69 0.021190
26 14 13.82 0.019310
27 14 13.93 0.017590
28 14 14.02 0.016030
29 14 14.09 0.014600
30 14 14.15 0.013310
31 15 14.20 0.012120
32 17 14.25 0.011050
33 17 14.28 0.010070
34 17 14.32 0.009172
35 18 14.34 0.008357
36 19 14.36 0.007614
37 19 14.39 0.006938
38 19 14.40 0.006322
39 19 14.42 0.005760
40 20 14.43 0.005248
41 20 14.44 0.004782
42 20 14.45 0.004357
43 20 14.46 0.003970
44 20 14.47 0.003617
45 20 14.48 0.003296
46 20 14.48 0.003003
47 20 14.49 0.002736
48 20 14.49 0.002493
49 20 14.49 0.002272
50 20 14.50 0.002070
51 20 14.50 0.001886
52 20 14.50 0.001719
53 20 14.50 0.001566
54 20 14.50 0.001427
55 20 14.50 0.001300
56 20 14.50 0.001185
57 20 14.50 0.001079
58 20 14.51 0.000983
59 20 14.51 0.000896
60 20 14.51 0.000816
61 20 14.51 0.000744
62 20 14.51 0.000678
63 20 14.51 0.000618
64 20 14.51 0.000563
65 20 14.51 0.000513
66 20 14.51 0.000467
> 
> 
> 
> cleanEx()
> nameEx("stratifySurv")
> ### * stratifySurv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: stratifySurv
> ### Title: Add strata to a Surv object
> ### Aliases: stratifySurv
> 
> ### ** Examples
> 
> y <- survival::Surv(1:10, rep(0:1, length.out = 10))
> strata <- rep(1:3, length.out = 10)
> y2 <- stratifySurv(y, strata)  # returns stratifySurv object
> 
> 
> 
> 
> cleanEx()
> nameEx("survfit.coxnet")
> ### * survfit.coxnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survfit.coxnet
> ### Title: Compute a survival curve from a coxnet object
> ### Aliases: survfit.coxnet
> 
> ### ** Examples
> 
> set.seed(2)
> nobs <- 100; nvars <- 15
> xvec <- rnorm(nobs * nvars)
> xvec[sample.int(nobs * nvars, size = 0.4 * nobs * nvars)] <- 0
> x <- matrix(xvec, nrow = nobs)
> beta <- rnorm(nvars / 3)
> fx <- x[, seq(nvars / 3)] %*% beta / 3
> ty <- rexp(nobs, exp(fx))
> tcens <- rbinom(n = nobs, prob = 0.3, size = 1)
> y <- survival::Surv(ty, tcens)
> fit1 <- glmnet(x, y, family = "cox")
> 
> # survfit object for Cox model where lambda = 0.1
> sf1 <- survival::survfit(fit1, s = 0.1, x = x, y = y)
> plot(sf1)
> 
> # example with new data
> sf2 <- survival::survfit(fit1, s = 0.1, x = x, y = y, newx = x[1:3, ])
> plot(sf2)
> 
> # example with strata
> y2 <- stratifySurv(y, rep(1:2, length.out = nobs))
> fit2 <- glmnet(x, y2, family = "cox")
> sf3 <- survival::survfit(fit2, s = 0.1, x = x, y = y2)
> sf4 <- survival::survfit(fit2, s = 0.1, x = x, y = y2,
+                newx = x[1:3, ], newstrata = c(1, 1, 1))
> 
> 
> 
> 
> cleanEx()
> nameEx("survfit.cv.glmnet")
> ### * survfit.cv.glmnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survfit.cv.glmnet
> ### Title: Compute a survival curve from a cv.glmnet object
> ### Aliases: survfit.cv.glmnet
> 
> ### ** Examples
> 
> set.seed(2)
> nobs <- 100; nvars <- 15
> xvec <- rnorm(nobs * nvars)
> x <- matrix(xvec, nrow = nobs)
> beta <- rnorm(nvars / 3)
> fx <- x[, seq(nvars / 3)] %*% beta / 3
> ty <- rexp(nobs, exp(fx))
> tcens <- rbinom(n = nobs, prob = 0.3, size = 1)
> y <- survival::Surv(ty, tcens)
> cvfit <- cv.glmnet(x, y, family = "cox")
> # default: s = "lambda.1se"
> survival::survfit(cvfit, x = x, y = y)
Call: survfit.cv.glmnet(formula = cvfit, x = x, y = y)

       n events median
[1,] 100     25   3.53
> 
> # s = "lambda.min"
> survival::survfit(cvfit, s = "lambda.min", x = x, y = y)
Call: survfit.cv.glmnet(formula = cvfit, s = "lambda.min", x = x, y = y)

       n events median
[1,] 100     25   3.53
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  10.778 1.57 10.712 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
