
R version 4.3.0 (2023-04-21) -- "Already Tomorrow"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> require(optimx)
Loading required package: optimx
> # Try testing calls to see what is transferred (eventually test also ...)
> # setup
> x0<-c(1,2,3,4)
> fnt <- function(x, fscale=10){
+   yy <- length(x):1
+   val <- sum((yy*x)^2)*fscale
+ }
> grt <- function(x, fscale=10){
+   nn <- length(x)
+   yy <- nn:1
+   #    gg <- rep(NA,nn)
+   gg <- 2*(yy^2)*x*fscale
+   gg
+ }
> 
> hesst <- function(x, fscale=10){
+   nn <- length(x)
+   yy <- nn:1
+   hh <- diag(2*yy^2*fscale)
+   hh
+ }
> 
> # library(snewton)
> t1 <- snewton(x0, fnt, grt, hesst, control=list(trace=2), fscale=3.0)
0   1   0   fbest= 312 
[1] 1 2 3 4
current gradient norm = 108 
Gradient projection =  -624 
f(xnew)= 0  at [1] 0 0 0 0
end major loop
1   2   1   fbest= 0 
[1] 0 0 0 0
current gradient norm = 0 
Small gradient norm 
> print(t1)
$par
[1] 0 0 0 0

$value
[1] 0

$grad
[1] 0 0 0 0

$Hess
     [,1] [,2] [,3] [,4]
[1,]   96    0    0    0
[2,]    0   54    0    0
[3,]    0    0   24    0
[4,]    0    0    0    6

$counts
$counts$niter
[1] 2

$counts$nfn
[1] 2

$counts$ngr
[1] 2

$counts$nhess
[1] 1


$convcode
[1] 0

$message
[1] "Small gradient norm"

> # we can also use nlm and nlminb
> fght <- function(x, fscale=10){
+   ## combine f, g and h into single function for nlm
+      ff <- fnt(x, fscale)
+      gg <- grt(x, fscale)
+      hh <- hesst(x, fscale)
+      attr(ff, "gradient") <- gg
+      attr(ff, "hessian") <- hh
+      ff
+ }
> 
> t1nlm <- nlm(fght, x0, fscale=3.0, hessian=TRUE, print.level=1)
iteration = 0
Step:
[1] 0 0 0 0
Parameter:
[1] 1 2 3 4
Function Value
[1] 1040
Gradient:
[1] 320 360 240  80

iteration = 1
Parameter:
[1] 0.000000e+00 2.220446e-16 4.440892e-16 0.000000e+00
Function Value
[1] 1.232595e-29
Gradient:
[1] 0.000000e+00 3.996803e-14 3.552714e-14 0.000000e+00

Relative gradient close to zero.
Current iterate is probably solution.

> print(t1nlm)
$minimum
[1] 1.232595e-29

$estimate
[1] 0.000000e+00 2.220446e-16 4.440892e-16 0.000000e+00

$gradient
[1] 0.000000e+00 3.996803e-14 3.552714e-14 0.000000e+00

$hessian
             [,1] [,2] [,3]         [,4]
[1,] 3.200000e+02    0    0 4.235165e-14
[2,] 0.000000e+00  180    0 0.000000e+00
[3,] 0.000000e+00    0   80 0.000000e+00
[4,] 4.235165e-14    0    0 2.000000e+01

$code
[1] 1

$iterations
[1] 1

> 
> ## BUT ... it looks like nlminb is NOT using a true Newton-type method
> t1nlminb <- nlminb(x0, fnt, gradient=grt, hessian=hesst, fscale=3.0, control=list(trace=1))
  0:     312.00000:  1.00000  2.00000  3.00000  4.00000
  1:     31.277896: 0.0946591 0.313486 0.884680  2.50350
  2: 7.4880156e-31: -1.38778e-17  0.00000 -1.11022e-16 -4.44089e-16
  3: 3.6918767e-62: 3.08149e-33  0.00000 2.46519e-32 9.86076e-32
  4: 1.8202358e-93: -6.84228e-49  0.00000 -5.47382e-48 -2.18953e-47
  5:8.9744552e-125: 1.51929e-64  0.00000 1.21543e-63 4.86173e-63
  6:4.4247480e-156: -3.37350e-80  0.00000 -2.69880e-79 -1.07952e-78
  7:2.1815692e-187: 7.49068e-96  0.00000 5.99255e-95 2.39702e-94
  8:1.0755967e-218: -1.66327e-111  0.00000 -1.33061e-110 -5.32245e-110
  9:5.3031010e-250: 3.69319e-127  0.00000 2.95455e-126 1.18182e-125
 10:2.6146307e-281: -8.20053e-143  0.00000 -6.56043e-142 -2.62417e-141
 11:1.2891124e-312: 1.82088e-158  0.00000 1.45671e-157 5.82683e-157
 12:     0.0000000: -4.04317e-174  0.00000 -3.23454e-173 -1.29382e-172
 13:     0.0000000: -4.04317e-174  0.00000 -3.23454e-173 -1.29382e-172
> print(t1nlminb)
$par
[1] -4.043175e-174   0.000000e+00 -3.234540e-173 -1.293816e-172

$objective
[1] 0

$convergence
[1] 0

$iterations
[1] 13

$evaluations
function gradient 
      15       13 

$message
[1] "relative convergence (4)"

> # and call them from optimx (i.e., test this gives same results)
> 
> t1nlmo <- optimr(x0, fnt, grt, hess=hesst, method="nlm", fscale=3.0, control=list(trace=1))
Parameter scaling:[1] 1 1 1 1
iteration = 0
Step:
[1] 0 0 0 0
Parameter:
[1] 1 2 3 4
Function Value
[1] 1040
Gradient:
[1] 320 360 240  80

iteration = 1
Parameter:
[1] 0.000000e+00 2.220446e-16 4.440892e-16 0.000000e+00
Function Value
[1] 1.232595e-29
Gradient:
[1] 0.000000e+00 3.996803e-14 3.552714e-14 0.000000e+00

Relative gradient close to zero.
Current iterate is probably solution.

> print(t1nlmo)
$convergence
[1] 0

$value
[1] 1.232595e-29

$par
[1] 0.000000e+00 2.220446e-16 4.440892e-16 0.000000e+00

$counts
[1] NA  1

$message
[1] "nlm: Convergence indicator (code) =  1"

> 
> ## FOLLOWING SHOWS UP ERRORS??
> t1nlminbo <- optimr(x0, fnt, grt, hess=hesst, method="nlminb", fscale=3.0, control=list(trace=1))
Parameter scaling:[1] 1 1 1 1
  0:     312.00000:  1.00000  2.00000  3.00000  4.00000
  1:     31.277896: 0.0946591 0.313486 0.884680  2.50350
  2: 7.4880156e-31: -1.38778e-17  0.00000 -1.11022e-16 -4.44089e-16
  3: 3.6918767e-62: 3.08149e-33  0.00000 2.46519e-32 9.86076e-32
  4: 1.8202358e-93: -6.84228e-49  0.00000 -5.47382e-48 -2.18953e-47
  5:8.9744552e-125: 1.51929e-64  0.00000 1.21543e-63 4.86173e-63
  6:4.4247480e-156: -3.37350e-80  0.00000 -2.69880e-79 -1.07952e-78
  7:2.1815692e-187: 7.49068e-96  0.00000 5.99255e-95 2.39702e-94
  8:1.0755967e-218: -1.66327e-111  0.00000 -1.33061e-110 -5.32245e-110
  9:5.3031010e-250: 3.69319e-127  0.00000 2.95455e-126 1.18182e-125
 10:2.6146307e-281: -8.20053e-143  0.00000 -6.56043e-142 -2.62417e-141
 11:1.2891124e-312: 1.82088e-158  0.00000 1.45671e-157 5.82683e-157
 12:     0.0000000: -4.04317e-174  0.00000 -3.23454e-173 -1.29382e-172
 13:     0.0000000: -4.04317e-174  0.00000 -3.23454e-173 -1.29382e-172
> print(t1nlminb)
$par
[1] -4.043175e-174   0.000000e+00 -3.234540e-173 -1.293816e-172

$objective
[1] 0

$convergence
[1] 0

$iterations
[1] 13

$evaluations
function gradient 
      15       13 

$message
[1] "relative convergence (4)"

> 
