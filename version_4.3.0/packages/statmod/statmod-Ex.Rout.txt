
R version 4.3.0 (2023-04-21) -- "Already Tomorrow"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "statmod"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('statmod')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("digammaf")
> ### * digammaf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Digamma
> ### Title: Digamma Generalized Linear Model Family
> ### Aliases: Digamma canonic.digamma d2cumulant.digamma
> ###   unitdeviance.digamma cumulant.digamma meanval.digamma varfun.digamma
> ### Keywords: models
> 
> ### ** Examples
> 
> # Test for log-linear dispersion trend in gamma regression
> y <- rchisq(20,df=1)
> x <- 1:20
> out.gam <- glm(y~x,family=Gamma(link="log"))
> d <- residuals(out.gam)^2
> out.dig <- glm(d~x,family=Digamma(link="log"))
> summary(out.dig,dispersion=2)

Call:
glm(formula = d ~ x, family = Digamma(link = "log"))

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.89184    0.63193   1.411    0.158
x           -0.02461    0.05302  -0.464    0.642

(Dispersion parameter for Digamma family taken to be 2)

    Null deviance: 36.144  on 19  degrees of freedom
Residual deviance: 35.766  on 18  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 8

> 
> 
> 
> cleanEx()
> nameEx("elda")
> ### * elda
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: elda
> ### Title: Extreme Limiting Dilution Analysis
> ### Aliases: elda limdil eldaOneGroup limdil.class limdil-class
> ### Keywords: regression
> 
> ### ** Examples
> 
> # When there is one group
> Dose <- c(50,100,200,400,800)
> Responses <- c(2,6,9,15,21)
> Tested <- c(24,24,24,24,24)
> out <- elda(Responses,Dose,Tested,test.unit.slope=TRUE)
> out
Confidence intervals for frequency:

           Lower Estimate    Upper
Group 1 537.7392 403.0632 302.1166

Goodness of fit (test log-Dose slope equals 1):
                      Estimate Std. Error z value Pr(>|z|)
Wald test               1.0464     0.1804 0.25704   0.7971
LR test                     NA         NA 0.25826   0.7962
Score test: log(Dose)       NA         NA 0.25554   0.7983
Score test: Dose            NA         NA 0.22552   0.8216
> plot(out)
> 
> # When there are four groups
> Dose <- c(30000,20000,4000,500,30000,20000,4000,500,30000,20000,4000,500,30000,20000,4000,500)
> Responses <- c(2,3,2,1,6,5,6,1,2,3,4,2,6,6,6,1)
> Tested <- c(6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6)
> Group <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)
> elda(Responses,Dose,Tested,Group,test.unit.slope=TRUE)
Confidence intervals for frequency:

           Lower  Estimate     Upper
Group 1 67165.44 31863.532 15116.177
Group 2 10251.72  4508.941  1983.135
Group 3 44524.68 22490.226 11360.221
Group 4  3371.15  1369.054   555.985

Differences between groups:
Chisq 41.13922 on 3 DF, p-value: 6.109e-09 

Goodness of fit (test log-Dose slope equals 1):
                      Estimate Std. Error z value  Pr(>|z|)    
Wald test               0.4198    0.11104 -5.2254 1.738e-07 ***
LR test                     NA         NA -4.3044 1.674e-05 ***
Score test: log(Dose)       NA         NA -5.7179 1.079e-08 ***
Score test: Dose            NA         NA -6.5719 4.968e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> 
> 
> cleanEx()
> nameEx("expectedDeviance")
> ### * expectedDeviance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expectedDeviance
> ### Title: Expected Value of Scaled Unit Deviance for Linear Exponential
> ###   Families
> ### Aliases: expectedDeviance
> ### Keywords: distributions
> 
> ### ** Examples
> 
> # Poisson example
> lambda <- 3
> nsim <- 1e4
> y <- rpois(nsim, lambda=lambda)
> d <- poisson()$dev.resids(y=y, mu=rep(lambda,nsim), wt=1)
> c(mean=mean(d), variance=var(d))
    mean variance 
1.111685 2.383397 
> unlist(expectedDeviance(mu=lambda, family="poisson"))
    mean variance 
1.094585 2.405685 
> 
> # binomial example
> n <- 10
> p <- 0.01
> y <- rbinom(nsim, prob=p, size=n)
> d <- binomial()$dev.resids(y=y/n, mu=rep(p,nsim), wt=n)
> c(mean=mean(d), variance=var(d))
     mean  variance 
0.4894045 0.8874054 
> unlist(expectedDeviance(mu=p, family="binomial", binom.size=n))
     mean  variance 
0.4831425 0.8992660 
> 
> # gamma example
> alpha <- 5
> beta <- 2
> y <- beta * rgamma(1e4, shape=alpha)
> d <- Gamma()$dev.resids(y=y, mu=rep(alpha*beta,n), wt=alpha)
> c(mean=mean(d), variance=var(d))
    mean variance 
1.039072 2.170940 
> unlist(expectedDeviance(mu=alpha*beta, family="Gamma", gamma.shape=alpha))
    mean variance 
1.033202 2.132296 
> 
> # negative binomial example
> library(MASS)
> mu <- 10
> phi <- 0.2
> y <- rnbinom(nsim, mu=mu, size=1/phi)
> f <- MASS::negative.binomial(theta=1/phi)
> d <- f$dev.resids(y=y, mu=rep(mu,nsim), wt=1)
> c(mean=mean(d), variance=var(d))
    mean variance 
1.067945 2.310279 
> unlist(expectedDeviance(mu=mu, family="negative.binomial", nbinom.size=1/phi))
    mean variance 
1.047080 2.241536 
> 
> # binomial expected deviance tends to zero for p small:
> p <- seq(from=0.001,to=0.11,len=200)
> ed <- expectedDeviance(mu=p,family="binomial",binom.size=10)
> plot(p,ed$mean,type="l")
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("fitNBP")
> ### * fitNBP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fitNBP
> ### Title: Negative Binomial Model for SAGE Libraries with Pearson
> ###   Estimation of Dispersion
> ### Aliases: fitNBP
> ### Keywords: regression
> 
> ### ** Examples
> 
> # True value for dispersion is 1/size=2/3
> # Note the Pearson method tends to under-estimate the dispersion
> y <- matrix(rnbinom(10*4,mu=4,size=1.5),10,4)
> lib.size <- rep(50000,4)
> group <- c(1,1,2,2)
> fit <- fitNBP(y,group=group,lib.size=lib.size)
> logratio <- fit$coef %*% c(-1,1)
> 
> 
> 
> cleanEx()
> nameEx("forward")
> ### * forward
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: forward
> ### Title: Forward Selection of Covariates for Multiple Regression
> ### Aliases: forward
> ### Keywords: regression
> 
> ### ** Examples
> 
> y <- rnorm(10)
> x <- matrix(rnorm(10*5),10,5)
> forward(y,x)
[1] 2 5 4 3 1
> 
> 
> 
> cleanEx()
> nameEx("gauss.quad")
> ### * gauss.quad
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gauss.quad
> ### Title: Gaussian Quadrature
> ### Aliases: gauss.quad
> ### Keywords: math
> 
> ### ** Examples
> 
> #  mean of gamma distribution with alpha=6
> out <- gauss.quad(10,"laguerre",alpha=5)
> sum(out$weights * out$nodes) / gamma(6)
[1] 6
> 
> 
> 
> cleanEx()
> nameEx("gauss.quad.prob")
> ### * gauss.quad.prob
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gauss.quad.prob
> ### Title: Gaussian Quadrature with Probability Distributions
> ### Aliases: gauss.quad.prob
> ### Keywords: math
> 
> ### ** Examples
> 
> #  the 4th moment of the standard normal is 3
> out <- gauss.quad.prob(10,"normal")
> sum(out$weights * out$nodes^4)
[1] 3
> 
> #  the expected value of log(X) where X is gamma is digamma(alpha)
> out <- gauss.quad.prob(32,"gamma",alpha=5)
> sum(out$weights * log(out$nodes))
[1] 1.506118
> 
> 
> 
> cleanEx()
> nameEx("glmgam")
> ### * glmgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmgam.fit
> ### Title: Fit Gamma Generalized Linear Model by Fisher Scoring with
> ###   Identity Link
> ### Aliases: glmgam.fit
> ### Keywords: regression
> 
> ### ** Examples
> 
> y <- rgamma(10, shape=5)
> X <- cbind(1, 1:10)
> fit <- glmgam.fit(X, y, trace=TRUE)
Iter = 0 , Dev = 1.561868  Beta 6.127364 -0.1990615 
Iter = 1 , Dev = 1.560572  Beta 6.129285 -0.2057179 
Iter = 2 , Dev = 1.559795  Beta 6.154427 -0.211027 
Iter = 3 , Dev = 1.557784  Beta 6.260201 -0.2254861 
Iter = 4 , Dev = 1.557186  Beta 6.340586 -0.2367725 
Iter = 5 , Dev = 1.557158  Beta 6.357378 -0.239427 
Iter = 6 , Dev = 1.557157  Beta 6.360288 -0.2399164 
> 
> 
> 
> cleanEx()
> nameEx("glmnbfit")
> ### * glmnbfit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmnb.fit
> ### Title: Fit Negative Binomial Generalized Linear Model with Log-Link
> ### Aliases: glmnb.fit
> ### Keywords: regression
> 
> ### ** Examples
> 
> y <- rnbinom(10, mu=1:10, size=5)
> X <- cbind(1, 1:10)
> fit <- glmnb.fit(X, y, dispersion=0.2, trace=TRUE)
Iter = 0 , Dev = 7.596909  Beta 1.504077 0 
Iter = 1 , Dev = 1.900036  Beta 0.5783221 0.1683256 
Convergence criterion 5.537027 4.440892e-15 32.89474 -0.9257553 0.1683256 
Iter = 2 , Dev = 1.511672  Beta 0.3538509 0.1862972 
Convergence criterion 0.362772 -2.683556 -13.33265 -0.2244713 0.01797161 
Iter = 3 , Dev = 1.508798  Beta 0.3273023 0.1893187 
Convergence criterion 0.002797809 -0.1665127 -0.537117 -0.02654861 0.003021447 
Iter = 4 , Dev = 1.508792  Beta 0.3262494 0.1894953 
Convergence criterion 5.372164e-06 0.00146112 0.03911717 -0.00105288 0.0001766628 
Iter = 5 , Dev = 1.508792  Beta 0.3261838 0.1895069 
Convergence criterion 2.379783e-08 0.0001673495 0.003011537 -6.559129e-05 1.15471e-05 
> 
> 
> 
> cleanEx()
> nameEx("glmscoretest")
> ### * glmscoretest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glm.scoretest
> ### Title: Score Test for Adding a Covariate to a GLM
> ### Aliases: glm.scoretest
> ### Keywords: regression
> 
> ### ** Examples
> 
> #  Pearson's chisquare test for independence
> #  in a contingency table is a score test.
> 
> #  First the usual test
> 
> y <- c(20,40,40,30)
> chisq.test(matrix(y,2,2), correct=FALSE)

	Pearson's Chi-squared test

data:  matrix(y, 2, 2)
X-squared = 7.3696, df = 1, p-value = 0.006634

> 
> #  Now same test using glm.scoretest
> 
> a <- gl(2,1,4)
> b <- gl(2,2,4)
> fit <- glm(y~a+b, family=poisson)
> x2 <- c(0,0,0,1)
> z <- glm.scoretest(fit, x2)
> z^2
[1] 7.369616
> 
> 
> 
> cleanEx()
> nameEx("growthcurve")
> ### * growthcurve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: growthcurve
> ### Title: Compare Groups of Growth Curves
> ### Aliases: compareGrowthCurves compareTwoGrowthCurves plotGrowthCurves
> ### Keywords: regression
> 
> ### ** Examples
> 
> # A example with only one time
> data(PlantGrowth)
> compareGrowthCurves(PlantGrowth$group,as.matrix(PlantGrowth$weight))
ctrl trt1  1.19 
ctrl trt2  -2.13 
trt1 trt2  -3.01 
  Group1 Group2      Stat     P.Value adj.P.Value
1   ctrl   trt1  1.191260 0.223880597  0.22388060
2   ctrl   trt2 -2.134020 0.054726368  0.10945274
3   trt1   trt2 -3.010099 0.004975124  0.01492537
> # Can make p-values more accurate by nsim=10000
> 
> 
> 
> cleanEx()
> nameEx("hommel.test")
> ### * hommel.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hommel.test
> ### Title: Test Multiple Comparisons Using Hommel's Method
> ### Aliases: hommel.test
> ### Keywords: htest
> 
> ### ** Examples
> 
> p <- sort(runif(100))[1:10]
> cbind(p,p.adjust(p,"hommel"),hommel.test(p))
               p            
 [1,] 0.01339033 0.1205130 1
 [2,] 0.02333120 0.1255551 1
 [3,] 0.05893438 0.1255551 1
 [4,] 0.06178627 0.1255551 1
 [5,] 0.07067905 0.1255551 1
 [6,] 0.08424691 0.1255551 1
 [7,] 0.09946616 0.1255551 1
 [8,] 0.10794363 0.1255551 1
 [9,] 0.12169192 0.1255551 1
[10,] 0.12555510 0.1255551 1
> 
> 
> 
> cleanEx()
> nameEx("invgauss")
> ### * invgauss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: invgauss
> ### Title: Inverse Gaussian Distribution
> ### Aliases: InverseGaussian dinvgauss pinvgauss qinvgauss rinvgauss
> ### Keywords: distribution
> 
> ### ** Examples
> 
> q <- rinvgauss(10, mean=1, disp=0.5) # generate vector of 10 random numbers
> p <- pinvgauss(q, mean=1, disp=0.5) # p should be uniformly distributed
> 
> # Quantile for small right tail probability:
> qinvgauss(1e-20, mean=1.5, disp=0.7, lower.tail=FALSE)
[1] 126.3493
> 
> # Same quantile, but represented in terms of left tail probability on log-scale
> qinvgauss(-1e-20, mean=1.5, disp=0.7, lower.tail=TRUE, log.p=TRUE)
[1] 126.3493
> 
> 
> 
> cleanEx()
> nameEx("logmdigamma")
> ### * logmdigamma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logmdigamma
> ### Title: Log Minus Digamma Function
> ### Aliases: logmdigamma
> ### Keywords: math
> 
> ### ** Examples
> 
> log(10^15) - digamma(10^15) # returns 0
[1] 0
> logmdigamma(10^15) # returns value correct to 15 figures
[1] 5e-16
> 
> 
> 
> cleanEx()
> nameEx("matvec")
> ### * matvec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: matvec
> ### Title: Multiply a Matrix by a Vector
> ### Aliases: matvec vecmat
> ### Keywords: array algebra
> 
> ### ** Examples
> 
> A <- matrix(1:12,3,4)
> A
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12
> matvec(A,c(1,2,3,4))
     [,1] [,2] [,3] [,4]
[1,]    1    8   21   40
[2,]    2   10   24   44
[3,]    3   12   27   48
> vecmat(c(1,2,3),A)
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    4   10   16   22
[3,]    9   18   27   36
> 
> 
> 
> cleanEx()
> nameEx("meanT")
> ### * meanT
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: meanT
> ### Title: Mean t-Statistic Between Two Groups of Growth Curves
> ### Aliases: meanT
> ### Keywords: regression
> 
> ### ** Examples
> 
> y1 <- matrix(rnorm(4*3),4,3)
> y2 <- matrix(rnorm(4*3),4,3)
> meanT(y1,y2)
[1] 0.1232123
> 
> data(PlantGrowth)
> compareGrowthCurves(PlantGrowth$group,as.matrix(PlantGrowth$weight))
ctrl trt1  1.19 
ctrl trt2  -2.13 
trt1 trt2  -3.01 
  Group1 Group2      Stat     P.Value adj.P.Value
1   ctrl   trt1  1.191260 0.233830846  0.23383085
2   ctrl   trt2 -2.134020 0.054726368  0.10945274
3   trt1   trt2 -3.010099 0.004975124  0.01492537
> # Can make p-values more accurate by nsim=10000
> 
> 
> 
> cleanEx()
> nameEx("mixedmodel")
> ### * mixedmodel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mixedModel2
> ### Title: Fit Mixed Linear Model with 2 Error Components
> ### Aliases: mixedModel2 mixedModel2Fit randomizedBlock randomizedBlockFit
> ### Keywords: regression
> 
> ### ** Examples
> 
> #  Compare with first data example from Venable and Ripley (2002),
> #  Chapter 10, "Linear Models"
> library(MASS)
> data(petrol)
> out <- mixedModel2(Y~SG+VP+V10+EP, random=No, data=petrol)
> cbind(varcomp=out$varcomp,se=out$se.varcomp)
          varcomp       se
Residual 3.505151 1.081110
Block    2.087317 1.908131
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("mscale")
> ### * mscale
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mscale
> ### Title: M Scale Estimation
> ### Aliases: mscale
> 
> ### ** Examples
> 
> u <- rnorm(100)
> sd(u)
[1] 0.8981994
> mscale(u)
[1] 0.8932594
> 
> 
> 
> cleanEx()
> nameEx("permp")
> ### * permp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: permp
> ### Title: Exact permutation p-values
> ### Aliases: permp
> ### Keywords: htest
> 
> ### ** Examples
> 
> x <- 0:5
> # Both calls give same results
> permp(x=x, nperm=99, n1=6, n2=6)
[1] 0.008956372 0.018917806 0.028917720 0.038917749 0.048917749 0.058917749
> permp(x=x, nperm=99, total.nperm=462)
[1] 0.008956372 0.018917806 0.028917720 0.038917749 0.048917749 0.058917749
> 
> 
> 
> cleanEx()
> nameEx("power")
> ### * power
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: power.fisher.test
> ### Title: Power of Fisher's Exact Test for Comparing Proportions
> ### Aliases: power.fisher.test
> ### Keywords: htest
> 
> ### ** Examples
> 
> power.fisher.test(0.5,0.9,20,20) # 70% chance of detecting difference
[1] 0.75
> 
> 
> 
> cleanEx()
> nameEx("qresiduals")
> ### * qresiduals
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qresiduals
> ### Title: Randomized Quantile Residuals
> ### Aliases: qresiduals qresid qres.binom qres.pois qres.nbinom qres.gamma
> ###   qres.invgauss qres.tweedie qres.default
> ### Keywords: regression
> 
> ### ** Examples
> 
> #  Poisson example: quantile residuals show no granularity
> y <- rpois(20,lambda=4)
> x <- 1:20
> fit <- glm(y~x, family=poisson)
> qr <- qresiduals(fit)
> qqnorm(qr)
> abline(0,1)
> 
> #  Gamma example:
> #  Quantile residuals are nearly normal while usual resids are not
> y <- rchisq(20, df=1)
> fit <- glm(y~1, family=Gamma)
> qr <- qresiduals(fit, dispersion=2)
> qqnorm(qr)
> abline(0,1)
> 
> #  Negative binomial example:
> if(require("MASS")) {
+ fit <- glm(Days~Age,family=negative.binomial(2),data=quine)
+ summary(qresiduals(fit))
+ fit <- glm.nb(Days~Age,link=log,data = quine)
+ summary(qresiduals(fit))
+ }
Loading required package: MASS
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-2.4068588 -0.5477100 -0.0307399  0.0003605  0.6765276  2.4679924 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("remlscor")
> ### * remlscor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: remlscore
> ### Title: REML for Heteroscedastic Regression
> ### Aliases: remlscore
> ### Keywords: regression
> 
> ### ** Examples
> 
> data(welding)
> attach(welding)
> y <- Strength
> # Reproduce results from Table 1 of Smyth (2002)
> X <- cbind(1,(Drying+1)/2,(Material+1)/2)
> colnames(X) <- c("1","B","C")
> Z <- cbind(1,(Material+1)/2,(Method+1)/2,(Preheating+1)/2)
> colnames(Z) <- c("1","C","H","I")
> out <- remlscore(y,X,Z)
> cbind(Estimate=out$gamma,SE=out$se.gam)
                        SE
[1,] -3.15886017 0.8313270
[2,] -2.73542576 0.8224828
[3,] -0.08588713 0.8351308
[4,]  3.33238821 0.8250499
> 
> 
> 
> cleanEx()

detaching ‘welding’

> nameEx("remlscorgamma")
> ### * remlscorgamma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: remlscoregamma
> ### Title: Approximate REML for Gamma Regression with Structured Dispersion
> ### Aliases: remlscoregamma
> ### Keywords: regression
> 
> ### ** Examples
> 
> data(welding)
> attach(welding)
> y <- Strength
> X <- cbind(1,(Drying+1)/2,(Material+1)/2)
> colnames(X) <- c("1","B","C")
> Z <- cbind(1,(Material+1)/2,(Method+1)/2,(Preheating+1)/2)
> colnames(Z) <- c("1","C","H","I")
> out <- remlscoregamma(y,X,Z)
> 
> 
> 
> cleanEx()

detaching ‘welding’

> nameEx("sage.test")
> ### * sage.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sage.test
> ### Title: Exact Binomial Tests For Comparing Two SAGE Libraries (Obsolete)
> ### Aliases: sage.test
> ### Keywords: htest
> 
> ### ** Examples
> 
> sage.test(c(0,5,10),c(0,30,50),n1=10000,n2=15000)
[1] 1.000000000 0.001542818 0.000168918
> #  Univariate equivalents:
> binom.test(5,5+30,p=10000/(10000+15000))$p.value
[1] 0.001542818
> binom.test(10,10+50,p=10000/(10000+15000))$p.value
[1] 0.000168918
> 
> 
> 
> cleanEx()
> nameEx("tweedie")
> ### * tweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tweedie
> ### Title: Tweedie Generalized Linear Models
> ### Aliases: tweedie
> ### Keywords: regression
> 
> ### ** Examples
> 
> y <- rgamma(20,shape=5)
> x <- 1:20
> # Fit a poisson generalized linear model with identity link
> glm(y~x,family=tweedie(var.power=1,link.power=1))

Call:  glm(formula = y ~ x, family = tweedie(var.power = 1, link.power = 1))

Coefficients:
(Intercept)            x  
    5.35647     -0.05365  

Degrees of Freedom: 19 Total (i.e. Null);  18 Residual
Null Deviance:	    14.88 
Residual Deviance: 14.47 	AIC: NA
> 
> # Fit an inverse-Gaussion glm with log-link
> glm(y~x,family=tweedie(var.power=3,link.power=0)) 

Call:  glm(formula = y ~ x, family = tweedie(var.power = 3, link.power = 0))

Coefficients:
(Intercept)            x  
      1.681       -0.011  

Degrees of Freedom: 19 Total (i.e. Null);  18 Residual
Null Deviance:	    1.017 
Residual Deviance: 0.9993 	AIC: NA
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  0.294 0.019 0.312 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
