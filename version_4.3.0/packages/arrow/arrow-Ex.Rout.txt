
R version 4.3.0 (2023-04-21) -- "Already Tomorrow"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "arrow"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('arrow')

Attaching package: ‘arrow’

The following object is masked from ‘package:utils’:

    timestamp

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("ChunkedArray")
> ### * ChunkedArray
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ChunkedArray
> ### Title: ChunkedArray class
> ### Aliases: ChunkedArray chunked_array
> 
> ### ** Examples
> 
> # Pass items into chunked_array as separate objects to create chunks
> class_scores <- chunked_array(c(87, 88, 89), c(94, 93, 92), c(71, 72, 73))
> class_scores$num_chunks
[1] 3
> 
> # When taking a Slice from a chunked_array, chunks are preserved
> class_scores$Slice(2, length = 5)
ChunkedArray
<double>
[
  [
    89
  ],
  [
    94,
    93,
    92
  ],
  [
    71
  ]
]
> 
> # You can combine Take and SortIndices to return a ChunkedArray with 1 chunk
> # containing all values, ordered.
> class_scores$Take(class_scores$SortIndices(descending = TRUE))
ChunkedArray
<double>
[
  [
    94,
    93,
    92,
    89,
    88,
    87,
    73,
    72,
    71
  ]
]
> 
> # If you pass a list into chunked_array, you get a list of length 1
> list_scores <- chunked_array(list(c(9.9, 9.6, 9.5), c(8.2, 8.3, 8.4), c(10.0, 9.9, 9.8)))
> list_scores$num_chunks
[1] 1
> 
> # When constructing a ChunkedArray, the first chunk is used to infer type.
> doubles <- chunked_array(c(1, 2, 3), c(5L, 6L, 7L))
> doubles$type
Float64
double
> 
> # Concatenating chunked arrays returns a new chunked array containing all chunks
> a <- chunked_array(c(1, 2), 3)
> b <- chunked_array(c(4, 5), 6)
> c(a, b)
ChunkedArray
<double>
[
  [
    1,
    2
  ],
  [
    3
  ],
  [
    4,
    5
  ],
  [
    6
  ]
]
> 
> 
> 
> cleanEx()
> nameEx("CsvFileFormat")
> ### * CsvFileFormat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: CsvFileFormat
> ### Title: CSV dataset file format
> ### Aliases: CsvFileFormat
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # Set up directory for examples
+ tf <- tempfile()
+ dir.create(tf)
+ on.exit(unlink(tf))
+ df <- data.frame(x = c("1", "2", "NULL"))
+ write.table(df, file.path(tf, "file1.txt"), sep = ",", row.names = FALSE)
+ 
+ # Create CsvFileFormat object with Arrow-style null_values option
+ format <- CsvFileFormat$create(convert_options = list(null_values = c("", "NA", "NULL")))
+ open_dataset(tf, format = format)
+ 
+ # Use readr-style options
+ format <- CsvFileFormat$create(na = c("", "NA", "NULL"))
+ open_dataset(tf, format = format)
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> dir.create(tf)
> on.exit(unlink(tf))
> df <- data.frame(x = c("1", "2", "NULL"))
> write.table(df, file.path(tf, "file1.txt"), sep = ",", row.names = FALSE)
> format <- CsvFileFormat$create(convert_options = list(null_values = c("", 
+     "NA", "NULL")))
> open_dataset(tf, format = format)
FileSystemDataset with 1 csv file
x: int64
> format <- CsvFileFormat$create(na = c("", "NA", "NULL"))
> open_dataset(tf, format = format)
FileSystemDataset with 1 csv file
x: int64
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("Field")
> ### * Field
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Field
> ### Title: Field class
> ### Aliases: Field field
> 
> ### ** Examples
> 
> field("x", int32())
Field
x: int32
> 
> 
> 
> cleanEx()
> nameEx("FileFormat")
> ### * FileFormat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FileFormat
> ### Title: Dataset file formats
> ### Aliases: FileFormat ParquetFileFormat IpcFileFormat
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ ## Semi-colon delimited files
+ # Set up directory for examples
+ tf <- tempfile()
+ dir.create(tf)
+ on.exit(unlink(tf))
+ write.table(mtcars, file.path(tf, "file1.txt"), sep = ";", row.names = FALSE)
+ 
+ # Create FileFormat object
+ format <- FileFormat$create(format = "text", delimiter = ";")
+ 
+ open_dataset(tf, format = format)
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> dir.create(tf)
> on.exit(unlink(tf))
> write.table(mtcars, file.path(tf, "file1.txt"), sep = ";", row.names = FALSE)
> format <- FileFormat$create(format = "text", delimiter = ";")
> open_dataset(tf, format = format)
FileSystemDataset with 1 csv file
mpg: double
cyl: int64
disp: double
hp: int64
drat: double
wt: double
qsec: double
vs: int64
am: int64
gear: int64
carb: int64
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("ParquetFileReader")
> ### * ParquetFileReader
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ParquetFileReader
> ### Title: ParquetFileReader class
> ### Aliases: ParquetFileReader
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_parquet()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ f <- system.file("v0.7.1.parquet", package = "arrow")
+ pq <- ParquetFileReader$create(f)
+ pq$GetSchema()
+ if (codec_is_available("snappy")) {
+   # This file has compressed data columns
+   tab <- pq$ReadTable()
+   tab$schema
+ }
+ ## Don't show: 
+ }) # examplesIf
> f <- system.file("v0.7.1.parquet", package = "arrow")
> pq <- ParquetFileReader$create(f)
> pq$GetSchema()
Schema
carat: double
cut: string
color: string
clarity: string
depth: double
table: double
price: int64
x: double
y: double
z: double
__index_level_0__: int64

See $metadata for additional Schema metadata
> if (codec_is_available("snappy")) {
+     tab <- pq$ReadTable()
+     tab$schema
+ }
Schema
carat: double
cut: string
color: string
clarity: string
depth: double
table: double
price: int64
x: double
y: double
z: double
__index_level_0__: int64

See $metadata for additional Schema metadata
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("RecordBatch")
> ### * RecordBatch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RecordBatch
> ### Title: RecordBatch class
> ### Aliases: RecordBatch record_batch
> 
> ### ** Examples
> 
> batch <- record_batch(name = rownames(mtcars), mtcars)
> dim(batch)
[1] 32 12
> dim(head(batch))
[1]  6 12
> names(batch)
 [1] "name" "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"  
[11] "gear" "carb"
> batch$mpg
Array
<double>
[
  21,
  21,
  22.8,
  21.4,
  18.7,
  18.1,
  14.3,
  24.4,
  22.8,
  19.2,
  ...
  15.2,
  13.3,
  19.2,
  27.3,
  26,
  30.4,
  15.8,
  19.7,
  15,
  21.4
]
> batch[["cyl"]]
Array
<double>
[
  6,
  6,
  4,
  6,
  8,
  6,
  8,
  4,
  4,
  6,
  ...
  8,
  8,
  8,
  4,
  4,
  4,
  8,
  6,
  8,
  4
]
> as.data.frame(batch[4:8, c("gear", "hp", "wt")])
  gear  hp    wt
1    3 110 3.215
2    3 175 3.440
3    3 105 3.460
4    3 245 3.570
5    4  62 3.190
> 
> 
> 
> cleanEx()
> nameEx("RecordBatchReader")
> ### * RecordBatchReader
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RecordBatchReader
> ### Title: RecordBatchReader classes
> ### Aliases: RecordBatchReader RecordBatchStreamReader
> ###   RecordBatchFileReader
> 
> ### ** Examples
> 
> tf <- tempfile()
> on.exit(unlink(tf))
> 
> batch <- record_batch(chickwts)
> 
> # This opens a connection to the file in Arrow
> file_obj <- FileOutputStream$create(tf)
> # Pass that to a RecordBatchWriter to write data conforming to a schema
> writer <- RecordBatchFileWriter$create(file_obj, batch$schema)
> writer$write(batch)
> # You may write additional batches to the stream, provided that they have
> # the same schema.
> # Call "close" on the writer to indicate end-of-file/stream
> writer$close()
> # Then, close the connection--closing the IPC message does not close the file
> file_obj$close()
> 
> # Now, we have a file we can read from. Same pattern: open file connection,
> # then pass it to a RecordBatchReader
> read_file_obj <- ReadableFile$create(tf)
> reader <- RecordBatchFileReader$create(read_file_obj)
> # RecordBatchFileReader knows how many batches it has (StreamReader does not)
> reader$num_record_batches
[1] 1
> # We could consume the Reader by calling $read_next_batch() until all are,
> # consumed, or we can call $read_table() to pull them all into a Table
> tab <- reader$read_table()
> # Call as.data.frame to turn that Table into an R data.frame
> df <- as.data.frame(tab)
> # This should be the same data we sent
> all.equal(df, chickwts, check.attributes = FALSE)
[1] TRUE
> # Unlike the Writers, we don't have to close RecordBatchReaders,
> # but we do still need to close the file connection
> read_file_obj$close()
> 
> 
> 
> cleanEx()
> nameEx("RecordBatchWriter")
> ### * RecordBatchWriter
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RecordBatchWriter
> ### Title: RecordBatchWriter classes
> ### Aliases: RecordBatchWriter RecordBatchStreamWriter
> ###   RecordBatchFileWriter
> 
> ### ** Examples
> 
> tf <- tempfile()
> on.exit(unlink(tf))
> 
> batch <- record_batch(chickwts)
> 
> # This opens a connection to the file in Arrow
> file_obj <- FileOutputStream$create(tf)
> # Pass that to a RecordBatchWriter to write data conforming to a schema
> writer <- RecordBatchFileWriter$create(file_obj, batch$schema)
> writer$write(batch)
> # You may write additional batches to the stream, provided that they have
> # the same schema.
> # Call "close" on the writer to indicate end-of-file/stream
> writer$close()
> # Then, close the connection--closing the IPC message does not close the file
> file_obj$close()
> 
> # Now, we have a file we can read from. Same pattern: open file connection,
> # then pass it to a RecordBatchReader
> read_file_obj <- ReadableFile$create(tf)
> reader <- RecordBatchFileReader$create(read_file_obj)
> # RecordBatchFileReader knows how many batches it has (StreamReader does not)
> reader$num_record_batches
[1] 1
> # We could consume the Reader by calling $read_next_batch() until all are,
> # consumed, or we can call $read_table() to pull them all into a Table
> tab <- reader$read_table()
> # Call as.data.frame to turn that Table into an R data.frame
> df <- as.data.frame(tab)
> # This should be the same data we sent
> all.equal(df, chickwts, check.attributes = FALSE)
[1] TRUE
> # Unlike the Writers, we don't have to close RecordBatchReaders,
> # but we do still need to close the file connection
> read_file_obj$close()
> 
> 
> 
> cleanEx()
> nameEx("Scalar")
> ### * Scalar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Scalar
> ### Title: Arrow scalars
> ### Aliases: Scalar
> 
> ### ** Examples
> 
> Scalar$create(pi)
Scalar
3.141592653589793
> Scalar$create(404)
Scalar
404
> # If you pass a vector into Scalar$create, you get a list containing your items
> Scalar$create(c(1, 2, 3))
Scalar
list<item: double>[1, 2, 3]
> 
> # Comparisons
> my_scalar <- Scalar$create(99)
> my_scalar$ApproxEquals(Scalar$create(99.00001)) # FALSE
[1] FALSE
> my_scalar$ApproxEquals(Scalar$create(99.000009)) # TRUE
[1] TRUE
> my_scalar$Equals(Scalar$create(99.000009)) # FALSE
[1] FALSE
> my_scalar$Equals(Scalar$create(99L)) # FALSE (types don't match)
[1] FALSE
> 
> my_scalar$ToString()
[1] "99"
> 
> 
> 
> cleanEx()
> nameEx("Scanner")
> ### * Scanner
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Scanner
> ### Title: Scan the contents of a dataset
> ### Aliases: Scanner ScannerBuilder
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset() & arrow_with_parquet()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # Set up directory for examples
+ tf <- tempfile()
+ dir.create(tf)
+ on.exit(unlink(tf))
+ 
+ write_dataset(mtcars, tf, partitioning="cyl")
+ 
+ ds <- open_dataset(tf)
+ 
+ scan_builder <- ds$NewScan()
+ scan_builder$Filter(Expression$field_ref("hp") > 100)
+ scan_builder$Project(list(hp_times_ten = 10 * Expression$field_ref("hp")))
+ 
+ # Once configured, call $Finish()
+ scanner <- scan_builder$Finish()
+ 
+ # Can get results as a table
+ as.data.frame(scanner$ToTable())
+ 
+ # Or as a RecordBatchReader
+ scanner$ToRecordBatchReader()
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> dir.create(tf)
> on.exit(unlink(tf))
> write_dataset(mtcars, tf, partitioning = "cyl")
> ds <- open_dataset(tf)
> scan_builder <- ds$NewScan()
> scan_builder$Filter(Expression$field_ref("hp") > 100)
ScannerBuilder
> scan_builder$Project(list(hp_times_ten = 10 * Expression$field_ref("hp")))
ScannerBuilder
> scanner <- scan_builder$Finish()
> as.data.frame(scanner$ToTable())
   hp_times_ten
1          1130
2          1090
3          1100
4          1100
5          1100
6          1050
7          1230
8          1230
9          1750
10         1750
11         2450
12         1800
13         1800
14         1800
15         2050
16         2150
17         2300
18         1500
19         1500
20         2450
21         1750
22         2640
23         3350
> scanner$ToRecordBatchReader()
RecordBatchReader
hp_times_ten: double

See $metadata for additional Schema metadata
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("Schema")
> ### * Schema
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Schema
> ### Title: Schema class
> ### Aliases: Schema schema
> 
> ### ** Examples
> 
> schema(a = int32(), b = float64())
Schema
a: int32
b: double
> 
> schema(
+   field("b", double()),
+   field("c", bool(), nullable = FALSE),
+   field("d", string())
+ )
Schema
b: double
c: bool not null
d: string
> 
> df <- data.frame(col1 = 2:4, col2 = c(0.1, 0.3, 0.5))
> tab1 <- arrow_table(df)
> tab1$schema
Schema
col1: int32
col2: double

See $metadata for additional Schema metadata
> tab2 <- arrow_table(df, schema = schema(col1 = int8(), col2 = float32()))
> tab2$schema
Schema
col1: int8
col2: float

See $metadata for additional Schema metadata
> 
> 
> 
> cleanEx()
> nameEx("Table")
> ### * Table
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Table
> ### Title: Table class
> ### Aliases: Table arrow_table
> 
> ### ** Examples
> 
> tbl <- arrow_table(name = rownames(mtcars), mtcars)
> dim(tbl)
[1] 32 12
> dim(head(tbl))
[1]  6 12
> names(tbl)
 [1] "name" "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"  
[11] "gear" "carb"
> tbl$mpg
ChunkedArray
<double>
[
  [
    21,
    21,
    22.8,
    21.4,
    18.7,
    18.1,
    14.3,
    24.4,
    22.8,
    19.2,
    ...
    15.2,
    13.3,
    19.2,
    27.3,
    26,
    30.4,
    15.8,
    19.7,
    15,
    21.4
  ]
]
> tbl[["cyl"]]
ChunkedArray
<double>
[
  [
    6,
    6,
    4,
    6,
    8,
    6,
    8,
    4,
    4,
    6,
    ...
    8,
    8,
    8,
    4,
    4,
    4,
    8,
    6,
    8,
    4
  ]
]
> as.data.frame(tbl[4:8, c("gear", "hp", "wt")])
# A tibble: 5 × 3
   gear    hp    wt
  <dbl> <dbl> <dbl>
1     3   110  3.22
2     3   175  3.44
3     3   105  3.46
4     3   245  3.57
5     4    62  3.19
> 
> 
> 
> cleanEx()
> nameEx("add_filename")
> ### * add_filename
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: add_filename
> ### Title: Add the data filename as a column
> ### Aliases: add_filename
> ### Keywords: internal
> 
> ### ** Examples
> 
> ## Not run: 
> ##D open_dataset("nyc-taxi") %>%
> ##D   mutate(file = add_filename())
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("array")
> ### * array
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: array
> ### Title: Arrow Arrays
> ### Aliases: array Array DictionaryArray StructArray ListArray
> ###   LargeListArray FixedSizeListArray MapArray StructScalar
> 
> ### ** Examples
> 
> my_array <- Array$create(1:10)
> my_array$type
Int32
int32
> my_array$cast(int8())
Array
<int8>
[
  1,
  2,
  3,
  4,
  5,
  6,
  7,
  8,
  9,
  10
]
> 
> # Check if value is null; zero-indexed
> na_array <- Array$create(c(1:5, NA))
> na_array$IsNull(0)
[1] FALSE
> na_array$IsNull(5)
[1] TRUE
> na_array$IsValid(5)
[1] FALSE
> na_array$null_count
[1] 1
> 
> # zero-copy slicing; the offset of the new Array will be the same as the index passed to $Slice
> new_array <- na_array$Slice(5)
> new_array$offset
[1] 5
> 
> # Compare 2 arrays
> na_array2 <- na_array
> na_array2 == na_array # element-wise comparison
Array
<bool>
[
  true,
  true,
  true,
  true,
  true,
  null
]
> na_array2$Equals(na_array) # overall comparison
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("as_arrow_array")
> ### * as_arrow_array
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_arrow_array
> ### Title: Convert an object to an Arrow Array
> ### Aliases: as_arrow_array as_arrow_array.Array as_arrow_array.Scalar
> ###   as_arrow_array.ChunkedArray
> 
> ### ** Examples
> 
> as_arrow_array(1:5)
Array
<int32>
[
  1,
  2,
  3,
  4,
  5
]
> 
> 
> 
> 
> cleanEx()
> nameEx("as_arrow_table")
> ### * as_arrow_table
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_arrow_table
> ### Title: Convert an object to an Arrow Table
> ### Aliases: as_arrow_table as_arrow_table.default as_arrow_table.Table
> ###   as_arrow_table.RecordBatch as_arrow_table.data.frame
> ###   as_arrow_table.RecordBatchReader as_arrow_table.Dataset
> ###   as_arrow_table.arrow_dplyr_query as_arrow_table.Schema
> 
> ### ** Examples
> 
> # use as_arrow_table() for a single object
> as_arrow_table(data.frame(col1 = 1, col2 = "two"))
Table
1 rows x 2 columns
$col1 <double>
$col2 <string>

See $metadata for additional Schema metadata
> 
> # use arrow_table() to create from columns
> arrow_table(col1 = 1, col2 = "two")
Table
1 rows x 2 columns
$col1 <double>
$col2 <string>
> 
> 
> 
> 
> cleanEx()
> nameEx("as_chunked_array")
> ### * as_chunked_array
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_chunked_array
> ### Title: Convert an object to an Arrow ChunkedArray
> ### Aliases: as_chunked_array as_chunked_array.ChunkedArray
> ###   as_chunked_array.Array
> 
> ### ** Examples
> 
> as_chunked_array(1:5)
ChunkedArray
<int32>
[
  [
    1,
    2,
    3,
    4,
    5
  ]
]
> 
> 
> 
> 
> cleanEx()
> nameEx("as_data_type")
> ### * as_data_type
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_data_type
> ### Title: Convert an object to an Arrow DataType
> ### Aliases: as_data_type as_data_type.DataType as_data_type.Field
> ###   as_data_type.Schema
> 
> ### ** Examples
> 
> as_data_type(int32())
Int32
int32
> 
> 
> 
> 
> cleanEx()
> nameEx("as_record_batch")
> ### * as_record_batch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_record_batch
> ### Title: Convert an object to an Arrow RecordBatch
> ### Aliases: as_record_batch as_record_batch.RecordBatch
> ###   as_record_batch.Table as_record_batch.arrow_dplyr_query
> ###   as_record_batch.data.frame
> 
> ### ** Examples
> 
> # use as_record_batch() for a single object
> as_record_batch(data.frame(col1 = 1, col2 = "two"))
RecordBatch
1 rows x 2 columns
$col1 <double>
$col2 <string>

See $metadata for additional Schema metadata
> 
> # use record_batch() to create from columns
> record_batch(col1 = 1, col2 = "two")
RecordBatch
1 rows x 2 columns
$col1 <double>
$col2 <string>
> 
> 
> 
> 
> cleanEx()
> nameEx("as_record_batch_reader")
> ### * as_record_batch_reader
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_record_batch_reader
> ### Title: Convert an object to an Arrow RecordBatchReader
> ### Aliases: as_record_batch_reader
> ###   as_record_batch_reader.RecordBatchReader as_record_batch_reader.Table
> ###   as_record_batch_reader.RecordBatch as_record_batch_reader.data.frame
> ###   as_record_batch_reader.Dataset as_record_batch_reader.function
> ###   as_record_batch_reader.arrow_dplyr_query
> ###   as_record_batch_reader.Scanner
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ reader <- as_record_batch_reader(data.frame(col1 = 1, col2 = "two"))
+ reader$read_next_batch()
+ ## Don't show: 
+ }) # examplesIf
> reader <- as_record_batch_reader(data.frame(col1 = 1, col2 = "two"))
> reader$read_next_batch()
RecordBatch
1 rows x 2 columns
$col1 <double>
$col2 <string>

See $metadata for additional Schema metadata
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("as_schema")
> ### * as_schema
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: as_schema
> ### Title: Convert an object to an Arrow DataType
> ### Aliases: as_schema as_schema.Schema as_schema.StructType
> 
> ### ** Examples
> 
> as_schema(schema(col1 = int32()))
Schema
col1: int32
> 
> 
> 
> 
> cleanEx()
> nameEx("buffer")
> ### * buffer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: buffer
> ### Title: Buffer class
> ### Aliases: buffer Buffer
> 
> ### ** Examples
> 
> my_buffer <- buffer(c(1, 2, 3, 4))
> my_buffer$is_mutable
[1] TRUE
> my_buffer$ZeroPadding()
> my_buffer$size
[1] 32
> my_buffer$capacity
[1] 32
> 
> 
> 
> cleanEx()
> nameEx("call_function")
> ### * call_function
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: call_function
> ### Title: Call an Arrow compute function
> ### Aliases: call_function
> 
> ### ** Examples
> 
> a <- Array$create(c(1L, 2L, 3L, NA, 5L))
> s <- Scalar$create(4L)
> call_function("coalesce", a, s)
Array
<int32>
[
  1,
  2,
  3,
  4,
  5
]
> 
> a <- Array$create(rnorm(10000))
> call_function("quantile", a, options = list(q = seq(0, 1, 0.25)))
Array
<double>
[
  -3.671299931846809,
  -0.6733943533540334,
  -0.01592882892007811,
  0.6776604744343349,
  3.810276680710665
]
> 
> 
> 
> cleanEx()
> nameEx("cast")
> ### * cast
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cast
> ### Title: Change the type of an array or column
> ### Aliases: cast
> ### Keywords: internal
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mtcars %>%
> ##D   arrow_table() %>%
> ##D   mutate(cyl = cast(cyl, string()))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("codec_is_available")
> ### * codec_is_available
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: codec_is_available
> ### Title: Check whether a compression codec is available
> ### Aliases: codec_is_available
> 
> ### ** Examples
> 
> codec_is_available("gzip")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("concat_arrays")
> ### * concat_arrays
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: concat_arrays
> ### Title: Concatenate zero or more Arrays
> ### Aliases: concat_arrays c.Array
> 
> ### ** Examples
> 
> concat_arrays(Array$create(1:3), Array$create(4:5))
Array
<int32>
[
  1,
  2,
  3,
  4,
  5
]
> 
> 
> 
> cleanEx()
> nameEx("concat_tables")
> ### * concat_tables
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: concat_tables
> ### Title: Concatenate one or more Tables
> ### Aliases: concat_tables
> 
> ### ** Examples
> 
> tbl <- arrow_table(name = rownames(mtcars), mtcars)
> prius <- arrow_table(name = "Prius", mpg = 58, cyl = 4, disp = 1.8)
> combined <- concat_tables(tbl, prius)
> tail(combined)$to_data_frame()
# A tibble: 6 × 12
  name           mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1 Lotus Europa  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2
2 Ford Panter…  15.8     8 351     264  4.22  3.17  14.5     0     1     5     4
3 Ferrari Dino  19.7     6 145     175  3.62  2.77  15.5     0     1     5     6
4 Maserati Bo…  15       8 301     335  3.54  3.57  14.6     0     1     5     8
5 Volvo 142E    21.4     4 121     109  4.11  2.78  18.6     1     1     4     2
6 Prius         58       4   1.8    NA NA    NA     NA      NA    NA    NA    NA
> 
> 
> 
> cleanEx()
> nameEx("copy_files")
> ### * copy_files
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: copy_files
> ### Title: Copy files between FileSystems
> ### Aliases: copy_files
> 
> ### ** Examples
> 
> ## Don't show: 
> if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # Copy an S3 bucket's files to a local directory:
+ copy_files("s3://your-bucket-name", "local-directory")
+ # Using a FileSystem object
+ copy_files(s3_bucket("your-bucket-name"), "local-directory")
+ # Or go the other way, from local to S3
+ copy_files("local-directory", s3_bucket("your-bucket-name"))
+ ## Don't show: 
+ }) # examplesIf
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("create_package_with_all_dependencies")
> ### * create_package_with_all_dependencies
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: create_package_with_all_dependencies
> ### Title: Create a source bundle that includes all thirdparty dependencies
> ### Aliases: create_package_with_all_dependencies
> 
> ### ** Examples
> 
> ## Not run: 
> ##D new_pkg <- create_package_with_all_dependencies()
> ##D # Note: this works when run in the same R session, but it's meant to be
> ##D # copied to a different computer.
> ##D install.packages(new_pkg, dependencies = c("Depends", "Imports", "LinkingTo"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("data-type")
> ### * data-type
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: data-type
> ### Title: Apache Arrow data types
> ### Aliases: data-type int8 int16 int32 int64 uint8 uint16 uint32 uint64
> ###   float16 halffloat float32 float float64 boolean bool utf8 large_utf8
> ###   binary large_binary fixed_size_binary string date32 date64 time32
> ###   time64 duration null timestamp decimal decimal128 decimal256 struct
> ###   list_of large_list_of FixedSizeListType fixed_size_list_of MapType
> ###   map_of
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_acero()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ bool()
+ struct(a = int32(), b = double())
+ timestamp("ms", timezone = "CEST")
+ time64("ns")
+ 
+ # Use the cast method to change the type of data contained in Arrow objects.
+ # Please check the documentation of each data object class for details.
+ my_scalar <- Scalar$create(0L, type = int64()) # int64
+ my_scalar$cast(timestamp("ns")) # timestamp[ns]
+ 
+ my_array <- Array$create(0L, type = int64()) # int64
+ my_array$cast(timestamp("s", timezone = "UTC")) # timestamp[s, tz=UTC]
+ 
+ my_chunked_array <- chunked_array(0L, 1L) # int32
+ my_chunked_array$cast(date32()) # date32[day]
+ 
+ # You can also use `cast()` in an Arrow dplyr query.
+ if (requireNamespace("dplyr", quietly = TRUE)) {
+   library(dplyr, warn.conflicts = FALSE)
+   arrow_table(mtcars) %>%
+     transmute(
+       col1 = cast(cyl, string()),
+       col2 = cast(cyl, int8())
+     ) %>%
+     compute()
+ }
+ ## Don't show: 
+ }) # examplesIf
> bool()
Boolean
bool
> struct(a = int32(), b = double())
StructType
struct<a: int32, b: double>
> timestamp("ms", timezone = "CEST")
Timestamp
timestamp[ms, tz=CEST]
> time64("ns")
Time64
time64[ns]
> my_scalar <- Scalar$create(0L, type = int64())
> my_scalar$cast(timestamp("ns"))
Scalar
1970-01-01 00:00:00.000000000
> my_array <- Array$create(0L, type = int64())
> my_array$cast(timestamp("s", timezone = "UTC"))
Array
<timestamp[s, tz=UTC]>
[
  1970-01-01 00:00:00
]
> my_chunked_array <- chunked_array(0L, 1L)
> my_chunked_array$cast(date32())
ChunkedArray
<date32[day]>
[
  [
    1970-01-01
  ],
  [
    1970-01-02
  ]
]
> if (requireNamespace("dplyr", quietly = TRUE)) {
+     library(dplyr, warn.conflicts = FALSE)
+     arrow_table(mtcars) %>% transmute(col1 = cast(cyl, string()), col2 = cast(cyl, 
+         int8())) %>% compute()
+ }
Table
32 rows x 2 columns
$col1 <string>
$col2 <int8>

See $metadata for additional Schema metadata
> ## End(Don't show)
> 
> 
> 
> cleanEx()

detaching ‘package:dplyr’

> nameEx("gs_bucket")
> ### * gs_bucket
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gs_bucket
> ### Title: Connect to a Google Cloud Storage (GCS) bucket
> ### Aliases: gs_bucket
> 
> ### ** Examples
> 
> ## Don't show: 
> if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ bucket <- gs_bucket("voltrondata-labs-datasets")
+ ## Don't show: 
+ }) # examplesIf
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("hive_partition")
> ### * hive_partition
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hive_partition
> ### Title: Construct Hive partitioning
> ### Aliases: hive_partition
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ hive_partition(year = int16(), month = int8())
+ ## Don't show: 
+ }) # examplesIf
> hive_partition(year = int16(), month = int8())
HivePartitioning
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("infer_type")
> ### * infer_type
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: infer_type
> ### Title: Infer the arrow Array type from an R object
> ### Aliases: infer_type type
> 
> ### ** Examples
> 
> infer_type(1:10)
Int32
int32
> infer_type(1L:10L)
Int32
int32
> infer_type(c(1, 1.5, 2))
Float64
double
> infer_type(c("A", "B", "C"))
Utf8
string
> infer_type(mtcars)
StructType
struct<mpg: double, cyl: double, disp: double, hp: double, drat: double, wt: double, qsec: double, vs: double, am: double, gear: double, carb: double>
> infer_type(Sys.Date())
Date32
date32[day]
> infer_type(as.POSIXlt(Sys.Date()))
VctrsExtensionType
POSIXlt of length 0
> infer_type(vctrs::new_vctr(1:5, class = "my_custom_vctr_class"))
VctrsExtensionType
<my_custom_vctr_class[0]>
> 
> 
> 
> cleanEx()
> nameEx("list_compute_functions")
> ### * list_compute_functions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: list_compute_functions
> ### Title: List available Arrow C++ compute functions
> ### Aliases: list_compute_functions
> 
> ### ** Examples
> 
> available_funcs <- list_compute_functions()
> utf8_funcs <- list_compute_functions(pattern = "^UTF8", ignore.case = TRUE)
> 
> 
> 
> cleanEx()
> nameEx("load_flight_server")
> ### * load_flight_server
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: load_flight_server
> ### Title: Load a Python Flight server
> ### Aliases: load_flight_server
> 
> ### ** Examples
> 
> ## Don't show: 
> if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ load_flight_server("demo_flight_server")
+ ## Don't show: 
+ }) # examplesIf
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("match_arrow")
> ### * match_arrow
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: match_arrow
> ### Title: 'match' and '%in%' for Arrow objects
> ### Aliases: match_arrow is_in
> 
> ### ** Examples
> 
> # note that the returned value is 0-indexed
> cars_tbl <- arrow_table(name = rownames(mtcars), mtcars)
> match_arrow(Scalar$create("Mazda RX4 Wag"), cars_tbl$name)
Scalar
1
> 
> is_in(Array$create("Mazda RX4 Wag"), cars_tbl$name)
Array
<bool>
[
  true
]
> 
> # Although there are multiple matches, you are returned the index of the first
> # match, as with the base R equivalent
> match(4, mtcars$cyl) # 1-indexed
[1] 3
> match_arrow(Scalar$create(4), cars_tbl$cyl) # 0-indexed
Scalar
2
> 
> # If `x` contains multiple values, you are returned the indices of the first
> # match for each value.
> match(c(4, 6, 8), mtcars$cyl)
[1] 3 1 5
> match_arrow(Array$create(c(4, 6, 8)), cars_tbl$cyl)
Array
<int32>
[
  2,
  0,
  4
]
> 
> # Return type matches type of `x`
> is_in(c(4, 6, 8), mtcars$cyl) # returns vector
Array
<bool>
[
  true,
  true,
  true
]
> is_in(Scalar$create(4), mtcars$cyl) # returns Scalar
Scalar
true
> is_in(Array$create(c(4, 6, 8)), cars_tbl$cyl) # returns Array
Array
<bool>
[
  true,
  true,
  true
]
> is_in(ChunkedArray$create(c(4, 6), 8), cars_tbl$cyl) # returns ChunkedArray
ChunkedArray
<bool>
[
  [
    true,
    true,
    true
  ]
]
> 
> 
> 
> cleanEx()
> nameEx("new_extension_type")
> ### * new_extension_type
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: new_extension_type
> ### Title: Extension types
> ### Aliases: new_extension_type new_extension_array register_extension_type
> ###   reregister_extension_type unregister_extension_type
> 
> ### ** Examples
> 
> # Create the R6 type whose methods control how Array objects are
> # converted to R objects, how equality between types is computed,
> # and how types are printed.
> QuantizedType <- R6::R6Class(
+   "QuantizedType",
+   inherit = ExtensionType,
+   public = list(
+     # methods to access the custom metadata fields
+     center = function() private$.center,
+     scale = function() private$.scale,
+ 
+     # called when an Array of this type is converted to an R vector
+     as_vector = function(extension_array) {
+       if (inherits(extension_array, "ExtensionArray")) {
+         unquantized_arrow <-
+           (extension_array$storage()$cast(float64()) / private$.scale) +
+           private$.center
+ 
+         as.vector(unquantized_arrow)
+       } else {
+         super$as_vector(extension_array)
+       }
+     },
+ 
+     # populate the custom metadata fields from the serialized metadata
+     deserialize_instance = function() {
+       vals <- as.numeric(strsplit(self$extension_metadata_utf8(), ";")[[1]])
+       private$.center <- vals[1]
+       private$.scale <- vals[2]
+     }
+   ),
+   private = list(
+     .center = NULL,
+     .scale = NULL
+   )
+ )
> 
> # Create a helper type constructor that calls new_extension_type()
> quantized <- function(center = 0, scale = 1, storage_type = int32()) {
+   new_extension_type(
+     storage_type = storage_type,
+     extension_name = "arrow.example.quantized",
+     extension_metadata = paste(center, scale, sep = ";"),
+     type_class = QuantizedType
+   )
+ }
> 
> # Create a helper array constructor that calls new_extension_array()
> quantized_array <- function(x, center = 0, scale = 1,
+                             storage_type = int32()) {
+   type <- quantized(center, scale, storage_type)
+   new_extension_array(
+     Array$create((x - center) * scale, type = storage_type),
+     type
+   )
+ }
> 
> # Register the extension type so that Arrow knows what to do when
> # it encounters this extension type
> reregister_extension_type(quantized())
> 
> # Create Array objects and use them!
> (vals <- runif(5, min = 19, max = 21))
[1] 19.53102 19.74425 20.14571 20.81642 19.40336
> 
> (array <- quantized_array(
+   vals,
+   center = 20,
+   scale = 2^15 - 1,
+   storage_type = int16()
+ )
+ )
ExtensionArray
<QuantizedType <20;32767>>
[
  -15367,
  -8380,
  4774,
  26751,
  -19549
]
> 
> array$type$center()
[1] 20
> array$type$scale()
[1] 32767
> 
> as.vector(array)
[1] 19.53102 19.74425 20.14570 20.81640 19.40339
> 
> 
> 
> cleanEx()
> nameEx("open_dataset")
> ### * open_dataset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: open_dataset
> ### Title: Open a multi-file dataset
> ### Aliases: open_dataset
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset() & arrow_with_parquet()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # Set up directory for examples
+ tf <- tempfile()
+ dir.create(tf)
+ on.exit(unlink(tf))
+ 
+ write_dataset(mtcars, tf, partitioning = "cyl")
+ 
+ # You can specify a directory containing the files for your dataset and
+ # open_dataset will scan all files in your directory.
+ open_dataset(tf)
+ 
+ # You can also supply a vector of paths
+ open_dataset(c(file.path(tf, "cyl=4/part-0.parquet"), file.path(tf, "cyl=8/part-0.parquet")))
+ 
+ ## You must specify the file format if using a format other than parquet.
+ tf2 <- tempfile()
+ dir.create(tf2)
+ on.exit(unlink(tf2))
+ write_dataset(mtcars, tf2, format = "ipc")
+ # This line will results in errors when you try to work with the data
+ ## Not run: 
+ ##D open_dataset(tf2)
+ ## End(Not run)
+ # This line will work
+ open_dataset(tf2, format = "ipc")
+ 
+ ## You can specify file partitioning to include it as a field in your dataset
+ # Create a temporary directory and write example dataset
+ tf3 <- tempfile()
+ dir.create(tf3)
+ on.exit(unlink(tf3))
+ write_dataset(airquality, tf3, partitioning = c("Month", "Day"), hive_style = FALSE)
+ 
+ # View files - you can see the partitioning means that files have been written
+ # to folders based on Month/Day values
+ tf3_files <- list.files(tf3, recursive = TRUE)
+ 
+ # With no partitioning specified, dataset contains all files but doesn't include
+ # directory names as field names
+ open_dataset(tf3)
+ 
+ # Now that partitioning has been specified, your dataset contains columns for Month and Day
+ open_dataset(tf3, partitioning = c("Month", "Day"))
+ 
+ # If you want to specify the data types for your fields, you can pass in a Schema
+ open_dataset(tf3, partitioning = schema(Month = int8(), Day = int8()))
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> dir.create(tf)
> on.exit(unlink(tf))
> write_dataset(mtcars, tf, partitioning = "cyl")
> open_dataset(tf)
FileSystemDataset with 3 Parquet files
mpg: double
disp: double
hp: double
drat: double
wt: double
qsec: double
vs: double
am: double
gear: double
carb: double
cyl: int32

See $metadata for additional Schema metadata
> open_dataset(c(file.path(tf, "cyl=4/part-0.parquet"), file.path(tf, "cyl=8/part-0.parquet")))
FileSystemDataset with 2 Parquet files
mpg: double
disp: double
hp: double
drat: double
wt: double
qsec: double
vs: double
am: double
gear: double
carb: double

See $metadata for additional Schema metadata
> tf2 <- tempfile()
> dir.create(tf2)
> on.exit(unlink(tf2))
> write_dataset(mtcars, tf2, format = "ipc")
> open_dataset(tf2, format = "ipc")
FileSystemDataset with 1 Feather file
mpg: double
cyl: double
disp: double
hp: double
drat: double
wt: double
qsec: double
vs: double
am: double
gear: double
carb: double

See $metadata for additional Schema metadata
> tf3 <- tempfile()
> dir.create(tf3)
> on.exit(unlink(tf3))
> write_dataset(airquality, tf3, partitioning = c("Month", "Day"), hive_style = FALSE)
> tf3_files <- list.files(tf3, recursive = TRUE)
> open_dataset(tf3)
FileSystemDataset with 153 Parquet files
Ozone: int32
Solar.R: int32
Wind: double
Temp: int32

See $metadata for additional Schema metadata
> open_dataset(tf3, partitioning = c("Month", "Day"))
FileSystemDataset with 153 Parquet files
Ozone: int32
Solar.R: int32
Wind: double
Temp: int32
Month: int32
Day: int32

See $metadata for additional Schema metadata
> open_dataset(tf3, partitioning = schema(Month = int8(), Day = int8()))
FileSystemDataset with 153 Parquet files
Ozone: int32
Solar.R: int32
Wind: double
Temp: int32
Month: int8
Day: int8

See $metadata for additional Schema metadata
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("open_delim_dataset")
> ### * open_delim_dataset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: open_delim_dataset
> ### Title: Open a multi-file dataset of CSV or other delimiter-separated
> ###   format
> ### Aliases: open_delim_dataset open_csv_dataset open_tsv_dataset
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # Set up directory for examples
+ tf <- tempfile()
+ dir.create(tf)
+ df <- data.frame(x = c("1", "2", "NULL"))
+ 
+ file_path <- file.path(tf, "file1.txt")
+ write.table(df, file_path, sep = ",", row.names = FALSE)
+ 
+ read_csv_arrow(file_path, na = c("", "NA", "NULL"), col_names = "y", skip = 1)
+ open_csv_dataset(file_path, na = c("", "NA", "NULL"), col_names = "y", skip = 1)
+ 
+ unlink(tf)
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> dir.create(tf)
> df <- data.frame(x = c("1", "2", "NULL"))
> file_path <- file.path(tf, "file1.txt")
> write.table(df, file_path, sep = ",", row.names = FALSE)
> read_csv_arrow(file_path, na = c("", "NA", "NULL"), col_names = "y", skip = 1)
# A tibble: 3 × 1
      y
  <int>
1     1
2     2
3    NA
> open_csv_dataset(file_path, na = c("", "NA", "NULL"), col_names = "y", 
+     skip = 1)
FileSystemDataset with 1 csv file
y: int64
> unlink(tf)
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("read_delim_arrow")
> ### * read_delim_arrow
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: read_delim_arrow
> ### Title: Read a CSV or other delimited file with Arrow
> ### Aliases: read_delim_arrow read_csv_arrow read_tsv_arrow
> 
> ### ** Examples
> 
> tf <- tempfile()
> on.exit(unlink(tf))
> write.csv(mtcars, file = tf)
> df <- read_csv_arrow(tf)
> dim(df)
[1] 32 12
> # Can select columns
> df <- read_csv_arrow(tf, col_select = starts_with("d"))
> 
> # Specifying column types and names
> write.csv(data.frame(x = c(1, 3), y = c(2, 4)), file = tf, row.names = FALSE)
> read_csv_arrow(tf, schema = schema(x = int32(), y = utf8()), skip = 1)
# A tibble: 2 × 2
      x y    
  <int> <chr>
1     1 2    
2     3 4    
> read_csv_arrow(tf, col_types = schema(y = utf8()))
# A tibble: 2 × 2
      x y    
  <int> <chr>
1     1 2    
2     3 4    
> read_csv_arrow(tf, col_types = "ic", col_names = c("x", "y"), skip = 1)
# A tibble: 2 × 2
      x y    
  <int> <chr>
1     1 2    
2     3 4    
> 
> # Note that if a timestamp column contains time zones,
> # the string "T" `col_types` specification won't work.
> # To parse timestamps with time zones, provide a [Schema] to `col_types`
> # and specify the time zone in the type object:
> tf <- tempfile()
> write.csv(data.frame(x = "1970-01-01T12:00:00+12:00"), file = tf, row.names = FALSE)
> read_csv_arrow(
+   tf,
+   col_types = schema(x = timestamp(unit = "us", timezone = "UTC"))
+ )
# A tibble: 1 × 1
  x                  
  <dttm>             
1 1970-01-01 00:00:00
> 
> # Read directly from strings with `I()`
> read_csv_arrow(I("x,y\n1,2\n3,4"))
# A tibble: 2 × 2
      x     y
  <int> <int>
1     1     2
2     3     4
> read_delim_arrow(I(c("x y", "1 2", "3 4")), delim = " ")
# A tibble: 2 × 2
      x     y
  <int> <int>
1     1     2
2     3     4
> 
> 
> 
> cleanEx()
> nameEx("read_feather")
> ### * read_feather
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: read_feather
> ### Title: Read a Feather file (an Arrow IPC file)
> ### Aliases: read_feather read_ipc_file
> 
> ### ** Examples
> 
> # We recommend the ".arrow" extension for Arrow IPC files (Feather V2).
> tf <- tempfile(fileext = ".arrow")
> on.exit(unlink(tf))
> write_feather(mtcars, tf)
> df <- read_feather(tf)
> dim(df)
[1] 32 11
> # Can select columns
> df <- read_feather(tf, col_select = starts_with("d"))
> 
> 
> 
> cleanEx()
> nameEx("read_json_arrow")
> ### * read_json_arrow
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: read_json_arrow
> ### Title: Read a JSON file
> ### Aliases: read_json_arrow
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_json()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ tf <- tempfile()
+ on.exit(unlink(tf))
+ writeLines('
+     { "hello": 3.5, "world": false, "yo": "thing" }
+     { "hello": 3.25, "world": null }
+     { "hello": 0.0, "world": true, "yo": null }
+   ', tf, useBytes = TRUE)
+ 
+ read_json_arrow(tf)
+ 
+ # Read directly from strings with `I()`
+ read_json_arrow(I(c('{"x": 1, "y": 2}', '{"x": 3, "y": 4}')))
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> on.exit(unlink(tf))
> writeLines("\n    { \"hello\": 3.5, \"world\": false, \"yo\": \"thing\" }\n    { \"hello\": 3.25, \"world\": null }\n    { \"hello\": 0.0, \"world\": true, \"yo\": null }\n  ", 
+     tf, useBytes = TRUE)
> read_json_arrow(tf)
# A tibble: 3 × 3
  hello world yo   
  <dbl> <lgl> <chr>
1  3.5  FALSE thing
2  3.25 NA    <NA> 
3  0    TRUE  <NA> 
> read_json_arrow(I(c("{\"x\": 1, \"y\": 2}", "{\"x\": 3, \"y\": 4}")))
# A tibble: 2 × 2
      x     y
  <int> <int>
1     1     2
2     3     4
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("read_parquet")
> ### * read_parquet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: read_parquet
> ### Title: Read a Parquet file
> ### Aliases: read_parquet
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_parquet() && !getFromNamespace("on_linux_dev", "arrow")()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ tf <- tempfile()
+ on.exit(unlink(tf))
+ write_parquet(mtcars, tf)
+ df <- read_parquet(tf, col_select = starts_with("d"))
+ head(df)
+ ## Don't show: 
+ }) # examplesIf
> tf <- tempfile()
> on.exit(unlink(tf))
> write_parquet(mtcars, tf)
> df <- read_parquet(tf, col_select = starts_with("d"))
> head(df)
  disp drat
1  160 3.90
2  160 3.90
3  108 3.85
4  258 3.08
5  360 3.15
6  225 2.76
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("register_scalar_function")
> ### * register_scalar_function
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: register_scalar_function
> ### Title: Register user-defined functions
> ### Aliases: register_scalar_function
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset() && identical(Sys.getenv("NOT_CRAN"), "true")) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ library(dplyr, warn.conflicts = FALSE)
+ 
+ some_model <- lm(mpg ~ disp + cyl, data = mtcars)
+ register_scalar_function(
+   "mtcars_predict_mpg",
+   function(context, disp, cyl) {
+     predict(some_model, newdata = data.frame(disp, cyl))
+   },
+   in_type = schema(disp = float64(), cyl = float64()),
+   out_type = float64(),
+   auto_convert = TRUE
+ )
+ 
+ as_arrow_table(mtcars) %>%
+   transmute(mpg, mpg_predicted = mtcars_predict_mpg(disp, cyl)) %>%
+   collect() %>%
+   head()
+ ## Don't show: 
+ }) # examplesIf
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("s3_bucket")
> ### * s3_bucket
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: s3_bucket
> ### Title: Connect to an AWS S3 bucket
> ### Aliases: s3_bucket
> 
> ### ** Examples
> 
> ## Don't show: 
> if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ bucket <- s3_bucket("voltrondata-labs-datasets")
+ ## Don't show: 
+ }) # examplesIf
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("show_exec_plan")
> ### * show_exec_plan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: show_exec_plan
> ### Title: Show the details of an Arrow Execution Plan
> ### Aliases: show_exec_plan
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset() && requireNamespace("dplyr", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ library(dplyr)
+ mtcars %>%
+   arrow_table() %>%
+   filter(mpg > 20) %>%
+   mutate(x = gear / carb) %>%
+   show_exec_plan()
+ ## Don't show: 
+ }) # examplesIf
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> mtcars %>% arrow_table() %>% filter(mpg > 20) %>% mutate(x = gear/carb) %>% 
+     show_exec_plan()
ExecPlan with 4 nodes:
3:SinkNode{}
  2:ProjectNode{projection=[mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, "x": divide(cast(gear, {to_type=double, allow_int_overflow=false, allow_time_truncate=false, allow_time_overflow=false, allow_decimal_truncate=false, allow_float_truncate=false, allow_invalid_utf8=false}), cast(carb, {to_type=double, allow_int_overflow=false, allow_time_truncate=false, allow_time_overflow=false, allow_decimal_truncate=false, allow_float_truncate=false, allow_invalid_utf8=false}))]}
    1:FilterNode{filter=(mpg > 20)}
      0:TableSourceNode{}
> ## End(Don't show)
> 
> 
> 
> cleanEx()

detaching ‘package:dplyr’

> nameEx("to_arrow")
> ### * to_arrow
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: to_arrow
> ### Title: Create an Arrow object from others
> ### Aliases: to_arrow
> 
> ### ** Examples
> 
> ## Don't show: 
> if (getFromNamespace("run_duckdb_examples", "arrow")()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ library(dplyr)
+ 
+ ds <- InMemoryDataset$create(mtcars)
+ 
+ ds %>%
+   filter(mpg < 30) %>%
+   to_duckdb() %>%
+   group_by(cyl) %>%
+   summarize(mean_mpg = mean(mpg, na.rm = TRUE)) %>%
+   to_arrow() %>%
+   collect()
+ ## Don't show: 
+ }) # examplesIf
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> ds <- InMemoryDataset$create(mtcars)
> ds %>% filter(mpg < 30) %>% to_duckdb() %>% group_by(cyl) %>% summarize(mean_mpg = mean(mpg, 
+     na.rm = TRUE)) %>% to_arrow() %>% collect()
# A tibble: 3 × 2
    cyl mean_mpg
  <dbl>    <dbl>
1     6     19.7
2     4     23.7
3     8     15.1
> ## End(Don't show)
> 
> 
> 
> cleanEx()

detaching ‘package:dplyr’

> nameEx("to_duckdb")
> ### * to_duckdb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: to_duckdb
> ### Title: Create a (virtual) DuckDB table from an Arrow object
> ### Aliases: to_duckdb
> 
> ### ** Examples
> 
> ## Don't show: 
> if (getFromNamespace("run_duckdb_examples", "arrow")()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ library(dplyr)
+ 
+ ds <- InMemoryDataset$create(mtcars)
+ 
+ ds %>%
+   filter(mpg < 30) %>%
+   group_by(cyl) %>%
+   to_duckdb() %>%
+   slice_min(disp)
+ ## Don't show: 
+ }) # examplesIf
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> ds <- InMemoryDataset$create(mtcars)
> ds %>% filter(mpg < 30) %>% group_by(cyl) %>% to_duckdb() %>% slice_min(disp)
# Source:   SQL [5 x 11]
# Database: DuckDB 0.8.0 [unknown@Linux 6.5.0-1025-azure:R 4.3.0/:memory:]
# Groups:   cyl
    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1  27.3     4   79     66  4.08  1.94  18.9     1     1     4     1
2  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6
3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3
4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3
5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3
> ## End(Don't show)
> 
> 
> 
> cleanEx()

detaching ‘package:dplyr’

> nameEx("unify_schemas")
> ### * unify_schemas
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: unify_schemas
> ### Title: Combine and harmonize schemas
> ### Aliases: unify_schemas
> 
> ### ** Examples
> 
> a <- schema(b = double(), c = bool())
> z <- schema(b = double(), k = utf8())
> unify_schemas(a, z)
Schema
b: double
c: bool
k: string
> 
> 
> 
> cleanEx()
> nameEx("value_counts")
> ### * value_counts
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: value_counts
> ### Title: 'table' for Arrow objects
> ### Aliases: value_counts
> 
> ### ** Examples
> 
> cyl_vals <- Array$create(mtcars$cyl)
> counts <- value_counts(cyl_vals)
> 
> 
> 
> cleanEx()
> nameEx("vctrs_extension_array")
> ### * vctrs_extension_array
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vctrs_extension_array
> ### Title: Extension type for generic typed vectors
> ### Aliases: vctrs_extension_array vctrs_extension_type
> 
> ### ** Examples
> 
> (array <- vctrs_extension_array(as.POSIXlt("2022-01-02 03:45", tz = "UTC")))
ExtensionArray
<POSIXlt of length 0>
-- is_valid: all not null
-- child 0 type: double
  [
    0
  ]
-- child 1 type: int32
  [
    45
  ]
-- child 2 type: int32
  [
    3
  ]
-- child 3 type: int32
  [
    2
  ]
-- child 4 type: int32
  [
    0
  ]
-- child 5 type: int32
  [
    122
  ]
-- child 6 type: int32
  [
    0
  ]
-- child 7 type: int32
  [
    1
  ]
-- child 8 type: int32
  [
    0
  ]
-- child 9 type: string
  [
    "UTC"
  ]
-- child 10 type: int32
  [
    0
  ]
> array$type
VctrsExtensionType
POSIXlt of length 0
> as.vector(array)
[1] "2022-01-02 03:45:00 UTC"
> 
> temp_feather <- tempfile()
> write_feather(arrow_table(col = array), temp_feather)
> read_feather(temp_feather)
# A tibble: 1 × 1
  col                
  <dttm>             
1 2022-01-02 03:45:00
> unlink(temp_feather)
> 
> 
> 
> cleanEx()
> nameEx("write_csv_arrow")
> ### * write_csv_arrow
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_csv_arrow
> ### Title: Write CSV file to disk
> ### Aliases: write_csv_arrow
> 
> ### ** Examples
> 
> tf <- tempfile()
> on.exit(unlink(tf))
> write_csv_arrow(mtcars, tf)
> 
> 
> 
> cleanEx()
> nameEx("write_dataset")
> ### * write_dataset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_dataset
> ### Title: Write a dataset
> ### Aliases: write_dataset
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_dataset() & arrow_with_parquet() & requireNamespace("dplyr", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ # You can write datasets partitioned by the values in a column (here: "cyl").
+ # This creates a structure of the form cyl=X/part-Z.parquet.
+ one_level_tree <- tempfile()
+ write_dataset(mtcars, one_level_tree, partitioning = "cyl")
+ list.files(one_level_tree, recursive = TRUE)
+ 
+ # You can also partition by the values in multiple columns
+ # (here: "cyl" and "gear").
+ # This creates a structure of the form cyl=X/gear=Y/part-Z.parquet.
+ two_levels_tree <- tempfile()
+ write_dataset(mtcars, two_levels_tree, partitioning = c("cyl", "gear"))
+ list.files(two_levels_tree, recursive = TRUE)
+ 
+ # In the two previous examples we would have:
+ # X = {4,6,8}, the number of cylinders.
+ # Y = {3,4,5}, the number of forward gears.
+ # Z = {0,1,2}, the number of saved parts, starting from 0.
+ 
+ # You can obtain the same result as as the previous examples using arrow with
+ # a dplyr pipeline. This will be the same as two_levels_tree above, but the
+ # output directory will be different.
+ library(dplyr)
+ two_levels_tree_2 <- tempfile()
+ mtcars %>%
+   group_by(cyl, gear) %>%
+   write_dataset(two_levels_tree_2)
+ list.files(two_levels_tree_2, recursive = TRUE)
+ 
+ # And you can also turn off the Hive-style directory naming where the column
+ # name is included with the values by using `hive_style = FALSE`.
+ 
+ # Write a structure X/Y/part-Z.parquet.
+ two_levels_tree_no_hive <- tempfile()
+ mtcars %>%
+   group_by(cyl, gear) %>%
+   write_dataset(two_levels_tree_no_hive, hive_style = FALSE)
+ list.files(two_levels_tree_no_hive, recursive = TRUE)
+ ## Don't show: 
+ }) # examplesIf
> one_level_tree <- tempfile()
> write_dataset(mtcars, one_level_tree, partitioning = "cyl")
> list.files(one_level_tree, recursive = TRUE)
[1] "cyl=4/part-0.parquet" "cyl=6/part-0.parquet" "cyl=8/part-0.parquet"
> two_levels_tree <- tempfile()
> write_dataset(mtcars, two_levels_tree, partitioning = c("cyl", "gear"))
> list.files(two_levels_tree, recursive = TRUE)
[1] "cyl=4/gear=3/part-0.parquet" "cyl=4/gear=4/part-0.parquet"
[3] "cyl=4/gear=5/part-0.parquet" "cyl=6/gear=3/part-0.parquet"
[5] "cyl=6/gear=4/part-0.parquet" "cyl=6/gear=5/part-0.parquet"
[7] "cyl=8/gear=3/part-0.parquet" "cyl=8/gear=5/part-0.parquet"
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> two_levels_tree_2 <- tempfile()
> mtcars %>% group_by(cyl, gear) %>% write_dataset(two_levels_tree_2)
> list.files(two_levels_tree_2, recursive = TRUE)
[1] "cyl=4/gear=3/part-0.parquet" "cyl=4/gear=4/part-0.parquet"
[3] "cyl=4/gear=5/part-0.parquet" "cyl=6/gear=3/part-0.parquet"
[5] "cyl=6/gear=4/part-0.parquet" "cyl=6/gear=5/part-0.parquet"
[7] "cyl=8/gear=3/part-0.parquet" "cyl=8/gear=5/part-0.parquet"
> two_levels_tree_no_hive <- tempfile()
> mtcars %>% group_by(cyl, gear) %>% write_dataset(two_levels_tree_no_hive, 
+     hive_style = FALSE)
> list.files(two_levels_tree_no_hive, recursive = TRUE)
[1] "4/3/part-0.parquet" "4/4/part-0.parquet" "4/5/part-0.parquet"
[4] "6/3/part-0.parquet" "6/4/part-0.parquet" "6/5/part-0.parquet"
[7] "8/3/part-0.parquet" "8/5/part-0.parquet"
> ## End(Don't show)
> 
> 
> 
> cleanEx()

detaching ‘package:dplyr’

> nameEx("write_feather")
> ### * write_feather
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_feather
> ### Title: Write a Feather file (an Arrow IPC file)
> ### Aliases: write_feather write_ipc_file
> 
> ### ** Examples
> 
> # We recommend the ".arrow" extension for Arrow IPC files (Feather V2).
> tf1 <- tempfile(fileext = ".feather")
> tf2 <- tempfile(fileext = ".arrow")
> tf3 <- tempfile(fileext = ".arrow")
> on.exit({
+   unlink(tf1)
+   unlink(tf2)
+   unlink(tf3)
+ })
> write_feather(mtcars, tf1, version = 1)
> write_feather(mtcars, tf2)
> write_ipc_file(mtcars, tf3)
> 
> 
> 
> cleanEx()
> nameEx("write_ipc_stream")
> ### * write_ipc_stream
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_ipc_stream
> ### Title: Write Arrow IPC stream format
> ### Aliases: write_ipc_stream
> 
> ### ** Examples
> 
> tf <- tempfile()
> on.exit(unlink(tf))
> write_ipc_stream(mtcars, tf)
> 
> 
> 
> cleanEx()
> nameEx("write_parquet")
> ### * write_parquet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_parquet
> ### Title: Write Parquet file to disk
> ### Aliases: write_parquet
> 
> ### ** Examples
> 
> ## Don't show: 
> if (arrow_with_parquet()) (if (getRversion() >= "3.4") withAutoprint else force)({ # examplesIf
+ ## End(Don't show)
+ tf1 <- tempfile(fileext = ".parquet")
+ write_parquet(data.frame(x = 1:5), tf1)
+ 
+ # using compression
+ if (codec_is_available("gzip")) {
+   tf2 <- tempfile(fileext = ".gz.parquet")
+   write_parquet(data.frame(x = 1:5), tf2, compression = "gzip", compression_level = 5)
+ }
+ ## Don't show: 
+ }) # examplesIf
> tf1 <- tempfile(fileext = ".parquet")
> write_parquet(data.frame(x = 1:5), tf1)
> if (codec_is_available("gzip")) {
+     tf2 <- tempfile(fileext = ".gz.parquet")
+     write_parquet(data.frame(x = 1:5), tf2, compression = "gzip", compression_level = 5)
+ }
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("write_to_raw")
> ### * write_to_raw
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: write_to_raw
> ### Title: Write Arrow data to a raw vector
> ### Aliases: write_to_raw
> 
> ### ** Examples
> 
> # The default format is "stream"
> mtcars_raw <- write_to_raw(mtcars)
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  2.235 0.254 2.398 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
Warning: Connection is garbage-collected, use dbDisconnect() to avoid this.
Warning: Database is garbage-collected, use dbDisconnect(con, shutdown=TRUE) or duckdb::duckdb_shutdown(drv) to avoid this.
