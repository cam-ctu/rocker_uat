
R version 4.4.0 (2024-04-24) -- "Puppy Cup"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "Hmisc"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('Hmisc')

Attaching package: ‘Hmisc’

The following objects are masked from ‘package:base’:

    format.pval, units

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Cs")
> ### * Cs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Cs
> ### Title: Character strings from unquoted names
> ### Aliases: Cs .q
> ### Keywords: character utilities
> 
> ### ** Examples
> 
> Cs(a,cat,dog)
[1] "a"   "cat" "dog"
> # subset.data.frame <- dataframe[,Cs(age,sex,race,bloodpressure,height)]
> .q(a, b, c, 'this and that')
[1] "a"             "b"             "c"             "this and that"
> .q(dog=a, giraffe=b, cat=c)
    dog giraffe     cat 
    "a"     "b"     "c" 
> 
> 
> 
> cleanEx()
> nameEx("Ecdf")
> ### * Ecdf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Ecdf
> ### Title: Empirical Cumulative Distribution Plot
> ### Aliases: Ecdf Ecdf.default Ecdf.data.frame Ecdf.formula panel.Ecdf
> ###   prepanel.Ecdf
> ### Keywords: nonparametric hplot methods distribution
> 
> ### ** Examples
> 
> set.seed(1)
> ch <- rnorm(1000, 200, 40)
> Ecdf(ch, xlab="Serum Cholesterol")
> scat1d(ch)                       # add rug plot
> histSpike(ch, add=TRUE, frac=.15)   # add spike histogram
> # Better: add a data density display automatically:
> Ecdf(ch, datadensity='density')
> 
> 
> label(ch) <- "Serum Cholesterol"
> Ecdf(ch)
> other.ch <- rnorm(500, 220, 20)
> Ecdf(other.ch,add=TRUE,lty=2)
> 
> 
> sex <- factor(sample(c('female','male'), 1000, TRUE))
> Ecdf(ch, q=c(.25,.5,.75))  # show quartiles
> Ecdf(ch, group=sex,
+      label.curves=list(method='arrow'))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> 
> 
> # Example showing how to draw multiple ECDFs from paired data
> pre.test <- rnorm(100,50,10)
> post.test <- rnorm(100,55,10)
> x <- c(pre.test, post.test)
> g <- c(rep('Pre',length(pre.test)),rep('Post',length(post.test)))
> Ecdf(x, group=g, xlab='Test Results', label.curves=list(keys=1:2))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> # keys=1:2 causes symbols to be drawn periodically on top of curves
> 
> 
> # Draw a matrix of ECDFs for a data frame
> m <- data.frame(pre.test, post.test, 
+                 sex=sample(c('male','female'),100,TRUE))
> Ecdf(m, group=m$sex, datadensity='rug')
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in par(oldmf) : argument 1 does not name a graphical parameter
> 
> 
> freqs <- sample(1:10, 1000, TRUE)
> Ecdf(ch, weights=freqs)  # weighted estimates
> 
> 
> # Trellis/Lattice examples:
> 
> 
> region <- factor(sample(c('Europe','USA','Australia'),100,TRUE))
> year <- factor(sample(2001:2002,1000,TRUE))
> Ecdf(~ch | region*year, groups=sex)
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> Key()           # draw a key for sex at the default location
> # Key(locator(1)) # user-specified positioning of key
> age <- rnorm(1000, 50, 10)
> Ecdf(~ch | lattice::equal.count(age), groups=sex)  # use overlapping shingles
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> Ecdf(~ch | sex, datadensity='hist', side=3)  # add spike histogram at top
> 
> 
> 
> cleanEx()
> nameEx("GiniMd")
> ### * GiniMd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GiniMd
> ### Title: Gini's Mean Difference
> ### Aliases: GiniMd
> ### Keywords: robust univar
> 
> ### ** Examples
> 
> set.seed(1)
> x <- rnorm(40)
> # Test GiniMd against a brute-force solution
> gmd <- function(x) {
+   n <- length(x)
+   sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)
+   }
> GiniMd(x)
[1] 0.9954155
> gmd(x)
[1] 0.9954155
> 
> z <- c(rep(0,17), rep(1,6))
> n <- length(z)
> GiniMd(z)
[1] 0.4031621
> 2*mean(z)*(1-mean(z))*n/(n-1)
[1] 0.4031621
> 
> a <- 12; b <- 13; c <- 7; n <- a + b + c
> A <- -.123; B <- -.707; C <- 0.523
> xx <- c(rep(A, a), rep(B, b), rep(C, c))
> GiniMd(xx)
[1] 0.518746
> 2*(a*b*abs(A-B) + a*c*abs(A-C) + b*c*abs(B-C))/n/(n-1)
[1] 0.518746
> 
> 
> 
> cleanEx()
> nameEx("Lag")
> ### * Lag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Lag
> ### Title: Lag a Numeric, Character, or Factor Vector
> ### Aliases: Lag
> ### Keywords: manip
> 
> ### ** Examples
> 
> Lag(1:5,2)
[1] NA NA  1  2  3
> Lag(letters[1:4],2)
[1] ""  ""  "a" "b"
> Lag(factor(letters[1:4]),-2)
[1] c    d    <NA> <NA>
Levels: a b c d
> # Find which observations are the first for a given subject
> id <- c('a','a','b','b','b','c')
> id != Lag(id)
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE
> !duplicated(id)
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE
> 
> 
> 
> cleanEx()
> nameEx("Merge")
> ### * Merge
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Merge
> ### Title: Merge Multiple Data Frames or Data Tables
> ### Aliases: Merge
> 
> ### ** Examples
> 
> ## Not run: 
> ##D a <- data.frame(sid=1:3, age=c(20,30,40))
> ##D b <- data.frame(sid=c(1,2,2), bp=c(120,130,140))
> ##D d <- data.frame(sid=c(1,3,4), wt=c(170,180,190))
> ##D all <- Merge(a, b, d, id = ~ sid)
> ##D # First file should be the master file and must
> ##D # contain all ids that ever occur.  ids not in the master will
> ##D # not be merged from other datasets.
> ##D a <- data.table(a); setkey(a, sid)
> ##D # data.table also does not allow duplicates without allow.cartesian=TRUE
> ##D b <- data.table(sid=1:2, bp=c(120,130)); setkey(b, sid)
> ##D d <- data.table(d); setkey(d, sid)
> ##D all <- Merge(a, b, d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Misc")
> ### * Misc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Misc
> ### Title: Miscellaneous Functions
> ### Aliases: clowess confbar getLatestSource grType prType htmlSpecialType
> ###   inverseFunction james.stein keepHattrib km.quick latexBuild
> ###   lm.fit.qr.bare matxv makeSteps nomiss outerText plotlyParm plotp
> ###   rendHTML restoreHattrib sepUnitsTrans strgraphwrap tobase64image
> ###   trap.rule trellis.strip.blank unPaste whichClosest whichClosePW
> ###   whichClosek xless
> ### Keywords: programming utilities iplot
> 
> ### ** Examples
> 
> 
> 
> trap.rule(1:100,1:100)
[1] 4999.5
> 
> unPaste(c('a;b or c','ab;d','qr;s'), ';')
[[1]]
[1] "a"  "ab" "qr"

[[2]]
[1] "b or c" "d"      "s"     

> 
> sepUnitsTrans(c('3 days','4 months','2 years','7'))
[1]   3.00 121.75 730.50   7.00
attr(,"units")
[1] "day"
> 
> set.seed(1)
> whichClosest(1:100, 3:5)
[1] 3 4 5
> whichClosest(1:100, rep(3,20))
 [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
> 
> whichClosePW(1:100, rep(3,20))
 [1]  3  3  5  8  2  8  9  6  6  1  2  2  6  4  7  5  6 11  4  7
> whichClosePW(1:100, rep(3,20), f=.05)
 [1] 4 2 3 2 2 3 1 3 4 3 3 3 3 2 4 3 4 2 4 3
> whichClosePW(1:100, rep(3,20), f=1e-10)
 [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
> 
> x <- seq(-1, 1, by=.01)
> y <- x^2
> h <- inverseFunction(x,y)
> formals(h)$turns   # vertex
[1] 0.005
> a <- seq(0, 1, by=.01)
> plot(0, 0, type='n', xlim=c(-.5,1.5))
> lines(a, h(a)[,1])            ## first inverse
> lines(a, h(a)[,2], col='red') ## second inverse
> a <- c(-.1, 1.01, 1.1, 1.2)
> points(a, h(a)[,1])
> 
> d <- data.frame(x=1:2, y=3:4, z=5:6)
> d <- upData(d, labels=c(x='X', z='Z lab'), units=c(z='mm'))
Input object size:	 1008 bytes;	 3 variables	 2 observations
New object size:	2272 bytes;	3 variables	2 observations
> a <- keepHattrib(d)
> 
> d <- data.frame(x=1:2, y=3:4, z=5:6)
> d2 <- restoreHattrib(d, a)
> sapply(d2, attributes)
$x
$x$label
[1] "X"

$x$class
[1] "labelled" "integer" 


$y
NULL

$z
$z$label
[1] "Z lab"

$z$class
[1] "labelled" "integer" 

$z$units
[1] "mm"


> 
> ## Not run: 
> ##D getLatestSource(recent=5)  # source() most recent 5 revised files in Hmisc
> ##D getLatestSource('cut2')    # fetch and source latest cut2.s
> ##D getLatestSource('all')     # get everything
> ##D getLatestSource(avail=TRUE) # list available files and latest versions
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("R2Measures")
> ### * R2Measures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: R2Measures
> ### Title: R2Measures
> ### Aliases: R2Measures
> 
> ### ** Examples
> 
> x <- c(rep(0, 50), rep(1, 50))
> y <- x
> # f <- lrm(y ~ x)
> # f   # Nagelkerke R^2=1.0
> # lr <- f$stats['Model L.R.']
> # 1 - exp(- lr / 100)  # Maddala-Cox-Snell (MCS) 0.75
> lr <- 138.6267  # manually so don't need rms package
> 
> R2Measures(lr, 1, 100, c(50, 50))  # 0.84 Effective n=75
  R2(100) R2(1,100)    R2(75)  R2(1,75) 
0.7499932 0.7474805 0.8425041 0.8403901 
> R2Measures(lr, 1, 100, 50)         # 0.94
  R2(100) R2(1,100)    R2(50)  R2(1,50) 
0.7499932 0.7474805 0.9374966 0.9362339 
> # MCS requires unreasonable effective sample size = minimum outcome
> # frequency to get close to the 1.0 that Nagelkerke R^2 achieves
> 
> 
> 
> cleanEx()
> nameEx("Save")
> ### * Save
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Save
> ### Title: Faciliate Use of save and load to Remote Directories
> ### Aliases: Save Load
> ### Keywords: data file utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D d <- data.frame(x=1:3, y=11:13)
> ##D options(LoadPath='../data/rda')
> ##D Save(d)   # creates ../data/rda/d.rda
> ##D Load(d)   # reads   ../data/rda/d.rda
> ##D Save(d, 'D')   # creates object D and saves it in .../D.rda
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("abs.error.pred")
> ### * abs.error.pred
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: abs.error.pred
> ### Title: Indexes of Absolute Prediction Error for Linear Models
> ### Aliases: abs.error.pred print.abs.error.pred
> ### Keywords: robust regression models
> 
> ### ** Examples
> 
> set.seed(1)         # so can regenerate results
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> y  <- exp(x1+x2+rnorm(100))
> f <- lm(log(y) ~ x1 + poly(x2,3), y=TRUE)
> abs.error.pred(lp=exp(fitted(f)), y=y)

Mean/Median |Differences|

                             Mean    Median
|Yi hat - median(Y hat)| 1.983447 0.8651185
|Yi hat - Yi|            2.184563 0.5436367
|Yi - median(Y)|         2.976277 1.0091661


Ratios of Mean/Median |Differences|

                                               Mean    Median
|Yi hat - median(Y hat)|/|Yi - median(Y)| 0.6664189 0.8572607
|Yi hat - Yi|/|Yi - median(Y)|            0.7339920 0.5386989
> rm(x1,x2,y,f)
> 
> 
> 
> cleanEx()
> nameEx("addMarginal")
> ### * addMarginal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: addMarginal
> ### Title: Add Marginal Observations
> ### Aliases: addMarginal
> ### Keywords: utilities manip
> 
> ### ** Examples
> 
> d <- expand.grid(sex=c('female', 'male'), country=c('US', 'Romania'),
+                  reps=1:2)
> addMarginal(d, sex, country)
      sex country reps  .marginal.
1  female      US    1            
2    male      US    1            
3  female Romania    1            
4    male Romania    1            
5  female      US    2            
6    male      US    2            
7  female Romania    2            
8    male Romania    2            
9     All      US    1         sex
10    All      US    1         sex
11    All Romania    1         sex
12    All Romania    1         sex
13    All      US    2         sex
14    All      US    2         sex
15    All Romania    2         sex
16    All Romania    2         sex
17 female     All    1     country
18   male     All    1     country
19 female     All    1     country
20   male     All    1     country
21 female     All    2     country
22   male     All    2     country
23 female     All    2     country
24   male     All    2     country
25    All     All    1 sex,country
26    All     All    1 sex,country
27    All     All    1 sex,country
28    All     All    1 sex,country
29    All     All    2 sex,country
30    All     All    2 sex,country
31    All     All    2 sex,country
32    All     All    2 sex,country
> 
> # Example of nested variables
> d <- data.frame(state=c('AL', 'AL', 'GA', 'GA', 'GA'),
+                 city=c('Mobile', 'Montgomery', 'Valdosto',
+                        'Augusta', 'Atlanta'),
+                 x=1:5, stringsAsFactors=TRUE)
> addMarginal(d, state, nested=city) # cite set to 'All' when state is
   state       city x .marginal.
1     AL     Mobile 1           
2     AL Montgomery 2           
3     GA   Valdosto 3           
4     GA    Augusta 4           
5     GA    Atlanta 5           
6    All        All 1      state
7    All        All 2      state
8    All        All 3      state
9    All        All 4      state
10   All        All 5      state
> 
> 
> 
> cleanEx()
> nameEx("all.is.numeric")
> ### * all.is.numeric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: all.is.numeric
> ### Title: Check if All Elements in Character Vector are Numeric
> ### Aliases: all.is.numeric
> ### Keywords: character
> 
> ### ** Examples
> 
> all.is.numeric(c('1','1.2','3'))
[1] TRUE
> all.is.numeric(c('1','1.2','3a'))
[1] FALSE
> all.is.numeric(c('1','1.2','3'),'vector')
[1] 1.0 1.2 3.0
> all.is.numeric(c('1','1.2','3a'),'vector')
[1] "1"   "1.2" "3a" 
> all.is.numeric(c('1','',' .'),'vector')
[1]  1 NA NA
> all.is.numeric(c('1', '1.2', '3a'), 'nonnum')
[1] "3a"
> 
> 
> 
> cleanEx()
> nameEx("approxExtrap")
> ### * approxExtrap
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: approxExtrap
> ### Title: Linear Extrapolation
> ### Aliases: approxExtrap
> ### Keywords: arith dplot
> 
> ### ** Examples
> 
> approxExtrap(1:3,1:3,xout=c(0,4))
$x
[1] 0 4

$y
[1] 0 4

> 
> 
> 
> cleanEx()
> nameEx("areg")
> ### * areg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: areg
> ### Title: Additive Regression with Optimal Transformations on Both Sides
> ###   using Canonical Variates
> ### Aliases: areg print.areg predict.areg plot.areg
> ### Keywords: smooth regression multivariate models
> 
> ### ** Examples
> 
> set.seed(1)
> 
> ns <- c(30,300,3000)
> for(n in ns) {
+   y <- sample(1:5, n, TRUE)
+   x <- abs(y-3) + runif(n)
+   par(mfrow=c(3,4))
+   for(k in c(0,3:5)) {
+     z <- areg(x, y, ytype='c', nk=k)
+     plot(x, z$tx)
+ 	title(paste('R2=',format(z$rsquared)))
+     tapply(z$ty, y, range)
+     a <- tapply(x,y,mean)
+     b <- tapply(z$ty,y,mean)
+     plot(a,b)
+ 	abline(lsfit(a,b))
+     # Should get same result to within linear transformation if reverse x and y
+     w <- areg(y, x, xtype='c', nk=k)
+     plot(z$ty, w$tx)
+     title(paste('R2=',format(w$rsquared)))
+     abline(lsfit(z$ty, w$tx))
+  }
+ }
> 
> par(mfrow=c(2,2))
> # Example where one category in y differs from others but only in variance of x
> n <- 50
> y <- sample(1:5,n,TRUE)
> x <- rnorm(n)
> x[y==1] <- rnorm(sum(y==1), 0, 5)
> z <- areg(x,y,xtype='l',ytype='c')
> z

N: 50 	 0  observations with NAs deleted.
R^2: 0.155	nk: 4	Mean and Median |error|: 2.2, 2


  type d.f.
x    l    1

y type: c 	d.f.: 4 

> plot(z)
> z <- areg(x,y,ytype='c')
> z

N: 50 	 0  observations with NAs deleted.
R^2: 0.756	nk: 4	Mean and Median |error|: 2.2, 2


  type d.f.
x    s    3

y type: c 	d.f.: 4 

> plot(z)
> 
> ## Not run: 
> ##D 		
> ##D # Examine overfitting when true transformations are linear
> ##D par(mfrow=c(4,3))
> ##D for(n in c(200,2000)) {
> ##D   x <- rnorm(n); y <- rnorm(n) + x
> ##D     for(nk in c(0,3,5)) {
> ##D     z <- areg(x, y, nk=nk, crossval=10, B=100)
> ##D     print(z)
> ##D     plot(z)
> ##D     title(paste('n=',n))
> ##D   }
> ##D }
> ##D par(mfrow=c(1,1))
> ##D 
> ##D # Underfitting when true transformation is quadratic but overfitting
> ##D # when y is allowed to be transformed
> ##D set.seed(49)
> ##D n <- 200
> ##D x <- rnorm(n); y <- rnorm(n) + .5*x^2
> ##D #areg(x, y, nk=0, crossval=10, B=100)
> ##D #areg(x, y, nk=4, ytype='l', crossval=10, B=100)
> ##D z <- areg(x, y, nk=4) #, crossval=10, B=100)
> ##D z
> ##D # Plot x vs. predicted value on original scale.  Since y-transform is
> ##D # not monotonic, there are multiple y-inverses
> ##D xx <- seq(-3.5,3.5,length=1000)
> ##D yhat <- predict(z, xx, type='fitted')
> ##D plot(x, y, xlim=c(-3.5,3.5))
> ##D for(j in 1:ncol(yhat)) lines(xx, yhat[,j], col=j)
> ##D # Plot a random sample of possible y inverses
> ##D yhats <- predict(z, xx, type='fitted', what='sample')
> ##D points(xx, yhats, pch=2)
> ## End(Not run)
> 
> # True transformation of x1 is quadratic, y is linear
> n <- 200
> x1 <- rnorm(n); x2 <- rnorm(n); y <- rnorm(n) + x1^2
> z <- areg(cbind(x1,x2),y,xtype=c('s','l'),nk=3)
> par(mfrow=c(2,2))
> plot(z)
> 
> # y transformation is inverse quadratic but areg gets the same answer by
> # making x1 quadratic
> n <- 5000
> x1 <- rnorm(n); x2 <- rnorm(n); y <- (x1 + rnorm(n))^2
> z <- areg(cbind(x1,x2),y,nk=5)
> par(mfrow=c(2,2))
> plot(z)
> 
> # Overfit 20 predictors when no true relationships exist
> n <- 1000
> x <- matrix(runif(n*20),n,20)
> y <- rnorm(n)
> z <- areg(x, y, nk=5)  # add crossval=4 to expose the problem
> 
> # Test predict function
> n <- 50
> x <- rnorm(n)
> y <- rnorm(n) + x
> g <- sample(1:3, n, TRUE)
> z <- areg(cbind(x,g),y,xtype=c('s','c'))
> range(predict(z, cbind(x,g)) - z$linear.predictors)
[1] 0 0
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("aregImpute")
> ### * aregImpute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: aregImpute
> ### Title: Multiple Imputation using Additive Regression, Bootstrapping,
> ###   and Predictive Mean Matching
> ### Aliases: aregImpute print.aregImpute plot.aregImpute reformM
> ### Keywords: smooth regression multivariate methods models
> 
> ### ** Examples
> 
> # Check that aregImpute can almost exactly estimate missing values when
> # there is a perfect nonlinear relationship between two variables
> # Fit restricted cubic splines with 4 knots for x1 and x2, linear for x3
> set.seed(3)
> x1 <- rnorm(200)
> x2 <- x1^2
> x3 <- runif(200)
> m <- 30
> x2[1:m] <- NA
> a <- aregImpute(~x1+x2+I(x3), n.impute=5, nk=4, match='closest')
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 
> a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~x1 + x2 + I(x3), n.impute = 5, nk = 4, 
    match = "closest")

n: 200 	p: 3 	Imputations: 5  	nk: 4 

Number of NAs:
x1 x2 x3 
 0 30  0 

   type d.f.
x1    s    3
x2    s    1
x3    l    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x2 
0.984 
> matplot(x1[1:m]^2, a$imputed$x2)
> abline(a=0, b=1, lty=2)
> 
> x1[1:m]^2
 [1] 0.925315897 0.085571299 0.066971341 1.327407883 0.038330915 0.000907452
 [7] 0.007296189 1.246818367 1.485613400 1.606223478 0.554699626 1.279655455
[13] 0.513169486 0.063833220 0.023117897 0.094652479 0.908242033 0.420218743
[19] 1.498943851 0.039924679 0.334643416 0.887930672 0.041505171 2.777138392
[25] 0.234696753 0.549188688 1.347028987 1.024279865 0.005195306 1.292273993
> a$imputed$x2
          [,1]        [,2]         [,3]         [,4]       [,5]
1  0.958996702 0.930577069 9.762484e-01 9.799896e-01 0.93057707
2  0.149052972 0.146963621 1.661947e-01 1.353046e-01 0.12388046
3  0.056748811 0.056748811 7.148054e-02 2.631055e-02 0.12140884
4  1.278855629 1.262597928 1.379068e+00 1.379068e+00 1.26259793
5  0.035671951 0.035671951 4.655437e-02 3.567195e-02 0.04379575
6  0.004982439 0.035671951 1.463217e-03 4.099071e-05 0.05595187
7  0.035671951 0.034669484 4.099071e-05 4.099071e-05 0.04379575
8  1.262597928 1.262597928 1.262598e+00 1.262598e+00 1.18470752
9  1.379068228 1.379068228 1.499170e+00 1.591427e+00 1.37906823
10 1.661538043 1.667583069 1.661538e+00 1.667583e+00 1.48316856
11 0.618593060 0.618593060 6.164482e-01 6.164482e-01 0.51992134
12 1.262597928 1.184707523 1.355782e+00 1.354657e+00 1.18470752
13 0.594890076 0.594890076 5.948901e-01 5.948901e-01 0.55889449
14 0.026310553 0.046554372 3.883560e-02 5.108231e-02 0.10438988
15 0.020493742 0.002514871 3.466948e-02 2.514871e-03 0.06568317
16 0.166194673 0.168263518 1.682635e-01 1.469636e-01 0.14905297
17 0.930577069 0.930577069 9.589967e-01 9.698616e-01 0.91224047
18 0.497752140 0.497752140 4.659193e-01 4.505450e-01 0.45543010
19 1.479656909 1.591426720 1.479657e+00 1.543022e+00 1.37906823
20 0.042739772 0.034669484 4.273977e-02 4.982439e-03 0.10438988
21 0.422393723 0.422393723 4.223937e-01 4.223937e-01 0.38112108
22 0.930577069 0.912240467 9.589967e-01 9.589967e-01 0.86794372
23 0.104389875 0.078255416 4.379575e-02 7.133446e-02 0.10438988
24 2.562633045 2.228492325 2.921218e+00 2.973363e+00 2.66392498
25 0.250603132 0.250603132 3.289231e-01 3.289231e-01 0.26743794
26 0.618593060 0.618593060 6.300569e-01 6.300569e-01 0.51992134
27 1.337073813 1.354657372 1.337074e+00 1.355782e+00 1.27885563
28 0.980088775 1.035227246 9.799896e-01 9.799896e-01 0.97998959
29 0.034669484 0.034669484 4.273977e-02 1.634461e-02 0.02460284
30 1.262597928 1.262597928 1.355782e+00 1.354657e+00 1.26259793
> 
> 
> # Multiple imputation and estimation of variances and covariances of
> # regression coefficient estimates accounting for imputation
> # Example 1: large sample size, much missing data, no overlap in
> # NAs across variables
> x1 <- factor(sample(c('a','b','c'),1000,TRUE))
> x2 <- (x1=='b') + 3*(x1=='c') + rnorm(1000,0,2)
> x3 <- rnorm(1000)
> y  <- x2 + 1*(x1=='c') + .2*x3 + rnorm(1000,0,2)
> orig.x1 <- x1[1:250]
> orig.x2 <- x2[251:350]
> x1[1:250] <- NA
> x2[251:350] <- NA
> d <- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
> # Find value of nk that yields best validating imputation models
> # tlinear=FALSE means to not force the target variable to be linear
> f <- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), tlinear=FALSE,
+                 data=d, B=10) # normally B=75
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> f

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~y + x1 + x2 + x3, data = d, nk = c(0, 3:5), 
    tlinear = FALSE, B = 10)

n: 1000 	p: 4 	Imputations: 5  	nk: 0 

Number of NAs:
  y  x1  x2  x3 
  0 250 100   0 

   type d.f.
y     s    1
x1    c    2
x2    s    1
x3    s    1

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x1    x2 
0.331 0.611 

Resampling results for determining the complexity of imputation models

Variable being imputed: x1 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.327 0.332 0.351 0.346
10-fold cross-validated  R^2            0.340 0.351 0.351 0.342
Bootstrap bias-corrected mean   |error| 1.072 1.069 1.062 1.067
10-fold cross-validated  mean   |error| 0.489 0.486 0.494 0.498
Bootstrap bias-corrected median |error| 1.000 1.000 1.000 1.000
10-fold cross-validated  median |error| 0.000 0.200 0.000 0.200

Variable being imputed: x2 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.652 0.638 0.635 0.647
10-fold cross-validated  R^2            0.635 0.643 0.635 0.634
Bootstrap bias-corrected mean   |error| 1.140 1.196 1.200 1.193
10-fold cross-validated  mean   |error| 1.712 1.203 1.202 1.202
Bootstrap bias-corrected median |error| 0.984 0.962 0.997 0.985
10-fold cross-validated  median |error| 1.436 0.980 1.007 0.995


> # Try forcing target variable (x1, then x2) to be linear while allowing
> # predictors to be nonlinear (could also say tlinear=TRUE)
> f <- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), data=d, B=10)
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> f

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~y + x1 + x2 + x3, data = d, nk = c(0, 3:5), 
    B = 10)

n: 1000 	p: 4 	Imputations: 5  	nk: 0 

Number of NAs:
  y  x1  x2  x3 
  0 250 100   0 

   type d.f.
y     s    1
x1    c    2
x2    s    1
x3    s    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x1    x2 
0.358 0.621 

Resampling results for determining the complexity of imputation models

Variable being imputed: x1 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.334 0.336 0.341 0.342
10-fold cross-validated  R^2            0.329 0.334 0.337 0.336
Bootstrap bias-corrected mean   |error| 1.058 1.061 1.052 1.052
10-fold cross-validated  mean   |error| 0.492 0.486 0.485 0.488
Bootstrap bias-corrected median |error| 1.000 1.000 1.000 1.000
10-fold cross-validated  median |error| 0.100 0.300 0.100 0.100

Variable being imputed: x2 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.633 0.631 0.635 0.618
10-fold cross-validated  R^2            0.629 0.629 0.626 0.627
Bootstrap bias-corrected mean   |error| 1.165 1.168 1.174 1.199
10-fold cross-validated  mean   |error| 1.736 1.731 1.742 1.737
Bootstrap bias-corrected median |error| 1.005 1.007 1.010 1.020
10-fold cross-validated  median |error| 1.450 1.445 1.454 1.464


> 
> ## Not run: 
> ##D # Use 100 imputations to better check against individual true values
> ##D f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)
> ##D f
> ##D par(mfrow=c(2,1))
> ##D plot(f)
> ##D modecat <- function(u) {
> ##D  tab <- table(u)
> ##D  as.numeric(names(tab)[tab==max(tab)][1])
> ##D }
> ##D table(orig.x1,apply(f$imputed$x1, 1, modecat))
> ##D par(mfrow=c(1,1))
> ##D plot(orig.x2, apply(f$imputed$x2, 1, mean))
> ##D fmi <- fit.mult.impute(y ~ x1 + x2 + x3, lm, f, 
> ##D                        data=d)
> ##D sqrt(diag(vcov(fmi)))
> ##D fcc <- lm(y ~ x1 + x2 + x3)
> ##D summary(fcc)   # SEs are larger than from mult. imputation
> ## End(Not run)
> ## Not run: 
> ##D # Example 2: Very discriminating imputation models,
> ##D # x1 and x2 have some NAs on the same rows, smaller n
> ##D set.seed(5)
> ##D x1 <- factor(sample(c('a','b','c'),100,TRUE))
> ##D x2 <- (x1=='b') + 3*(x1=='c') + rnorm(100,0,.4)
> ##D x3 <- rnorm(100)
> ##D y  <- x2 + 1*(x1=='c') + .2*x3 + rnorm(100,0,.4)
> ##D orig.x1 <- x1[1:20]
> ##D orig.x2 <- x2[18:23]
> ##D x1[1:20] <- NA
> ##D x2[18:23] <- NA
> ##D #x2[21:25] <- NA
> ##D d <- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
> ##D n <- naclus(d)
> ##D plot(n); naplot(n)  # Show patterns of NAs
> ##D # 100 imputations to study them; normally use 5 or 10
> ##D f  <- aregImpute(~y + x1 + x2 + x3, n.impute=100, nk=0, data=d)
> ##D par(mfrow=c(2,3))
> ##D plot(f, diagnostics=TRUE, maxn=2)
> ##D # Note: diagnostics=TRUE makes graphs similar to those made by:
> ##D # r <- range(f$imputed$x2, orig.x2)
> ##D # for(i in 1:6) {  # use 1:2 to mimic maxn=2
> ##D #   plot(1:100, f$imputed$x2[i,], ylim=r,
> ##D #        ylab=paste("Imputations for Obs.",i))
> ##D #   abline(h=orig.x2[i],lty=2)
> ##D # }
> ##D 
> ##D table(orig.x1,apply(f$imputed$x1, 1, modecat))
> ##D par(mfrow=c(1,1))
> ##D plot(orig.x2, apply(f$imputed$x2, 1, mean))
> ##D 
> ##D 
> ##D fmi <- fit.mult.impute(y ~ x1 + x2, lm, f, 
> ##D                        data=d)
> ##D sqrt(diag(vcov(fmi)))
> ##D fcc <- lm(y ~ x1 + x2)
> ##D summary(fcc)   # SEs are larger than from mult. imputation
> ## End(Not run)
> 
> ## Not run: 
> ##D # Study relationship between smoothing parameter for weighting function
> ##D # (multiplier of mean absolute distance of transformed predicted
> ##D # values, used in tricube weighting function) and standard deviation
> ##D # of multiple imputations.  SDs are computed from average variances
> ##D # across subjects.  match="closest" same as match="weighted" with
> ##D # small value of fweighted.
> ##D # This example also shows problems with predicted mean
> ##D # matching almost always giving the same imputed values when there is
> ##D # only one predictor (regression coefficients change over multiple
> ##D # imputations but predicted values are virtually 1-1 functions of each
> ##D # other)
> ##D 
> ##D set.seed(23)
> ##D x <- runif(200)
> ##D y <- x + runif(200, -.05, .05)
> ##D r <- resid(lsfit(x,y))
> ##D rmse <- sqrt(sum(r^2)/(200-2))   # sqrt of residual MSE
> ##D 
> ##D y[1:20] <- NA
> ##D d <- data.frame(x,y)
> ##D f <- aregImpute(~ x + y, n.impute=10, match='closest', data=d)
> ##D # As an aside here is how to create a completed dataset for imputation
> ##D # number 3 as fit.mult.impute would do automatically.  In this degenerate
> ##D # case changing 3 to 1-2,4-10 will not alter the results.
> ##D imputed <- impute.transcan(f, imputation=3, data=d, list.out=TRUE,
> ##D                            pr=FALSE, check=FALSE)
> ##D sd <- sqrt(mean(apply(f$imputed$y, 1, var)))
> ##D 
> ##D ss <- c(0, .01, .02, seq(.05, 1, length=20))
> ##D sds <- ss; sds[1] <- sd
> ##D 
> ##D for(i in 2:length(ss)) {
> ##D   f <- aregImpute(~ x + y, n.impute=10, fweighted=ss[i])
> ##D   sds[i] <- sqrt(mean(apply(f$imputed$y, 1, var)))
> ##D }
> ##D 
> ##D plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
> ##D      type='b')
> ##D abline(v=.2,  lty=2)  # default value of fweighted
> ##D abline(h=rmse, lty=2)  # root MSE of residuals from linear regression
> ## End(Not run)
> 
> ## Not run: 
> ##D # Do a similar experiment for the Titanic dataset
> ##D getHdata(titanic3)
> ##D h <- lm(age ~ sex + pclass + survived, data=titanic3)
> ##D rmse <- summary(h)$sigma
> ##D set.seed(21)
> ##D f <- aregImpute(~ age + sex + pclass + survived, n.impute=10,
> ##D                 data=titanic3, match='closest')
> ##D sd <- sqrt(mean(apply(f$imputed$age, 1, var)))
> ##D 
> ##D ss <- c(0, .01, .02, seq(.05, 1, length=20))
> ##D sds <- ss; sds[1] <- sd
> ##D 
> ##D for(i in 2:length(ss)) {
> ##D   f <- aregImpute(~ age + sex + pclass + survived, data=titanic3,
> ##D                   n.impute=10, fweighted=ss[i])
> ##D   sds[i] <- sqrt(mean(apply(f$imputed$age, 1, var)))
> ##D }
> ##D 
> ##D plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
> ##D      type='b')
> ##D abline(v=.2,   lty=2)  # default value of fweighted
> ##D abline(h=rmse, lty=2)  # root MSE of residuals from linear regression
> ## End(Not run)
> 
> 
> set.seed(2)
> d <- data.frame(x1=runif(50), x2=c(rep(NA, 10), runif(40)),
+                 x3=c(runif(4), rep(NA, 11), runif(35)))
> reformM(~ x1 + x2 + x3, data=d)
Recommended number of imputations: 30 
~x3 + x2 + x1
<environment: 0x5781d233e060>
> reformM(~ x1 + x2 + x3, data=d, nperm=2)
Recommended number of imputations: 30 
[[1]]
~x1 + x2 + x3
<environment: 0x5781d237d938>

[[2]]
~x1 + x3 + x2
<environment: 0x5781d237d938>

> # Give result or one of the results as the first argument to aregImpute
> 
> # Constrain imputed values for two variables
> # Require imputed values for x2 to be above 0.2
> # Assume x1 is never missing and require imputed values for
> # x3 to be less than the recipient's value of x1
> a <- aregImpute(~ x1 + x2 + x3, data=d,
+                 constraint=list(x2 = expression(d$x2 > 0.2),
+                                 x3 = expression(d$x3 < r$x1)))
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~x1 + x2 + x3, data = d, constraint = list(x2 = expression(d$x2 > 
    0.2), x3 = expression(d$x3 < r$x1)))

n: 50 	p: 3 	Imputations: 5  	nk: 3 

Number of NAs:
x1 x2 x3 
 0 10 11 

   type d.f.
x1    s    2
x2    s    2
x3    s    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x2    x3 
0.128 0.100 

Frequency distributions of number of potential donor observations
meeting constraints

x2 

32 
10 

x3 

 6  7  9 16 21 29 32 35 
 1  1  1  1  3  1  1  2 

> 
> 
> 
> cleanEx()
> nameEx("biVar")
> ### * biVar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biVar
> ### Title: Bivariate Summaries Computed Separately by a Series of
> ###   Predictors
> ### Aliases: biVar print.biVar plot.biVar spearman2 spearman2.default
> ###   spearman2.formula spearman spearman.test chiSquare
> ### Keywords: nonparametric htest category
> 
> ### ** Examples
> 
> x <- c(-2, -1, 0, 1, 2)
> y <- c(4,   1, 0, 1, 4)
> z <- c(1,   2, 3, 4, NA)
> v <- c(1,   2, 3, 4, 5)
> 
> spearman2(x, y)
         rho2             F           df1           df2             P 
    0.0000000     0.0000000     1.0000000     3.0000000     1.0000000 
            n Adjusted rho2 
    5.0000000    -0.3333333 
> plot(spearman2(z ~ x + y + v, p=2))
> 
> f <- chiSquare(z ~ x + y + v)
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
> f

Pearson Chi-square Tests    Response variable:z

  chisquare df chisquare-df      P n
x         8  6            2 0.2381 4
y         8  6            2 0.2381 4
v         8  6            2 0.2381 4
> 
> 
> 
> cleanEx()
> nameEx("binconf")
> ### * binconf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binconf
> ### Title: Confidence Intervals for Binomial Probabilities
> ### Aliases: binconf
> ### Keywords: category htest
> 
> ### ** Examples
> 
> binconf(0:10,10,include.x=TRUE,include.n=TRUE)
  X  N PointEst       Lower     Upper
  0 10      0.0 0.000000000 0.2775328
  1 10      0.1 0.005129329 0.4041500
  2 10      0.2 0.056682151 0.5098375
  3 10      0.3 0.107791267 0.6032219
  4 10      0.4 0.168180330 0.6873262
  5 10      0.5 0.236593091 0.7634069
  6 10      0.6 0.312673770 0.8318197
  7 10      0.7 0.396778147 0.8922087
  8 10      0.8 0.490162472 0.9433178
  9 10      0.9 0.595849973 0.9948707
 10 10      1.0 0.722467200 1.0000000
> binconf(46,50,method="all")
           PointEst     Lower     Upper
Exact          0.92 0.8076572 0.9777720
Wilson         0.92 0.8116175 0.9684505
Asymptotic     0.92 0.8448027 0.9951973
> 
> 
> 
> cleanEx()
> nameEx("bootkm")
> ### * bootkm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootkm
> ### Title: Bootstrap Kaplan-Meier Estimates
> ### Aliases: bootkm
> ### Keywords: survival nonparametric
> 
> ### ** Examples
> 
> # Compute 0.95 nonparametric confidence interval for the difference in
> # median survival time between females and males (two-sample problem)
> set.seed(1)
> library(survival)
> S <- Surv(runif(200))      # no censoring
> sex <- c(rep('female',100),rep('male',100))
> med.female <- bootkm(S[sex=='female',], B=100) # normally B=500
10 20 30 40 50 60 70 80 90 100 
> med.male   <- bootkm(S[sex=='male',],   B=100)
10 20 30 40 50 60 70 80 90 100 
> describe(med.female-med.male)
med.female - med.male 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
     100        0       87        1 -0.01575  0.08495 -0.12179 -0.11216 
     .25      .50      .75      .90      .95 
-0.08030 -0.01734  0.01819  0.09412  0.15126 

lowest : -0.139027 -0.136873 -0.136775 -0.122804 -0.121741
highest: 0.151802  0.152362  0.154363  0.157939  0.160357 
> quantile(med.female-med.male, c(.025,.975), na.rm=TRUE)
      2.5%      97.5% 
-0.1301387  0.1534129 
> # na.rm needed because some bootstrap estimates of median survival
> # time may be missing when a bootstrap sample did not include the
> # longer survival times
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("bpower")
> ### * bpower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bpower
> ### Title: Power and Sample Size for Two-Sample Binomial Test
> ### Aliases: bpower bsamsize ballocation bpower.sim
> ### Keywords: htest category
> 
> ### ** Examples
> 
> bpower(.1, odds.ratio=.9, n=1000, alpha=c(.01,.05))
    Power1     Power2 
0.01953539 0.07780432 
> bpower.sim(.1, odds.ratio=.9, n=1000)
     Power      Lower      Upper 
0.07920000 0.07390701 0.08449299 
> bsamsize(.1, .05, power=.95)
      n1       n2 
718.2381 718.2381 
> ballocation(.1, .5, n=100)
   fraction.group1.min.var.diff   fraction.group1.min.var.ratio 
                      0.3750000                       0.7500000 
fraction.group1.min.var.logodds       fraction.group1.max.power 
                      0.6250000                       0.4745255 
> 
> 
> # Plot power vs. n for various odds ratios  (base prob.=.1)
> n  <- seq(10, 1000, by=10)
> OR <- seq(.2,.9,by=.1)
> plot(0, 0, xlim=range(n), ylim=c(0,1), xlab="n", ylab="Power", type="n")
> for(or in OR) {
+   lines(n, bpower(.1, odds.ratio=or, n=n))
+   text(350, bpower(.1, odds.ratio=or, n=350)-.02, format(or))
+ }
> 
> 
> # Another way to plot the same curves, but letting labcurve do the
> # work, including labeling each curve at points of maximum separation
> pow <- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
+               n=n)
> names(pow) <- format(OR)
> labcurve(pow, pl=TRUE, xlab='n', ylab='Power')
> 
> 
> # Contour graph for various probabilities of outcome in the control
> # group, fixing the odds ratio at .8 ([p2/(1-p2) / p1/(1-p1)] = .8)
> # n is varied also
> p1 <- seq(.01,.99,by=.01)
> n  <- seq(100,5000,by=250)
> pow <- outer(p1, n, function(p1,n) bpower(p1, n=n, odds.ratio=.8))
> # This forms a length(p1)*length(n) matrix of power estimates
> contour(p1, n, pow)
> 
> 
> 
> cleanEx()
> nameEx("bpplot")
> ### * bpplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bpplot
> ### Title: Box-percentile plots
> ### Aliases: bpplot
> ### Keywords: nonparametric hplot
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(500)
> x2 <- runif(500, -2, 2)
> x3 <- abs(rnorm(500))-2
> bpplot(x1, x2, x3)
> g <- sample(1:2, 500, replace=TRUE)
> bpplot(split(x2, g), name=c('Group 1','Group 2'))
> rm(x1,x2,x3,g)
> 
> 
> 
> cleanEx()
> nameEx("bystats")
> ### * bystats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bystats
> ### Title: Statistics by Categories
> ### Aliases: bystats print.bystats latex.bystats bystats2 print.bystats2
> ###   latex.bystats2
> ### Keywords: category
> 
> ### ** Examples
> 
> ## Not run: 
> ##D bystats(sex==2, county, city)
> ##D bystats(death, race)
> ##D bystats(death, cut2(age,g=5), race)
> ##D bystats(cholesterol, cut2(age,g=4), sex, fun=median)
> ##D bystats(cholesterol, sex, fun=quantile)
> ##D bystats(cholesterol, sex, fun=function(x)c(Mean=mean(x),Median=median(x)))
> ##D latex(bystats(death,race,nmiss=FALSE,subset=sex=="female"), digits=2)
> ##D f <- function(y) c(Hazard=sum(y[,2])/sum(y[,1]))
> ##D # f() gets the hazard estimate for right-censored data from exponential dist.
> ##D bystats(cbind(d.time, death), race, sex, fun=f)
> ##D bystats(cbind(pressure, cholesterol), age.decile, 
> ##D         fun=function(y) c(Median.pressure   =median(y[,1]),
> ##D                           Median.cholesterol=median(y[,2])))
> ##D y <- cbind(pressure, cholesterol)
> ##D bystats(y, age.decile, 
> ##D         fun=function(y) apply(y, 2, median))   # same result as last one
> ##D bystats(y, age.decile, fun=function(y) apply(y, 2, quantile, c(.25,.75)))
> ##D # The last one computes separately the 0.25 and 0.75 quantiles of 2 vars.
> ##D latex(bystats2(death, race, sex, fun=table))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("capitalize")
> ### * capitalize
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: capitalize
> ### Title: capitalize the first letter of a string
> ### Aliases: capitalize
> ### Keywords: manip character
> 
> ### ** Examples
> 
> capitalize(c("Hello", "bob", "daN"))
[1] "Hello" "Bob"   "DaN"  
> 
> 
> 
> cleanEx()
> nameEx("ciapower")
> ### * ciapower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ciapower
> ### Title: Power of Interaction Test for Exponential Survival
> ### Aliases: ciapower
> ### Keywords: survival htest
> 
> ### ** Examples
> 
> # Find the power of a race x treatment test.  25% of patients will
> # be non-white and the total sample size is 14000.  
> # Accrual is for 1.5 years and minimum follow-up is 5y.
> # Reduction in 5-year mortality is 15% for whites, 0% or -5% for
> # non-whites.  5-year mortality for control subjects if assumed to
> # be 0.18 for whites, 0.23 for non-whites.
> n <- 14000
> for(nonwhite.reduction in c(0,-5)) {
+   cat("\n\n\n% Reduction in 5-year mortality for non-whites:",
+       nonwhite.reduction, "\n\n")
+   pow <- ciapower(5,  .75*n, .25*n,  .18, .23,  15, nonwhite.reduction,  
+                   1.5, 5)
+   cat("\n\nPower:",format(pow),"\n")
+ }



% Reduction in 5-year mortality for non-whites: 0 


Accrual duration: 1.5 y  Minimum follow-up: 5 y

Sample size Stratum 1: 10500   Stratum 2: 3500 

Alpha= 0.05 

5-year Mortalities
          Control Intervention
Stratum 1    0.18        0.153
Stratum 2    0.23        0.230

Hazard Rates
             Control Intervention
Stratum 1 0.03969019   0.03321092
Stratum 2 0.05227295   0.05227295

Probabilities of an Event During Study
            Control Intervention
Stratum 1 0.2039322    0.1737512
Stratum 2 0.2594139    0.2594139

Expected Number of Events
          Control Intervention
Stratum 1  1070.6        912.2
Stratum 2   454.0        454.0

Ratio of hazard ratios: 0.8367538 
Standard deviation of log ratio of ratios: 0.08022351 


Power: 0.6032173 



% Reduction in 5-year mortality for non-whites: -5 


Accrual duration: 1.5 y  Minimum follow-up: 5 y

Sample size Stratum 1: 10500   Stratum 2: 3500 

Alpha= 0.05 

5-year Mortalities
          Control Intervention
Stratum 1    0.18       0.1530
Stratum 2    0.23       0.2415

Hazard Rates
             Control Intervention
Stratum 1 0.03969019   0.03321092
Stratum 2 0.05227295   0.05528250

Probabilities of an Event During Study
            Control Intervention
Stratum 1 0.2039322    0.1737512
Stratum 2 0.2594139    0.2720973

Expected Number of Events
          Control Intervention
Stratum 1  1070.6        912.2
Stratum 2   454.0        476.2

Ratio of hazard ratios: 0.7912015 
Standard deviation of log ratio of ratios: 0.07958098 


Power: 0.8371925 
> 
> 
> 
> cleanEx()
> nameEx("cnvrt.coords")
> ### * cnvrt.coords
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cnvrt.coords
> ### Title: Convert between the 5 different coordinate sytems on a graphical
> ###   device
> ### Aliases: cnvrt.coords
> ### Keywords: dplot aplot
> 
> ### ** Examples
> 
> 
> old.par <- par(no.readonly=TRUE)
> 
> par(mfrow=c(2,2),xpd=NA)
> 
> # generate some sample data
> tmp.x <- rnorm(25, 10, 2)
> tmp.y <- rnorm(25, 50, 10)
> tmp.z <- rnorm(25, 0, 1)
> 
> plot( tmp.x, tmp.y)
> 
> # draw a diagonal line across the plot area
> tmp1 <- cnvrt.coords( c(0,1), c(0,1), input='plt' )
> lines(tmp1$usr, col='blue')
> 
> # draw a diagonal line accross figure region
> tmp2 <- cnvrt.coords( c(0,1), c(1,0), input='fig')
> lines(tmp2$usr, col='red')
> 
> # save coordinate of point 1 and y value near top of plot for future plots
> tmp.point1 <- cnvrt.coords(tmp.x[1], tmp.y[1])
> tmp.range1 <- cnvrt.coords(NA, 0.98, input='plt')
> 
> # make a second plot and draw a line linking point 1 in each plot
> plot(tmp.y, tmp.z)
> 
> tmp.point2 <- cnvrt.coords( tmp.point1$dev, input='dev' )
> arrows( tmp.y[1], tmp.z[1], tmp.point2$usr$x, tmp.point2$usr$y,
+  col='green')
> 
> # draw another plot and add rectangle showing same range in 2 plots
> 
> plot(tmp.x, tmp.z)
> tmp.range2 <- cnvrt.coords(NA, 0.02, input='plt')
> tmp.range3 <- cnvrt.coords(NA, tmp.range1$dev$y, input='dev')
> rect( 9, tmp.range2$usr$y, 11, tmp.range3$usr$y, border='yellow')
> 
> # put a label just to the right of the plot and
> #  near the top of the figure region.
> text( cnvrt.coords(1.05, NA, input='plt')$usr$x,
+ 	cnvrt.coords(NA, 0.75, input='fig')$usr$y,
+ 	"Label", adj=0)
> 
> par(mfrow=c(1,1))
> 
> ## create a subplot within another plot (see also subplot)
> 
> plot(1:10, 1:10)
> 
> tmp <- cnvrt.coords( c( 1, 4, 6, 9), c(6, 9, 1, 4) )
> 
> par(plt = c(tmp$dev$x[1:2], tmp$dev$y[1:2]), new=TRUE)
> hist(rnorm(100))
> 
> par(fig = c(tmp$dev$x[3:4], tmp$dev$y[3:4]), new=TRUE)
> hist(rnorm(100))
> 
> par(old.par)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("combine.levels")
> ### * combine.levels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: combine.levels
> ### Title: combine.levels
> ### Aliases: combine.levels
> 
> ### ** Examples
> 
> x <- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1))
> combine.levels(x, m=3)
 [1] OTHER B     B     B     C     C     C     C     OTHER OTHER
Levels: OTHER B C
> combine.levels(x, m=3, plevels=TRUE)
 [1] A,D,E B     B     B     C     C     C     C     A,D,E A,D,E
Levels: A,D,E B C
> combine.levels(x, ord=TRUE, m=3)
 [1] A,B   A,B   A,B   A,B   C,D,E C,D,E C,D,E C,D,E C,D,E C,D,E
Levels: A,B < C,D,E
> x <- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1),
+        rep('F',1))
> combine.levels(x, ord=TRUE, m=3)
 [1] A,B   A,B   A,B   A,B   C     C     C     C     D,E,F D,E,F D,E,F
Levels: A,B < C < D,E,F
> 
> 
> 
> cleanEx()
> nameEx("combplotp")
> ### * combplotp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: combplotp
> ### Title: Combination Plot
> ### Aliases: combplotp
> 
> ### ** Examples
> 
> if (requireNamespace("plotly")) {
+   g <- function() sample(0:1, n, prob=c(1 - p, p), replace=TRUE)
+   set.seed(2); n <- 100; p <- 0.5
+   x1 <- g(); label(x1) <- 'A long label for x1 that describes it'
+   x2 <- g()
+   x3 <- g(); label(x3) <- 'This is<br>a label for x3'
+   x4 <- g()
+   combplotp(~ x1 + x2 + x3 + x4, showno=TRUE, includenone=TRUE)
+ 
+   n <- 1500; p <- 0.05
+   pain       <- g()
+   anxiety    <- g()
+   depression <- g()
+   soreness   <- g()
+   numbness   <- g()
+   tiredness  <- g()
+   sleepiness <- g()
+   combplotp(~ pain + anxiety + depression + soreness + numbness +
+             tiredness + sleepiness, showno=TRUE)
+ }
Loading required namespace: plotly
Failed with error:  ‘there is no package called ‘plotly’’
> 
> 
> 
> cleanEx()
> nameEx("completer")
> ### * completer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: completer
> ### Title: completer
> ### Aliases: completer
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mtcars$hp[1:5]    <- NA
> ##D mtcars$wt[1:10]   <- NA
> ##D myrform <- ~ wt + hp + I(carb)
> ##D mytranscan  <- transcan( myrform,  data = mtcars, imputed = TRUE,
> ##D   pl = FALSE, pr = FALSE, trantab = TRUE, long = TRUE)
> ##D myareg      <- aregImpute(myrform, data = mtcars, x=TRUE, n.impute = 5)
> ##D completer(mytranscan)                    # single completed dataset
> ##D completer(myareg, 3, oneimpute = TRUE)
> ##D # single completed dataset based on the `n.impute`th set of multiple imputation
> ##D completer(myareg, 3)
> ##D # list of completed datasets based on first `nimpute` sets of multiple imputation
> ##D completer(myareg)
> ##D # list of completed datasets based on all available sets of multiple imputation
> ##D # To get a stacked data frame of all completed datasets use
> ##D # do.call(rbind, completer(myareg, data=mydata))
> ##D # or use rbindlist in data.table
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("consolidate")
> ### * consolidate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: consolidate
> ### Title: Element Merging
> ### Aliases: consolidate consolidate<- consolidate.default
> ### Keywords: utilities
> 
> ### ** Examples
> 
> x <- 1:5
> names(x) <- LETTERS[x]
> 
> y <- 6:10
> names(y) <- LETTERS[y-2]
> 
> x                  # c(A=1,B=2,C=3,D=4,E=5)
A B C D E 
1 2 3 4 5 
> y                  # c(D=6,E=7,F=8,G=9,H=10)
 D  E  F  G  H 
 6  7  8  9 10 
> 
> consolidate(x, y)      # c(A=1,B=2,C=3,D=6,E=7,F=8,G=9,H=10)
 A  B  C  D  E  F  G  H 
 1  2  3  6  7  8  9 10 
> consolidate(x, y, protect=TRUE)      # c(A=1,B=2,C=3,D=4,E=5,F=8,G=9,H=10)
 A  B  C  D  E  F  G  H 
 1  2  3  4  5  8  9 10 
> 
> 
> 
> 
> cleanEx()
> nameEx("contents")
> ### * contents
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: contents
> ### Title: Metadata for a Data Frame
> ### Aliases: contents contents.data.frame print.contents.data.frame
> ###   html.contents.data.frame contents.list print.contents.list
> ### Keywords: data interface
> 
> ### ** Examples
> 
> set.seed(1)
> dfr <- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE),
+                   stringsAsFactors=TRUE)
> contents(dfr)

Data frame:dfr	400 observations and 2 variables    Maximum # NAs:0


  Levels Storage
x         double
y      2 integer

+--------+-----------+
|Variable|Levels     |
+--------+-----------+
|    y   |female,male|
+--------+-----------+
> dfr <- upData(dfr, labels=c(x='Label for x', y='Label for y'))
Input object size:	 6160 bytes;	 2 variables	 400 observations
New object size:	6992 bytes;	2 variables	400 observations
> attr(dfr$x, 'longlabel') <-
+  'A very long label for x that can continue onto multiple long lines of text'
> 
> k <- contents(dfr)
> print(k, sort='names', prlevels=FALSE)

Data frame:dfr	400 observations and 2 variables    Maximum # NAs:0


       Labels Levels   Class Storage
x Label for x        numeric  double
y Label for y      2         integer
+--------+------------------------------------------------------------------+
|Variable|                            Long Label                            |
+--------+------------------------------------------------------------------+
|x       |A very long label for x that can continue onto multiple long lines|
|        |                                                           of text|
+--------+------------------------------------------------------------------+
> ## Not run: 
> ##D html(k)
> ##D html(contents(dfr))            # same result
> ##D latex(k$contents)              # latex.default just the main information
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cpower")
> ### * cpower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cpower
> ### Title: Power of Cox/log-rank Two-Sample Test
> ### Aliases: cpower
> ### Keywords: htest survival
> 
> ### ** Examples
> 
> #In this example, 4 plots are drawn on one page, one plot for each
> #combination of noncompliance percentage.  Within a plot, the
> #5-year mortality % in the control group is on the x-axis, and
> #separate curves are drawn for several % reductions in mortality
> #with the intervention.  The accrual period is 1.5y, with all
> #patients followed at least 5y and some 6.5y.
> 
> 
> par(mfrow=c(2,2),oma=c(3,0,3,0))
> 
> 
> morts <- seq(10,25,length=50)
> red <- c(10,15,20,25)
> 
> 
> for(noncomp in c(0,10,15,-1)) {
+   if(noncomp>=0) nc.i <- nc.c <- noncomp else {nc.i <- 25; nc.c <- 15}
+   z <- paste("Drop-in ",nc.c,"%, Non-adherence ",nc.i,"%",sep="")
+   plot(0,0,xlim=range(morts),ylim=c(0,1),
+            xlab="5-year Mortality in Control Patients (%)",
+            ylab="Power",type="n")
+   title(z)
+   cat(z,"\n")
+   lty <- 0
+   for(r in red) {
+         lty <- lty+1
+         power <- morts
+         i <- 0
+         for(m in morts) {
+           i <- i+1
+           power[i] <- cpower(5, 14000, m/100, r, 1.5, 5, nc.c, nc.i, pr=FALSE)
+         }
+         lines(morts, power, lty=lty)
+   }
+   if(noncomp==0)legend(18,.55,rev(paste(red,"% reduction",sep="")),
+            lty=4:1,bty="n")
+ }
Drop-in 0%, Non-adherence 0% 
Drop-in 10%, Non-adherence 10% 
Drop-in 15%, Non-adherence 15% 
Drop-in 15%, Non-adherence 25% 
> mtitle("Power vs Non-Adherence for Main Comparison",
+            ll="alpha=.05, 2-tailed, Total N=14000",cex.l=.8)
> #
> # Point sample size requirement vs. mortality reduction
> # Root finder (uniroot()) assumes needed sample size is between
> # 1000 and 40000
> #
> nc.i <- 25; nc.c <- 15; mort <- .18
> red <- seq(10,25,by=.25)
> samsiz <- red
> 
> 
> i <- 0
> for(r in red) {
+   i <- i+1
+   samsiz[i] <- uniroot(function(x) cpower(5, x, mort, r, 1.5, 5,
+                                           nc.c, nc.i, pr=FALSE) - .8,
+                        c(1000,40000))$root
+ }
> 
> 
> samsiz <- samsiz/1000
> par(mfrow=c(1,1))
> plot(red, samsiz, xlab='% Reduction in 5-Year Mortality',
+ 	 ylab='Total Sample Size (Thousands)', type='n')
> lines(red, samsiz, lwd=2)
> title('Sample Size for Power=0.80\nDrop-in 15%, Non-adherence 25%')
> title(sub='alpha=0.05, 2-tailed', adj=0)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("csv.get")
> ### * csv.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: csv.get
> ### Title: Read Comma-Separated Text Data Files
> ### Aliases: csv.get
> ### Keywords: manip IO file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat <- csv.get('myfile.csv')
> ##D 
> ##D # Read a csv file with junk in the first row, variable names in the
> ##D # second, long variable labels in the third, and junk in the 4th row
> ##D dat <- csv.get('myfile.csv', vnames=2, labels=3, skip=4)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("curveRep")
> ### * curveRep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: curveRep
> ### Title: Representative Curves
> ### Aliases: curveRep print.curveRep plot.curveRep curveSmooth
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Simulate 200 curves with per-curve sample sizes ranging from 1 to 10
> ##D # Make curves with odd-numbered IDs have an x-distribution that is random
> ##D # uniform [0,1] and those with even-numbered IDs have an x-dist. that is
> ##D # half as wide but still centered at 0.5.  Shift y values higher with
> ##D # increasing IDs
> ##D set.seed(1)
> ##D N <- 200
> ##D nc <- sample(1:10, N, TRUE)
> ##D id <- rep(1:N, nc)
> ##D x <- y <- id
> ##D for(i in 1:N) {
> ##D   x[id==i] <- if(i %% 2) runif(nc[i]) else runif(nc[i], c(.25, .75))
> ##D   y[id==i] <- i + 10*(x[id==i] - .5) + runif(nc[i], -10, 10)
> ##D }
> ##D 
> ##D w <- curveRep(x, y, id, kxdist=2, p=10)
> ##D w
> ##D par(ask=TRUE, mfrow=c(4,5))
> ##D plot(w)                # show everything, profiles going across
> ##D par(mfrow=c(2,5))
> ##D plot(w,1)              # show n=1 results
> ##D # Use a color assignment table, assigning low curves to green and
> ##D # high to red.  Unique curve (subject) IDs are the names of the vector.
> ##D cols <- c(rep('green', N/2), rep('red', N/2))
> ##D names(cols) <- as.character(1:N)
> ##D plot(w, 3, idcol=cols)
> ##D par(ask=FALSE, mfrow=c(1,1))
> ##D 
> ##D plot(w, 1, 'lattice')  # show n=1 results
> ##D plot(w, 3, 'lattice')  # show n=4-5 results
> ##D plot(w, 3, 'lattice', idcol=cols)  # same but different color mapping
> ##D plot(w, 3, 'lattice', m=1)  # show a single "representative" curve
> ##D # Show median, 10th, and 90th percentiles of supposedly representative curves
> ##D plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9))
> ##D # Same plot but with much less grouping of x variable
> ##D plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9), nx=2)
> ##D 
> ##D # Use ggplot2 for one sample size interval
> ##D z <- plot(w, 2, 'data')
> ##D require(ggplot2)
> ##D ggplot(z, aes(x, y, color=curve)) + geom_line() +
> ##D        facet_grid(distribution ~ cluster) +
> ##D        theme(legend.position='none') +
> ##D        labs(caption=z$ninterval[1])
> ##D 
> ##D 
> ##D # Smooth data before profiling.  This allows later plotting to plot
> ##D # smoothed representative curves rather than raw curves (which
> ##D # specifying smooth=TRUE to curveRep would do, if curveSmooth was not used)
> ##D d <- curveSmooth(x, y, id)
> ##D w <- with(d, curveRep(x, y, id))
> ##D 
> ##D # Example to show that curveRep can cluster profiles correctly when
> ##D # there is no noise.  In the data there are four profiles - flat, flat
> ##D # at a higher mean y, linearly increasing then flat, and flat at the
> ##D # first height except for a sharp triangular peak
> ##D 
> ##D set.seed(1)
> ##D x <- 0:100
> ##D m <- length(x)
> ##D profile <- matrix(NA, nrow=m, ncol=4)
> ##D profile[,1] <- rep(0, m)
> ##D profile[,2] <- rep(3, m)
> ##D profile[,3] <- c(0:3, rep(3, m-4))
> ##D profile[,4] <- c(0,1,3,1,rep(0,m-4))
> ##D col <- c('black','blue','green','red')
> ##D matplot(x, profile, type='l', col=col)
> ##D xeval <- seq(0, 100, length.out=5)
> ##D s <- x ##D 
> ##D matplot(x[s], profile[s,], type='l', col=col)
> ##D 
> ##D id <- rep(1:100, each=m)
> ##D X <- Y <- id
> ##D cols <- character(100)
> ##D names(cols) <- as.character(1:100)
> ##D for(i in 1:100) {
> ##D   s <- id==i
> ##D   X[s] <- x
> ##D   j <- sample(1:4,1)
> ##D   Y[s] <- profile[,j]
> ##D   cols[i] <- col[j]
> ##D }
> ##D table(cols)
> ##D yl <- c(-1,4)
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=4)
> ##D plot(w, 1, 'lattice', idcol=cols, ylim=yl)
> ##D # Found 4 clusters but two have same profile
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=3)
> ##D plot(w, 1, 'lattice', idcol=cols, freq=cols, plotfreq=TRUE, ylim=yl)
> ##D # Incorrectly combined black and red because default value p=5 did
> ##D # not result in different profiles at x=xeval
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=4, p=40)
> ##D plot(w, 1, 'lattice', idcol=cols, ylim=yl)
> ##D # Found correct clusters because evaluated curves at 40 equally
> ##D # spaced points and could find the sharp triangular peak in profile 4
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cut2")
> ### * cut2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cut2
> ### Title: Cut a Numeric Variable into Intervals
> ### Aliases: cut2
> ### Keywords: category nonparametric
> 
> ### ** Examples
> 
> set.seed(1)
> x <- runif(1000, 0, 100)
> z <- cut2(x, c(10,20,30))
> table(z)
z
[  0.131, 10.000) [ 10.000, 20.000) [ 20.000, 30.000) [ 30.000, 99.993] 
               96               104                93               707 
> table(cut2(x, g=10))      # quantile groups

[ 0.131, 10.5) [10.505, 20.2) [20.168, 31.2) [31.204, 39.8) [39.784, 48.4) 
           100            100            100            100            100 
[48.435, 59.6) [59.645, 70.7) [70.666, 79.7) [79.731, 91.0) [91.037,100.0] 
           100            100            100            100            100 
> table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

[ 0.131,  5.52) [ 5.516, 10.51) [10.505, 15.48) [15.483, 20.17) [20.168, 25.82) 
             50              50              50              50              50 
[25.817, 31.20) [31.204, 35.32) [35.320, 39.78) [39.784, 44.15) [44.146, 48.43) 
             50              50              50              50              50 
[48.435, 52.78) [52.778, 59.64) [59.645, 65.09) [65.087, 70.67) [70.666, 74.76) 
             50              50              50              50              50 
[74.764, 79.73) [79.731, 85.51) [85.508, 91.04) [91.037, 95.37) [95.373, 99.99] 
             50              50              50              50              50 
> 
> 
> 
> cleanEx()
> nameEx("data.frame.create.modify.check")
> ### * data.frame.create.modify.check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: data.frame.create.modify.check
> ### Title: Tips for Creating, Modifying, and Checking Data Frames
> ### Aliases: data.frame.create.modify.check
> ### Keywords: data manip programming interface htest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # First, we do steps that create or manipulate the data
> ##D # frame in its entirety.  For S-Plus, these are done with
> ##D # .Data in search position one (the default at the
> ##D # start of the session).
> ##D #
> ##D # -----------------------------------------------------------------------
> ##D # Step 1: Create initial draft of data frame
> ##D # 
> ##D # We usually begin by importing a dataset from
> ##D # # another application.  ASCII files may be imported
> ##D # using the scan and read.table functions.  SAS
> ##D # datasets may be imported using the Hmisc sas.get
> ##D # function (which will carry more attributes from
> ##D # SAS than using File \dots  Import) from the GUI
> ##D # menus.  But for most applications (especially
> ##D # Excel), File \dots Import will suffice.  If using
> ##D # the GUI, it is often best to provide variable
> ##D # names during the import process, using the Options
> ##D # tab, rather than renaming all fields later Of
> ##D # course, if the data to be imported already have
> ##D # field names (e.g., in Excel), let S use those
> ##D # automatically.  If using S-Plus, you can use a
> ##D # command to execute File \dots  Import, e.g.:
> ##D 
> ##D 
> ##D import.data(FileName = "/windows/temp/fev.asc",
> ##D             FileType = "ASCII", DataFrame = "FEV")
> ##D 
> ##D 
> ##D # Here we name the new data frame FEV rather than
> ##D # fev, because we wanted to distinguish a variable
> ##D # in the data frame named fev from the data frame
> ##D # name.  For S-Plus the command will look
> ##D # instead like the following:
> ##D 
> ##D 
> ##D FEV <- importData("/tmp/fev.asc")
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 2: Clean up data frame / make it be more
> ##D # efficiently stored
> ##D # 
> ##D # Unless using sas.get to import your dataset
> ##D # (sas.get already stores data efficiently), it is
> ##D # usually a good idea to run the data frame through
> ##D # the Hmisc cleanup.import function to change
> ##D # numeric variables that are always whole numbers to
> ##D # be stored as integers, the remaining numerics to
> ##D # single precision, strange values from Excel to
> ##D # NAs, and character variables that always contain
> ##D # legal numeric values to numeric variables.
> ##D # cleanup.import typically halves the size of the
> ##D # data frame.  If you do not specify any parameters
> ##D # to cleanup.import, the function assumes that no
> ##D # numeric variable needs more than 7 significant
> ##D # digits of precision, so all non-integer-valued
> ##D # variables will be converted to single precision.
> ##D 
> ##D 
> ##D FEV <- cleanup.import(FEV)
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 3: Make global changes to the data frame
> ##D # 
> ##D # A data frame has attributes that are "external" to
> ##D # its variables.  There are the vector of its
> ##D # variable names ("names" attribute), the
> ##D # observation identifiers ("row.names"), and the
> ##D # "class" (whose value is "data.frame").  The
> ##D # "names" attribute is the one most commonly in need
> ##D # of modification.  If we had wanted to change all
> ##D # the variable names to lower case, we could have
> ##D # specified lowernames=TRUE to the cleanup.import
> ##D # invocation above, or type
> ##D 
> ##D 
> ##D names(FEV) <- casefold(names(FEV))
> ##D 
> ##D 
> ##D # The upData function can also be used to change
> ##D # variable names in two ways (see below).
> ##D # To change names in a non-systematic way we use
> ##D # other options.  Under Windows/NT the most
> ##D # straigtforward approach is to change the names
> ##D # interactively.  Click on the data frame in the
> ##D # left panel of the Object Browser, then in the
> ##D # right pane click twice (slowly) on a variable.
> ##D # Use the left arrow and other keys to edit the
> ##D # name.  Click outside that name field to commit the
> ##D # change.  You can also rename columns while in a
> ##D # Data Sheet.  To instead use programming commands
> ##D # to change names, use something like:
> ##D 
> ##D 
> ##D names(FEV)[6] <- 'smoke'   # assumes you know the positions!  
> ##D names(FEV)[names(FEV)=='smoking'] <- 'smoke' 
> ##D names(FEV) <- edit(names(FEV))
> ##D 
> ##D 
> ##D # The last example is useful if you are changing
> ##D # many names.  But none of the interactive
> ##D # approaches such as edit() are handy if you will be
> ##D # re-importing the dataset after it is updated in
> ##D # its original application.  This problem can be
> ##D # addressed by saving the new names in a permanent
> ##D # vector in .Data:
> ##D 
> ##D 
> ##D new.names <- names(FEV)
> ##D 
> ##D 
> ##D # Then if the data are re-imported, you can type
> ##D 
> ##D 
> ##D names(FEV) <- new.names
> ##D 
> ##D 
> ##D # to rename the variables.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 4: Delete unneeded variables
> ##D # 
> ##D # To delete some of the variables, you can
> ##D # right-click on variable names in the Object
> ##D # Browser's right pane, then select Delete.  You can
> ##D # also set variables to have NULL values, which
> ##D # causes the system to delete them.  We don't need
> ##D # to delete any variables from FEV but suppose we
> ##D # did need to delete some from mydframe.
> ##D 
> ##D 
> ##D mydframe$x1 <- NULL 
> ##D mydframe$x2 <- NULL
> ##D mydframe[c('age','sex')] <- NULL   # delete 2 variables 
> ##D mydframe[Cs(age,sex)]    <- NULL   # same thing
> ##D 
> ##D 
> ##D # The last example uses the Hmisc short-cut quoting
> ##D # function Cs.  See also the drop parameter to upData.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 5: Make changes to individual variables
> ##D #         within the data frame
> ##D # 
> ##D # After importing data, the resulting variables are
> ##D # seldom self - documenting, so we commonly need to
> ##D # change or enhance attributes of individual
> ##D # variables within the data frame.
> ##D # 
> ##D # If you are only changing a few variables, it is
> ##D # efficient to change them directly without
> ##D # attaching the entire data frame.
> ##D 
> ##D 
> ##D FEV$sex   <- factor(FEV$sex,   0:1, c('female','male')) 
> ##D FEV$smoke <- factor(FEV$smoke, 0:1, 
> ##D                     c('non-current smoker','current smoker')) 
> ##D units(FEV$age)    <- 'years'
> ##D units(FEV$fev)    <- 'L' 
> ##D label(FEV$fev)    <- 'Forced Expiratory Volume' 
> ##D units(FEV$height) <- 'inches'
> ##D 
> ##D 
> ##D # When changing more than one or two variables it is
> ##D # more convenient change the data frame using the
> ##D # Hmisc upData function.
> ##D 
> ##D 
> ##D FEV2 <- upData(FEV,
> ##D   rename=c(smoking='smoke'), 
> ##D   # omit if renamed above
> ##D   drop=c('var1','var2'),
> ##D   levels=list(sex  =list(female=0,male=1),
> ##D               smoke=list('non-current smoker'=0,
> ##D                          'current smoker'=1)),
> ##D   units=list(age='years', fev='L', height='inches'),
> ##D   labels=list(fev='Forced Expiratory Volume'))
> ##D 
> ##D 
> ##D # An alternative to levels=list(\dots) is for example
> ##D # upData(FEV, sex=factor(sex,0:1,c('female','male'))).
> ##D # 
> ##D # Note that we saved the changed data frame into a
> ##D # new data frame FEV2.  If we were confident of the
> ##D # correctness of our changes we could have stored
> ##D # the new data frame on top of the old one, under
> ##D # the original name FEV.
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 6:  Check the data frame
> ##D # 
> ##D # The Hmisc describe function is perhaps the first
> ##D # function that should be used on the new data
> ##D # frame.  It provides documentation of all the
> ##D # variables and the frequency tabulation, counts of
> ##D # NAs,  and 5 largest and smallest values are
> ##D # helpful in detecting data errors.  Typing
> ##D # describe(FEV) will write the results to the
> ##D # current output window.  To put the results in a
> ##D # new window that can persist, even upon exiting
> ##D # S, we use the page function.  The describe
> ##D # output can be minimized to an icon but kept ready
> ##D # for guiding later steps of the analysis.
> ##D 
> ##D 
> ##D page(describe(FEV2), multi=TRUE) 
> ##D # multi=TRUE allows that window to persist while
> ##D # control is returned to other windows
> ##D 
> ##D 
> ##D # The new data frame is OK.  Store it on top of the
> ##D # old FEV and then use the graphical user interface
> ##D # to delete FEV2 (click on it and hit the Delete
> ##D # key) or type rm(FEV2) after the next statement.
> ##D 
> ##D 
> ##D FEV <- FEV2
> ##D 
> ##D 
> ##D # Next, we can use a variety of other functions to
> ##D # check and describe all of the variables.  As we
> ##D # are analyzing all or almost all of the variables,
> ##D # this is best done without attaching the data
> ##D # frame.  Note that plot.data.frame plots inverted
> ##D # CDFs for continuous variables and dot plots
> ##D # showing frequency distributions of categorical
> ##D # ones.
> ##D 
> ##D 
> ##D summary(FEV)
> ##D # basic summary function (summary.data.frame) 
> ##D 
> ##D 
> ##D plot(FEV)                # plot.data.frame 
> ##D datadensity(FEV)         
> ##D # rug plots and freq. bar charts for all var.
> ##D 
> ##D 
> ##D hist.data.frame(FEV)     
> ##D # for variables having > 2 values 
> ##D 
> ##D 
> ##D by(FEV, FEV$smoke, summary)  
> ##D # use basic summary function with stratification
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 7:  Do detailed analyses involving individual
> ##D #          variables
> ##D # 
> ##D # Analyses based on the formula language can use
> ##D # data= so attaching the data frame may not be
> ##D # required.  This saves memory.  Here we use the
> ##D # Hmisc summary.formula function to compute 5
> ##D # statistics on height, stratified separately by age
> ##D # quartile and by sex.
> ##D 
> ##D 
> ##D options(width=80) 
> ##D summary(height ~ age + sex, data=FEV,
> ##D         fun=function(y)c(smean.sd(y),
> ##D                          smedian.hilow(y,conf.int=.5)))
> ##D # This computes mean height, S.D., median, outer quartiles
> ##D 
> ##D 
> ##D fit <- lm(height ~ age*sex, data=FEV) 
> ##D summary(fit)
> ##D 
> ##D 
> ##D # For this analysis we could also have attached the
> ##D # data frame in search position 2.  For other
> ##D # analyses, it is mandatory to attach the data frame
> ##D # unless FEV$ prefixes each variable name.
> ##D # Important: DO NOT USE attach(FEV, 1) or
> ##D # attach(FEV, pos=1, \dots) if you are only analyzing
> ##D # and not changing the variables, unless you really
> ##D # need to avoid conflicts with variables in search
> ##D # position 1 that have the same names as the
> ##D # variables in FEV.  Attaching into search position
> ##D # 1 will cause S-Plus to be more of a memory hog.
> ##D 
> ##D 
> ##D attach(FEV)
> ##D # Use e.g. attach(FEV[,Cs(age,sex)]) if you only
> ##D # want to analyze a small subset of the variables
> ##D # Use e.g. attach(FEV[FEV$sex=='male',]) to
> ##D # analyze a subset of the observations
> ##D 
> ##D 
> ##D summary(height ~ age + sex,
> ##D         fun=function(y)c(smean.sd(y),
> ##D           smedian.hilow(y,conf.int=.5)))
> ##D fit <- lm(height ~ age*sex)
> ##D 
> ##D 
> ##D # Run generic summary function on height and fev, 
> ##D # stratified by sex
> ##D by(data.frame(height,fev), sex, summary)
> ##D 
> ##D 
> ##D # Cross-classify into 4 sex x smoke groups
> ##D by(FEV, list(sex,smoke), summary)
> ##D 
> ##D 
> ##D # Plot 5 quantiles
> ##D s <- summary(fev ~ age + sex + height,
> ##D               fun=function(y)quantile(y,c(.1,.25,.5,.75,.9)))
> ##D 
> ##D 
> ##D plot(s, which=1:5, pch=c(1,2,15,2,1), #pch=c('=','[','o',']','='), 
> ##D      main='A Discovery', xlab='FEV')
> ##D 
> ##D 
> ##D # Use the nonparametric bootstrap to compute a 
> ##D # 0.95 confidence interval for the population mean fev
> ##D smean.cl.boot(fev)    # in Hmisc
> ##D 
> ##D 
> ##D # Use the Statistics \dots Compare Samples \dots One Sample 
> ##D # keys to get a normal-theory-based C.I.  Then do it 
> ##D # more manually.  The following method assumes that 
> ##D # there are no NAs in fev
> ##D 
> ##D 
> ##D sd <- sqrt(var(fev))
> ##D xbar <- mean(fev)
> ##D xbar
> ##D sd
> ##D n <- length(fev)
> ##D qt(.975,n-1)     
> ##D # prints 0.975 critical value of t dist. with n-1 d.f.
> ##D 
> ##D 
> ##D xbar + c(-1,1)*sd/sqrt(n)*qt(.975,n-1)   
> ##D # prints confidence limits
> ##D 
> ##D 
> ##D # Fit a linear model
> ##D # fit <- lm(fev ~ other variables \dots)
> ##D 
> ##D 
> ##D detach()
> ##D 
> ##D 
> ##D # The last command is only needed if you want to
> ##D # start operating on another data frame and you want
> ##D # to get FEV out of the way.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Creating data frames from scratch
> ##D # 
> ##D # Data frames can be created from within S.  To
> ##D # create a small data frame containing ordinary
> ##D # data, you can use something like
> ##D 
> ##D 
> ##D dframe <- data.frame(age=c(10,20,30), 
> ##D                      sex=c('male','female','male'),
> ##D                      stringsAsFactors=TRUE)
> ##D 
> ##D 
> ##D # You can also create a data frame using the Data
> ##D # Sheet.  Create an empty data frame with the
> ##D # correct variable names and types, then edit in the
> ##D # data.
> ##D 
> ##D 
> ##D dd <- data.frame(age=numeric(0),sex=character(0),
> ##D                  stringsAsFactors=TRUE)
> ##D 
> ##D 
> ##D # The sex variable will be stored as a factor, and
> ##D # levels will be automatically added to it as you
> ##D # define new values for sex in the Data Sheet's sex
> ##D # column.
> ##D # 
> ##D # When the data frame you need to create is defined
> ##D # by systematically varying variables (e.g., all
> ##D # possible combinations of values of each variable),
> ##D # the expand.grid function is useful for quickly
> ##D # creating the data.  Then you can add
> ##D # non-systematically-varying variables to the object
> ##D # created by expand.grid, using programming
> ##D # statements or editing the Data Sheet.  This
> ##D # process is useful for creating a data frame
> ##D # representing all the values in a printed table.
> ##D # In what follows we create a data frame
> ##D # representing the combinations of values from an 8
> ##D # x 2 x 2 x 2 (event x method x sex x what) table,
> ##D # and add a non-systematic variable percent to the
> ##D # data.
> ##D 
> ##D 
> ##D jcetable <- expand.grid(
> ##D  event=c('Wheezing at any time',
> ##D          'Wheezing and breathless',
> ##D          'Wheezing without a cold',
> ##D          'Waking with tightness in the chest',
> ##D          'Waking with shortness of breath',
> ##D          'Waking with an attack of cough',
> ##D          'Attack of asthma',
> ##D          'Use of medication'),
> ##D  method=c('Mail','Telephone'), 
> ##D  sex=c('Male','Female'),
> ##D  what=c('Sensitivity','Specificity'))
> ##D 
> ##D 
> ##D jcetable$percent <- 
> ##D c(756,618,706,422,356,578,289,333,
> ##D   576,421,789,273,273,212,212,212,
> ##D   613,763,713,403,377,541,290,226,
> ##D   613,684,632,290,387,613,258,129,
> ##D   656,597,438,780,732,679,938,919,
> ##D   714,600,494,877,850,703,963,987,
> ##D   755,420,480,794,779,647,956,941,
> ##D   766,423,500,833,833,604,955,986) / 10
> ##D 
> ##D 
> ##D # In jcetable, event varies most rapidly, then
> ##D # method, then sex, and what.
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("dataRep")
> ### * dataRep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dataRep
> ### Title: Representativeness of Observations in a Data Set
> ### Aliases: dataRep print.dataRep predict.dataRep print.predict.dataRep
> ###   roundN [.roundN
> ### Keywords: datasets category cluster manip models
> 
> ### ** Examples
> 
> set.seed(13)
> num.symptoms <- sample(1:4, 1000,TRUE)
> sex <- factor(sample(c('female','male'), 1000,TRUE))
> x    <- runif(1000)
> x[1] <- NA
> table(num.symptoms, sex, .25*round(x/.25))
, ,  = 0

            sex
num.symptoms female male
           1     15   11
           2     22   13
           3     14   18
           4     16   18

, ,  = 0.25

            sex
num.symptoms female male
           1     37   28
           2     33   24
           3     32   35
           4     27   32

, ,  = 0.5

            sex
num.symptoms female male
           1     30   29
           2     36   35
           3     20   34
           4     27   36

, ,  = 0.75

            sex
num.symptoms female male
           1     38   28
           2     23   25
           3     32   29
           4     30   31

, ,  = 1

            sex
num.symptoms female male
           1     19   24
           2     25   18
           3     17   15
           4      6   17

> 
> 
> d <- dataRep(~ num.symptoms + sex + roundN(x,.25))
> print(d, long=TRUE)

Data Representativeness    n=999

dataRep(formula = ~num.symptoms + sex + roundN(x, 0.25))

Frequencies of Missing Values Due to Each Variable
   num.symptoms             sex roundN(x, 0.25) 
              0               0               1 

Specifications for Matching

                          Type      Parameters
num.symptoms     exact numeric                
sex          exact categorical     female male
x                        round to nearest 0.25

Unique Combinations of Descriptor Variables

   num.symptoms    sex    x Frequency
1             1 female 0.00        15
2             2 female 0.00        22
3             3 female 0.00        14
4             4 female 0.00        16
5             1   male 0.00        11
6             2   male 0.00        13
7             3   male 0.00        18
8             4   male 0.00        18
9             1 female 0.25        37
10            2 female 0.25        33
11            3 female 0.25        32
12            4 female 0.25        27
13            1   male 0.25        28
14            2   male 0.25        24
15            3   male 0.25        35
16            4   male 0.25        32
17            1 female 0.50        30
18            2 female 0.50        36
19            3 female 0.50        20
20            4 female 0.50        27
21            1   male 0.50        29
22            2   male 0.50        35
23            3   male 0.50        34
24            4   male 0.50        36
25            1 female 0.75        38
26            2 female 0.75        23
27            3 female 0.75        32
28            4 female 0.75        30
29            1   male 0.75        28
30            2   male 0.75        25
31            3   male 0.75        29
32            4   male 0.75        31
33            1 female 1.00        19
34            2 female 1.00        25
35            3 female 1.00        17
36            4 female 1.00         6
37            1   male 1.00        24
38            2   male 1.00        18
39            3   male 1.00        15
40            4   male 1.00        17
> 
> 
> predict(d, data.frame(num.symptoms=1:3, sex=c('male','male','female'),
+                       x=c(.03,.5,1.5)))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values

Descriptor Variable Values, Estimated Frequency in Original Dataset,
and Minimum Marginal Frequency for any Variable

  num.symptoms    sex    x Frequency Marginal.Freq
1            1   male 0.03        11           127
2            2   male 0.50        35           247
3            3 female 1.50         0             0


Percentiles for Continuous Descriptor Variables,
Percentage in Category for Categorical Variables

  num.symptoms sex   x
1           12  50   3
2           38  50  50
3           64  50 100
> 
> 
> 
> cleanEx()
> nameEx("deff")
> ### * deff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deff
> ### Title: Design Effect and Intra-cluster Correlation
> ### Aliases: deff
> ### Keywords: htest
> 
> ### ** Examples
> 
> set.seed(1)
> blood.pressure <- rnorm(1000, 120, 15)
> clinic <- sample(letters, 1000, replace=TRUE)
> deff(blood.pressure, clinic)
            n      clusters           rho          deff 
 1.000000e+03  2.600000e+01 -7.080501e-03  7.289867e-01 
> 
> 
> 
> cleanEx()
> nameEx("describe")
> ### * describe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: describe
> ### Title: Concise Statistical Description of a Vector, Matrix, Data Frame,
> ###   or Formula
> ### Aliases: describe describe.default describe.vector describe.matrix
> ###   describe.formula describe.data.frame plot.describe print.describe
> ###   print.describe.single [.describe latex.describe latex.describe.single
> ###   html.describe html.describe.single formatdescribeSingle
> ### Keywords: interface nonparametric category distribution robust models
> ###   hplot
> 
> ### ** Examples
> 
> set.seed(1)
> describe(runif(200),dig=2)    #single variable, continuous
runif(200) 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
     200        0      200        1     0.52     0.31    0.084    0.142 
     .25      .50      .75      .90      .95 
   0.294    0.505    0.742    0.881    0.927 

lowest : 0.0130776 0.0133903 0.0233312 0.0355406 0.0589344
highest: 0.976171  0.985095  0.991839  0.991906  0.992684 
>                               #get quantiles .05,.10,\dots
> 
> dfr <- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE))
> describe(dfr)
dfr 

 2  Variables      400  Observations
--------------------------------------------------------------------------------
x 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
     400        0      400        1 0.001083    1.167 -1.64182 -1.32308 
     .25      .50      .75      .90      .95 
-0.64280 -0.05831  0.67754  1.35234  1.72182 

lowest : -3.00805 -2.88892 -2.59233 -2.4031  -2.34272
highest: 2.35055  2.44653  2.49766  2.64917  3.81028 
--------------------------------------------------------------------------------
y 
       n  missing distinct 
     400        0        2 
                        
Value      female   male
Frequency     212    188
Proportion   0.53   0.47
--------------------------------------------------------------------------------
> 
> ## Not run: 
> ##D options(grType='plotly')
> ##D d <- describe(mydata)
> ##D p <- plot(d)   # create plots for both types of variables
> ##D p[[1]]; p[[2]] # or p$Categorical; p$Continuous
> ##D plotly::subplot(p[[1]], p[[2]], nrows=2)  # plot both in one
> ##D plot(d, which='categorical')    # categorical ones
> ##D 
> ##D d <- sas.get(".","mydata",special.miss=TRUE,recode=TRUE)
> ##D describe(d)      #describe entire data frame
> ##D attach(d, 1)
> ##D describe(relig)  #Has special missing values .D .F .M .R .T
> ##D                  #attr(relig,"label") is "Religious preference"
> ##D 
> ##D #relig : Religious preference  Format:relig
> ##D #    n missing  D  F M R T distinct 
> ##D # 4038     263 45 33 7 2 1        8
> ##D #
> ##D #0:none (251, 6%), 1:Jewish (372, 9%), 2:Catholic (1230, 30%) 
> ##D #3:Jehovah's Witnes (25, 1%), 4:Christ Scientist (7, 0%) 
> ##D #5:Seventh Day Adv (17, 0%), 6:Protestant (2025, 50%), 7:other (111, 3%) 
> ##D 
> ##D 
> ##D # Method for describing part of a data frame:
> ##D  describe(death.time ~ age*sex + rcs(blood.pressure))
> ##D  describe(~ age+sex)
> ##D  describe(~ age+sex, weights=freqs)  # weighted analysis
> ##D 
> ##D  fit <- lrm(y ~ age*sex + log(height))
> ##D  describe(formula(fit))
> ##D  describe(y ~ age*sex, na.action=na.delete)   
> ##D # report on number deleted for each variable
> ##D  options(na.detail.response=TRUE)  
> ##D # keep missings separately for each x, report on dist of y by x=NA
> ##D  describe(y ~ age*sex)
> ##D  options(na.fun.response="quantile")
> ##D  describe(y ~ age*sex)   # same but use quantiles of y by x=NA
> ##D 
> ##D  d <- describe(my.data.frame)
> ##D  d$age                   # print description for just age
> ##D  d[c('age','sex')]       # print description for two variables
> ##D  d[sort(names(d))]       # print in alphabetic order by var. names
> ##D  d2 <- d[20:30]          # keep variables 20-30
> ##D  page(d2)                # pop-up window for these variables
> ##D 
> ##D # Test date/time formats and suppression of times when they don't vary
> ##D  library(chron)
> ##D  d <- data.frame(a=chron((1:20)+.1),
> ##D                  b=chron((1:20)+(1:20)/100),
> ##D                  d=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
> ##D                                hour=rep(11,20),min=rep(17,20),sec=rep(11,20)),
> ##D                  f=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
> ##D                                hour=1:20,min=1:20,sec=1:20),
> ##D                  g=ISOdate(year=2001:2020,month=rep(3,20),day=1:20))
> ##D  describe(d)
> ##D 
> ##D # Make a function to run describe, latex.describe, and use the kdvi
> ##D # previewer in Linux to view the result and easily make a pdf file
> ##D 
> ##D  ldesc <- function(data) {
> ##D   options(xdvicmd='kdvi')
> ##D   d <- describe(data, desc=deparse(substitute(data)))
> ##D   dvi(latex(d, file='/tmp/z.tex'), nomargins=FALSE, width=8.5, height=11)
> ##D  }
> ##D 
> ##D  ldesc(d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("discrete")
> ### * discrete
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: discrete
> ### Title: Discrete Vector tools
> ### Aliases: as.discrete as.discrete.default discrete [<-.discrete
> ###   [.discrete [[.discrete is.discrete is.na<-.discrete length<-.discrete
> ### Keywords: manip
> 
> ### ** Examples
> 
> a <- discrete(1:25)
> a
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
attr(,"levels")
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
attr(,"class")
[1] "discrete"
> 
> is.discrete(a)
[1] TRUE
> 
> b <- as.discrete(2:4)
> b
[1] 2 3 4
attr(,"levels")
[1] 2 3 4
attr(,"class")
[1] "discrete"
> 
> 
> 
> cleanEx()
> nameEx("dotchart2")
> ### * dotchart2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchart2
> ### Title: Enhanced Dot Chart
> ### Aliases: dotchart2
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(135)
> maj <- factor(c(rep('North',13),rep('South',13)))
> g <- paste('Category',rep(letters[1:13],2))
> n <- sample(1:15000, 26, replace=TRUE)
> y1 <- runif(26)
> y2 <- pmax(0, y1 - runif(26, 0, .1))
> dotchart2(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y')
> dotchart2(y2, g, groups=maj, pch=17, add=TRUE)
> ## Compare with dotchart function (no superpositioning or auxdata allowed):
> ## dotchart(y1, g, groups=maj, xlab='Y')
> 
> ## To plot using a transformed scale add for example
> ## axisat=sqrt(pretty(y)), axislabels=pretty(y)
> 
> 
> 
> cleanEx()
> nameEx("dotchart3")
> ### * dotchart3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchart3
> ### Title: Enhanced Version of dotchart Function
> ### Aliases: dotchart3 dotchartp summaryD summaryDp
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(135)
> maj <- factor(c(rep('North',13),rep('South',13)))
> g <- paste('Category',rep(letters[1:13],2))
> n <- sample(1:15000, 26, replace=TRUE)
> y1 <- runif(26)
> y2 <- pmax(0, y1 - runif(26, 0, .1))
> dotchart3(cbind(y1,y2), g, groups=maj, auxdata=n, auxtitle='n',
+           xlab='Y', pch=c(1,17))
> ## Compare with dotchart function (no superpositioning or auxdata allowed):
> ## dotchart(y1, g, groups=maj, xlab='Y')
> 
> ## Not run: 
> ##D dotchartp(cbind(y1, y2), g, groups=maj, auxdata=n, auxtitle='n',
> ##D           xlab='Y', gdata=cbind(c(0,.1), c(.23,.44)), auxgdata=c(-1,-2),
> ##D           symbol=c('circle', 'line-ns-open'))
> ##D 
> ##D summaryDp(sbp ~ region + sex + race + cut2(age, g=5), data=mydata)
> ## End(Not run)
> 
> ## Put options(grType='plotly') to have the following use dotchartp
> ## (rlegend will not apply)
> ## Add argument auxwhere='hover' to summaryD or dotchartp to put
> ## aux info in hover text instead of right margin
> summaryD(y1 ~ maj + g, xlab='Mean')
> summaryD(y1 ~ maj + g, groupsummary=FALSE)
> summaryD(y1 ~ g, fmtvals=function(x) sprintf('%4.2f', x))
> Y <- cbind(y1, y2)   # summaryD cannot handle cbind(...) ~ ...
> summaryD(Y  ~ maj + g, fun=function(y) y[1,], symbol=c(1,17))
> rlegend(.1, 26, c('y1','y2'), pch=c(1,17))
> 
> summaryD(y1 ~ maj, fun=function(y) c(Mean=mean(y), n=length(y)),
+          auxvar='n', auxtitle='N')
> 
> 
> 
> cleanEx()
> nameEx("dotchartpl")
> ### * dotchartpl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchartpl
> ### Title: Enhanced Version of dotchart Function for plotly
> ### Aliases: dotchartpl
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D d <- expand.grid(major=c('Alabama', 'Alaska', 'Arkansas'),
> ##D                  minor=c('East', 'West'),
> ##D                  group=c('Female', 'Male'),
> ##D                  city=0:2)
> ##D n <- nrow(d)
> ##D d$num   <- round(100*runif(n))
> ##D d$denom <- d$num + round(100*runif(n))
> ##D d$x     <- d$num / d$denom
> ##D d$lower <- d$x - runif(n)
> ##D d$upper <- d$x + runif(n)
> ##D 
> ##D with(d,
> ##D  dotchartpl(x, major, minor, group, city, lower=lower, upper=upper,
> ##D             big=city==0, num=num, denom=denom, xlab='x'))
> ##D 
> ##D # Show half-width confidence intervals for Female - Male differences
> ##D # after subsetting the data to have only one record per
> ##D # state/region/group
> ##D d <- subset(d, city == 0)
> ##D with(d,
> ##D  dotchartpl(x, major, minor, group, num=num, denom=denom,
> ##D             lower=lower, upper=upper, refgroup='Male')
> ##D )
> ##D 
> ##D n <- 500
> ##D set.seed(1)
> ##D d <- data.frame(
> ##D   race         = sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
> ##D   sex          = sample(c('Female', 'Male'), n, TRUE),
> ##D   treat        = sample(c('A', 'B'), n, TRUE),
> ##D   smoking      = sample(c('Smoker', 'Non-smoker'), n, TRUE),
> ##D   hypertension = sample(c('Hypertensive', 'Non-Hypertensive'), n, TRUE),
> ##D   region       = sample(c('North America','Europe','South America',
> ##D                           'Europe', 'Asia', 'Central America'), n, TRUE))
> ##D 
> ##D d <- upData(d, labels=c(race='Race', sex='Sex'))
> ##D 
> ##D dm <- addMarginal(d, region)
> ##D s <- summaryP(race + sex + smoking + hypertension ~
> ##D                 region + treat,  data=dm)
> ##D 
> ##D s$region <- ifelse(s$region == 'All', 'All Regions', as.character(s$region))
> ##D 
> ##D with(s, 
> ##D  dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
> ##D             big=region == 'All Regions', num=freq, denom=denom)
> ##D )
> ##D 
> ##D s2 <- s[- attr(s, 'rows.to.exclude1'), ]
> ##D with(s2, 
> ##D      dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
> ##D                 big=region == 'All Regions', num=freq, denom=denom)
> ##D )
> ##D # Note these plots can be created by plot.summaryP when options(grType='plotly')
> ##D 
> ##D # Plot hazard rates and ratios with confidence limits, on log scale
> ##D d <- data.frame(tx=c('a', 'a', 'b', 'b'),
> ##D                 event=c('MI', 'stroke', 'MI', 'stroke'),
> ##D                 count=c(10, 5, 5, 2),
> ##D                 exposure=c(1000, 1000, 900, 900))
> ##D # There were no zero event counts in this dataset.  In general we
> ##D # want to handle that, hence the 0.5 below
> ##D d <- upData(d, hazard = pmax(0.5, count) / exposure,
> ##D                selog  = sqrt(1. / pmax(0.5, count)),
> ##D                lower  = log(hazard) - 1.96 * selog,
> ##D                upper  = log(hazard) + 1.96 * selog)
> ##D with(d,
> ##D      dotchartpl(log(hazard), minor=event, group=tx, num=count, denom=exposure,
> ##D                 lower=lower, upper=upper,
> ##D                 fun=exp, ifun=log, op='/',
> ##D                 numlabel='events', denomlabel='years',
> ##D                 refgroup='a', xlab='Events Per Person-Year')
> ##D )
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ebpcomp")
> ### * ebpcomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ebpcomp
> ### Title: ebpcomp
> ### Aliases: ebpcomp
> 
> ### ** Examples
> 
> ebpcomp(1:1000)
$segments
$segments$x
   50%    25%    75% 
500.50 250.75 750.25 

$segments$y1
[1] -1.0000000 -0.6666667 -0.6666667

$segments$y2
[1] 1.0000000 0.6666667 0.6666667


$lines
$lines$x
     5%   12.5%   12.5%     25%     25%   37.5%   37.5%   62.5%   62.5%     75% 
 50.950 125.875 125.875 250.750 250.750 375.625 375.625 625.375 625.375 750.250 
    75%   87.5%   87.5%     95%     95%   87.5%   87.5%     75%     75%   62.5% 
750.250 875.125 875.125 950.050 950.050 875.125 875.125 750.250 750.250 625.375 
  62.5%   37.5%   37.5%     25%     25%   12.5%   12.5%      5%      5% 
625.375 375.625 375.625 250.750 250.750 125.875 125.875  50.950  50.950 

$lines$y
 [1]  0.1333333  0.1333333  0.3333333  0.3333333  0.6666667  0.6666667
 [7]  1.0000000  1.0000000  0.6666667  0.6666667  0.3333333  0.3333333
[13]  0.1333333  0.1333333 -0.1333333 -0.1333333 -0.3333333 -0.3333333
[19] -0.6666667 -0.6666667 -1.0000000 -1.0000000 -0.6666667 -0.6666667
[25] -0.3333333 -0.3333333 -0.1333333 -0.1333333  0.1333333


$points
$points$x
[1] 500.5

$points$y
[1] 0

$points$N
[1] 1000


$points2
$points2$x
    1%    99% 
 10.99 990.01 

$points2$y
[1] 0


> 
> 
> 
> cleanEx()
> nameEx("ecdfSteps")
> ### * ecdfSteps
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ecdfSteps
> ### Title: ecdfSteps
> ### Aliases: ecdfSteps
> 
> ### ** Examples
> 
> ecdfSteps(0:10)
$x
 [1] -0.5  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0 10.5

$y
 [1] 0.00000000 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545
 [7] 0.54545455 0.63636364 0.72727273 0.81818182 0.90909091 1.00000000
[13] 1.00000000

> ## Not run: 
> ##D # Use data.table for obtaining ECDFs by country and region
> ##D w <- d[, ecdfSteps(z, extend=c(1,11)), by=.(country, region)]  # d is a DT
> ##D # Use ggplot2 to make one graph with multiple regions' ECDFs
> ##D # and use faceting for countries
> ##D ggplot(w, aes(x, y, color=region)) + geom_step() +
> ##D        facet_wrap(~ country)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("epi")
> ### * epi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mhgr
> ### Title: Miscellaneous Functions for Epidemiology
> ### Aliases: mhgr print.mhgr lrcum print.lrcum
> ### Keywords: category htest
> 
> ### ** Examples
> 
> # Greate Migraine dataset used in Example 28.6 in the SAS PROC FREQ guide
> d <- expand.grid(response=c('Better','Same'),
+                  treatment=c('Active','Placebo'),
+                  sex=c('female','male'))
> d$count <- c(16, 11, 5, 20, 12, 16, 7, 19)
> d
  response treatment    sex count
1   Better    Active female    16
2     Same    Active female    11
3   Better   Placebo female     5
4     Same   Placebo female    20
5   Better    Active   male    12
6     Same    Active   male    16
7   Better   Placebo   male     7
8     Same   Placebo   male    19
> # Expand data frame to represent raw data
> r <- rep(1:8, d$count)
> d <- d[r,]
> with(d, mhgr(response=='Better', treatment, sex))
Mantel-Haenszel Risk Ratio and 0.95 Greenland-Robins Confidence Interval

Common Relative Risk: 2.163597 CI: 1.233568 3.794806 

N in Each Group

group
 Active Placebo 
     55      51 
> 
> # Discrete survival time example, to get Cox-Mantel relative risk and CL
> # From Stokes ME, Davis CS, Koch GG, Categorical Data Analysis Using the
> # SAS System, 2nd Edition, Sectino 17.3, p. 596-599
> #
> # Input data in Table 17.5
> d <- expand.grid(treatment=c('A','P'), center=1:3)
> d$healed2w    <- c(15,15,17,12, 7, 3)
> d$healed4w    <- c(17,17,17,13,17,17)
> d$notHealed4w <- c( 2, 7,10,15,16,18)
> d
  treatment center healed2w healed4w notHealed4w
1         A      1       15       17           2
2         P      1       15       17           7
3         A      2       17       17          10
4         P      2       12       13          15
5         A      3        7       17          16
6         P      3        3       17          18
> # Reformat to the way most people would collect raw data
> d1 <- d[rep(1:6, d$healed2w),]
> d1$time <- '2'
> d1$y <- 1
> d2 <- d[rep(1:6, d$healed4w),]
> d2$time <- '4'
> d2$y <- 1
> d3 <- d[rep(1:6, d$notHealed4w),]
> d3$time <- '4'
> d3$y <- 0
> d <- rbind(d1, d2, d3)
> d$healed2w <- d$healed4w <- d$notHealed4w <- NULL
> d
      treatment center time y
1             A      1    2 1
1.1           A      1    2 1
1.2           A      1    2 1
1.3           A      1    2 1
1.4           A      1    2 1
1.5           A      1    2 1
1.6           A      1    2 1
1.7           A      1    2 1
1.8           A      1    2 1
1.9           A      1    2 1
1.10          A      1    2 1
1.11          A      1    2 1
1.12          A      1    2 1
1.13          A      1    2 1
1.14          A      1    2 1
2             P      1    2 1
2.1           P      1    2 1
2.2           P      1    2 1
2.3           P      1    2 1
2.4           P      1    2 1
2.5           P      1    2 1
2.6           P      1    2 1
2.7           P      1    2 1
2.8           P      1    2 1
2.9           P      1    2 1
2.10          P      1    2 1
2.11          P      1    2 1
2.12          P      1    2 1
2.13          P      1    2 1
2.14          P      1    2 1
3             A      2    2 1
3.1           A      2    2 1
3.2           A      2    2 1
3.3           A      2    2 1
3.4           A      2    2 1
3.5           A      2    2 1
3.6           A      2    2 1
3.7           A      2    2 1
3.8           A      2    2 1
3.9           A      2    2 1
3.10          A      2    2 1
3.11          A      2    2 1
3.12          A      2    2 1
3.13          A      2    2 1
3.14          A      2    2 1
3.15          A      2    2 1
3.16          A      2    2 1
4             P      2    2 1
4.1           P      2    2 1
4.2           P      2    2 1
4.3           P      2    2 1
4.4           P      2    2 1
4.5           P      2    2 1
4.6           P      2    2 1
4.7           P      2    2 1
4.8           P      2    2 1
4.9           P      2    2 1
4.10          P      2    2 1
4.11          P      2    2 1
5             A      3    2 1
5.1           A      3    2 1
5.2           A      3    2 1
5.3           A      3    2 1
5.4           A      3    2 1
5.5           A      3    2 1
5.6           A      3    2 1
6             P      3    2 1
6.1           P      3    2 1
6.2           P      3    2 1
11            A      1    4 1
1.17          A      1    4 1
1.21          A      1    4 1
1.31          A      1    4 1
1.41          A      1    4 1
1.51          A      1    4 1
1.61          A      1    4 1
1.71          A      1    4 1
1.81          A      1    4 1
1.91          A      1    4 1
1.101         A      1    4 1
1.111         A      1    4 1
1.121         A      1    4 1
1.131         A      1    4 1
1.141         A      1    4 1
1.15          A      1    4 1
1.16          A      1    4 1
21            P      1    4 1
2.17          P      1    4 1
2.21          P      1    4 1
2.31          P      1    4 1
2.41          P      1    4 1
2.51          P      1    4 1
2.61          P      1    4 1
2.71          P      1    4 1
2.81          P      1    4 1
2.91          P      1    4 1
2.101         P      1    4 1
2.111         P      1    4 1
2.121         P      1    4 1
2.131         P      1    4 1
2.141         P      1    4 1
2.15          P      1    4 1
2.16          P      1    4 1
31            A      2    4 1
3.17          A      2    4 1
3.21          A      2    4 1
3.31          A      2    4 1
3.41          A      2    4 1
3.51          A      2    4 1
3.61          A      2    4 1
3.71          A      2    4 1
3.81          A      2    4 1
3.91          A      2    4 1
3.101         A      2    4 1
3.111         A      2    4 1
3.121         A      2    4 1
3.131         A      2    4 1
3.141         A      2    4 1
3.151         A      2    4 1
3.161         A      2    4 1
41            P      2    4 1
4.15          P      2    4 1
4.21          P      2    4 1
4.31          P      2    4 1
4.41          P      2    4 1
4.51          P      2    4 1
4.61          P      2    4 1
4.71          P      2    4 1
4.81          P      2    4 1
4.91          P      2    4 1
4.101         P      2    4 1
4.111         P      2    4 1
4.12          P      2    4 1
51            A      3    4 1
5.17          A      3    4 1
5.21          A      3    4 1
5.31          A      3    4 1
5.41          A      3    4 1
5.51          A      3    4 1
5.61          A      3    4 1
5.7           A      3    4 1
5.8           A      3    4 1
5.9           A      3    4 1
5.10          A      3    4 1
5.11          A      3    4 1
5.12          A      3    4 1
5.13          A      3    4 1
5.14          A      3    4 1
5.15          A      3    4 1
5.16          A      3    4 1
61            P      3    4 1
6.18          P      3    4 1
6.21          P      3    4 1
6.3           P      3    4 1
6.4           P      3    4 1
6.5           P      3    4 1
6.6           P      3    4 1
6.7           P      3    4 1
6.8           P      3    4 1
6.9           P      3    4 1
6.10          P      3    4 1
6.11          P      3    4 1
6.12          P      3    4 1
6.13          P      3    4 1
6.14          P      3    4 1
6.15          P      3    4 1
6.16          P      3    4 1
12            A      1    4 0
1.18          A      1    4 0
22            P      1    4 0
2.18          P      1    4 0
2.22          P      1    4 0
2.32          P      1    4 0
2.42          P      1    4 0
2.52          P      1    4 0
2.62          P      1    4 0
32            A      2    4 0
3.18          A      2    4 0
3.22          A      2    4 0
3.32          A      2    4 0
3.42          A      2    4 0
3.52          A      2    4 0
3.62          A      2    4 0
3.72          A      2    4 0
3.82          A      2    4 0
3.92          A      2    4 0
42            P      2    4 0
4.16          P      2    4 0
4.22          P      2    4 0
4.32          P      2    4 0
4.42          P      2    4 0
4.52          P      2    4 0
4.62          P      2    4 0
4.72          P      2    4 0
4.82          P      2    4 0
4.92          P      2    4 0
4.102         P      2    4 0
4.112         P      2    4 0
4.121         P      2    4 0
4.13          P      2    4 0
4.14          P      2    4 0
52            A      3    4 0
5.18          A      3    4 0
5.22          A      3    4 0
5.32          A      3    4 0
5.42          A      3    4 0
5.52          A      3    4 0
5.62          A      3    4 0
5.71          A      3    4 0
5.81          A      3    4 0
5.91          A      3    4 0
5.101         A      3    4 0
5.111         A      3    4 0
5.121         A      3    4 0
5.131         A      3    4 0
5.141         A      3    4 0
5.151         A      3    4 0
62            P      3    4 0
6.19          P      3    4 0
6.22          P      3    4 0
6.31          P      3    4 0
6.41          P      3    4 0
6.51          P      3    4 0
6.61          P      3    4 0
6.71          P      3    4 0
6.81          P      3    4 0
6.91          P      3    4 0
6.101         P      3    4 0
6.111         P      3    4 0
6.121         P      3    4 0
6.131         P      3    4 0
6.141         P      3    4 0
6.151         P      3    4 0
6.161         P      3    4 0
6.17          P      3    4 0
> # Finally, duplicate appropriate observations to create 2 and 4-week
> # risk sets.  Healed and not healed at 4w need to be in the 2-week
> # risk set as not healed
> d2w      <- subset(d, time=='4')
> d2w$time <- '2'
> d2w$y    <- 0
> d24      <- rbind(d, d2w)
> with(d24, table(y, treatment, time, center))
, , time = 2, center = 1

   treatment
y    A  P
  0 19 24
  1 15 15

, , time = 4, center = 1

   treatment
y    A  P
  0  2  7
  1 17 17

, , time = 2, center = 2

   treatment
y    A  P
  0 27 28
  1 17 12

, , time = 4, center = 2

   treatment
y    A  P
  0 10 15
  1 17 13

, , time = 2, center = 3

   treatment
y    A  P
  0 33 35
  1  7  3

, , time = 4, center = 3

   treatment
y    A  P
  0 16 18
  1 17 17

> # Matches Table 17.6
> 
> with(d24, mhgr(y, treatment, interaction(center, time, sep=';')))
Mantel-Haenszel Risk Ratio and 0.95 Greenland-Robins Confidence Interval

Common Relative Risk: 1.255945 CI: 1.012105 1.558532 

N in Each Group

group
  A   P 
197 204 
> 
> # Get cumulative likelihood ratios and their 0.95 confidence intervals
> # based on the following two tables
> #
> #          Disease       Disease
> #          +     -       +     -
> # Test +   39    3       20    5
> # Test -   21   17       22   15
> 
> lrcum(c(39,20), c(3,5), c(21,22), c(17,15))
   LR+ Lower 0.95 Upper 0.95 Cum. LR+ Lower 0.95 Upper 0.95
 4.333      1.502     12.503    4.333      1.502     12.503
 1.905      0.837      4.336    8.254      2.158     31.571

   LR- Lower 0.95 Upper 0.95 Cum. LR- Lower 0.95 Upper 0.95
 0.412      0.279      0.609    0.412      0.279      0.609
 0.698      0.476      1.025    0.288      0.166      0.497
> 
> 
> 
> cleanEx()
> nameEx("equalBins")
> ### * equalBins
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: equalBins
> ### Title: Multicolumn Formating
> ### Aliases: equalBins
> ### Keywords: print
> 
> ### ** Examples
> 
> mcols <- c("Group 1", "Group 2")
> mwidth <- nchar(mcols, type="width")
> spancols <- c(3,3)
> ccols <- c("a", "deer", "ad", "cat", "help", "bob")
> cwidth <- nchar(ccols, type="width")
> 
> subwidths <- partition.vector(cwidth, spancols)
> 
> equalBins(mwidth, subwidths)
11 12 13 21 22 23 
 1  4  2  3  4  3 
> 
> 
> 
> cleanEx()
> nameEx("errbar")
> ### * errbar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: errbar
> ### Title: Plot Error Bars
> ### Aliases: errbar
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(1)
> x <- 1:10
> y <- x + rnorm(10)
> delta <- runif(10)
> errbar( x, y, y + delta, y - delta )
> 
> 
> # Show bootstrap nonparametric CLs for 3 group means and for
> # pairwise differences on same graph
> group <- sample(c('a','b','d'), 200, TRUE)
> y     <- runif(200) + .25*(group=='b') + .5*(group=='d')
> cla <- smean.cl.boot(y[group=='a'],B=100,reps=TRUE)  # usually B=1000
> a   <- attr(cla,'reps')
> clb <- smean.cl.boot(y[group=='b'],B=100,reps=TRUE)
> b   <- attr(clb,'reps')
> cld <- smean.cl.boot(y[group=='d'],B=100,reps=TRUE)
> d   <- attr(cld,'reps')
> a.b <- quantile(a-b,c(.025,.975))
> a.d <- quantile(a-d,c(.025,.975))
> b.d <- quantile(b-d,c(.025,.975))
> errbar(c('a','b','d','a - b','a - d','b - d'),
+        c(cla[1],clb[1],cld[1],cla[1]-clb[1],cla[1]-cld[1],clb[1]-cld[1]),
+        c(cla[3],clb[3],cld[3],a.b[2],a.d[2],b.d[2]),
+        c(cla[2],clb[2],cld[2],a.b[1],a.d[1],b.d[1]),
+        Type=c(1,1,1,2,2,2), xlab='', ylab='')
>        
> 
> 
> 
> cleanEx()
> nameEx("escapeRegex")
> ### * escapeRegex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: escapeRegex
> ### Title: Escapes any characters that would have special meaning in a
> ###   reqular expression.
> ### Aliases: escapeRegex escapeBS
> ### Keywords: manip character programming
> 
> ### ** Examples
> 
> string <- "this\\(system) {is} [full]."
> escapeRegex(string)
[1] "this\\\\\\(system\\) \\{is\\} \\[full\\]\\."
> 
> escapeBS(string)
[1] "this\\\\(system) {is} [full]."
> 
> ## Don't show: 
> if(!any(grep(escapeRegex(string), string))) {
+   stop("function escapeRegex failed test")
+ }
> 
> if(escapeBS(string) != "this\\\\(system) {is} [full].") {
+   stop("function escapeBS failed test")
+ }
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("estSeqSim")
> ### * estSeqSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: estSeqSim
> ### Title: estSeqSim
> ### Aliases: estSeqSim
> 
> ### ** Examples
> 
> if (requireNamespace("rms", quietly = TRUE)) {
+   # Run 100 simulations, 5 looks, 2 true parameter values
+   # Total simulation time: 2s
+   lfit <- function(x, y) {
+   f <- rms::lrm.fit(x, y)
+     k <- length(coef(f))
+     c(coef(f)[k], vcov(f)[k, k])
+   }
+   gdat <- function(beta, n1, n2) {
+     # Cell probabilities for a 7-category ordinal outcome for the control group
+     p <- c(2, 1, 2, 7, 8, 38, 42) / 100
+ 
+     # Compute cell probabilities for the treated group
+     p2 <- pomodm(p=p, odds.ratio=exp(beta))
+     y1 <- sample(1 : 7, n1, p,  replace=TRUE)
+     y2 <- sample(1 : 7, n2, p2, replace=TRUE)
+     list(y1=y1, y2=y2)
+   }
+ 
+   set.seed(1)
+   est <- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
+                     gendat=gdat,
+                     fitter=lfit, nsim=100)
+   head(est)
+ }
  sim  parameter look         est       vest
1   1  0.0000000   50 -0.07834606 0.27969284
2   1  0.0000000   75 -0.48653077 0.18703883
3   1  0.0000000   95 -0.30167932 0.14752954
4   1  0.0000000  100 -0.26988751 0.14021910
5   1  0.0000000  200 -0.43421591 0.07182057
6   1 -0.3566749   50  0.10563461 0.27823395
> 
> 
> 
> cleanEx()
> nameEx("event.chart")
> ### * event.chart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.chart
> ### Title: Flexible Event Chart for Time-to-Event Data
> ### Aliases: event.chart
> ### Keywords: hplot survival
> 
> ### ** Examples
> 
> # The sample data set is an augmented CDC AIDS dataset (ASCII)
> # which is used in the examples in the help file.  This dataset is 
> # described in Kalbfleisch and Lawless (JASA, 1989).
> # Here, we have included only children 4 years old and younger.
> # We have also added a new field, dethdate, which
> # represents a fictitious death date for each patient.  There was
> # no recording of death date on the original dataset.  In addition, we have
> # added a fictitious viral load reading (copies/ml) for each patient at time of AIDS diagnosis,
> # noting viral load was also not part of the original dataset.
> #   
> # All dates are julian with julian=0 being 
> # January 1, 1960, and julian=14000 being 14000 days beyond
> # January 1, 1960 (i.e., May 1, 1998).
> 
> 
> cdcaids <- data.frame(
+ age=c(4,2,1,1,2,2,2,4,2,1,1,3,2,1,3,2,1,2,4,2,2,1,4,2,4,1,4,2,1,1,3,3,1,3),
+ infedate=c(
+ 7274,7727,7949,8037,7765,8096,8186,7520,8522,8609,8524,8213,8455,8739,
+ 8034,8646,8886,8549,8068,8682,8612,9007,8461,8888,8096,9192,9107,9001,
+ 9344,9155,8800,8519,9282,8673),
+ diagdate=c(
+ 8100,8158,8251,8343,8463,8489,8554,8644,8713,8733,8854,8855,8863,8983,
+ 9035,9037,9132,9164,9186,9221,9224,9252,9274,9404,9405,9433,9434,9470,
+ 9470,9472,9489,9500,9585,9649),
+ diffdate=c(
+ 826,431,302,306,698,393,368,1124,191,124,330,642,408,244,1001,391,246,
+ 615,1118,539,612,245,813,516,1309,241,327,469,126,317,689,981,303,976),
+ dethdate=c(
+ 8434,8304,NA,8414,8715,NA,8667,9142,8731,8750,8963,9120,9005,9028,9445,
+ 9180,9189,9406,9711,9453,9465,9289,9640,9608,10010,9488,9523,9633,9667,
+ 9547,9755,NA,9686,10084),
+ censdate=c(
+ NA,NA,8321,NA,NA,8519,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,
+ NA,NA,NA,NA,NA,NA,NA,NA,NA,10095,NA,NA),
+ viralload=c(
+ 13000,36000,70000,90000,21000,110000,75000,12000,125000,110000,13000,39000,79000,135000,14000,
+ 42000,123000,20000,12000,18000,16000,140000,16000,58000,11000,120000,85000,31000,24000,115000,
+ 17000,13100,72000,13500)
+ )
> 
> cdcaids <- upData(cdcaids,
+  labels=c(age     ='Age, y', infedate='Date of blood transfusion',
+           diagdate='Date of AIDS diagnosis',
+           diffdate='Incubation period (days from HIV to AIDS)',
+           dethdate='Fictitious date of death',
+           censdate='Fictitious censoring date',
+ 	  viralload='Fictitious viral load'))
Input object size:	 3416 bytes;	 7 variables	 34 observations
New object size:	6264 bytes;	7 variables	34 observations
> 
> 
> # Note that the style options listed with these
> # examples are best suited for output to a postscript file (i.e., using
> # the postscript function with horizontal=TRUE) as opposed to a graphical
> # window (e.g., motif).
> 
> 
> # To produce simple calendar event chart (with internal legend):
> # postscript('example1.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'observation dates',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data calendar event chart 1',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   legend.point.at = list(c(7210, 8100), c(35, 27)), legend.bty='o')
> 
> 
> # To produce simple interval event chart (with internal legend):
> # postscript('example2.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data interval event chart 1',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))
> 
> 
> # To produce simple interval event chart (with internal legend),
> # but now with flexible diagdate symbol size based on viral load variable:
> # postscript('example2a.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data interval event chart 1a, with viral load at diagdate represented',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   point.cex.mult = 0.00002, point.cex.mult.var = 'viralload', extra.points.no.mult = c(1,NA,1,1), 
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))
> 
> 
> # To produce more complicated interval chart which is
> # referenced by infection date, and sorted by age and incubation period:
> # postscript('example3.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since diagnosis of AIDS (in days)',
+   y.lab='patients (sorted by age and incubation length)',
+   titl='AIDS data interval event chart 2 (sorted by age, incubation)',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i',legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='diagdate', x.julian=TRUE, sort.by=c('age','diffdate'),
+   line.by='age', line.lty=c(1,3,2,4), line.lwd=rep(1,4), line.col=rep(1,4),
+   legend.bty='o', legend.point.at = list(c(-1350, -800), c(7, -1)),
+   legend.line.at = list(c(-1350, -800), c(16, 8)),
+   legend.line.text=c('age = 1', '       = 2', '       = 3', '       = 4'))
> 
> 
> # To produce the Goldman chart:
> # postscript('example4.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)', y.lab='dates of observation',
+   titl='AIDS data Goldman event chart 1',
+   y.var = c('infedate'), y.var.type='d', now.line=TRUE, y.jitter=FALSE,
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8), mgp = c(3.1,1.6,0),
+   legend.plot=TRUE, legend.location='i',legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1500, 2800), c(9300, 10000)))
> 
> 
> # To convert coded time-to-event data, then, draw an event chart:
> surv.time <- c(5,6,3,1,2)
> cens.ind   <- c(1,0,1,1,0)
> surv.data  <- cbind(surv.time,cens.ind)
> event.data <- event.convert(surv.data)
> event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
> 
> 
> 
> cleanEx()
> nameEx("event.convert")
> ### * event.convert
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.convert
> ### Title: Event Conversion for Time-to-Event Data
> ### Aliases: event.convert
> ### Keywords: hplot survival
> 
> ### ** Examples
> 
> # To convert coded time-to-event data, then, draw an event chart:
> surv.time <- c(5,6,3,1,2)
> cens.ind   <- c(1,0,1,1,0)
> surv.data  <- cbind(surv.time,cens.ind)
> event.data <- event.convert(surv.data)
> event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
> 
> 
> 
> cleanEx()
> nameEx("event.history")
> ### * event.history
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.history
> ### Title: Produces event.history graph for survival data
> ### Aliases: event.history
> ### Keywords: survival
> 
> ### ** Examples
> 
> # Code to produce event history graphs for SIM paper
> #
> # before generating plots, some pre-processing needs to be performed,
> #  in order to get dataset in proper form for event.history function;
> #  need to create one line per subject and sort by time under observation, 
> #  with those experiencing event coming before those tied with censoring time;
> require('survival')
Loading required package: survival
> data(heart)
> 
> # creation of event.history version of heart dataset (call heart.one):
> 
> heart.one <- matrix(nrow=length(unique(heart$id)), ncol=8)
> for(i in 1:length(unique(heart$id)))
+  {
+   if(length(heart$id[heart$id==i]) == 1)
+    heart.one[i,] <- as.numeric(unlist(heart[heart$id==i, ]))
+   else if(length(heart$id[heart$id==i]) == 2)
+    heart.one[i,] <- as.numeric(unlist(heart[heart$id==i,][2,]))
+  }
> 
> heart.one[,3][heart.one[,3] == 0] <- 2 	## converting censored events to 2, from 0
> if(is.factor(heart$transplant))
+  heart.one[,7] <- heart.one[,7] - 1
>  ## getting back to correct transplantation coding
> heart.one <- as.data.frame(heart.one[order(unlist(heart.one[,2]), unlist(heart.one[,3])),])
> names(heart.one) <- names(heart)
> # back to usual censoring indicator:
> heart.one[,3][heart.one[,3] == 2] <- 0 
> # note: transplant says 0 (for no transplants) or 1 (for one transplant)
> #        and event = 1 is death, while event = 0 is censored
> 
> # plot single Kaplan-Meier curve from heart data, first creating survival object
> heart.surv <- survfit(Surv(stop, event) ~ 1, data=heart.one, conf.int = FALSE)
> 
> # figure 3: traditional Kaplan-Meier curve
> # postscript('ehgfig3.ps', horiz=TRUE)
> # omi <- par(omi=c(0,1.25,0.5,1.25))
>  plot(heart.surv, ylab='estimated survival probability',
+       xlab='observation time (in days)')
>  title('Figure 3: Kaplan-Meier curve for Stanford data', cex=0.8)
> # dev.off()
> 
> ## now, draw event history graph for Stanford heart data; use as Figure 4
> 
> # postscript('ehgfig4.ps', horiz=TRUE, colors = seq(0, 1, len=20))
> # par(omi=c(0,1.25,0.5,1.25))
>  event.history(heart.one, 
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),
+ 		x.lab = 'time under observation (in days)',
+ 		title='Figure 4: Event history graph for\nStanford data',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 30.0, cens.mark.cex = 0.85)
> # dev.off()
> 
> 
> 
> # now, draw age-stratified event history graph for Stanford heart data; 
> #  use as Figure 5
> 
> # two plots, stratified by age status
> # postscript('c:\temp\ehgfig5.ps', horiz=TRUE, colors = seq(0, 1, len=20))
> # par(omi=c(0,1.25,0.5,1.25))
>  par(mfrow=c(1,2))
> 
>  event.history(data=heart.one, subset.rows = (heart.one[,4] < 0),
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),  
+ 		x.lab = 'time under observation\n(in days)',
+ 		title = 'Figure 5a:\nStanford data\n(age < 48)',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
+ 		xlim=c(0,1900))
> 
>  event.history(data=heart.one, subset.rows = (heart.one[,4] >= 0),
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),
+ 		x.lab = 'time under observation\n(in days)',
+ 		title = 'Figure 5b:\nStanford data\n(age >= 48)',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
+ 		xlim=c(0,1900))
> # dev.off()
> # par(omi=omi)
> 
> # we will not show liver cirrhosis data manipulation, as it was 
> #  a bit detailed; however, here is the 
> #  event.history code to produce Figure 7 / Plate 1
> 
> # Figure 7 / Plate 1 : prothrombin ehg with color
> ## Not run: 
> ##D second.arg <- 1				### second.arg is for shading
> ##D third.arg <- c(rep(1,18),0,1)		### third.arg is for intensity
> ##D 
> ##D # postscript('c:\temp\ehgfig7.ps', horiz=TRUE, 
> ##D # colors = cbind(seq(0, 1, len = 20), second.arg, third.arg)) 
> ##D # par(omi=c(0,1.25,0.5,1.25), col=19)
> ##D  event.history(cirrhos2.eh, subset.rows = NULL,
> ##D                survtime.col=cirrhos2.eh$time, surv.col=cirrhos2.eh$event,
> ##D 		covtime.cols = as.matrix(cirrhos2.eh[, ((2:18)*2)]),
> ##D 		cov.cols = as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
> ##D 		cut.cov =  as.numeric(quantile(as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
> ##D 				c(0,.2,.4,.6,.8,1), na.rm=TRUE) + c(-1,0,0,0,0,1)),	
> ##D  		colors=c(20,4,8,11,14),
> ##D 		x.lab = 'time under observation (in days)',
> ##D 		title='Figure 7: Event history graph for liver cirrhosis data (color)',
> ##D 		cens.mark.right =TRUE, cens.mark = '-', 
> ##D 		cens.mark.ahead = 100.0, cens.mark.cex = 0.85)
> ##D # dev.off()
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:survival’

> nameEx("extractlabs")
> ### * extractlabs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extractlabs
> ### Title: extractlabs
> ### Aliases: extractlabs
> 
> ### ** Examples
> 
> d <- data.frame(x=1:10, y=(1:10)/10)
> d <- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
> d2 <- d
> units(d2$x) <- 'cm'
> LabelsUnits <- extractlabs(d, d2)
Warning in extractlabs(d, d2) :
  1 variables have conflicting labels/units from different datasets
Variable names with inconsistent attributes:
Key: <name>
     name  label  units
   <char> <char> <char>
1:      x      X   mmHg
2:      x      X     cm

> LabelsUnits
Key: <name>
     name  label  units
   <char> <char> <char>
1:      x      X   mmHg
2:      x      X     cm
3:      y      Y       
> 
> 
> 
> cleanEx()
> nameEx("fImport")
> ### * fImport
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fImport
> ### Title: fImport
> ### Aliases: fImport
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Get a Stata dataset
> ##D d <- fImport('http://www.principlesofeconometrics.com/stata/alcohol.dta')
> ##D contents(d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("find.matches")
> ### * find.matches
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: find.matches
> ### Title: Find Close Matches
> ### Aliases: find.matches summary.find.matches print.find.matches
> ###   matchCases
> ### Keywords: math multivariate htest
> 
> ### ** Examples
> 
> y <- rbind(c(.1, .2),c(.11, .22), c(.3, .4), c(.31, .41), c(.32, 5))
> x <- rbind(c(.09,.21), c(.29,.39))
> y
     [,1] [,2]
[1,] 0.10 0.20
[2,] 0.11 0.22
[3,] 0.30 0.40
[4,] 0.31 0.41
[5,] 0.32 5.00
> x
     [,1] [,2]
[1,] 0.09 0.21
[2,] 0.29 0.39
> w <- find.matches(x, y, maxmatch=5, tol=c(.05,.05))
> 
> 
> set.seed(111)       # so can replicate results
> x <- matrix(runif(500), ncol=2)
> y <- matrix(runif(2000), ncol=2)
> w <- find.matches(x, y, maxmatch=5, tol=c(.02,.03))
> w$matches[1:5,]
     Match #1 Match #2 Match #3 Match #4 Match #5
[1,]      999      694        0        0        0
[2,]        0        0        0        0        0
[3,]      235        0        0        0        0
[4,]      964      139        0        0        0
[5,]      906      427      204        0        0
> w$distance[1:5,]
     Distance #1 Distance #2 Distance #3 Distance #4 Distance #5
[1,]   0.1042884   0.1562084          NA          NA          NA
[2,]          NA          NA          NA          NA          NA
[3,]   0.7272258          NA          NA          NA          NA
[4,]   0.2815041   0.7973284          NA          NA          NA
[5,]   0.6135293   0.7162828   0.7189297          NA          NA
> # Find first x with 3 or more y-matches
> num.match <- apply(w$matches, 1, function(x)sum(x > 0))
> j <- ((1:length(num.match))[num.match > 2])[1]
> x[j,]
[1] 0.3776632 0.6833354
> y[w$matches[j,],]
          [,1]      [,2]
[1,] 0.3708767 0.7045144
[2,] 0.3687917 0.7049588
[3,] 0.3821378 0.7078709
> 
> 
> summary(w)
Frequency table of number of matches found per observation

m
 0  1  2  3  4  5 
27 53 64 54 32 20 

Median minimum distance by number of matches

        1         2         3         4         5 
0.5859325 0.3376432 0.1917933 0.1407859 0.1398928 

Observations selected first more than once (with frequencies)


 57  73  91 101 116 165 191 251 256 292 415 422 438 443 467 552 592 593 650 691 
  2   2   2   2   2   3   2   2   2   3   2   2   2   2   2   2   2   2   2   2 
719 733 747 754 818 820 824 849 871 926 945 964 970 
  2   2   2   2   2   2   3   2   2   2   2   2   2 
> 
> 
> # For many applications would do something like this:
> # attach(df1)
> # x <- cbind(age, sex) # Just do as.matrix(df1) if df1 has no factor objects
> # attach(df2)
> # y <- cbind(age, sex)
> # mat <- find.matches(x, y, tol=c(5,0)) # exact match on sex, 5y on age
> 
> 
> # Demonstrate matchCases
> xcase     <- c(1,3,5,12)
> xcontrol  <- 1:6
> idcase    <- c('A','B','C','D')
> idcontrol <- c('a','b','c','d','e','f')
> ycase     <- c(11,33,55,122)
> ycontrol  <- c(11,22,33,44,55,66)
> matchCases(xcase, ycase, idcase,
+            xcontrol, ycontrol, idcontrol, tol=1)

Frequencies of Number of Matched Controls per Case:

matches
0 2 3 
1 1 2 

   idcase    type id x  y
1       A    case  A 1 11
2       A control  a 1 11
3       A control  b 2 22
4       B    case  B 3 33
5       B control  b 2 22
6       B control  c 3 33
7       B control  d 4 44
8       C    case  C 5 55
9       C control  d 4 44
10      C control  e 5 55
11      C control  f 6 66
> 
> 
> # If y is a binary response variable, the following code
> # will produce a Mantel-Haenszel summary odds ratio that 
> # utilizes the matching.
> # Standard variance formula will not work here because
> # a control will match more than one case
> # WARNING: The M-H procedure exemplified here is suspect 
> # because of the small strata and widely varying number
> # of controls per case.
> 
> 
> x    <- c(1, 2, 3, 3, 3, 6, 7, 12,  1, 1:7)
> y    <- c(0, 0, 0, 1, 0, 1, 1,  1,  1, 0, 0, 0, 0, 1, 1, 1)
> case <- c(rep(TRUE, 8), rep(FALSE, 8))
> id   <- 1:length(x)
> 
> 
> m <- matchCases(x[case],  y[case],  id[case],
+                 x[!case], y[!case], id[!case], tol=1)

Frequencies of Number of Matched Controls per Case:

matches
0 2 3 4 
1 1 5 1 

> iscase <- m$type=='case'
> # Note: the first tapply on insures that event indicators are
> # sorted by case id.  The second actually does something.
> event.case    <- tapply(m$y[iscase],  m$idcase[iscase],  sum)
> event.control <- tapply(m$y[!iscase], m$idcase[!iscase], sum)
> n.control     <- tapply(!iscase,      m$idcase,          sum)
> n             <- tapply(m$y,          m$idcase,          length)
> or <- sum(event.case * (n.control - event.control) / n) /
+       sum(event.control * (1 - event.case) / n)
> or
[1] 1.666667
> 
> 
> # Bootstrap this estimator by sampling with replacement from
> # subjects.  Assumes id is unique when combine cases+controls
> # (id was constructed this way above).  The following algorithms
> # puts all sampled controls back with the cases to whom they were
> # originally matched.
> 
> 
> ids <- unique(m$id)
> idgroups <- split(1:nrow(m), m$id)
> B   <- 50   # in practice use many more
> ors <- numeric(B)
> # Function to order w by ids, leaving unassigned elements zero
> align <- function(ids, w) {
+   z <- structure(rep(0, length(ids)), names=ids)
+   z[names(w)] <- w
+   z
+ }
> for(i in 1:B) {
+   j <- sample(ids, replace=TRUE)
+   obs <- unlist(idgroups[j])
+   u <- m[obs,]
+   iscase <- u$type=='case'
+   n.case <- align(ids, tapply(u$type, u$idcase, 
+                               function(v)sum(v=='case')))
+   n.control <- align(ids, tapply(u$type, u$idcase,
+                                  function(v)sum(v=='control')))
+   event.case <- align(ids, tapply(u$y[iscase],  u$idcase[iscase],  sum))
+   event.control <- align(ids, tapply(u$y[!iscase], u$idcase[!iscase], sum))
+   n <- n.case + n.control
+   # Remove sets having 0 cases or 0 controls in resample
+   s             <- n.case > 0 & n.control > 0
+   denom <- sum(event.control[s] * (n.case[s] - event.case[s]) / n[s])
+   or <- if(denom==0) NA else 
+    sum(event.case[s] * (n.control[s] - event.control[s]) / n[s]) / denom
+   ors[i] <- or
+ }
> describe(ors)
ors 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
      25       25       14    0.936    1.442    1.935   0.0000   0.0000 
     .25      .50      .75      .90      .95 
  0.0000   0.9375   1.6667   3.6000   5.0667 

0 (10, 0.40), 0.5 (1, 0.04), 0.777777777777778 (1, 0.04), 0.9375 (1, 0.04),
1.16666666666667 (1, 0.04), 1.2 (1, 0.04), 1.5 (2, 0.08), 1.66666666666667 (2,
0.08), 2 (1, 0.04), 2.4 (1, 0.04), 3 (1, 0.04), 4 (1, 0.04), 5.33333333333333
(1, 0.04), 8.4 (1, 0.04)

For the frequency table, variable is rounded to the nearest 0
> 
> 
> 
> cleanEx()
> nameEx("first.word")
> ### * first.word
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: first.word
> ### Title: First Word in a String or Expression
> ### Aliases: first.word
> ### Keywords: character manip
> 
> ### ** Examples
> 
> first.word(expr=expression(y ~ x + log(w)))
[1] "y"
> 
> 
> 
> cleanEx()
> nameEx("format.df")
> ### * format.df
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: format.df
> ### Title: Format a Data Frame or Matrix for LaTeX or HTML
> ### Aliases: format.df
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- data.frame(a=1:2, b=3:4)
> ##D x$m <- 10000*matrix(5:8,nrow=2)
> ##D names(x)
> ##D dim(x)
> ##D x
> ##D format.df(x, big.mark=",")
> ##D dim(format.df(x))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("format.pval")
> ### * format.pval
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: format.pval
> ### Title: Format P Values
> ### Aliases: format.pval
> ### Keywords: print
> 
> ### ** Examples
> 
> format.pval(c(runif(5), pi^-100, NA))
[1] "0.26551" "0.37212" "0.57285" "0.90821" "0.20168" "< 2e-16" "NA"     
> format.pval(c(0.1, 0.0001, 1e-27))
[1] "1e-01"  "1e-04"  "<2e-16"
> format.pval(c(0.1, 1e-27), nsmall=3)
[1] "0.100"  "<2e-16"
> 
> 
> 
> cleanEx()
> nameEx("gbayes")
> ### * gbayes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbayes
> ### Title: Gaussian Bayesian Posterior and Predictive Distributions
> ### Aliases: gbayes plot.gbayes gbayes2 gbayesMixPredNoData gbayesMixPost
> ###   gbayesMixPowerNP gbayes1PowerNP
> ### Keywords: htest
> 
> ### ** Examples
> 
> # Compare 2 proportions using the var stabilizing transformation
> # arcsin(sqrt((x+3/8)/(n+3/4))) (Anscombe), which has variance 
> # 1/[4(n+.5)]
> 
> 
> m1 <- 100;     m2 <- 150
> deaths1 <- 10; deaths2 <- 30
> 
> 
> f <- function(events,n) asin(sqrt((events+3/8)/(n+3/4)))
> stat <- f(deaths1,m1) - f(deaths2,m2)
> var.stat <- function(m1, m2) 1/4/(m1+.5) + 1/4/(m2+.5)
> cat("Test statistic:",format(stat),"  s.d.:",
+     format(sqrt(var.stat(m1,m2))), "\n")
Test statistic: -0.1388297   s.d.: 0.06441034 
> #Use unbiased prior with variance 1000 (almost flat)
> b <- gbayes(0, 1000, m1, m2, stat, var.stat, 2*m1, 2*m2)
> print(b)
$mean.prior
[1] 0

$var.prior
[1] 1000

$mean.post
[1] -0.1388291

$var.post
[1] 0.004148675

$mean.pred
[1] -0.1388291

$var.pred
[1] 0.006227504

attr(,"class")
[1] "gbayes"
> plot(b)
> #To get posterior Prob[parameter > w] use 
> # 1-pnorm(w, b$mean.post, sqrt(b$var.post))
> 
> 
> #If g(effect, n1, n2) is the power function to
> #detect an effect of 'effect' with samples size for groups 1 and 2
> #of n1,n2, estimate the expected power by getting 1000 random
> #draws from the posterior distribution, computing power for
> #each value of the population effect, and averaging the 1000 powers
> #This code assumes that g will accept vector-valued 'effect'
> #For the 2-sample proportion problem just addressed, 'effect'
> #could be taken approximately as the change in the arcsin of
> #the square root of the probability of the event
> 
> 
> g <- function(effect, n1, n2, alpha=.05) {
+   sd <- sqrt(var.stat(n1,n2))
+   z <- qnorm(1 - alpha/2)
+   effect <- abs(effect)
+   1 - pnorm(z - effect/sd) + pnorm(-z - effect/sd)
+ }
> 
> 
> effects <- rnorm(1000, b$mean.post, sqrt(b$var.post))
> powers <- g(effects, 500, 500)
> hist(powers, nclass=35, xlab='Power')
> describe(powers)
powers 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
    1000        0      997        1   0.8567   0.2236   0.1581   0.4029 
     .25      .50      .75      .90      .95 
  0.8509   0.9939   0.9999   1.0000   1.0000 

lowest : 0.0500474 0.0500896 0.0501411 0.0505619 0.0512243
highest: 1         1         1         1         1        
> 
> 
> 
> 
> # gbayes2 examples
> # First consider a study with a binary response where the
> # sample size is n1=500 in the new treatment arm and n2=300
> # in the control arm.  The parameter of interest is the 
> # treated:control log odds ratio, which has variance
> # 1/[n1 p1 (1-p1)] + 1/[n2 p2 (1-p2)].  This is not
> # really constant so we average the variance over plausible
> # values of the probabilities of response p1 and p2.  We
> # think that these are between .4 and .6 and we take a 
> # further short cut
> 
> 
> v <- function(n1, n2, p1, p2) 1/(n1*p1*(1-p1)) + 1/(n2*p2*(1-p2))
> n1 <- 500; n2 <- 300
> ps <- seq(.4, .6, length=100)
> vguess <- quantile(v(n1, n2, ps, ps), .75)
> vguess
       75% 
0.02183459 
> #        75% 
> # 0.02183459
> 
> 
> # The minimally interesting treatment effect is an odds ratio
> # of 1.1.  The prior distribution on the log odds ratio is
> # a 50:50 mixture of a vague Gaussian (mean 0, sd 100) and
> # an informative prior from a previous study (mean 1, sd 1)
> 
> 
> prior <- function(delta) 
+   0.5*dnorm(delta, 0, 100)+0.5*dnorm(delta, 1, 1)
> deltas <- seq(-5, 5, length=150)
> plot(deltas, prior(deltas), type='l')
> 
> 
> # Now compute the power, averaged over this prior
> gbayes2(sqrt(vguess), prior, log(1.1))
[1] 0.6133338
> # [1] 0.6133338
> 
> 
> # See how much power is lost by ignoring the previous
> # study completely
> 
> 
> gbayes2(sqrt(vguess), function(delta)dnorm(delta, 0, 100), log(1.1))
[1] 0.4984588
> # [1] 0.4984588
> 
> 
> # What happens to the power if we really don't believe the treatment
> # is very effective?  Let's use a prior distribution for the log
> # odds ratio that is uniform between log(1.2) and log(1.3).
> # Also check the power against a true null hypothesis
> 
> 
> prior2 <- function(delta) dunif(delta, log(1.2), log(1.3))
> gbayes2(sqrt(vguess), prior2, log(1.1))
[1] 0.1385113
> # [1] 0.1385113
> 
> 
> gbayes2(sqrt(vguess), prior2, 0)
[1] 0.3264065
> # [1] 0.3264065
> 
> 
> # Compare this with the power of a two-sample binomial test to
> # detect an odds ratio of 1.25
> bpower(.5, odds.ratio=1.25, n1=500, n2=300)
    Power 
0.3307486 
> #     Power 
> # 0.3307486
> 
> 
> # For the original prior, consider a new study with equal
> # sample sizes n in the two arms.  Solve for n to get a
> # power of 0.9.  For the variance of the log odds ratio
> # assume a common p in the center of a range of suspected
> # probabilities of response, 0.3.  For this example we
> # use a zero null value and the uniform prior above
> 
> 
> v   <- function(n) 2/(n*.3*.7)
> pow <- function(n) gbayes2(sqrt(v(n)), prior2)
> uniroot(function(n) pow(n)-0.9, c(50,10000))$root
[1] 2119.688
> # [1] 2119.675
> # Check this value
> pow(2119.675)
[1] 0.8999984
> # [1] 0.9
> 
> 
> # Get the posterior density when there is a mixture of two priors,
> # with mixing probability 0.5.  The first prior is almost
> # non-informative (normal with mean 0 and variance 10000) and the
> # second has mean 2 and variance 0.3.  The test statistic has a value
> # of 3 with variance 0.4.
> f <- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3)
> 
> 
> args(f)
function (delta = numeric(0), x = 3, v = 4, mix = 0.5, d0 = 0, 
    v0 = 10000, d1 = 2, v1 = 0.3, dist = function (x, mean = 0, 
        sd = 1, log = FALSE) 
    .Call(C_dnorm, x, mean, sd, log)) 
NULL
> 
> 
> # Plot this density
> delta <- seq(-2, 6, length=150)
> plot(delta, f(delta), type='l')
> 
> 
> # Add to the plot the posterior density that used only
> # the almost non-informative prior
> lines(delta, f(delta, mix=1), lty=2)
> 
> 
> # The same but for an observed statistic of zero
> lines(delta, f(delta, mix=1, x=0), lty=3)
> 
> 
> # Derive the CDF instead of the density
> g <- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3,
+                    what='cdf')
> # Had mix=0 or 1, gbayes1PowerNP could have been used instead
> # of gbayesMixPowerNP below
> 
> 
> # Compute the power to detect an effect of delta=1 if the variance
> # of the test statistic is 0.2
> gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12))
Critical value          Power 
     0.3335535      0.9319167 
> 
> 
> # Do the same thing by simulation
> gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12), nsim=20000)
     Power Lower 0.95 Upper 0.95 
 0.9327000  0.9292277  0.9361723 
> 
> 
> # Compute by what factor the sample size needs to be larger
> # (the variance needs to be smaller) so that the power is 0.9
> ratios <- seq(1, 4, length=50)
> pow <- single(50)
> for(i in 1:50) 
+   pow[i] <- gbayesMixPowerNP(g, 1, 0.2/ratios[i], interval=c(-10,12))[2]
> 
> 
> # Solve for ratio using reverse linear interpolation
> approx(pow, ratios, xout=0.9)$y
[1] NA
> 
> 
> # Check this by computing power
> gbayesMixPowerNP(g, 1, 0.2/2.1, interval=c(-10,12))
Critical value          Power 
     0.3422667      0.9834678 
> # So the study will have to be 2.1 times as large as earlier thought
> 
> 
> 
> cleanEx()
> nameEx("gbayesSeqSim")
> ### * gbayesSeqSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbayesSeqSim
> ### Title: gbayesSeqSim
> ### Aliases: gbayesSeqSim
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Simulate Bayesian operating characteristics for an unadjusted
> ##D # proportional odds comparison (Wilcoxon test)
> ##D # For 100 simulations, 5 looks, 2 true parameter values, and
> ##D # 2 assertion/prior combinations, compute the posterior probability
> ##D # Use a low-level logistic regression call to speed up simuluations
> ##D # Use data.table to compute various summary measures
> ##D # Total simulation time: 2s
> ##D lfit <- function(x, y) {
> ##D f <- rms::lrm.fit(x, y)
> ##D   k <- length(coef(f))
> ##D   c(coef(f)[k], vcov(f)[k, k])
> ##D }
> ##D gdat <- function(beta, n1, n2) {
> ##D   # Cell probabilities for a 7-category ordinal outcome for the control group
> ##D   p <- c(2, 1, 2, 7, 8, 38, 42) / 100
> ##D 
> ##D   # Compute cell probabilities for the treated group
> ##D   p2 <- pomodm(p=p, odds.ratio=exp(beta))
> ##D   y1 <- sample(1 : 7, n1, p,  replace=TRUE)
> ##D   y2 <- sample(1 : 7, n2, p2, replace=TRUE)
> ##D   list(y1=y1, y2=y2)
> ##D }
> ##D 
> ##D # Assertion 1: log(OR) < 0 under prior with prior mean 0.1 and sigma 1 on log OR scale
> ##D # Assertion 2: OR between 0.9 and 1/0.9 with prior mean 0 and sigma computed so that
> ##D # P(OR > 2) = 0.05
> ##D asserts <- list(list('Efficacy', '<', 0, mu=0.1, sigma=1),
> ##D                 list('Similarity', 'in', log(c(0.9, 1/0.9)),
> ##D                      cutprior=log(2), tailprob=0.05))
> ##D 
> ##D set.seed(1)
> ##D est <- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
> ##D                    gendat=gdat,
> ##D                    fitter=lfit, nsim=100)
> ##D z <- gbayesSeqSim(est, asserts)
> ##D head(z)
> ##D attr(z, 'asserts')
> ##D 
> ##D # Compute the proportion of simulations that hit targets (different target posterior
> ##D # probabilities for efficacy vs. similarity)
> ##D 
> ##D # For the efficacy assessment compute the first look at which the target
> ##D # was hit (set to infinity if never hit)
> ##D require(data.table)
> ##D z <- data.table(z)
> ##D u <- z[, .(first=min(p1 > 0.95)), by=.(parameter, sim)]
> ##D # Compute the proportion of simulations that ever hit the target and
> ##D # that hit it by the 100th subject
> ##D u[, .(ever=mean(first < Inf)),  by=.(parameter)]
> ##D u[, .(by75=mean(first <= 100)), by=.(parameter)]
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getHdata")
> ### * getHdata
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getHdata
> ### Title: Download and Install Datasets for 'Hmisc', 'rms', and
> ###   Statistical Modeling
> ### Aliases: getHdata
> ### Keywords: interface data
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getHdata()          # download list of available datasets
> ##D getHdata(prostate)  # downloads, load( ) or data.restore( )
> ##D                     # runs cleanup.import for S-Plus 6
> ##D getHdata(valung, "contents")   # open browser (options(browser="whatever"))
> ##D                     # after downloading valung.html
> ##D                     # (result of html(contents()))
> ##D getHdata(support, "all")  # download and open one browser window
> ##D datadensity(support)
> ##D attach(support)     # make individual variables available
> ##D getHdata(plasma,  "all")  # download and open two browser windows
> ##D                           # (description file is available for plasma)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getRs")
> ### * getRs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getRs
> ### Title: Interact with github rscripts Project
> ### Aliases: getRs
> ### Keywords: interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getRs()             # list available scripts
> ##D scripts <- getRs()  # likewise, but store in an object that can easily
> ##D                     # be viewed on demand in RStudio
> ##D getRs('introda.r')  # download introda.r and put in script editor
> ##D getRs(cats=TRUE)    # list available major and minor categories
> ##D categories <- getRs(cats=TRUE)
> ##D # likewise but store results in a list for later viewing
> ##D getRs(cats='reg')   # list all scripts in a major category containing 'reg'
> ##D getRs('importREDCap.r')   # source() to define a function
> ##D # source() a new version of the Hmisc package's cut2 function:
> ##D getRs('cut2.s', grepo='Hmisc', dir='R')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getZip")
> ### * getZip
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getZip
> ### Title: Open a Zip File From a URL
> ### Aliases: getZip
> ### Keywords: file IO
> 
> ### ** Examples
> 
> ## Not run: 
> ##D read.csv(getZip('http://test.com/z.zip'))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ggMisc")
> ### * ggMisc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: colorFacet
> ### Title: Miscellaneous ggplot2 and grid Helper Functions
> ### Aliases: colorFacet arrGrob print.arrGrob
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D require(ggplot2)
> ##D s <- summaryP(age + sex ~ region + treatment)
> ##D colorFacet(ggplot(s))   # prints directly
> ##D # arrGrob is called by rms::ggplot.Predict and others
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ggfreqScatter")
> ### * ggfreqScatter
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ggfreqScatter
> ### Title: Frequency Scatterplot
> ### Aliases: ggfreqScatter
> ### Keywords: hplot
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> set.seed(1)
> x <- rnorm(1000)
> y <- rnorm(1000)
> count <- sample(1:100, 1000, TRUE)
> x <- rep(x, count)
> y <- rep(y, count)
> # color=alpha=NULL below makes loess smooth over all points
> g <- ggfreqScatter(x, y) +   # might add g=0 if using plotly
+       geom_smooth(aes(color=NULL, alpha=NULL), se=FALSE) +
+       ggtitle("Using Deciles of Frequency Counts, 2500 Bins")
> g
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
Warning: The following aesthetics were dropped during statistical transformation: label.
ℹ This can happen when ggplot fails to infer the correct grouping structure in
  the data.
ℹ Did you forget to specify a `group` aesthetic or to convert a numerical
  variable into a factor?
> # plotly::ggplotly(g, tooltip='label')  # use plotly, hover text = freq. only
> # Plotly makes it somewhat interactive, with hover text tooltips
> 
> # Instead use varying-height sticks to depict frequencies
> ggfreqScatter(x, y, stick=TRUE) +
+  labs(subtitle='Relative height of black lines to gray lines
+ is proportional to cell frequency.
+ Note that points with even tiny frequency are visable
+ (gray line with no visible black line).')
Warning: Removed 1 row containing missing values or values outside the scale range
(`geom_segment()`).
Warning: Removed 1 row containing missing values or values outside the scale range
(`geom_segment()`).
> 
> 
> # Try with x categorical
> x1 <- sample(c('cat', 'dog', 'giraffe'), length(x), TRUE)
> ggfreqScatter(x1, y)
> 
> # Try with y categorical
> y1 <- sample(LETTERS[1:10], length(x), TRUE)
> ggfreqScatter(x, y1)
> 
> # Both categorical, larger point symbols, box instead of circle
> ggfreqScatter(x1, y1, shape=15, size=7)
> # Vary box size instead
> ggfreqScatter(x1, y1, nsize=TRUE, shape=15)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("hdquantile")
> ### * hdquantile
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hdquantile
> ### Title: Harrell-Davis Distribution-Free Quantile Estimator
> ### Aliases: hdquantile
> ### Keywords: univar
> 
> ### ** Examples
> 
> set.seed(1)
> x <- runif(100)
> hdquantile(x, (1:3)/4, se=TRUE)
     0.25      0.50      0.75 
0.3064350 0.5054821 0.7571213 
attr(,"se")
      0.25       0.50       0.75 
0.03931114 0.04878268 0.02997025 
> 
> ## Not run: 
> ##D # Compare jackknife standard errors with those from the bootstrap
> ##D library(boot)
> ##D boot(x, function(x,i) hdquantile(x[i], probs=(1:3)/4), R=400)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hidingTOC")
> ### * hidingTOC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hidingTOC
> ### Title: Moving and Hiding Table of Contents
> ### Aliases: hidingTOC
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hidingTOC()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hist.data.frame")
> ### * hist.data.frame
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hist.data.frame
> ### Title: Histograms for Variables in a Data Frame
> ### Aliases: hist.data.frame
> ### Keywords: hplot dplot distribution
> 
> ### ** Examples
> 
> d <- data.frame(a=runif(200), b=rnorm(200),
+                 w=factor(sample(c('green','red','blue'), 200, TRUE)))
> hist.data.frame(d)   # in R, just say hist(d)
> 
> 
> 
> cleanEx()
> nameEx("histbackback")
> ### * histbackback
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: histbackback
> ### Title: Back to Back Histograms
> ### Aliases: histbackback
> ### Keywords: dplot hplot distribution
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(1)
> histbackback(rnorm(20), rnorm(30))
> 
> 
> fool <- list(x=rnorm(40), y=rnorm(40))
> histbackback(fool)
> age <- rnorm(1000,50,10)
> sex <- sample(c('female','male'),1000,TRUE)
> histbackback(split(age, sex))
> agef <- age[sex=='female']; agem <- age[sex=='male']
> histbackback(list(Female=agef,Male=agem), probability=TRUE, xlim=c(-.06,.06))
> 
> 
> 
> cleanEx()
> nameEx("histboxp")
> ### * histboxp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: histboxp
> ### Title: Use plotly to Draw Stratified Spike Histogram and Box Plot
> ###   Statistics
> ### Aliases: histboxp histboxpM dhistboxp
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dist <- c(rep(1, 500), rep(2, 250), rep(3, 600))
> ##D Distribution <- factor(dist, 1 : 3, c('Unimodal', 'Bimodal', 'Trimodal'))
> ##D x <- c(rnorm(500, 6, 1),
> ##D        rnorm(200, 3, .7), rnorm(50, 7, .4),
> ##D        rnorm(200, 2, .7), rnorm(300, 5.5, .4), rnorm(100, 8, .4))
> ##D histboxp(x=x, group=Distribution, sd=TRUE)
> ##D X <- data.frame(x, x2=runif(length(x)))
> ##D histboxpM(x=X, group=Distribution, ncols=2)  # separate plots
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hlab")
> ### * hlab
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hlab
> ### Title: hlab
> ### Aliases: hlab
> 
> ### ** Examples
> 
> d <- data.frame(x=1:10, y=(1:10)/10)
> d <- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
> hlab(x)
expression(list(X, scriptstyle(mmHg)))
> hlab(x, html=TRUE)
[1] "X <span style='font-family:Verdana;font-size:75%;'>mmHg</span>"
> hlab(z)
expression(z)
> require(ggplot2)
Loading required package: ggplot2
> ggplot(d, aes(x, y)) + geom_point() + labs(x=hlab(x), y=hlab(y))
> # Can use xlab(hlab(x)) + ylab(hlab(y)) also
> # Store names, labels, units for all variables in d in object
> LabelsUnits <- extractlabs(d)
> # Remove d; labels/units still found
> rm(d)
> hlab(x)
expression(list(X, scriptstyle(mmHg)))
> # Remove LabelsUnits and use a current dataset named
> # d2 instead of the default d
> rm(LabelsUnits)
> options(current_ds='d2')
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("hlabs")
> ### * hlabs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hlabs
> ### Title: hlabs
> ### Aliases: hlabs
> 
> ### ** Examples
> 
> # Name the current dataset d, or specify a name with
> # options(curr_ds='...') or run `extractlabs`, then
> # ggplot(d, aes(x,y)) + geom_point() + hlabs(x,y)
> # to specify only the x-axis label use hlabs(x), or to
> # specify only the y-axis label use hlabs(y=...)
> 
> 
> 
> cleanEx()
> nameEx("hoeffd")
> ### * hoeffd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hoeffd
> ### Title: Matrix of Hoeffding's D Statistics
> ### Aliases: hoeffd print.hoeffd
> ### Keywords: nonparametric htest
> 
> ### ** Examples
> 
> x <- c(-2, -1, 0, 1, 2)
> y <- c(4,   1, 0, 1, 4)
> z <- c(1,   2, 3, 4, NA)
> q <- c(1,   2, 3, 4, 5)
> hoeffd(cbind(x,y,z,q))
D
   x  y  z  q
x  1  0 NA  1
y  0  1 NA  0
z NA NA  1 NA
q  1  0 NA  1

avg|F(x,y)-G(x)H(y)|
     x    y z    q
x 0.00 0.04 0 0.16
y 0.04 0.00 0 0.04
z 0.00 0.00 0 0.00
q 0.16 0.04 0 0.00

max|F(x,y)-G(x)H(y)|
     x   y z    q
x 0.00 0.1 0 0.24
y 0.10 0.0 0 0.10
z 0.00 0.0 0 0.00
q 0.24 0.1 0 0.00

n
  x y z q
x 5 5 4 5
y 5 5 4 5
z 4 4 4 4
q 5 5 4 5

P
  x     y     z q    
x       0.363   0.000
y 0.363         0.363
z                    
q 0.000 0.363        
> 
> 
> # Hoeffding's test can detect even one-to-many dependency
> set.seed(1)
> x <- seq(-10,10,length=200)
> y <- x*sign(runif(200,-1,1))
> plot(x,y)
> hoeffd(x,y)
D
     x    y
x 1.00 0.06
y 0.06 1.00

avg|F(x,y)-G(x)H(y)|
       x      y
x 0.0000 0.0407
y 0.0407 0.0000

max|F(x,y)-G(x)H(y)|
       x      y
x 0.0000 0.0763
y 0.0763 0.0000

n= 200 

P
  x  y 
x     0
y  0   
> 
> 
> 
> cleanEx()
> nameEx("html")
> ### * html
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: html
> ### Title: Convert an S object to HTML
> ### Aliases: html html.latex html.data.frame html.default htmlVerbatim
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','e')))
> ##D w <- latex(x)
> ##D h <- html(w) # run HeVeA to convert .tex to .html
> ##D h <- html(x) # convert x directly to html
> ##D w <- html(x, link=c('','B'))   # hyperlink first row first col to B
> ##D 
> ##D # Assuming system package tex4ht is installed, easily convert advanced
> ##D # LaTeX tables to html
> ##D getHdata(pbc)
> ##D s <- summaryM(bili + albumin + stage + protime + sex + age + spiders ~ drug,
> ##D               data=pbc, test=TRUE)
> ##D w <- latex(s, npct='slash', file='s.tex')
> ##D z <- html(w)
> ##D browseURL(z$file)
> ##D 
> ##D d <- describe(pbc)
> ##D w <- latex(d, file='d.tex')
> ##D z <- html(w)
> ##D browseURL(z$file)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("impute")
> ### * impute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: impute
> ### Title: Generic Functions and Methods for Imputation
> ### Aliases: impute impute.default print.impute summary.impute [.impute
> ###   is.imputed
> ### Keywords: methods math htest models
> 
> ### ** Examples
> 
> age <- c(1,2,NA,4)
> age.i <- impute(age)
> # Could have used impute(age,2.5), impute(age,mean), impute(age,"random")
> age.i
 1  2  3  4 
 1  2 2*  4 
> summary(age.i)

 1 values imputed to 2 

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    1.75    2.00    2.25    2.50    4.00 
> is.imputed(age.i)
[1] FALSE FALSE  TRUE FALSE
> 
> 
> 
> cleanEx()
> nameEx("knitrSet")
> ### * knitrSet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: knitrSet
> ### Title: knitr Setup and plotly Service Function
> ### Aliases: knitrSet plotlySave
> ### Keywords: interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Typical call (without # comment symbols):
> ##D # <<echo=FALSE>>=
> ##D # require(Hmisc)
> ##D # knitrSet()
> ##D # @
> ##D 
> ##D knitrSet()    # use all defaults and don't use a graphics file prefix
> ##D knitrSet('modeling')   # use modeling- prefix for a major section or chapter
> ##D knitrSet(cache=TRUE, echo=FALSE)  # global default to cache and not print code
> ##D knitrSet(w=5,h=3.75)   # override default figure width, height
> ##D 
> ##D # ```{r chunkname}
> ##D # p <- plotly::plot_ly(...)
> ##D # plotlySave(p)   # creates fig.path/chunkname.png
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("labcurve")
> ### * labcurve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: labcurve
> ### Title: Label Curves, Make Keys, and Interactively Draw Points and
> ###   Curves
> ### Aliases: labcurve putKey putKeyEmpty largest.empty drawPlot
> ###   plot.drawPlot bezier
> ### Keywords: hplot aplot dplot iplot
> 
> ### ** Examples
> 
> n <- 2:8
> m <-  length(n)
> type <- c('l','l','l','l','s','l','l')
> # s=step function l=ordinary line (polygon)
> curves <- vector('list', m)
> 
> 
> plot(0,1,xlim=c(0,1),ylim=c(-2.5,4),type='n')
> 
> 
> set.seed(39)
> 
> 
> for(i in 1:m) {
+   x <- sort(runif(n[i]))
+   y <- rnorm(n[i])
+   lines(x, y, lty=i, type=type[i], col=i)
+   curves[[i]] <- list(x=x,y=y)
+ }
> 
> 
> labels <- paste('Label for',letters[1:m])
> labcurve(curves, labels, tilt=TRUE, type=type, col=1:m)
> 
> 
> # Put only single letters on curves at points of 
> # maximum space, and use key() to define the letters,
> # with automatic positioning of the key in the most empty
> # part of the plot
> # Have labcurve do the plotting, leaving extra space for key
> 
> 
> names(curves) <- labels
> labcurve(curves, keys=letters[1:m], type=type, col=1:m,
+          pl=TRUE, ylim=c(-2.5,4))
> 
> 
> # Put plotting symbols at equally-spaced points,
> # with a key for the symbols, ignoring line types
> 
> 
> labcurve(curves, keys=1:m, lty=1, type=type, col=1:m,
+          pl=TRUE, ylim=c(-2.5,4))
> 
> 
> 
> 
> # Plot and label two curves, with line parameters specified with data
> set.seed(191)
> ages.f <- sort(rnorm(50,20,7))
> ages.m <- sort(rnorm(40,19,7))
> height.f <- pmin(ages.f,21)*.2+60
> height.m <- pmin(ages.m,21)*.16+63
> 
> 
> labcurve(list(Female=list(ages.f,height.f,col=2),
+               Male  =list(ages.m,height.m,col=3,lty='dashed')),
+          xlab='Age', ylab='Height', pl=TRUE)
> # add ,keys=c('f','m') to label curves with single letters
> # For S-Plus use lty=2
> 
> 
> # Plot power for testing two proportions vs. n for various odds ratios, 
> # using 0.1 as the probability of the event in the control group.  
> # A separate curve is plotted for each odds ratio, and the curves are
> # labeled at points of maximum separation
> 
> 
> n  <- seq(10, 1000, by=10)
> OR <- seq(.2,.9,by=.1)
> pow <- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
+               n=n)
> names(pow) <- format(OR)
> labcurve(pow, pl=TRUE, xlab='n', ylab='Power')
> 
> 
> # Plot some random data and find the largest empty rectangle
> # that is at least .1 wide and .1 tall
> 
> 
> x <- runif(50)
> y <- runif(50)
> plot(x, y)
> z <- largest.empty(x, y, .1, .1)
> z
$x
[1] 0.754

$y
[1] 0.0705

$rect
$rect$x
[1] 0.479 1.028 1.028 0.479

$rect$y
[1] -0.0246 -0.0246  0.1656  0.1656


$area
[1] 0.104

> points(z,pch=3)  # mark center of rectangle, or
> polygon(z$rect, col='blue')  # to draw the rectangle, or
> #key(z$x, z$y, \dots stuff for legend)
> 
> 
> 
> 
> # Use the mouse to draw a series of points using one symbol, and
> # two smooth curves or straight lines (if two points are clicked), 
> # none of these being labeled
> 
> 
> # d <- drawPlot(Points(), Curve(), Curve())
> # plot(d)
> 
> 
> ## Not run: 
> ##D # Use the mouse to draw a Gaussian density, two series of points
> ##D # using 2 symbols, one Bezier curve, a step function, and raw data
> ##D # along the x-axis as a 1-d scatter plot (rug plot).  Draw a key.
> ##D # The density function is fit to 3 mouse clicks
> ##D # Abline draws a dotted horizontal reference line
> ##D d <- drawPlot(Curve('Normal',type='gauss'),
> ##D               Points('female'), Points('male'), 
> ##D               Curve('smooth',ask=TRUE,lty=2), Curve('step',type='s',lty=3), 
> ##D               Points(type='r'), Abline(h=.5, lty=2),
> ##D               xlab='X', ylab='y', xlim=c(0,100), key=TRUE)
> ##D plot(d, ylab='Y')
> ##D plot(d, key=FALSE)  # label groups using labcurve
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("label")
> ### * label
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: label
> ### Title: Label Attribute of an Object
> ### Aliases: label label<- label.default label.Surv label<-.default
> ###   labelPlotmath labelLatex [.labelled print.labelled Label
> ###   Label.data.frame llist prList putHcap putHfig plotmathTranslate
> ###   as.data.frame.labelled data.frame.labelled reLabelled
> ###   label.data.frame label<-.data.frame relevel.labelled combineLabels
> ### Keywords: attribute misc utilities
> 
> ### ** Examples
> 
> age <- c(21,65,43)
> y   <- 1:3
> label(age) <- "Age in Years"
> plot(age, y, xlab=label(age))
> 
> data <- data.frame(age=age, y=y)
> label(data)
           age              y 
"Age in Years"             "" 
> 
> label(data, self=TRUE) <- "A data frame"
> label(data, self=TRUE)
[1] "A data frame"
> 
> x1 <- 1:10
> x2 <- 10:1
> label(x2) <- 'Label for x2'
> units(x2) <- 'mmHg'
> x2
Label for x2 [mmHg] 
 [1] 10  9  8  7  6  5  4  3  2  1
> x2[1:5]
Label for x2 [mmHg] 
[1] 10  9  8  7  6
> dframe <- data.frame(x1, x2)
> Label(dframe)
label(x1)	<- ''
label(x2)	<- 'Label for x2'
> 
> labelLatex(x2, hfill=TRUE, bold=TRUE)
[1] "{\\textbf Label for x2}\\hfill {\\smaller[2] mmHg}"
> labelLatex(label='Velocity', units='m/s')
[1] "Velocity {\\smaller[2] m/s}"
> 
> ##In these examples of llist, note that labels are printed after
> ##variable names, because of print.labelled
> a <- 1:3
> b <- 4:6
> label(b) <- 'B Label'
> llist(a,b)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

> llist(a,b,d=0)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

$d
d 
[1] 0

> llist(a,b,0)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

$`0`
0 
[1] 0

> 
> 
> w <- llist(a, b>5, d=101:103)
> sapply(w, function(x){
+   hist(as.numeric(x), xlab=label(x))
+   # locator(1)   ## wait for mouse click
+ })
         a               b > 5           d              
breaks   numeric,5       numeric,3       numeric,5      
counts   integer,4       integer,2       integer,4      
density  numeric,4       numeric,2       numeric,4      
mids     numeric,4       numeric,2       numeric,4      
xname    "as.numeric(x)" "as.numeric(x)" "as.numeric(x)"
equidist TRUE            TRUE            TRUE           
> 
> # Or: for(u in w) {hist(u); title(label(u))}
> 
> 
> 
> cleanEx()
> nameEx("latex")
> ### * latex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latex
> ### Title: Convert an S object to LaTeX, and Related Utilities
> ### Aliases: latex latex.default latex.function latex.list latexTranslate
> ###   htmlTranslate latexSN htmlSN latexVerbatim dvi print.dvi dvi.latex
> ###   dvips dvips.latex dvips.dvi dvigv dvigv.latex dvigv.dvi print.latex
> ###   show.latex show.dvi
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
> ## Not run: 
> ##D latex(x)   # creates x.tex in working directory
> ##D # The result of the above command is an object of class "latex"
> ##D # which here is automatically printed by the latex print method.
> ##D # The latex print method prepends and appends latex headers and
> ##D # calls the latex program in the PATH.  If the latex program is
> ##D # not in the PATH, you will get error messages from the operating
> ##D # system.
> ##D 
> ##D w <- latex(x, file='/tmp/my.tex')
> ##D # Does not call the latex program as the print method was not invoked
> ##D print.default(w)
> ##D # Shows the contents of the w variable without attempting to latex it.
> ##D 
> ##D d <- dvi(w)  # compile LaTeX document, make .dvi
> ##D              # latex assumed to be in path
> ##D d            # or show(d) : run xdvi (assumed in path) to display
> ##D w            # or show(w) : run dvi then xdvi
> ##D dvips(d)     # run dvips to print document
> ##D dvips(w)     # run dvi then dvips
> ##D library(tools)
> ##D texi2dvi('/tmp/my.tex')   # compile and produce pdf file in working dir.
> ## End(Not run)
> latex(x, file="")   # just write out LaTeX code to screen
\begin{table}[!tbp]
\begin{center}
\begin{tabular}{lrrr}
\hline\hline
\multicolumn{1}{l}{x}&\multicolumn{1}{c}{c}&\multicolumn{1}{c}{d}&\multicolumn{1}{c}{this that}\tabularnewline
\hline
a&$1$&$3$&$5$\tabularnewline
b&$2$&$4$&$6$\tabularnewline
\hline
\end{tabular}\end{center}
\end{table}
> 
> ## Not run: 
> ##D # Use paragraph formatting to wrap text to 3 in. wide in a column
> ##D d <- data.frame(x=1:2,
> ##D                 y=c(paste("a",
> ##D                     paste(rep("very",30),collapse=" "),"long string"),
> ##D                 "a short string"))
> ##D latex(d, file="", col.just=c("l", "p{3in}"), table.env=FALSE)
> ## End(Not run)
> 
> ## Not run: 
> ##D # After running latex( ) multiple times with different special styles in
> ##D # effect, make a file that will call for the needed LaTeX packages when
> ##D # latex is run (especially when using Sweave with R)
> ##D if(exists(latexStyles))
> ##D   cat(paste('\usepackage{',latexStyles,'}',sep=''),
> ##D       file='stylesused.tex', sep='\n')
> ##D # Then in the latex job have something like:
> ##D # \documentclass{article}
> ##D # \input{stylesused}
> ##D # \begin{document}
> ##D # ...
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("latexDotchart")
> ### * latexDotchart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexDotchart
> ### Title: Enhanced Dot Chart for LaTeX Picture Environment with epic
> ### Aliases: latexDotchart
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D z <- latexDotchart(c(.1,.2), c('a','bbAAb'), xlab='This Label',
> ##D                    auxdata=c(.1,.2), auxtitle='Zcriteria')
> ##D f <- '/tmp/t.tex'
> ##D cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n', file=f)
> ##D cat(z, sep='\n', file=f, append=TRUE)
> ##D cat('\end{document}\n', file=f, append=TRUE)
> ##D 
> ##D set.seed(135)
> ##D maj <- factor(c(rep('North',13),rep('South',13)))
> ##D g <- paste('Category',rep(letters[1:13],2))
> ##D n <- sample(1:15000, 26, replace=TRUE)
> ##D y1 <- runif(26)
> ##D y2 <- pmax(0, y1 - runif(26, 0, .1))
> ##D z <- latexDotchart(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y',
> ##D                    size.group.labels='large', ttlabels=TRUE)
> ##D f <- '/tmp/t2.tex'
> ##D cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n\framebox{', file=f)
> ##D cat(z, sep='\n', file=f, append=TRUE)
> ##D cat('}\end{document}\n', file=f, append=TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("latexTabular")
> ### * latexTabular
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexTabular
> ### Title: Convert a Data Frame or Matrix to a LaTeX Tabular
> ### Aliases: latexTabular
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
> latexTabular(x)   # a character string with LaTeX markup
[1] "{\\fontfamily{phv}\\selectfont \\begin{tabular}{ccc}\nc&d&this that\\\\\n1&3&5\\\\\n2&4&6\\\\\n\\end{tabular}}"
> 
> 
> 
> cleanEx()
> nameEx("latexTherm")
> ### * latexTherm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexTherm
> ### Title: Create LaTeX Thermometers and Colored Needles
> ### Aliases: latexTherm latexNeedle pngNeedle
> ### Keywords: utilities interface file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # The following is in the Hmisc tests directory
> ##D # For a knitr example see latexTherm.Rnw in that directory
> ##D ct <- function(...) cat(..., sep='')
> ##D ct('\documentclass{report}\begin{document}\n')
> ##D latexTherm(c(1, 1, 1, 1), name='lta')
> ##D latexTherm(c(.5, .7, .4, .2), name='ltb')
> ##D latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltc', extra=0)
> ##D latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltcc')
> ##D latexTherm(c(0, 0, 0, 0), name='ltd')
> ##D ct('This is a the first:\lta and the second:\ltb\\ and the third
> ##D without extra:\ltc END\\\nThird with extra:\ltcc END\\ 
> ##D \vspace{2in}\\ 
> ##D All data = zero, frame only:\ltd\\
> ##D \end{document}\n')
> ##D w <- pngNeedle(c(.2, .5, .7))
> ##D cat(tobase64image(w))  # can insert this directly into an html file
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("list.tree")
> ### * list.tree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: list.tree
> ### Title: Pretty-print the Structure of a Data Object
> ### Aliases: list.tree
> ### Keywords: documentation
> 
> ### ** Examples
> 
> X <- list(a=ordered(c(1:30,30:1)),b=c("Rick","John","Allan"),
+           c=diag(300),e=cbind(p=1008:1019,q=4))
> list.tree(X)
 X = list 4 (724376 bytes)
.  a = integer 60= factor (30 levels)( ordered factor )= 1 2 3 4 5 6 7 8 ...
.  b = character 3= Rick John Allan 
.  c = double 90000= array 300 X 300= 1 0 0 0 0 0 0 0 ...
.  e = double 24= named array 12 X 2= 1008 1009 1010 1011 ...
> # In R you can say str(X)
> 
> 
> 
> cleanEx()
> nameEx("mApply")
> ### * mApply
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mApply
> ### Title: Apply a Function to Rows of a Matrix or Vector
> ### Aliases: mApply
> ### Keywords: iteration category
> 
> ### ** Examples
> 
> require(datasets, TRUE)
> a <- mApply(iris[,-5], iris$Species, mean)
> 
> 
> 
> cleanEx()
> nameEx("mChoice")
> ### * mChoice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mChoice
> ### Title: Methods for Storing and Analyzing Multiple Choice Variables
> ### Aliases: mChoice format.mChoice print.mChoice summary.mChoice
> ###   as.character.mChoice as.double.mChoice inmChoice inmChoicelike
> ###   nmChoice match.mChoice [.mChoice print.summary.mChoice is.mChoice
> ###   Math.mChoice Ops.mChoice Summary.mChoice
> ### Keywords: category manip
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(3)
> n <- 20
> sex <- factor(sample(c("m","f"), n, rep=TRUE))
> age <- rnorm(n, 50, 5)
> treatment <- factor(sample(c("Drug","Placebo"), n, rep=TRUE))
> 
> 
> # Generate a 3-choice variable; each of 3 variables has 5 possible levels
> symp <- c('Headache','Stomach Ache','Hangnail',
+           'Muscle Ache','Depressed')
> symptom1 <- sample(symp, n, TRUE)
> symptom2 <- sample(symp, n, TRUE)
> symptom3 <- sample(symp, n, TRUE)
> cbind(symptom1, symptom2, symptom3)[1:5,]
     symptom1       symptom2      symptom3      
[1,] "Hangnail"     "Headache"    "Headache"    
[2,] "Muscle Ache"  "Depressed"   "Depressed"   
[3,] "Muscle Ache"  "Hangnail"    "Muscle Ache" 
[4,] "Stomach Ache" "Muscle Ache" "Stomach Ache"
[5,] "Headache"     "Muscle Ache" "Depressed"   
> Symptoms <- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
> Symptoms
 [1] Hangnail;Headache                 Muscle Ache;Depressed            
 [3] Hangnail;Muscle Ache              Muscle Ache;Stomach Ache         
 [5] Muscle Ache;Headache;Depressed    Hangnail;Muscle Ache;Headache    
 [7] Stomach Ache;Headache;Depressed   Stomach Ache;Depressed           
 [9] Stomach Ache;Depressed            Muscle Ache;Depressed            
[11] Hangnail;Stomach Ache;Headache    Hangnail;Muscle Ache;Stomach Ache
[13] Hangnail;Stomach Ache;Depressed   Muscle Ache;Headache             
[15] Hangnail;Muscle Ache;Stomach Ache Hangnail;Stomach Ache;Headache   
[17] Depressed                         Hangnail;Muscle Ache;Headache    
[19] Stomach Ache;Headache             Muscle Ache;Stomach Ache;Headache
attr(,"label")
[1] Primary Symptoms
Levels: Hangnail Muscle Ache Stomach Ache Headache Depressed
> print(Symptoms, long=TRUE)
 [1] Hangnail;Headache                 Muscle Ache;Depressed            
 [3] Hangnail;Muscle Ache              Muscle Ache;Stomach Ache         
 [5] Muscle Ache;Headache;Depressed    Hangnail;Muscle Ache;Headache    
 [7] Stomach Ache;Headache;Depressed   Stomach Ache;Depressed           
 [9] Stomach Ache;Depressed            Muscle Ache;Depressed            
[11] Hangnail;Stomach Ache;Headache    Hangnail;Muscle Ache;Stomach Ache
[13] Hangnail;Stomach Ache;Depressed   Muscle Ache;Headache             
[15] Hangnail;Muscle Ache;Stomach Ache Hangnail;Stomach Ache;Headache   
[17] Depressed                         Hangnail;Muscle Ache;Headache    
[19] Stomach Ache;Headache             Muscle Ache;Stomach Ache;Headache
attr(,"label")
[1] Primary Symptoms
Levels: Hangnail Muscle Ache Stomach Ache Headache Depressed
> format(Symptoms[1:5])
[1] "Hangnail;Headache"              "Muscle Ache;Depressed"         
[3] "Hangnail;Muscle Ache"           "Muscle Ache;Stomach Ache"      
[5] "Muscle Ache;Headache;Depressed"
> inmChoice(Symptoms,'Headache')
 [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE
> inmChoicelike(Symptoms, 'head', ignore.case=TRUE)
 [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE
> levels(Symptoms)
[1] "Hangnail"     "Muscle Ache"  "Stomach Ache" "Headache"     "Depressed"   
> inmChoice(Symptoms, 3)
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[13]  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE
> # Find all subjects with either of two symptoms
> inmChoice(Symptoms, c('Headache','Hangnail'))
 [1]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> # Note: In this example, some subjects have the same symptom checked
> # multiple times; in practice these redundant selections would be NAs
> # mChoice will ignore these redundant selections
> # Find all subjects with both symptoms
> inmChoice(Symptoms, c('Headache', 'Hangnail'), condition='all')
 [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE
> 
> meanage <- N <- numeric(5)
> for(j in 1:5) {
+  meanage[j] <- mean(age[inmChoice(Symptoms,j)])
+  N[j] <- sum(inmChoice(Symptoms,j))
+ }
> names(meanage) <- names(N) <- levels(Symptoms)
> meanage
    Hangnail  Muscle Ache Stomach Ache     Headache    Depressed 
        47.9         47.8         48.1         47.5         49.9 
> N
    Hangnail  Muscle Ache Stomach Ache     Headache    Depressed 
           9           11           11           10            8 
> 
> # Manually compute mean age for 2 symptoms
> mean(age[symptom1=='Headache' | symptom2=='Headache' | symptom3=='Headache'])
[1] 47.5
> mean(age[symptom1=='Hangnail' | symptom2=='Hangnail' | symptom3=='Hangnail'])
[1] 47.9
> 
> summary(Symptoms)

15 unique combinations

Primary Symptoms

Frequencies of Numbers of Choices Per Observation

 1  2  3 
 1  9 10 

Pairwise Frequencies (Diagonal Contains Marginal Frequencies)

             Hangnail Muscle Ache Stomach Ache Headache Depressed
Hangnail      9        5           5            5        1       
Muscle Ache           11           4            5        3       
Stomach Ache                      11            5        4       
Headache                                       10        2       
Depressed                                                8       

 Frequencies of Top 5 Combinations 

    Hangnail;Muscle Ache;Headache Hangnail;Muscle Ache;Stomach Ache 
                                2                                 2 
   Hangnail;Stomach Ache;Headache             Muscle Ache;Depressed 
                                2                                 2 
           Stomach Ache;Depressed 
                                2 
> 
> #Frequency table sex*treatment, sex*Symptoms
> summary(sex ~ treatment + Symptoms, fun=table)
sex      N= 20  

+---------+------------+--+--+-+
|         |            | N| f|m|
+---------+------------+--+--+-+
|treatment|        Drug|11| 6|5|
|         |     Placebo| 9| 7|2|
+---------+------------+--+--+-+
| Symptoms|    Hangnail| 9| 6|3|
|         | Muscle Ache|11| 8|3|
|         |Stomach Ache|11| 6|5|
|         |    Headache|10| 7|3|
|         |   Depressed| 8| 5|3|
+---------+------------+--+--+-+
|  Overall|            |20|13|7|
+---------+------------+--+--+-+
> # Check:
> ma <- inmChoice(Symptoms, 'Muscle Ache')
> table(sex[ma])

f m 
8 3 
> 
> # could also do:
> # summary(sex ~ treatment + mChoice(symptom1,symptom2,symptom3), fun=table)
> 
> #Compute mean age, separately by 3 variables
> summary(age ~ sex + treatment + Symptoms)
age      N= 20  

+---------+------------+--+----+
|         |            | N| age|
+---------+------------+--+----+
|      sex|           f|13|47.7|
|         |           m| 7|49.8|
+---------+------------+--+----+
|treatment|        Drug|11|47.4|
|         |     Placebo| 9|49.7|
+---------+------------+--+----+
| Symptoms|    Hangnail| 9|47.9|
|         | Muscle Ache|11|47.8|
|         |Stomach Ache|11|48.1|
|         |    Headache|10|47.5|
|         |   Depressed| 8|49.9|
+---------+------------+--+----+
|  Overall|            |20|48.4|
+---------+------------+--+----+
> 
> 
> summary(age ~ sex + treatment + Symptoms, method="cross")

 mean by sex, treatment, Symptoms 

    sex treatment                          Symptoms  N  age
1     f      Drug                         Depressed  0   NA
2     m      Drug                         Depressed  0   NA
3   ALL      Drug                         Depressed  0   NA
4     f   Placebo                         Depressed  1 55.8
5     m   Placebo                         Depressed  0   NA
6   ALL   Placebo                         Depressed  1 55.8
7     f       ALL                         Depressed  1 55.8
8     m       ALL                         Depressed  0   NA
9   ALL       ALL                         Depressed  1 55.8
10    f      Drug                 Hangnail;Headache  0   NA
11    m      Drug                 Hangnail;Headache  1 46.3
12  ALL      Drug                 Hangnail;Headache  1 46.3
13    f   Placebo                 Hangnail;Headache  0   NA
14    m   Placebo                 Hangnail;Headache  0   NA
15  ALL   Placebo                 Hangnail;Headache  0   NA
16    f       ALL                 Hangnail;Headache  0   NA
17    m       ALL                 Hangnail;Headache  1 46.3
18  ALL       ALL                 Hangnail;Headache  1 46.3
19    f      Drug              Hangnail;Muscle Ache  1 46.4
20    m      Drug              Hangnail;Muscle Ache  0   NA
21  ALL      Drug              Hangnail;Muscle Ache  1 46.4
22    f   Placebo              Hangnail;Muscle Ache  0   NA
23    m   Placebo              Hangnail;Muscle Ache  0   NA
24  ALL   Placebo              Hangnail;Muscle Ache  0   NA
25    f       ALL              Hangnail;Muscle Ache  1 46.4
26    m       ALL              Hangnail;Muscle Ache  0   NA
27  ALL       ALL              Hangnail;Muscle Ache  1 46.4
28    f      Drug     Hangnail;Muscle Ache;Headache  1 48.5
29    m      Drug     Hangnail;Muscle Ache;Headache  0   NA
30  ALL      Drug     Hangnail;Muscle Ache;Headache  1 48.5
31    f   Placebo     Hangnail;Muscle Ache;Headache  0   NA
32    m   Placebo     Hangnail;Muscle Ache;Headache  1 55.1
33  ALL   Placebo     Hangnail;Muscle Ache;Headache  1 55.1
34    f       ALL     Hangnail;Muscle Ache;Headache  1 48.5
35    m       ALL     Hangnail;Muscle Ache;Headache  1 55.1
36  ALL       ALL     Hangnail;Muscle Ache;Headache  2 51.8
37    f      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
38    m      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
39  ALL      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
40    f   Placebo Hangnail;Muscle Ache;Stomach Ache  2 46.4
41    m   Placebo Hangnail;Muscle Ache;Stomach Ache  0   NA
42  ALL   Placebo Hangnail;Muscle Ache;Stomach Ache  2 46.4
43    f       ALL Hangnail;Muscle Ache;Stomach Ache  2 46.4
44    m       ALL Hangnail;Muscle Ache;Stomach Ache  0   NA
45  ALL       ALL Hangnail;Muscle Ache;Stomach Ache  2 46.4
46    f      Drug   Hangnail;Stomach Ache;Depressed  0   NA
47    m      Drug   Hangnail;Stomach Ache;Depressed  1 49.0
48  ALL      Drug   Hangnail;Stomach Ache;Depressed  1 49.0
49    f   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
50    m   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
51  ALL   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
52    f       ALL   Hangnail;Stomach Ache;Depressed  0   NA
53    m       ALL   Hangnail;Stomach Ache;Depressed  1 49.0
54  ALL       ALL   Hangnail;Stomach Ache;Depressed  1 49.0
55    f      Drug    Hangnail;Stomach Ache;Headache  1 46.3
56    m      Drug    Hangnail;Stomach Ache;Headache  0   NA
57  ALL      Drug    Hangnail;Stomach Ache;Headache  1 46.3
58    f   Placebo    Hangnail;Stomach Ache;Headache  1 47.1
59    m   Placebo    Hangnail;Stomach Ache;Headache  0   NA
60  ALL   Placebo    Hangnail;Stomach Ache;Headache  1 47.1
61    f       ALL    Hangnail;Stomach Ache;Headache  2 46.7
62    m       ALL    Hangnail;Stomach Ache;Headache  0   NA
63  ALL       ALL    Hangnail;Stomach Ache;Headache  2 46.7
64    f      Drug             Muscle Ache;Depressed  0   NA
65    m      Drug             Muscle Ache;Depressed  0   NA
66  ALL      Drug             Muscle Ache;Depressed  0   NA
67    f   Placebo             Muscle Ache;Depressed  2 47.7
68    m   Placebo             Muscle Ache;Depressed  0   NA
69  ALL   Placebo             Muscle Ache;Depressed  2 47.7
70    f       ALL             Muscle Ache;Depressed  2 47.7
71    m       ALL             Muscle Ache;Depressed  0   NA
72  ALL       ALL             Muscle Ache;Depressed  2 47.7
73    f      Drug              Muscle Ache;Headache  1 41.7
74    m      Drug              Muscle Ache;Headache  0   NA
75  ALL      Drug              Muscle Ache;Headache  1 41.7
76    f   Placebo              Muscle Ache;Headache  0   NA
77    m   Placebo              Muscle Ache;Headache  0   NA
78  ALL   Placebo              Muscle Ache;Headache  0   NA
79    f       ALL              Muscle Ache;Headache  1 41.7
80    m       ALL              Muscle Ache;Headache  0   NA
81  ALL       ALL              Muscle Ache;Headache  1 41.7
82    f      Drug    Muscle Ache;Headache;Depressed  1 50.8
83    m      Drug    Muscle Ache;Headache;Depressed  0   NA
84  ALL      Drug    Muscle Ache;Headache;Depressed  1 50.8
85    f   Placebo    Muscle Ache;Headache;Depressed  0   NA
86    m   Placebo    Muscle Ache;Headache;Depressed  0   NA
87  ALL   Placebo    Muscle Ache;Headache;Depressed  0   NA
88    f       ALL    Muscle Ache;Headache;Depressed  1 50.8
89    m       ALL    Muscle Ache;Headache;Depressed  0   NA
90  ALL       ALL    Muscle Ache;Headache;Depressed  1 50.8
91    f      Drug          Muscle Ache;Stomach Ache  0   NA
92    m      Drug          Muscle Ache;Stomach Ache  0   NA
93  ALL      Drug          Muscle Ache;Stomach Ache  0   NA
94    f   Placebo          Muscle Ache;Stomach Ache  0   NA
95    m   Placebo          Muscle Ache;Stomach Ache  1 51.3
96  ALL   Placebo          Muscle Ache;Stomach Ache  1 51.3
97    f       ALL          Muscle Ache;Stomach Ache  0   NA
98    m       ALL          Muscle Ache;Stomach Ache  1 51.3
99  ALL       ALL          Muscle Ache;Stomach Ache  1 51.3
100   f      Drug Muscle Ache;Stomach Ache;Headache  0   NA
101   m      Drug Muscle Ache;Stomach Ache;Headache  1 44.3
102 ALL      Drug Muscle Ache;Stomach Ache;Headache  1 44.3
103   f   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
104   m   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
105 ALL   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
106   f       ALL Muscle Ache;Stomach Ache;Headache  0   NA
107   m       ALL Muscle Ache;Stomach Ache;Headache  1 44.3
108 ALL       ALL Muscle Ache;Stomach Ache;Headache  1 44.3
109   f      Drug            Stomach Ache;Depressed  0   NA
110   m      Drug            Stomach Ache;Depressed  2 51.4
111 ALL      Drug            Stomach Ache;Depressed  2 51.4
112   f   Placebo            Stomach Ache;Depressed  0   NA
113   m   Placebo            Stomach Ache;Depressed  0   NA
114 ALL   Placebo            Stomach Ache;Depressed  0   NA
115   f       ALL            Stomach Ache;Depressed  0   NA
116   m       ALL            Stomach Ache;Depressed  2 51.4
117 ALL       ALL            Stomach Ache;Depressed  2 51.4
118   f      Drug             Stomach Ache;Headache  0   NA
119   m      Drug             Stomach Ache;Headache  0   NA
120 ALL      Drug             Stomach Ache;Headache  0   NA
121   f   Placebo             Stomach Ache;Headache  1 49.6
122   m   Placebo             Stomach Ache;Headache  0   NA
123 ALL   Placebo             Stomach Ache;Headache  1 49.6
124   f       ALL             Stomach Ache;Headache  1 49.6
125   m       ALL             Stomach Ache;Headache  0   NA
126 ALL       ALL             Stomach Ache;Headache  1 49.6
127   f      Drug   Stomach Ache;Headache;Depressed  1 45.2
128   m      Drug   Stomach Ache;Headache;Depressed  0   NA
129 ALL      Drug   Stomach Ache;Headache;Depressed  1 45.2
130   f   Placebo   Stomach Ache;Headache;Depressed  0   NA
131   m   Placebo   Stomach Ache;Headache;Depressed  0   NA
132 ALL   Placebo   Stomach Ache;Headache;Depressed  0   NA
133   f       ALL   Stomach Ache;Headache;Depressed  1 45.2
134   m       ALL   Stomach Ache;Headache;Depressed  0   NA
135 ALL       ALL   Stomach Ache;Headache;Depressed  1 45.2
136   f      Drug                               ALL  6 46.5
137   m      Drug                               ALL  5 48.5
138 ALL      Drug                               ALL 11 47.4
139   f   Placebo                               ALL  7 48.7
140   m   Placebo                               ALL  2 53.2
141 ALL   Placebo                               ALL  9 49.7
142   f       ALL                               ALL 13 47.7
143   m       ALL                               ALL  7 49.8
144 ALL       ALL                               ALL 20 48.4
> 
> f <- summary(treatment ~ age + sex + Symptoms, method="reverse", test=TRUE)
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
> f


Descriptive Statistics by treatment

+---------------------------+----------------------+----------------------+------------------------------+
|                           |Drug                  |Placebo               |  Test                        |
|                           |(N=11)                |(N=9)                 |Statistic                     |
+---------------------------+----------------------+----------------------+------------------------------+
|age                        |        45.8/46.4/48.7|        47.1/49.6/51.3|   F=2.09 d.f.=1,18 P=0.166   |
+---------------------------+----------------------+----------------------+------------------------------+
|sex : m                    |           45%  (5)   |           22%  (2)   |Chi-square=1.17 d.f.=1 P=0.279|
+---------------------------+----------------------+----------------------+------------------------------+
|Primary Symptoms : Hangnail|           45%  (5)   |           44%  (4)   |Chi-square=0.00 d.f.=1 P=0.964|
+---------------------------+----------------------+----------------------+------------------------------+
|    Muscle Ache            |           45%  (5)   |           67%  (6)   |Chi-square=0.90 d.f.=1 P=0.343|
+---------------------------+----------------------+----------------------+------------------------------+
|    Stomach Ache           |           55%  (6)   |           56%  (5)   |Chi-square=0.00 d.f.=1 P=0.964|
+---------------------------+----------------------+----------------------+------------------------------+
|    Headache               |           64%  (7)   |           33%  (3)   |Chi-square=1.82 d.f.=1 P=0.178|
+---------------------------+----------------------+----------------------+------------------------------+
|    Depressed              |           45%  (5)   |           33%  (3)   |Chi-square=0.30 d.f.=1 P=0.582|
+---------------------------+----------------------+----------------------+------------------------------+
> # trio of numbers represent 25th, 50th, 75th percentile
> print(f, long=TRUE)


Descriptive Statistics by treatment

+----------------+----------------------+----------------------+------------------------------+
|                |Drug                  |Placebo               |  Test                        |
|                |(N=11)                |(N=9)                 |Statistic                     |
+----------------+----------------------+----------------------+------------------------------+
|age             |        45.8/46.4/48.7|        47.1/49.6/51.3|   F=2.09 d.f.=1,18 P=0.166   |
+----------------+----------------------+----------------------+------------------------------+
|sex             |                      |                      |Chi-square=1.17 d.f.=1 P=0.279|
+----------------+----------------------+----------------------+------------------------------+
|    m           |           45%  (5)   |           22%  (2)   |                              |
+----------------+----------------------+----------------------+------------------------------+
|Primary Symptoms|                      |                      |                              |
+----------------+----------------------+----------------------+------------------------------+
|    Hangnail    |           45%  (5)   |           44%  (4)   |Chi-square=0.00 d.f.=1 P=0.964|
+----------------+----------------------+----------------------+------------------------------+
|    Muscle Ache |           45%  (5)   |           67%  (6)   |Chi-square=0.90 d.f.=1 P=0.343|
+----------------+----------------------+----------------------+------------------------------+
|    Stomach Ache|           55%  (6)   |           56%  (5)   |Chi-square=0.00 d.f.=1 P=0.964|
+----------------+----------------------+----------------------+------------------------------+
|    Headache    |           64%  (7)   |           33%  (3)   |Chi-square=1.82 d.f.=1 P=0.178|
+----------------+----------------------+----------------------+------------------------------+
|    Depressed   |           45%  (5)   |           33%  (3)   |Chi-square=0.30 d.f.=1 P=0.582|
+----------------+----------------------+----------------------+------------------------------+
> 
> 
> 
> cleanEx()
> nameEx("makeNstr")
> ### * makeNstr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeNstr
> ### Title: creates a string that is a repeat of a substring
> ### Aliases: makeNstr
> ### Keywords: manip character
> 
> ### ** Examples
> 
> makeNstr(" ", 5)
[1] "     "
> 
> ## Don't show: 
> if(makeNstr(" ", 5) != "     ") stop("makeNstr failed test")
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("mdb.get")
> ### * mdb.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mdb.get
> ### Title: Read Tables in a Microsoft Access Database
> ### Aliases: mdb.get
> ### Keywords: manip IO file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Read all tables in the Microsoft Access database Nwind.mdb
> ##D d <- mdb.get('Nwind.mdb')
> ##D contents(d)
> ##D for(z in d) print(contents(z))
> ##D # Just print the names of tables in the database
> ##D mdb.get('Nwind.mdb', tables=TRUE)
> ##D # Import one table
> ##D Orders <- mdb.get('Nwind.mdb', tables='Orders')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("meltData")
> ### * meltData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: meltData
> ### Title: meltData
> ### Aliases: meltData
> 
> ### ** Examples
> 
> d <- data.frame(y1=(1:10)/10, y2=(1:10)/100, x1=1:10, x2=101:110)
> label(d$x1) <- 'X1'
> units(d$x1) <- 'mmHg'
> m=meltData(y1 + y2 ~ x1 + x2, data=d, units=TRUE) # consider also html=TRUE
> print(m)
> m=meltData(y1 + y2 ~ x1 + x2, data=d, tall='left')
> print(m)
> 
> 
> 
> cleanEx()
> nameEx("mgp.axis")
> ### * mgp.axis
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgp.axis
> ### Title: Draw Axes With Side-Specific mgp Parameters
> ### Aliases: mgp.axis mgp.axis.labels
> ### Keywords: iplot dplot environment
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mgp.axis.labels(type='x')  # get default value for x-axis
> ##D mgp.axis.labels(type='y')  # get value for y-axis
> ##D mgp.axis.labels(type='xy') # get 2nd element of both mgps
> ##D mgp.axis.labels(type='x and y')  # get a list with 2 elements
> ##D mgp.axis.labels(c(3,.5,0), type='x')  # set
> ##D options('mgp.axis.labels')            # retrieve
> ##D 
> ##D plot(..., axes=FALSE)
> ##D mgp.axis(1, "X Label")
> ##D mgp.axis(2, "Y Label")
> ##D 
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("minor.tick")
> ### * minor.tick
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: minor.tick
> ### Title: Minor Tick Marks
> ### Aliases: minor.tick
> ### Keywords: aplot hplot
> 
> ### ** Examples
> 
> # Plot with default settings
> plot(runif(20), runif(20))
> minor.tick()
> 
> # Plot with arguments passed to axis()
> plot(c(0,1), c(0,1), type = 'n', axes = FALSE, ann = FALSE)
> # setting up a plot without axes and annotation
> points(runif(20), runif(20))                       # plotting data
> axis(1, pos = 0.5, lwd = 2)                        # showing X-axis at Y = 0.5 with formatting
> axis(2, col = 2)                                   # formatted Y-axis
> minor.tick( nx = 4, ny = 4, tick.ratio = 0.3,
+             x.args = list(pos = 0.5, lwd = 2),     # X-minor tick format argumnets
+             y.args = list(col = 2))                # Y-minor tick format arguments
> 
> 
> 
> cleanEx()
> nameEx("mtitle")
> ### * mtitle
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mtitle
> ### Title: Margin Titles
> ### Aliases: mtitle
> ### Keywords: hplot
> 
> ### ** Examples
> 
> #Set up for 1 plot on figure, give a main title,
> #use date for lr
> plot(runif(20),runif(20))
> mtitle("Main Title")
> 
> 
> #Set up for 2 x 2 matrix of plots with a lower left subtitle and overall title
> par(mfrow=c(2,2), oma=c(3,0,3,0))
> plot(runif(20),runif(20))
> plot(rnorm(20),rnorm(20))
> plot(exp(rnorm(20)),exp(rnorm(20)))
> mtitle("Main Title",ll="n=20")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("multLines")
> ### * multLines
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multLines
> ### Title: Plot Multiple Lines
> ### Aliases: multLines
> ### Keywords: hplot
> 
> ### ** Examples
> 
> if (requireNamespace("plotly")) {
+   x <- 1:4
+   y <- cbind(x, x-3, x-2, x-1, x+1, x+2, x+3)
+   plot(NA, NA, xlim=c(1,4), ylim=c(-2, 7))
+   multLines(x, y, col='blue')
+   multLines(x, y, col='red', pos='right')
+ }
Loading required namespace: plotly
Failed with error:  ‘there is no package called ‘plotly’’
> 
> 
> 
> cleanEx()
> nameEx("nCoincident")
> ### * nCoincident
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nCoincident
> ### Title: nCoincident
> ### Aliases: nCoincident
> 
> ### ** Examples
> 
> nCoincident(c(1:5, 4:5), c(1:5, 4:5)/10)
[1] 2
> 
> 
> 
> cleanEx()
> nameEx("na.delete")
> ### * na.delete
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.delete
> ### Title: Row-wise Deletion na.action
> ### Aliases: na.delete
> ### Keywords: models
> 
> ### ** Examples
> 
> # options(na.action="na.delete")
> # ols(y ~ x)
> 
> 
> 
> cleanEx()
> nameEx("na.detail.response")
> ### * na.detail.response
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.detail.response
> ### Title: Detailed Response Variable Information
> ### Aliases: na.detail.response
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # sex
> # [1] m f f m f f m m m m m m m m f f f m f m
> # age
> # [1] NA 41 23 30 44 22 NA 32 37 34 38 36 36 50 40 43 34 22 42 30
> # y
> # [1] 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0
> # options(na.detail.response=TRUE, na.action="na.delete", digits=3)
> # lrm(y ~ age*sex)
> #
> # Logistic Regression Model
> # 
> # lrm(formula = y ~ age * sex)
> #
> #
> # Frequencies of Responses
> #   0 1 
> #  10 8
> #
> # Frequencies of Missing Values Due to Each Variable
> #  y age sex 
> #  0   2   0
> #
> #
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #     age=NA age!=NA sex!=NA Any NA  No NA 
> #   N    2.0  18.000   20.00    2.0 18.000
> # Mean    0.5   0.444    0.45    0.5  0.444
> #
> # \dots\dots
> # options(na.action="na.keep")
> # describe(y ~ age*sex)
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #      age=NA age!=NA sex!=NA Any NA  No NA 
> #    N    2.0  18.000   20.00    2.0 18.000
> # Mean    0.5   0.444    0.45    0.5  0.444
> #
> # \dots
> # options(na.fun.response="table")  #built-in function table()
> # describe(y ~ age*sex)
> #
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #   age=NA age!=NA sex!=NA Any NA No NA 
> # 0      1      10      11      1    10
> # 1      1       8       9      1     8
> #
> # \dots
> 
> 
> 
> cleanEx()
> nameEx("na.keep")
> ### * na.keep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.keep
> ### Title: Do-nothing na.action
> ### Aliases: na.keep
> ### Keywords: models
> 
> ### ** Examples
> 
> options(na.action="na.keep", na.detail.response=TRUE)
> x1 <- runif(20)
> x2 <- runif(20)
> x2[1:4] <- NA
> y <- rnorm(20)
> describe(y ~ x1*x2)
y ~ x1 * x2 

 3  Variables      20  Observations
--------------------------------------------------------------------------------
y 
        n   missing  distinct      Info      Mean       Gmd       .05       .10 
       20         0        20         1 -0.006472    0.9847  -1.49668  -1.38643 
      .25       .50       .75       .90       .95 
 -0.39947  -0.05497   0.65566   0.93708   1.11296 

-1.98935169586337 (1, 0.05), -1.47075238389927 (1, 0.05), -1.37705955682861 (1,
0.05), -0.47815005510862 (1, 0.05), -0.41499456329968 (1, 0.05),
-0.394289953710349 (1, 0.05), -0.155795506705329 (1, 0.05), -0.102787727342996
(1, 0.05), -0.0593133967111857 (1, 0.05), -0.0561287395290008 (1, 0.05),
-0.0538050405829051 (1, 0.05), 0.0745649833651906 (1, 0.05), 0.387671611559369
(1, 0.05), 0.417941560199702 (1, 0.05), 0.61982574789471 (1, 0.05),
0.763175748457544 (1, 0.05), 0.782136300731067 (1, 0.05), 0.918977371608218 (1,
0.05), 1.10002537198388 (1, 0.05), 1.35867955152904 (1, 0.05)

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
x1 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
      20        0       20        1   0.5552   0.3367   0.1708   0.1992 
     .25      .50      .75      .90      .95 
  0.3455   0.6010   0.7717   0.9119   0.9470 

0.0617862704675645 (1, 0.05), 0.176556752528995 (1, 0.05), 0.201681931037456
(1, 0.05), 0.205974574899301 (1, 0.05), 0.2655086631421 (1, 0.05),
0.37212389963679 (1, 0.05), 0.380035179434344 (1, 0.05), 0.384103718213737 (1,
0.05), 0.497699242085218 (1, 0.05), 0.572853363351896 (1, 0.05),
0.62911404389888 (1, 0.05), 0.660797792486846 (1, 0.05), 0.687022846657783 (1,
0.05), 0.717618508264422 (1, 0.05), 0.769841419998556 (1, 0.05),
0.777445221319795 (1, 0.05), 0.898389684967697 (1, 0.05), 0.908207789994776 (1,
0.05), 0.944675268605351 (1, 0.05), 0.991906094830483 (1, 0.05)

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
x2 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
      16        4       16        1   0.4721   0.3064  0.08431  0.14708 
     .25      .50      .75      .90      .95 
 0.32207  0.44668  0.68228  0.81081  0.83795 
                                                                         
Value      0.0134 0.1079 0.1862 0.2672 0.3403 0.3824 0.3861 0.4113 0.4821
Frequency       1      1      1      1      1      1      1      1      1
Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062
                                                           
Value      0.4935 0.5996 0.6685 0.7237 0.7942 0.8274 0.8697
Frequency       1      1      1      1      1      1      1
Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
> 
> 
> 
> cleanEx()
> nameEx("nin")
> ### * nin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: %nin%
> ### Title: Find Matching (or Non-Matching) Elements
> ### Aliases: %nin%
> ### Keywords: manip character
> 
> ### ** Examples
> 
> c('a','b','c') %nin% c('a','b')
[1] FALSE FALSE  TRUE
> 
> 
> 
> cleanEx()
> nameEx("nobsY")
> ### * nobsY
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nobsY
> ### Title: Compute Number of Observations for Left Hand Side of Formula
> ### Aliases: nobsY
> ### Keywords: utilities manip
> 
> ### ** Examples
> 
> d <- expand.grid(sex=c('female', 'male', NA),
+                  country=c('US', 'Romania'),
+                  reps=1:2)
> d$subject.id <- c(0, 0, 3:12)
> dm <- addMarginal(d, sex, country)
> dim(dm)
[1] 48  5
> nobsY(sex + country ~ 1, data=d)
$nobs
    sex country 
      8      12 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ 1

> nobsY(sex + country ~ id(subject.id), data=d)
$nobs
    sex country 
      7      11 

$nobsg
NULL

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ 1
<environment: 0x5781d8441b60>

> nobsY(sex + country ~ id(subject.id) + reps, group='reps', data=d)
$nobs
    sex country 
      7      11 

$nobsg
  sex country
1   3       5
2   4       6

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ id(subject.id) + reps
<environment: 0x5781dc42ec38>

> nobsY(sex ~ 1, data=d)
$nobs
sex 
  8 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12

$formula
sex ~ 1

> nobsY(sex ~ 1, data=dm)
$nobs
sex 
  8 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48

$formula
sex ~ 1

> nobsY(sex ~ id(subject.id), data=dm)
$nobs
sex 
  7 

$nobsg
NULL

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12  0  0  3  4  5  6  7  8  9 10 11 12  0
[26]  0  3  4  5  6  7  8  9 10 11 12  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex ~ id(subject.id)
<environment: 0x5781dc6de8a0>

> 
> 
> 
> cleanEx()
> nameEx("nstr")
> ### * nstr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nstr
> ### Title: Creates a string of arbitry length
> ### Aliases: nstr
> ### Keywords: manip character utilities
> 
> ### ** Examples
> 
> nstr(c("a"), c(0,3,4))
[1] ""     "aaa"  "aaaa"
> 
> nstr(c("a", "b", "c"), c(1,2,3))
[1] "a"   "bb"  "ccc"
> 
> nstr(c("a", "b", "c"), 4)
[1] "aaaa" "bbbb" "cccc"
> 
> 
> 
> cleanEx()
> nameEx("pairUpDiff")
> ### * pairUpDiff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pairUpDiff
> ### Title: pairUpDiff
> ### Aliases: pairUpDiff
> 
> ### ** Examples
> 
> x <- c(1, 4, 7, 2, 5, 3, 6)
> pairUpDiff(x, c(rep('A', 4), rep('B', 3)),
+   c('u','u','v','v','z','z','q'),
+   c('a','b','a','b','a','b','a'), 'a', x-.1, x+.1)
$X
  x major minor group lower upper subscripts
1 1     A     u     a   0.9   1.1          1
2 4     A     u     b   3.9   4.1          2
3 7     A     v     a   6.9   7.1          3
4 2     A     v     b   1.9   2.1          4
7 6     B     q     a   5.9   6.1          7
5 5     B     z     a   4.9   5.1          5
6 3     B     z     b   2.9   3.1          6

$D
    diff major minor     sd lower upper mid lowermid uppermid
A:u    3     A     u 0.0722  2.86  3.14 2.5     2.43     2.57
A:v   -5     A     v 0.0722 -5.14 -4.86 4.5     4.43     4.57
B:q   NA     B     q     NA    NA    NA  NA       NA       NA
B:z   -2     B     z 0.0722 -2.14 -1.86 4.0     3.93     4.07

$dropped
NULL

> 
> 
> 
> cleanEx()
> nameEx("panel.bpplot")
> ### * panel.bpplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: panel.bpplot
> ### Title: Box-Percentile Panel Function for Trellis
> ### Aliases: panel.bpplot bpplotM bpplt bppltp
> ### Keywords: nonparametric hplot distribution
> 
> ### ** Examples
> 
> set.seed(13)
> x <- rnorm(1000)
> g <- sample(1:6, 1000, replace=TRUE)
> x[g==1][1:20] <- rnorm(20)+3   # contaminate 20 x's for group 1
> 
> 
> # default trellis box plot
> require(lattice)
Loading required package: lattice
> bwplot(g ~ x)
> 
> 
> # box-percentile plot with data density (rug plot)
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.49,by=.01), datadensity=TRUE)
> # add ,scat1d.opts=list(tfrac=1) to make all tick marks the same size
> # when a group has > 125 observations
> 
> 
> # small dot for means, show only .05,.125,.25,.375,.625,.75,.875,.95 quantiles
> bwplot(g ~ x, panel=panel.bpplot, cex.means=.3)
> 
> 
> # suppress means and reference lines for lower and upper quartiles
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,.1,.25), means=FALSE, qref=FALSE)
> 
> 
> # continuous plot up until quartiles ("Tootsie Roll plot")
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.25,by=.01))
> 
> 
> # start at quartiles then make it continuous ("coffin plot")
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.25,.49,by=.01))
> 
> 
> # same as previous but add a spike to give 0.95 interval
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,seq(.25,.49,by=.01)))
> 
> 
> # decile plot with reference lines at outer quintiles and median
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.1,.2,.3,.4), qref=c(.5,.2,.8))
> 
> 
> # default plot with tick marks showing all observations outside the outer
> # box (.05 and .95 quantiles), with very small ticks
> bwplot(g ~ x, panel=panel.bpplot, nout=.05, scat1d.opts=list(frac=.01))
> 
> 
> # show 5 smallest and 5 largest observations
> bwplot(g ~ x, panel=panel.bpplot, nout=5)
> 
> 
> # Use a scat1d option (preserve=TRUE) to ensure that the right peak extends 
> # to the same position as the extreme scat1d
> bwplot(~x , panel=panel.bpplot, probs=seq(.00,.5,by=.001), 
+        datadensity=TRUE, scat1d.opt=list(preserve=TRUE))
> 
> # Add an extended box plot to an existing base graphics plot
> plot(x, 1:length(x))
> panel.bpplot(x, 1070, nogrid=TRUE, pch=19, height=15, cex.means=.5)
> 
> # Draw a prototype showing how to interpret the plots
> bpplt()
> 
> # Example for bpplotM
> set.seed(1)
> n <- 800
> d <- data.frame(treatment=sample(c('a','b'), n, TRUE),
+                 sex=sample(c('female','male'), n, TRUE),
+                 age=rnorm(n, 40, 10),
+                 bp =rnorm(n, 120, 12),
+                 wt =rnorm(n, 190, 30))
> label(d$bp) <- 'Systolic Blood Pressure'
> units(d$bp) <- 'mmHg'
> bpplotM(age + bp + wt ~ treatment, data=d)
> bpplotM(age + bp + wt ~ treatment * sex, data=d, cex.strip=.8)
> bpplotM(age + bp + wt ~ treatment*sex, data=d,
+         violin=TRUE,
+         violin.opts=list(col=adjustcolor('blue', alpha.f=.15),
+                          border=FALSE))
> 
> 
> bpplotM(c('age', 'bp', 'wt'), groups='treatment', data=d)
> # Can use Hmisc Cs function, e.g. Cs(age, bp, wt)
> bpplotM(age + bp + wt ~ treatment, data=d, nloc='left')
> 
> # Without treatment: bpplotM(age + bp + wt ~ 1, data=d)
> 
> ## Not run: 
> ##D # Automatically find all variables that appear to be continuous
> ##D getHdata(support)
> ##D bpplotM(data=support, group='dzgroup',
> ##D         cex.strip=.4, cex.means=.3, cex.n=.45)
> ##D 
> ##D # Separate displays for categorical vs. continuous baseline variables
> ##D getHdata(pbc)
> ##D pbc <- upData(pbc, moveUnits=TRUE)
> ##D 
> ##D s <- summaryM(stage + sex + spiders ~ drug, data=pbc)
> ##D plot(s)
> ##D Key(0, .5)
> ##D s <- summaryP(stage + sex + spiders ~ drug, data=pbc)
> ##D plot(s, val ~ freq | var, groups='drug', pch=1:3, col=1:3,
> ##D      key=list(x=.6, y=.8))
> ##D 
> ##D bpplotM(bili + albumin + protime + age ~ drug, data=pbc)
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’

> nameEx("partition")
> ### * partition
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: partition
> ### Title: Patitions an object into different sets
> ### Aliases: partition partition.vector partition.matrix
> ### Keywords: manip
> 
> ### ** Examples
> 
> a <- 1:7
> partition.vector(a, sep=c(1,3,2,1))
$`1`
[1] 1

$`2`
[1] 2 3 4

$`3`
[1] 5 6

$`4`
[1] 7

> 
> 
> 
> cleanEx()
> nameEx("pc1")
> ### * pc1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pc1
> ### Title: First Principal Component
> ### Aliases: pc1
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(100)
> x2 <- x1 + rnorm(100)
> w <- pc1(cbind(x1,x2))
Fraction variance explained by PC1: 0.842 

Coefficients to obtain PC1:

Intercept        x1        x2 
   -0.124     0.787     0.539 
> attr(w,'coef')
Intercept        x1        x2 
   -0.124     0.787     0.539 
> 
> 
> 
> cleanEx()
> nameEx("plotCorrM")
> ### * plotCorrM
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotCorrM
> ### Title: plotCorrM
> ### Aliases: plotCorrM
> 
> ### ** Examples
> 
> set.seed(1)
> r <- cor(matrix(rnorm(100), ncol=10))
> g <- plotCorrM(r)
> g[[1]]  # plot matrix
> g[[2]]  # plot correlation vs gap time
`geom_smooth()` using formula = 'y ~ x'
Warning: The following aesthetics were dropped during statistical transformation: label.
ℹ This can happen when ggplot fails to infer the correct grouping structure in
  the data.
ℹ Did you forget to specify a `group` aesthetic or to convert a numerical
  variable into a factor?
> # ggplotlyr(g[[2]])
> # ggplotlyr uses ggplotly with tooltip='label' then removes
> # txt: from hover text
> 
> 
> 
> cleanEx()
> nameEx("plotCorrPrecision")
> ### * plotCorrPrecision
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotCorrPrecision
> ### Title: Plot Precision of Estimate of Pearson Correlation Coefficient
> ### Aliases: plotCorrPrecision
> ### Keywords: htest
> 
> ### ** Examples
> 
> plotCorrPrecision()
> plotCorrPrecision(rho=0)
> 
> 
> 
> cleanEx()
> nameEx("plotlyM")
> ### * plotlyM
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotlyM
> ### Title: plotly Multiple
> ### Aliases: plotlyM
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D pts     <- expand.grid(v=c('y1', 'y2', 'y3'), x=1:4, g=c('a', 'b'), yhi=NA,
> ##D                        tracename='mean', legendgroup='mean',
> ##D                        connect=TRUE, size=4)
> ##D 
> ##D pts$y   <- round(runif(nrow(pts)), 2)
> ##D 
> ##D segs     <- expand.grid(v=c('y1', 'y2', 'y3'), x=1:4, g=c('a', 'b'),
> ##D                         tracename='limits', legendgroup='limits',
> ##D                         connect=NA, size=6)
> ##D segs$y   <- runif(nrow(pts))
> ##D segs$yhi <- segs$y + runif(nrow(pts), .05, .15)
> ##D 
> ##D z <- rbind(pts, segs)
> ##D 
> ##D xlab <- labelPlotmath('X<sub>12</sub>', 'm/sec<sup>2</sup>', html=TRUE)
> ##D ylab <- c(y1=labelPlotmath('Y1', 'cm', html=TRUE),
> ##D           y2='Y2',
> ##D           y3=labelPlotmath('Y3', 'mm', html=TRUE))
> ##D 
> ##D W=plotlyM(z, multplot=~v, color=~g, xlab=xlab, ylab=ylab, ncols=2,
> ##D           colors=c('black', 'blue'))
> ##D 
> ##D W2=plotlyM(z, multplot=~v, color=~I('black'), xlab=xlab, ylab=ylab,
> ##D            colors=c('black', 'blue'))
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plsmo")
> ### * plsmo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plsmo
> ### Title: Plot smoothed estimates
> ### Aliases: plsmo panel.plsmo
> ### Keywords: smooth nonparametric hplot
> 
> ### ** Examples
> 
> set.seed(1)
> x <- 1:100
> y <- x + runif(100, -10, 10)
> plsmo(x, y, "supsmu", xlab="Time of Entry") 
> #Use label(y) or "y" for ylab
> 
> 
> plsmo(x, y, add=TRUE, lty=2)
> #Add lowess smooth to existing plot, with different line type
> 
> 
> age <- rnorm(500, 50, 15)
> survival.time <- rexp(500)
> sex <- sample(c('female','male'), 500, TRUE)
> race <- sample(c('black','non-black'), 500, TRUE)
> plsmo(age, survival.time < 1, fun=qlogis, group=sex) # plot logit by sex
> 
> #Bivariate Y
> sbp <- 120 + (age - 50)/10 + rnorm(500, 0, 8) + 5 * (sex == 'male')
> dbp <-  80 + (age - 50)/10 + rnorm(500, 0, 8) - 5 * (sex == 'male')
> Y <- cbind(sbp, dbp)
> plsmo(age, Y)
> plsmo(age, Y, group=sex)
> 
> 
> #Plot points and smooth trend line using trellis 
> # (add type='l' to suppress points or type='p' to suppress trend lines)
> require(lattice)
Loading required package: lattice
> xyplot(survival.time ~ age, panel=panel.plsmo)
> 
> 
> #Do this for multiple panels
> xyplot(survival.time ~ age | sex, panel=panel.plsmo)
> 
> #Repeat this using equal sample size intervals (n=25 each) summarized by
> #the median, then a proportion (mean of binary y)
> xyplot(survival.time ~ age | sex, panel=panel.plsmo, type='l',
+        method='intervals', mobs=25, ifun=median)
> ybinary <- ifelse(runif(length(sex)) < 0.5, 1, 0)
> xyplot(ybinary ~ age, groups=sex, panel=panel.plsmo, type='l',
+        method='intervals', mobs=75, ifun=mean, xlim=c(0, 120))
> 
> 
> #Do this for subgroups of points on each panel, show the data
> #density on each curve, and draw a key at the default location
> xyplot(survival.time ~ age | sex, groups=race, panel=panel.plsmo,
+        datadensity=TRUE)
> Key()
> 
> 
> #Use wloess.noiter to do a fast weighted smooth
> plot(x, y)
> lines(wtd.loess.noiter(x, y))
> lines(wtd.loess.noiter(x, y, weights=c(rep(1,50), 100, rep(1,49))), col=2)
> points(51, y[51], pch=18)   # show overly weighted point
> #Try to duplicate this smooth by replicating 51st observation 100 times
> lines(wtd.loess.noiter(c(x,rep(x[51],99)),c(y,rep(y[51],99)),
+       type='ordered all'), col=3)
> #Note: These two don't agree exactly
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’

> nameEx("popower")
> ### * popower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: popower
> ### Title: Power and Sample Size for Ordinal Response
> ### Aliases: multEventChart popower posamsize print.popower print.posamsize
> ###   pomodm simPOcuts propsPO propsTrans
> ### Keywords: htest category
> 
> ### ** Examples
> 
> # For a study of back pain (none, mild, moderate, severe) here are the
> # expected proportions (averaged over 2 treatments) that will be in
> # each of the 4 categories:
> 
> 
> p <- c(.1,.2,.4,.3)
> popower(p, 1.2, 1000)   # OR=1.2, total n=1000
Power: 0.351 
Efficiency of design compared with continuous response: 0.9 
Approximate standard error of log odds ratio: 0.116 

> posamsize(p, 1.2)
Total sample size: 3148 
Efficiency of design compared with continuous response: 0.9 

> popower(p, 1.2, 3148)
Power: 0.8 
Efficiency of design compared with continuous response: 0.9 
Approximate standard error of log odds ratio: 0.0651 

> # If p was the vector of probabilities for group 1, here's how to
> # compute the average over the two groups:
> # p2   <- pomodm(p=p, odds.ratio=1.2)
> # pavg <- (p + p2) / 2
> 
> # Compare power to test for proportions for binary case,
> # proportion of events in control group of 0.1
> p <- 0.1; or <- 0.85; n <- 4000
> popower(c(1 - p, p), or, n)    # 0.338
Power: 0.338 
Efficiency of design compared with continuous response: 0.27 
Approximate standard error of log odds ratio: 0.105 

> bpower(p, odds.ratio=or, n=n)  # 0.320
Power 
 0.32 
> # Add more categories, starting with 0.1 in middle
> p <- c(.8, .1, .1)
> popower(p, or, n)   # 0.543
Power: 0.543 
Efficiency of design compared with continuous response: 0.486 
Approximate standard error of log odds ratio: 0.0786 

> p <- c(.7, .1, .1, .1)
> popower(p, or, n)   # 0.67
Power: 0.67 
Efficiency of design compared with continuous response: 0.654 
Approximate standard error of log odds ratio: 0.0677 

> # Continuous scale with final level have prob. 0.1
> p <- c(rep(1 / n, 0.9 * n), 0.1)
> popower(p, or, n)   # 0.843
Power: 0.843 
Efficiency of design compared with continuous response: 0.999 
Approximate standard error of log odds ratio: 0.0548 

> 
> # Compute the mean and median x after shifting the probability
> # distribution by an odds ratio under the proportional odds model
> x <- 1 : 5
> p <- c(.05, .2, .2, .3, .25)
> # For comparison make up a sample that looks like this
> X <- rep(1 : 5, 20 * p)
> c(mean=mean(X), median=median(X))
  mean median 
   3.5    4.0 
> pomodm(x, p, odds.ratio=1)  # still have to figure out the right median
  mean median 
  3.50   3.67 
> pomodm(x, p, odds.ratio=0.5)
  mean median 
  3.03   2.95 
> 
> # Show variation of odds ratios over possible cutoffs of Y even when PO
> # truly holds.  Run 5 simulations for a total sample size of 300.
> # The two groups have 150 subjects each.
> s <- simPOcuts(300, nsim=5, odds.ratio=2, p=p)
> round(s, 2)
              y>=2 y>=3 y>=4 y>=5
Simulation 1  6.21 1.98 2.51 2.31
Simulation 2  4.26 2.39 1.85 1.39
Simulation 3  1.00 1.60 1.66 1.74
Simulation 4 10.64 2.34 1.76 2.38
Simulation 5  7.29 1.91 1.75 2.28
> 
> # An ordinal outcome with levels a, b, c, d, e is measured at 3 times
> # Show the proportion of values in each outcome category stratified by
> # time.  Then compute what the proportions would be had the proportions
> # at times 2 and 3 been the proportions at time 1 modified by two odds ratios 
> 
> set.seed(1)
> d   <- expand.grid(time=1:3, reps=1:30)
> d$y <- sample(letters[1:5], nrow(d), replace=TRUE)
> propsPO(y ~ time, data=d, odds.ratio=function(time) c(1, 2, 4)[time])
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
> # To show with plotly, save previous result as object p and then:
> # plotly::ggplotly(p, tooltip='label')
> 
> # Add a stratification variable and don't consider an odds ratio
> d   <- expand.grid(time=1:5, sex=c('female', 'male'), reps=1:30)
> d$y <- sample(letters[1:5], nrow(d), replace=TRUE)
> propsPO(y ~ time + sex, data=d)  # may add nrow= or ncol=
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
> 
> # Show all successive transition proportion matrices
> d   <- expand.grid(id=1:30, time=1:10)
> d$state <- sample(LETTERS[1:4], nrow(d), replace=TRUE)
> propsTrans(state ~ time + id, data=d)
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 7 ➔ 8' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 8 ➔ 9' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 9 ➔ 10' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 4 ➔ 5' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 5 ➔ 6' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 6 ➔ 7' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 1 ➔ 2' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 2 ➔ 3' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  for 'time 3 ➔ 4' in 'mbcsToSbcs': -> substituted for ➔ (U+2794)
> 
> pt1 <- data.frame(pt=1, day=0:3,
+    status=c('well', 'well', 'sick', 'very sick'))
> pt2 <- data.frame(pt=2, day=c(1,2,4,6),
+    status=c('sick', 'very sick', 'coma', 'death'))
> pt3 <- data.frame(pt=3, day=1:5,
+    status=c('sick', 'very sick', 'sick', 'very sick', 'discharged'))
> pt4 <- data.frame(pt=4, day=c(1:4, 10),
+    status=c('well', 'sick', 'very sick', 'well', 'discharged'))
> d <- rbind(pt1, pt2, pt3, pt4)
> d$status <- factor(d$status, c('discharged', 'well', 'sick',
+                                'very sick', 'coma', 'death'))
> label(d$day) <- 'Day'
> require(ggplot2)
Loading required package: ggplot2
> multEventChart(status ~ day + pt, data=d,
+                absorb=c('death', 'discharged'),
+                colorTitle='Status', sortbylast=TRUE) +
+                theme_classic() +
+                theme(legend.position='bottom')
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
Warning in mean.default(x) :
  argument is not numeric or logical: returning NA
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("print.char.matrix")
> ### * print.char.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.char.matrix
> ### Title: Function to print a matrix with stacked cells
> ### Aliases: print.char.matrix
> ### Keywords: print array
> 
> ### ** Examples
> 
> data(HairEyeColor)
> print.char.matrix(HairEyeColor[ , , "Male"], col.names = TRUE)
+-----+-----+----+-----+-----+
| Hair|Brown|Blue|Hazel|Green|
+-----+-----+----+-----+-----+
|Black|  32 | 11 |  10 |   3 |
+-----+-----+----+-----+-----+
|Brown|  53 | 50 |  25 |  15 |
+-----+-----+----+-----+-----+
|  Red|  10 | 10 |   7 |   7 |
+-----+-----+----+-----+-----+
|Blond|   3 | 30 |   5 |   8 |
+-----+-----+----+-----+-----+
> print.char.matrix(HairEyeColor[ , , "Female"], col.txt.align = "left", col.names = TRUE)
+-----+-----+----+-----+-----+
| Hair|Brown|Blue|Hazel|Green|
+-----+-----+----+-----+-----+
|Black|  36 | 9  |  5  |  2  |
+-----+-----+----+-----+-----+
|Brown|  66 | 34 |  29 |  14 |
+-----+-----+----+-----+-----+
|Red  |  16 | 7  |  7  |  7  |
+-----+-----+----+-----+-----+
|Blond|  4  | 64 |  5  |  8  |
+-----+-----+----+-----+-----+
> 
> 
> z <- rbind(c("", "N", "y"),
+            c("[ 1.34,40.3)\n[40.30,48.5)\n[48.49,58.4)\n[58.44,87.8]",
+              " 50\n 50\n 50\n 50",
+              "0.530\n0.489\n0.514\n0.507"),
+            c("female\nmale", " 94\n106", "0.552\n0.473"  ),
+            c("", "200", "0.510"))
> dimnames(z) <- list(c("", "age", "sex", "Overall"),NULL)
> 
> print.char.matrix(z)
+-------+------------+---+-----+
|       |            |  N|    y|
+-------+------------+---+-----+
|    age|[ 1.34,40.3)| 50|0.530|
|       |[40.30,48.5)| 50|0.489|
|       |[48.49,58.4)| 50|0.514|
|       |[58.44,87.8]| 50|0.507|
+-------+------------+---+-----+
|    sex|      female| 94|0.552|
|       |        male|106|0.473|
+-------+------------+---+-----+
|Overall|            |200|0.510|
+-------+------------+---+-----+
> 
> 
> 
> cleanEx()
> nameEx("printL")
> ### * printL
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: printL
> ### Title: printL
> ### Aliases: printL
> 
> ### ** Examples
> 
> w <- pi + 1 : 2
> printL(w=w)
w: 4.14159265358979, 5.14159265358979

> printL(w, dec=3)
4.142, 5.142

> printL('this is it'=c(pi, pi, 1, 2),
+        yyy=pi,
+        z=data.frame(x=pi+1:2, y=3:4, z=c('a', 'b')),
+        qq=1:10,
+        dec=4)
this is it: 3.1416, 3.1416, 1, 2

yyy: 3.1416

z:
     x y z
1 4.14 3 a
2 5.14 4 b

qq:
 [1]  1  2  3  4  5  6  7  8  9 10

>        
> 
> 
> 
> cleanEx()
> nameEx("prnz")
> ### * prnz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prnz
> ### Title: Print and Object with its Name
> ### Aliases: prn
> ### Keywords: print
> 
> ### ** Examples
> 
> x <- 1:5
> prn(x)

x

[1] 1 2 3 4 5
> # prn(fit, 'Full Model Fit')
> 
> 
> 
> cleanEx()
> nameEx("prselect")
> ### * prselect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prselect
> ### Title: Selectively Print Lines of a Text Vector
> ### Aliases: prselect
> ### Keywords: manip utilities
> 
> ### ** Examples
> 
> x <- c('the','cat','ran','past','the','dog')
> prselect(x, 'big','bad')     # omit nothing- no match
the
cat
ran
past
the
dog
> prselect(x, 'the','past')    # omit first 4 lines
...
the
dog
> prselect(x,'the','junk')     # omit nothing- no match for stop
the
cat
ran
past
the
dog
> prselect(x,'ran','dog')      # omit last 4 lines
the
cat
...
> prselect(x,'cat')            # omit lines 2-
the
...
> prselect(x,'cat',i=1)        # omit lines 3-
the
cat
...
> prselect(x,'cat','past')     # omit lines 2-4
the
...
the
dog
> prselect(x,'cat','past',j=1) # omit lines 2-5
the
...
dog
> prselect(x,'cat','past',j=-1)# omit lines 2-3
the
...
past
the
dog
> prselect(x,'t$','dog')       # omit lines 2-6; t must be at end
the
...
> 
> # Example for Sweave: run a regression analysis with the rms package
> # then selectively output only a portion of what print.ols prints.
> # (Thanks to \email{romain.francois@dbmail.com})
> # <<z,eval=FALSE,echo=T>>=
> # library(rms)
> # y <- rnorm(20); x1 <- rnorm(20); x2 <- rnorm(20)
> # ols(y ~ x1 + x2)
> # <<echo=F>>=
> # z <- capture.output( {
> # <<z>>
> #    } )
> # prselect(z, 'Residuals:') # keep only summary stats; or:
> # prselect(z, stop='Coefficients', j=-1)  # keep coefficients, rmse, R^2; or:
> # prselect(z, 'Coefficients', 'Residual standard error', j=-1) # omit coef
> # @
> 
> 
> 
> cleanEx()
> nameEx("pstamp")
> ### * pstamp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pstamp
> ### Title: Date/Time/Directory Stamp the Current Plot
> ### Aliases: pstamp
> ### Keywords: aplot
> 
> ### ** Examples
> 
> plot(1:20)
> pstamp(pwd=TRUE, time=FALSE)
> 
> 
> 
> cleanEx()
> nameEx("qcrypt")
> ### * qcrypt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qcrypt
> ### Title: qcrypt
> ### Aliases: qcrypt
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Suppose x is a data.table or data.frame
> ##D # The first time qcrypt is run with a service a password will
> ##D # be requested.  It will be remembered across sessions thanks to
> ##D # the keyring package
> ##D qcrypt(x, 'x')   # creates x.qs.encrypted in current working directory
> ##D x <- qcrypt('x') # unencrypts x.qs.encrypted into a temporary
> ##D                  # directory, uses qs::qread to read it, and
> ##D                  # stores the result in x
> ##D # Encrypt a general file using a different password
> ##D qcrypt(file='report.pdf', service='pdfkey')
> ##D # Decrypt that file
> ##D fi <- qcrypt(file='report.pdf.encrypted', service='pdfkey')
> ##D fi contains the full unencrypted file name which is in a temporary directory
> ##D # Encrypt without using a keyring
> ##D qcrypt(x, 'x', service=NA)
> ##D x <- qcrypt('x', service=NA)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("r2describe")
> ### * r2describe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: r2describe
> ### Title: r2describe
> ### Aliases: r2describe
> 
> ### ** Examples
> 
> ## Not run: 
> ##D r <- redun(...)
> ##D r2describe(r$scores)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rMultinom")
> ### * rMultinom
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rMultinom
> ### Title: Generate Multinomial Random Variables with Varying Probabilities
> ### Aliases: rMultinom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> set.seed(1)
> w <- rMultinom(rbind(c(.1,.2,.3,.4),c(.4,.3,.2,.1)),200)
> t(apply(w, 1, table)/200)
         1     2     3    4
[1,] 0.080 0.205 0.315 0.40
[2,] 0.445 0.280 0.185 0.09
> 
> 
> 
> cleanEx()
> nameEx("rcorr")
> ### * rcorr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcorr
> ### Title: Matrix of Correlations and P-values
> ### Aliases: rcorr print.rcorr
> ### Keywords: nonparametric htest category
> 
> ### ** Examples
> 
> x <- c(-2, -1, 0, 1, 2)
> y <- c(4,   1, 0, 1, 4)
> z <- c(1,   2, 3, 4, NA)
> v <- c(1,   2, 3, 4, 5)
> rcorr(cbind(x,y,z,v))
  x     y     z v
x 1  0.00  1.00 1
y 0  1.00 -0.75 0
z 1 -0.75  1.00 1
v 1  0.00  1.00 1

n
  x y z v
x 5 5 4 5
y 5 5 4 5
z 4 4 4 4
v 5 5 4 5

P
  x     y     z     v    
x       1.000 0.000 0.000
y 1.000       0.255 1.000
z 0.000 0.255       0.000
v 0.000 1.000 0.000      
> 
> 
> 
> cleanEx()
> nameEx("rcorr.cens")
> ### * rcorr.cens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcorr.cens
> ### Title: Rank Correlation for Censored Data
> ### Aliases: rcorr.cens rcorrcens rcorrcens.formula
> ### Keywords: survival nonparametric
> 
> ### ** Examples
> 
> set.seed(1)
> x <- round(rnorm(200))
> y <- rnorm(200)
> rcorr.cens(x, y, outx=TRUE)   # can correlate non-censored variables
       C Index            Dxy           S.D.              n        missing 
      4.76e-01      -4.74e-02       6.49e-02       2.00e+02       0.00e+00 
    uncensored Relevant Pairs     Concordant      Uncertain 
      2.00e+02       2.83e+04       1.35e+04       0.00e+00 
> library(survival)
> age <- rnorm(400, 50, 10)
> bp  <- rnorm(400,120, 15)
> bp[1]  <- NA
> d.time <- rexp(400)
> cens   <- runif(400,.5,2)
> death  <- d.time <= cens
> d.time <- pmin(d.time, cens)
> rcorr.cens(age, Surv(d.time, death))
       C Index            Dxy           S.D.              n        missing 
      4.99e-01      -1.49e-03       3.67e-02       4.00e+02       0.00e+00 
    uncensored Relevant Pairs     Concordant      Uncertain 
      2.79e+02       1.33e+05       6.66e+04       2.63e+04 
> r <- rcorrcens(Surv(d.time, death) ~ age + bp)
> r

Somers' Rank Correlation for Censored Data    Response variable:Surv(d.time, death)

        C    Dxy  aDxy    SD    Z     P   n
age 0.499 -0.001 0.001 0.037 0.04 0.968 400
bp  0.497 -0.006 0.006 0.038 0.15 0.877 399
> plot(r)
> 
> # Show typical 0.95 confidence limits for ROC areas for a sample size
> # with 24 events and 62 non-events, for varying population ROC areas
> # Repeat for 138 events and 102 non-events
> set.seed(8)
> par(mfrow=c(2,1))
> for(i in 1:2) {
+  n1 <- c(24,138)[i]
+  n0 <- c(62,102)[i]
+  y <- c(rep(0,n0), rep(1,n1))
+  deltas <- seq(-3, 3, by=.25)
+  C <- se <- deltas
+  j <- 0
+  for(d in deltas) {
+   j <- j + 1
+   x <- c(rnorm(n0, 0), rnorm(n1, d))
+   w <- rcorr.cens(x, y)
+   C[j]  <- w['C Index']
+   se[j] <- w['S.D.']/2
+  }
+  low <- C-1.96*se; hi <- C+1.96*se
+  print(cbind(C, low, hi))
+  errbar(deltas, C, C+1.96*se, C-1.96*se,
+         xlab='True Difference in Mean X',
+         ylab='ROC Area and Approx. 0.95 CI')
+  title(paste('n1=',n1,'  n0=',n0,sep=''))
+  abline(h=.5, v=0, col='gray')
+  true <- 1 - pnorm(0, deltas, sqrt(2))
+  lines(deltas, true, col='blue')
+ }
           C       low     hi
 [1,] 0.0276 -0.001510 0.0566
 [2,] 0.0336 -0.000378 0.0676
 [3,] 0.0108 -0.004450 0.0260
 [4,] 0.1042  0.028482 0.1799
 [5,] 0.0679  0.014300 0.1215
 [6,] 0.0981  0.026285 0.1700
 [7,] 0.1472  0.068762 0.2256
 [8,] 0.0927  0.023150 0.1623
 [9,] 0.1532  0.060669 0.2458
[10,] 0.2386  0.132475 0.3447
[11,] 0.3085  0.194064 0.4229
[12,] 0.5141  0.385697 0.6425
[13,] 0.4335  0.297805 0.5691
[14,] 0.5410  0.409413 0.6726
[15,] 0.6136  0.486172 0.7410
[16,] 0.6418  0.508400 0.7752
[17,] 0.7695  0.666004 0.8730
[18,] 0.8233  0.718352 0.9282
[19,] 0.7977  0.689130 0.9063
[20,] 0.9079  0.849626 0.9662
[21,] 0.9362  0.882644 0.9897
[22,] 0.9577  0.921863 0.9935
[23,] 0.9630  0.928010 0.9981
[24,] 0.9906  0.978049 1.0031
[25,] 0.9718  0.935365 1.0082
           C     low     hi
 [1,] 0.0220 0.00644 0.0375
 [2,] 0.0239 0.00933 0.0385
 [3,] 0.0279 0.01162 0.0442
 [4,] 0.0635 0.03341 0.0936
 [5,] 0.0634 0.03448 0.0924
 [6,] 0.1398 0.09424 0.1854
 [7,] 0.1701 0.11980 0.2205
 [8,] 0.1961 0.14033 0.2518
 [9,] 0.2230 0.16465 0.2814
[10,] 0.2956 0.22882 0.3624
[11,] 0.3382 0.26882 0.4077
[12,] 0.3661 0.29566 0.4365
[13,] 0.5075 0.43368 0.5812
[14,] 0.6153 0.54412 0.6865
[15,] 0.6569 0.58852 0.7252
[16,] 0.6523 0.58239 0.7222
[17,] 0.7237 0.65994 0.7875
[18,] 0.8255 0.77347 0.8776
[19,] 0.8291 0.77867 0.8796
[20,] 0.8939 0.85444 0.9333
[21,] 0.8813 0.84016 0.9224
[22,] 0.9587 0.93522 0.9821
[23,] 0.9716 0.95470 0.9885
[24,] 0.9743 0.95601 0.9926
[25,] 0.9899 0.98147 0.9983
> par(mfrow=c(1,1))
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:survival’

> nameEx("rcorrp.cens")
> ### * rcorrp.cens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcorrp.cens
> ### Title: Rank Correlation for Paired Predictors with a Possibly Censored
> ###   Response, and Integrated Discrimination Index
> ### Aliases: rcorrp.cens improveProb print.improveProb
> ### Keywords: survival nonparametric regression
> 
> ### ** Examples
> 
> set.seed(1)
> library(survival)
> 
> x1 <- rnorm(400)
> x2 <- x1 + rnorm(400)
> d.time <- rexp(400) + (x1 - min(x1))
> cens   <- runif(400,.5,2)
> death  <- d.time <= cens
> d.time <- pmin(d.time, cens)
> rcorrp.cens(x1, x2, Surv(d.time, death))
               Dxy               S.D. x1 more concordant x2 more concordant 
         -8.21e-02           1.37e-01           4.59e-01           5.41e-01 
                 n            missing         uncensored     Relevant Pairs 
          4.00e+02           0.00e+00           1.10e+01           4.26e+03 
         Uncertain               C X1               C X2             Dxy X1 
          1.55e+05           9.92e-01           9.26e-01           9.84e-01 
            Dxy X2 
          8.52e-01 
> #rcorrp.cens(x1, x2, y) ## no censoring
> 
> set.seed(1)
> x1 <- runif(1000)
> x2 <- runif(1000)
> y  <- sample(0:1, 1000, TRUE)
> rcorrp.cens(x1, x2, y)
               Dxy               S.D. x1 more concordant x2 more concordant 
          6.00e-02           3.65e-02           5.30e-01           4.70e-01 
                 n            missing         uncensored     Relevant Pairs 
          1.00e+03           0.00e+00           1.00e+03           5.00e+05 
         Uncertain               C X1               C X2             Dxy X1 
          0.00e+00           5.10e-01           4.66e-01           1.91e-02 
            Dxy X2 
         -6.82e-02 
> improveProb(x1, x2, y)

Analysis of Proportions of Subjects with Improvement in Predicted Probability

Number of events: 508 	Number of non-events: 492 

Proportions of Positive and Negative Changes in Probabilities

                            Proportion
Increase for events     (1)      0.482
Increase for non-events (2)      0.528
Decrease for events     (3)      0.518
Decrease for non-events (4)      0.472


Net Reclassification Improvement

                           Index     SE      Z    2P Lower 0.95 Upper 0.95
NRI            (1-3+4-2) -0.0923 0.0632 -1.462 0.144     -0.216     0.0315
NRI for events     (1-3) -0.0354 0.0443 -0.799 0.424     -0.122     0.0515
NRI for non-events (4-2) -0.0569 0.0450 -1.264 0.206     -0.145     0.0313


Analysis of Changes in Predicted Probabilities

                                      Mean Change in Probability
Increase for events (sensitivity)                        -0.0308
Decrease for non-events (specificity)                    -0.0127


Integrated Discrimination Improvement
 (average of sensitivity and 1-specificity over [0,1];
 also is difference in Yates' discrimination slope)

       IDI         SE          Z         2P Lower 0.95 Upper 0.95 
   -0.0436     0.0265    -1.6433     0.1003    -0.0956     0.0084 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("rcspline.eval")
> ### * rcspline.eval
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcspline.eval
> ### Title: Restricted Cubic Spline Design Matrix
> ### Aliases: rcspline.eval
> ### Keywords: regression smooth
> 
> ### ** Examples
> 
> x <- 1:100
> rcspline.eval(x, nk=4, inclx=TRUE)
         x                  
  [1,]   1 0.00e+00 0.00e+00
  [2,]   2 0.00e+00 0.00e+00
  [3,]   3 0.00e+00 0.00e+00
  [4,]   4 0.00e+00 0.00e+00
  [5,]   5 0.00e+00 0.00e+00
  [6,]   6 1.57e-08 0.00e+00
  [7,]   7 1.46e-04 0.00e+00
  [8,]   8 1.09e-03 0.00e+00
  [9,]   9 3.57e-03 0.00e+00
 [10,]  10 8.37e-03 0.00e+00
 [11,]  11 1.62e-02 0.00e+00
 [12,]  12 2.79e-02 0.00e+00
 [13,]  13 4.41e-02 0.00e+00
 [14,]  14 6.57e-02 0.00e+00
 [15,]  15 9.34e-02 0.00e+00
 [16,]  16 1.28e-01 0.00e+00
 [17,]  17 1.70e-01 0.00e+00
 [18,]  18 2.20e-01 0.00e+00
 [19,]  19 2.80e-01 0.00e+00
 [20,]  20 3.49e-01 0.00e+00
 [21,]  21 4.29e-01 0.00e+00
 [22,]  22 5.21e-01 0.00e+00
 [23,]  23 6.24e-01 0.00e+00
 [24,]  24 7.41e-01 0.00e+00
 [25,]  25 8.71e-01 0.00e+00
 [26,]  26 1.02e+00 0.00e+00
 [27,]  27 1.17e+00 0.00e+00
 [28,]  28 1.35e+00 0.00e+00
 [29,]  29 1.54e+00 0.00e+00
 [30,]  30 1.75e+00 0.00e+00
 [31,]  31 1.98e+00 0.00e+00
 [32,]  32 2.23e+00 0.00e+00
 [33,]  33 2.49e+00 0.00e+00
 [34,]  34 2.78e+00 0.00e+00
 [35,]  35 3.09e+00 0.00e+00
 [36,]  36 3.42e+00 5.40e-06
 [37,]  37 3.77e+00 3.10e-04
 [38,]  38 4.15e+00 1.63e-03
 [39,]  39 4.55e+00 4.74e-03
 [40,]  40 4.97e+00 1.04e-02
 [41,]  41 5.42e+00 1.93e-02
 [42,]  42 5.90e+00 3.23e-02
 [43,]  43 6.41e+00 5.00e-02
 [44,]  44 6.94e+00 7.33e-02
 [45,]  45 7.50e+00 1.03e-01
 [46,]  46 8.09e+00 1.40e-01
 [47,]  47 8.71e+00 1.84e-01
 [48,]  48 9.37e+00 2.37e-01
 [49,]  49 1.00e+01 3.00e-01
 [50,]  50 1.08e+01 3.72e-01
 [51,]  51 1.15e+01 4.56e-01
 [52,]  52 1.23e+01 5.51e-01
 [53,]  53 1.31e+01 6.58e-01
 [54,]  54 1.40e+01 7.78e-01
 [55,]  55 1.49e+01 9.13e-01
 [56,]  56 1.58e+01 1.06e+00
 [57,]  57 1.68e+01 1.23e+00
 [58,]  58 1.78e+01 1.41e+00
 [59,]  59 1.88e+01 1.60e+00
 [60,]  60 1.99e+01 1.82e+00
 [61,]  61 2.10e+01 2.05e+00
 [62,]  62 2.22e+01 2.30e+00
 [63,]  63 2.34e+01 2.58e+00
 [64,]  64 2.46e+01 2.87e+00
 [65,]  65 2.59e+01 3.18e+00
 [66,]  66 2.73e+01 3.52e+00
 [67,]  67 2.87e+01 3.88e+00
 [68,]  68 3.01e+01 4.26e+00
 [69,]  69 3.16e+01 4.66e+00
 [70,]  70 3.31e+01 5.08e+00
 [71,]  71 3.46e+01 5.52e+00
 [72,]  72 3.62e+01 5.98e+00
 [73,]  73 3.78e+01 6.45e+00
 [74,]  74 3.94e+01 6.94e+00
 [75,]  75 4.11e+01 7.45e+00
 [76,]  76 4.28e+01 7.97e+00
 [77,]  77 4.46e+01 8.51e+00
 [78,]  78 4.63e+01 9.06e+00
 [79,]  79 4.81e+01 9.62e+00
 [80,]  80 5.00e+01 1.02e+01
 [81,]  81 5.18e+01 1.08e+01
 [82,]  82 5.37e+01 1.14e+01
 [83,]  83 5.55e+01 1.20e+01
 [84,]  84 5.74e+01 1.26e+01
 [85,]  85 5.94e+01 1.32e+01
 [86,]  86 6.13e+01 1.39e+01
 [87,]  87 6.32e+01 1.45e+01
 [88,]  88 6.52e+01 1.51e+01
 [89,]  89 6.72e+01 1.58e+01
 [90,]  90 6.91e+01 1.64e+01
 [91,]  91 7.11e+01 1.71e+01
 [92,]  92 7.31e+01 1.78e+01
 [93,]  93 7.51e+01 1.84e+01
 [94,]  94 7.71e+01 1.91e+01
 [95,]  95 7.91e+01 1.98e+01
 [96,]  96 8.11e+01 2.04e+01
 [97,]  97 8.31e+01 2.11e+01
 [98,]  98 8.51e+01 2.18e+01
 [99,]  99 8.71e+01 2.24e+01
[100,] 100 8.91e+01 2.31e+01
attr(,"knots")
[1]  5.95 35.65 65.35 95.05
> #lrm.fit(rcspline.eval(age,nk=4,inclx=TRUE), death)
> x <- 1:1000
> attributes(rcspline.eval(x))
$dim
[1] 1000    3

$knots
[1]  51 276 500 725 950

> x <- c(rep(0, 744),rep(1,6), rep(2,4), rep(3,10),rep(4,2),rep(6,6),
+   rep(7,3),rep(8,2),rep(9,4),rep(10,2),rep(11,9),rep(12,10),rep(13,13),
+   rep(14,5),rep(15,5),rep(16,10),rep(17,6),rep(18,3),rep(19,11),rep(20,16),
+   rep(21,6),rep(22,16),rep(23,17), 24, rep(25,8), rep(26,6),rep(27,3),
+   rep(28,7),rep(29,9),rep(30,10),rep(31,4),rep(32,4),rep(33,6),rep(34,6),
+   rep(35,4), rep(36,5), rep(38,6), 39, 39, 40, 40, 40, 41, 43, 44, 45)
> attributes(rcspline.eval(x, nk=3))
$dim
[1] 998   1

$knots
[1]  1 21 34

> attributes(rcspline.eval(x, nk=5))
$dim
[1] 998   3

$knots
[1]  1.0  3.0 17.0 24.6 38.0

> u <- c(rep(0,30), 1:4, rep(5,30))
> attributes(rcspline.eval(u))
Warning in rcspline.eval(u) :
  5 knots requested with 6 unique values of x.  knots set to 4 interior values.
$dim
[1] 64  2

$knots
[1] 1 2 3 4

> 
> 
> 
> cleanEx()
> nameEx("rcspline.plot")
> ### * rcspline.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcspline.plot
> ### Title: Plot Restricted Cubic Spline Function
> ### Aliases: rcspline.plot
> ### Keywords: regression models
> 
> ### ** Examples
> 
> #rcspline.plot(cad.dur, tvdlm, m=150)
> #rcspline.plot(log10(cad.dur+1), tvdlm, m=150)
> 
> 
> 
> cleanEx()
> nameEx("rcspline.restate")
> ### * rcspline.restate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcspline.restate
> ### Title: Re-state Restricted Cubic Spline Function
> ### Aliases: rcspline.restate rcsplineFunction
> ### Keywords: regression interface character
> 
> ### ** Examples
> 
> set.seed(1)
> x <- 1:100
> y <- (x - 50)^2 + rnorm(100, 0, 50)
> plot(x, y)
> xx <- rcspline.eval(x, inclx=TRUE, nk=4)
> knots <- attr(xx, "knots")
> coef <- lsfit(xx, y)$coef
> options(digits=4)
> # rcspline.restate must ignore intercept
> w <- rcspline.restate(knots, coef[-1], x="{\\rm BP}")
> # could also have used coef instead of coef[-1], to include intercept
> cat(attr(w,"latex"), sep="\n")
& & -69.685564  {\rm BP}+0.013443742 ({\rm BP}-5.95)_{+}^{3} \
& & -0.013754656({\rm BP}-35.65)_{+}^{3}-0.012821915({\rm BP}-65.35)_{+}^{3} \
& & +0.013132828 ({\rm BP}-95.05)_{+}^{3} \
> 
> 
> xtrans <- eval(attr(w, "function"))
> # This is an S function of a single argument
> lines(x, coef[1] + xtrans(x), type="l")
> # Plots fitted transformation
> 
> xtrans <- rcsplineFunction(knots, coef)
> xtrans
function (x = numeric(0), knots = c(5.95, 35.65, 65.35, 95.05
), coef = c(Intercept = 2329.20175140574, x = -69.6855639459135, 
106.727315060936, -109.195599900753), kd = 19.9488777705554, 
    type = "ordinary") 
{
    k <- length(knots)
    knotnk <- knots[k]
    knotnk1 <- knots[k - 1]
    knot1 <- knots[1]
    if (length(coef) < k) 
        coef <- c(0, coef)
    if (type == "ordinary") {
        y <- coef[1] + coef[2] * x
        for (j in 1:(k - 2)) y <- y + coef[j + 2] * (pmax((x - 
            knots[j])/kd, 0)^3 + ((knotnk1 - knots[j]) * pmax((x - 
            knotnk)/kd, 0)^3 - (knotnk - knots[j]) * (pmax((x - 
            knotnk1)/kd, 0)^3))/(knotnk - knotnk1))
        return(y)
    }
    y <- coef[1] * x + 0.5 * coef[2] * x * x
    for (j in 1:(k - 2)) y <- y + 0.25 * coef[j + 2] * kd * (pmax((x - 
        knots[j])/kd, 0)^4 + ((knotnk1 - knots[j]) * pmax((x - 
        knotnk)/kd, 0)^4 - (knotnk - knots[j]) * (pmax((x - knotnk1)/kd, 
        0)^4))/(knotnk - knotnk1))
    y
}
<environment: 0x5781db5e1dc8>
> lines(x, xtrans(x), col='blue')
> 
> 
> #x <- blood.pressure
> xx.simple <- cbind(x, pmax(x-knots[1],0)^3, pmax(x-knots[2],0)^3,
+                        pmax(x-knots[3],0)^3, pmax(x-knots[4],0)^3)
> pred.value <- coef[1] + xx.simple %*% w
> plot(x, pred.value, type='l')   # same as above
> 
> 
> 
> cleanEx()
> nameEx("reShape")
> ### * reShape
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: reShape
> ### Title: Reshape Matrices and Serial Data
> ### Aliases: reShape
> ### Keywords: manip array
> 
> ### ** Examples
> 
> set.seed(1)
> Solder  <- factor(sample(c('Thin','Thick'),200,TRUE),c('Thin','Thick'))
> Opening <- factor(sample(c('S','M','L'),  200,TRUE),c('S','M','L'))
> 
> tab <- table(Opening, Solder)
> tab
       Solder
Opening Thin Thick
      S   43    37
      M   26    37
      L   33    24
> reShape(tab)
$Opening
[1] "S" "M" "L" "S" "M" "L"

$Solder
[1] "Thin"  "Thin"  "Thin"  "Thick" "Thick" "Thick"

$tab
[1] 43 26 33 37 37 24

> # attach(tab)  # do further processing
> 
> # An example where a matrix is created from irregular vectors
> follow <- data.frame(id=c('a','a','b','b','b','d'),
+                      month=c(1, 2,  1,  2,  3,  2),
+                      cholesterol=c(225,226, 320,319,318, 270))
> follow
  id month cholesterol
1  a     1         225
2  a     2         226
3  b     1         320
4  b     2         319
5  b     3         318
6  d     2         270
> attach(follow)
> reShape(cholesterol, id=id, colvar=month)
    1   2   3
a 225 226  NA
b 320 319 318
d  NA 270  NA
> detach('follow')
> # Could have done :
> # reShape(cholesterol, triglyceride=trig, id=id, colvar=month)
> 
> # Create a data frame, reshaping a long dataset in which groups are
> # formed not just by subject id but by combinations of subject id and
> # visit number.  Also carry forward a variable that is supposed to be
> # constant within subject-visit number combinations.  In this example,
> # it is not constant, so an arbitrary visit number will be selected.
> w <- data.frame(id=c('a','a','a','a','b','b','b','d','d','d'),
+              visit=c(  1,  1,  2,  2,  1,  1,  2,  2,  2,  2),
+                  k=c('A','A','B','B','C','C','D','E','F','G'),
+                var=c('x','y','x','y','x','y','y','x','y','z'),
+                val=1:10)
> with(w,
+      reShape(val, id=data.frame(id,visit),
+              constant=data.frame(k), colvar=var))
  id visit k  x y  z
1  a     1 A  1 2 NA
3  a     2 B  3 4 NA
5  b     1 C  5 6 NA
7  b     2 D NA 7 NA
8  d     2 E  8 9 10
> 
> # Get predictions from a regression model for 2 systematically
> # varying predictors.  Convert the predictions into a matrix, with
> # rows corresponding to the predictor having the most values, and
> # columns corresponding to the other predictor
> # d <- expand.grid(x2=0:1, x1=1:100)
> # pred <- predict(fit, d)
> # reShape(pred, id=d$x1, colvar=d$x2)  # makes 100 x 2 matrix
> 
> 
> # Reshape a wide data frame containing multiple variables representing
> # repeated measurements (3 repeats on 2 variables; 4 subjects)
> set.seed(33)
> n <- 4
> w <- data.frame(age=rnorm(n, 40, 10),
+                 sex=sample(c('female','male'), n,TRUE),
+                 sbp1=rnorm(n, 120, 15),
+                 sbp2=rnorm(n, 120, 15),
+                 sbp3=rnorm(n, 120, 15),
+                 dbp1=rnorm(n,  80, 15),
+                 dbp2=rnorm(n,  80, 15),
+                 dbp3=rnorm(n,  80, 15), row.names=letters[1:n])
> options(digits=3)
> w
   age    sex sbp1 sbp2 sbp3 dbp1 dbp2  dbp3
a 38.6 female  109  123  131 81.8 90.9  88.9
b 39.6 female  132  120  120 71.1 86.9 110.0
c 50.1 female  131  148  118 73.4 82.8  52.4
d 38.4   male  104  124  125 84.4 83.5  67.1
> 
> 
> u <- reShape(w, base=c('sbp','dbp'), reps=3)
> u
    seqno  age    sex sbp   dbp
a 1     1 38.6 female 109  81.8
a 2     2 38.6 female 123  90.9
a 3     3 38.6 female 131  88.9
b 1     1 39.6 female 132  71.1
b 2     2 39.6 female 120  86.9
b 3     3 39.6 female 120 110.0
c 1     1 50.1 female 131  73.4
c 2     2 50.1 female 148  82.8
c 3     3 50.1 female 118  52.4
d 1     1 38.4   male 104  84.4
d 2     2 38.4   male 124  83.5
d 3     3 38.4   male 125  67.1
> reShape(w, base=c('sbp','dbp'), reps=3, timevar='week', times=c(0,3,12))
    week  age    sex sbp   dbp
a 1    0 38.6 female 109  81.8
a 2    3 38.6 female 123  90.9
a 3   12 38.6 female 131  88.9
b 1    0 39.6 female 132  71.1
b 2    3 39.6 female 120  86.9
b 3   12 39.6 female 120 110.0
c 1    0 50.1 female 131  73.4
c 2    3 50.1 female 148  82.8
c 3   12 50.1 female 118  52.4
d 1    0 38.4   male 104  84.4
d 2    3 38.4   male 124  83.5
d 3   12 38.4   male 125  67.1
> 
> 
> 
> cleanEx()
> nameEx("redun")
> ### * redun
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: redun
> ### Title: Redundancy Analysis
> ### Aliases: redun print.redun
> ### Keywords: smooth regression multivariate methods models
> 
> ### ** Examples
> 
> set.seed(1)
> n <- 100
> x1 <- runif(n)
> x2 <- runif(n)
> x3 <- x1 + x2 + runif(n)/10
> x4 <- x1 + x2 + x3 + runif(n)/10
> x5 <- factor(sample(c('a','b','c'),n,replace=TRUE))
> x6 <- 1*(x5=='a' | x5=='c')
> redun(~x1+x2+x3+x4+x5+x6, r2=.8)

Redundancy Analysis

~x1 + x2 + x3 + x4 + x5 + x6

n: 100 	p: 6 	nk: 3 

Number of NAs:	 0 

Transformation of target variables forced to be linear

R-squared cutoff: 0.8 	Type: ordinary 

R^2 with which each variable can be predicted from all other variables:

   x1    x2    x3    x4    x5    x6 
0.994 0.994 0.998 0.999 1.000 1.000 

Rendundant variables:

x5 x4 x3


Predicted from variables:

x1 x2 x6

  Variable Deleted   R^2 R^2 after later deletions
1               x5 1.000                       1 1
2               x4 0.999                     0.997
3               x3 0.995                          
> redun(~x1+x2+x3+x4+x5+x6, r2=.8, minfreq=40)

Redundancy Analysis

~x1 + x2 + x3 + x4 + x5 + x6

n: 100 	p: 4 	nk: 3 

Number of NAs:	 0 

Transformation of target variables forced to be linear

Minimum category frequency required for retention of a binary or
categorical variable: 40 

Binary or categorical variables removed because of inadequate frequencies:

x5 x6 

R-squared cutoff: 0.8 	Type: ordinary 

R^2 with which each variable can be predicted from all other variables:

   x1    x2    x3    x4 
0.994 0.994 0.998 0.999 

Rendundant variables:

x4 x3


Predicted from variables:

x1 x2

  Variable Deleted   R^2 R^2 after later deletions
1               x4 0.999                     0.997
2               x3 0.995                          
> redun(~x1+x2+x3+x4+x5+x6, r2=.8, allcat=TRUE)

Redundancy Analysis

~x1 + x2 + x3 + x4 + x5 + x6

n: 100 	p: 6 	nk: 3 

Number of NAs:	 0 

Transformation of target variables forced to be linear

All levels of a categorical variable had to be redundant before the
variable was declared redundant

R-squared cutoff: 0.8 	Type: ordinary 

R^2 with which each variable can be predicted from all other variables:

   x1    x2    x3    x4    x5    x6 
0.994 0.994 0.998 0.999 0.221 1.000 

(For categorical variables the minimum R^2 for any sufficiently
frequent dummy variable is displayed)


Rendundant variables:

x6 x4 x3


Predicted from variables:

x1 x2 x5

  Variable Deleted   R^2 R^2 after later deletions
1               x6 1.000                       1 1
2               x4 0.999                     0.997
3               x3 0.995                          
> # x5 is no longer redundant but x6 is
> redun(~x1+x2+x3+x4+x5+x6, r2=.8, rank=TRUE)

Redundancy Analysis

~x1 + x2 + x3 + x4 + x5 + x6

n: 100 	p: 6 	nk: 0 

Number of NAs:	 0 

Analysis used ranks 

Transformation of target variables forced to be linear in the ranks

R-squared cutoff: 0.8 	Type: ordinary 

R^2 with which each variable can be predicted from all other variables:

   x1    x2    x3    x4    x5    x6 
0.994 0.994 0.998 0.999 1.000 1.000 

Rendundant variables:

x5 x4 x3


Predicted from variables:

x1 x2 x6

  Variable Deleted   R^2 R^2 after later deletions
1               x5 1.000                       1 1
2               x4 0.999                     0.997
3               x3 0.995                          
> redun(~x1+x2+x3+x4+x5+x6, r2=.8, qrank=TRUE)

Redundancy Analysis

~x1 + x2 + x3 + x4 + x5 + x6

n: 100 	p: 6 	nk: 0 

Number of NAs:	 0 

Analysis used ranks and square of ranks 

Transformation of target variables forced to be linear in the ranks

R-squared cutoff: 0.8 	Type: ordinary 

R^2 with which each variable can be predicted from all other variables:

   x1    x2    x3    x4    x5    x6 
0.907 0.914 0.994 0.995 1.000 1.000 

Rendundant variables:

x5 x4 x3


Predicted from variables:

x1 x2 x6

  Variable Deleted   R^2 R^2 after later deletions
1               x5 1.000                       1 1
2               x4 0.995                     0.952
3               x3 0.949                          
> 
> # To help decode which variables made a particular variable redundant:
> # r <- redun(...)
> # r2describe(r$scores)
> 
> 
> 
> cleanEx()
> nameEx("rm.boot")
> ### * rm.boot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rm.boot
> ### Title: Bootstrap Repeated Measurements Model
> ### Aliases: rm.boot plot.rm.boot
> ### Keywords: regression multivariate htest hplot
> 
> ### ** Examples
> 
> # Generate multivariate normal responses with equal correlations (.7)
> # within subjects and no correlation between subjects
> # Simulate realizations from a piecewise linear population time-response
> # profile with large subject effects, and fit using a 6-knot spline
> # Estimate the correlation structure from the residuals, as a function
> # of the absolute time difference
> 
> 
> # Function to generate n p-variate normal variates with mean vector u and
> # covariance matrix S
> # Slight modification of function written by Bill Venables
> # See also the built-in function rmvnorm
> mvrnorm <- function(n, p = 1, u = rep(0, p), S = diag(p)) {
+   Z <- matrix(rnorm(n * p), p, n)
+   t(u + t(chol(S)) %*% Z)
+ }
> 
> 
> n     <- 20         # Number of subjects
> sub   <- .5*(1:n)   # Subject effects
> 
> 
> # Specify functional form for time trend and compute non-stochastic component
> times <- seq(0, 1, by=.1)
> g     <- function(times) 5*pmax(abs(times-.5),.3)
> ey    <- g(times)
> 
> 
> # Generate multivariate normal errors for 20 subjects at 11 times
> # Assume equal correlations of rho=.7, independent subjects
> 
> 
> nt    <- length(times)
> rho   <- .7
> 
> 
>         
> set.seed(19)        
> errors <- mvrnorm(n, p=nt, S=diag(rep(1-rho,nt))+rho)
> # Note:  first random number seed used gave rise to mean(errors)=0.24!
> 
> 
> # Add E[Y], error components, and subject effects
> y      <- matrix(rep(ey,n), ncol=nt, byrow=TRUE) + errors + 
+           matrix(rep(sub,nt), ncol=nt)
> 
> 
> # String out data into long vectors for times, responses, and subject ID
> y      <- as.vector(t(y))
> times  <- rep(times, n)
> id     <- sort(rep(1:n, nt))
> 
> 
> # Show lowess estimates of time profiles for individual subjects
> f <- rm.boot(times, y, id, plot.individual=TRUE, B=25, cor.pattern='estimate',
+              smoother=lowess, bootstrap.type='x fixed', nk=6)
10 20 

Spearman rank correlation between 26 log likelihoods  assuming independence and assuming dependence: 0.95 
> # In practice use B=400 or 500
> # This will compute a dependent-structure log-likelihood in addition
> # to one assuming independence.  By default, the dep. structure
> # objective will be used by the plot method  (could have specified rho=.7)
> # NOTE: Estimating the correlation pattern from the residual does not
> # work in cases such as this one where there are large subject effects
> 
> 
> # Plot fits for a random sample of 10 of the 25 bootstrap fits
> plot(f, individual.boot=TRUE, ncurves=10, ylim=c(6,8.5))
> 
> 
> # Plot pointwise and simultaneous confidence regions
> plot(f, pointwise.band=TRUE, col.pointwise=1, ylim=c(6,8.5))
> 
> 
> # Plot population response curve at average subject effect
> ts <- seq(0, 1, length=100)
> lines(ts, g(ts)+mean(sub), lwd=3)
> 
> 
> ## Not run: 
> ##D #
> ##D # Handle a 2-sample problem in which curves are fitted 
> ##D # separately for males and females and we wish to estimate the
> ##D # difference in the time-response curves for the two sexes.  
> ##D # The objective criterion will be taken by plot.rm.boot as the 
> ##D # total of the two sums of squared errors for the two models
> ##D #
> ##D knots <- rcspline.eval(c(time.f,time.m), nk=6, knots.only=TRUE)
> ##D # Use same knots for both sexes, and use a times vector that 
> ##D # uses a range of times that is included in the measurement 
> ##D # times for both sexes
> ##D #
> ##D tm <- seq(max(min(time.f),min(time.m)),
> ##D           min(max(time.f),max(time.m)),length=100)
> ##D 
> ##D 
> ##D f.female <- rm.boot(time.f, bp.f, id.f, knots=knots, times=tm)
> ##D f.male   <- rm.boot(time.m, bp.m, id.m, knots=knots, times=tm)
> ##D plot(f.female)
> ##D plot(f.male)
> ##D # The following plots female minus male response, with 
> ##D # a sequence of shaded confidence band for the difference
> ##D plot(f.female,f.male,multi=TRUE)
> ##D 
> ##D 
> ##D # Do 1000 simulated analyses to check simultaneous coverage 
> ##D # probability.  Use a null regression model with Gaussian errors
> ##D 
> ##D 
> ##D n.per.pt <- 30
> ##D n.pt     <- 10
> ##D 
> ##D 
> ##D null.in.region <- 0
> ##D 
> ##D 
> ##D for(i in 1:1000) {
> ##D   y    <- rnorm(n.pt*n.per.pt)
> ##D   time <- rep(1:n.per.pt, n.pt)
> ##D #  Add the following line and add ,id=id to rm.boot to use clustering
> ##D #  id   <- sort(rep(1:n.pt, n.per.pt))
> ##D #  Because we are ignoring patient id, this simulation is effectively
> ##D #  using 1 point from each of 300 patients, with times 1,2,3,,,30 
> ##D 
> ##D 
> ##D   f <- rm.boot(time, y, B=500, nk=5, bootstrap.type='x fixed')
> ##D   g <- plot(f, ylim=c(-1,1), pointwise=FALSE)
> ##D   null.in.region <- null.in.region + all(g$lower<=0 & g$upper>=0)
> ##D   prn(c(i=i,null.in.region=null.in.region))
> ##D }
> ##D 
> ##D 
> ##D # Simulation Results: 905/1000 simultaneous confidence bands 
> ##D # fully contained the horizontal line at zero
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("samplesize.bin")
> ### * samplesize.bin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: samplesize.bin
> ### Title: Sample Size for 2-sample Binomial
> ### Aliases: samplesize.bin
> ### Keywords: htest category
> 
> ### ** Examples
> 
> alpha <- .05
> beta <- c(.70,.80,.90,.95)
> 
> 
> # N1 is a matrix of total sample sizes whose
> # rows vary by hypothesized treatment success probability and
> # columns vary by power
> # See Meinert's book for formulae.
> 
> 
> N1 <- samplesize.bin(alpha, beta, pit=.55, pic=.5)
> N1 <- rbind(N1, samplesize.bin(alpha, beta, pit=.60, pic=.5))
> N1 <- rbind(N1, samplesize.bin(alpha, beta, pit=.65, pic=.5))
> N1 <- rbind(N1, samplesize.bin(alpha, beta, pit=.70, pic=.5))
> attr(N1,"dimnames") <- NULL
> 
> 
> #Accounting for 5% noncompliance in the treated group
> inflation <- (1/.95)**2
> print(round(N1*inflation+.5,0))
     [,1] [,2] [,3] [,4]
[1,] 2079 2732 3784 4782
[2,]  516  676  937 1184
[3,]  225  296  409  518
[4,]  125  163  225  284
> 
> 
> 
> cleanEx()
> nameEx("sas.get")
> ### * sas.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sas.get
> ### Title: Convert a SAS Dataset to an S Data Frame
> ### Aliases: sas.get is.special.miss [.special.miss print.special.miss
> ###   format.special.miss sas.codes code.levels timePOSIXt
> ### Keywords: interface manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D sas.contents("saslib", "mice")
> ##D # [1] "dose"  "ld50"  "strain"  "lab_no"
> ##D attr(, "n"):
> ##D # [1] 117
> ##D mice <- sas.get("saslib", mem="mice", var=c("dose", "strain", "ld50"))
> ##D plot(mice$dose, mice$ld50)
> ##D 
> ##D 
> ##D nude.mice <- sas.get(lib=unix("echo $HOME/saslib"), mem="mice",
> ##D 	ifs="if strain='nude'")
> ##D 
> ##D 
> ##D nude.mice.dl <- sas.get(lib=unix("echo $HOME/saslib"), mem="mice",
> ##D 	var=c("dose", "ld50"), ifs="if strain='nude'")
> ##D 
> ##D 
> ##D # Get a dataset from current directory, recode PROC FORMAT; VALUE \dots 
> ##D # variables into factors with labels of the form "good(1)" "better(2)",
> ##D # get special missing values, recode missing codes .D and .R into new
> ##D # factor levels "Don't know" and "Refused to answer" for variable q1
> ##D d <- sas.get(".", "mydata", recode=2, special.miss=TRUE)
> ##D attach(d)
> ##D nl <- length(levels(q1))
> ##D lev <- c(levels(q1), "Don't know", "Refused")
> ##D q1.new <- as.integer(q1)
> ##D q1.new[is.special.miss(q1,"D")] <- nl+1
> ##D q1.new[is.special.miss(q1,"R")] <- nl+2
> ##D q1.new <- factor(q1.new, 1:(nl+2), lev)
> ##D # Note: would like to use factor() in place of as.integer \dots but
> ##D # factor in this case adds "NA" as a category level
> ##D 
> ##D 
> ##D d <- sas.get(".", "mydata")
> ##D sas.codes(d$x)    # for PROC FORMATted variables returns original data codes
> ##D d$x <- code.levels(d$x)   # or attach(d); x <- code.levels(x)
> ##D # This makes levels such as "good" "better" "best" into e.g.
> ##D # "1:good" "2:better" "3:best", if the original SAS values were 1,2,3
> ##D 
> ##D 
> ##D # Retrieve the same variables from another dataset (or an update of
> ##D # the original dataset)
> ##D mydata2 <- sas.get('mydata2', var=names(d))
> ##D # This only works if none of the original SAS variable names contained _
> ##D mydata2 <- cleanup.import(mydata2) # will make true integer variables
> ##D 
> ##D # Code from Don MacQueen to generate SAS dataset to test import of
> ##D # date, time, date-time variables
> ##D # data ssd.test;
> ##D #     d1='3mar2002'd ;
> ##D #     dt1='3mar2002 9:31:02'dt;
> ##D #     t1='11:13:45't;
> ##D #     output;
> ##D #
> ##D #     d1='3jun2002'd ;
> ##D #     dt1='3jun2002 9:42:07'dt;
> ##D #     t1='11:14:13't;
> ##D #     output;
> ##D #     format d1 mmddyy10. dt1 datetime. t1 time.;
> ##D # run;
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sasxport.get")
> ### * sasxport.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sasxport.get
> ### Title: Enhanced Importing of SAS Transport Files using read.xport
> ### Aliases: sasxport.get sasdsLabels
> ### Keywords: interface manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # SAS code to generate test dataset:
> ##D # libname y SASV5XPT "test2.xpt";
> ##D #
> ##D # PROC FORMAT; VALUE race 1=green 2=blue 3=purple; RUN;
> ##D # PROC FORMAT CNTLOUT=format;RUN;  * Name, e.g. 'format', unimportant;
> ##D # data test;
> ##D # LENGTH race 3 age 4;
> ##D # age=30; label age="Age at Beginning of Study";
> ##D # race=2;
> ##D # d1='3mar2002'd ;
> ##D # dt1='3mar2002 9:31:02'dt;
> ##D # t1='11:13:45't;
> ##D # output;
> ##D #
> ##D # age=31;
> ##D # race=4;
> ##D # d1='3jun2002'd ;
> ##D # dt1='3jun2002 9:42:07'dt;
> ##D # t1='11:14:13't;
> ##D # output;
> ##D # format d1 mmddyy10. dt1 datetime. t1 time. race race.;
> ##D # run;
> ##D # data z; LENGTH x3 3 x4 4 x5 5 x6 6 x7 7 x8 8;
> ##D #    DO i=1 TO 100;
> ##D #        x3=ranuni(3);
> ##D #        x4=ranuni(5);
> ##D #        x5=ranuni(7);
> ##D #        x6=ranuni(9);
> ##D #        x7=ranuni(11);
> ##D #        x8=ranuni(13);
> ##D #        output;
> ##D #        END;
> ##D #    DROP i;
> ##D #    RUN;
> ##D # PROC MEANS; RUN;
> ##D # PROC COPY IN=work OUT=y;SELECT test format z;RUN; *Creates test2.xpt;
> ##D w <- sasxport.get('test2.xpt')
> ##D # To use an existing copy of test2.xpt available on the web:
> ##D w <- sasxport.get('https://github.com/harrelfe/Hmisc/raw/master/inst/tests/test2.xpt')
> ##D 
> ##D describe(w$test)   # see labels, format names for dataset test
> ##D # Note: if only one dataset (other than format) had been exported,
> ##D # just do describe(w) as sasxport.get would not create a list for that
> ##D lapply(w, describe)# see descriptive stats for both datasets
> ##D contents(w$test)   # another way to see variable attributes
> ##D lapply(w, contents)# show contents of both datasets
> ##D options(digits=7)  # compare the following matrix with PROC MEANS output
> ##D t(sapply(w$z, function(x)
> ##D  c(Mean=mean(x),SD=sqrt(var(x)),Min=min(x),Max=max(x))))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("scat1d")
> ### * scat1d
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scat1d
> ### Title: One-Dimensional Scatter Diagram, Spike Histogram, or Density
> ### Aliases: scat1d jitter2 jitter2.default jitter2.data.frame datadensity
> ###   datadensity.data.frame histSpike histSpikeg ecdfpM
> ### Keywords: dplot aplot hplot distribution
> 
> ### ** Examples
> 
> plot(x <- rnorm(50), y <- 3*x + rnorm(50)/2 )
> scat1d(x)                 # density bars on top of graph
> scat1d(y, 4)              # density bars at right
> histSpike(x, add=TRUE)       # histogram instead, 100 bins
> histSpike(y, 4, add=TRUE)
> histSpike(x, type='density', add=TRUE)  # smooth density at bottom
> histSpike(y, 4, type='density', add=TRUE)
> 
> 
> smooth <- lowess(x, y)    # add nonparametric regression curve
> lines(smooth)             # Note: plsmo() does this
> scat1d(x, y=approx(smooth, xout=x)$y) # data density on curve
> scat1d(x, curve=smooth)   # same effect as previous command
> histSpike(x, curve=smooth, add=TRUE) # same as previous but with histogram
> histSpike(x, curve=smooth, type='density', add=TRUE)  
> # same but smooth density over curve
> 
> 
> plot(x <- rnorm(250), y <- 3*x + rnorm(250)/2)
> scat1d(x, tfrac=0)        # dots randomly spaced from axis
> scat1d(y, 4, frac=-.03)   # bars outside axis
> scat1d(y, 2, tfrac=.2)    # same bars with smaller random fraction
> 
> 
> x <- c(0:3,rep(4,3),5,rep(7,10),9)
> plot(x, jitter2(x))       # original versus jittered values
> abline(0,1)               # unique values unjittered on abline
> points(x+0.1, jitter2(x, limit=FALSE), col=2)
>                           # allow locally maximum jittering
> points(x+0.2, jitter2(x, fill=1), col=3); abline(h=seq(0.5,9,1), lty=2)
>                           # fill 3/3 instead of 1/3
> x <- rnorm(200,0,2)+1; y <- x^2
> x2 <- round((x+rnorm(200))/2)*2
> x3 <- round((x+rnorm(200))/4)*4
> dfram <- data.frame(y,x,x2,x3)
> plot(dfram$x2, dfram$y)   # jitter2 via scat1d
> scat1d(dfram$x2, y=dfram$y, preserve=TRUE, col=2)
> scat1d(dfram$x2, preserve=TRUE, frac=-0.02, col=2)
> scat1d(dfram$y, 4, preserve=TRUE, frac=-0.02, col=2)
> 
> 
> pairs(jitter2(dfram))     # pairs for jittered data.frame
> # This gets reasonable pairwise scatter plots for all combinations of
> # variables where
> #
> # - continuous variables (with unique values) are not jittered at all, thus
> #   all relations between continuous variables are shown as they are,
> #   extreme values have exact positions.
> #
> # - discrete variables get a reasonable amount of jittering, whether they
> #   have 2, 3, 5, 10, 20 \dots levels
> #
> # - different from adding noise, jitter2() will use the available space
> #   optimally and no value will randomly mask another
> #
> # If you want a scatterplot with lowess smooths on the *exact* values and
> # the point clouds shown jittered, you just need
> #
> pairs( dfram ,panel=function(x,y) { points(jitter2(x),jitter2(y))
+                                     lines(lowess(x,y)) } )
> 
> 
> 
> 
> datadensity(dfram)     # graphical snapshot of entire data frame
> datadensity(dfram, group=cut2(dfram$x2,g=3))
>                           # stratify points and frequencies by
>                           # x2 tertiles and use 3 colors
> 
> 
> # datadensity.data.frame(split(x, grouping.variable))
> # need to explicitly invoke datadensity.data.frame when the
> # first argument is a list
> 
> ## Not run: 
> ##D require(rms)
> ##D require(ggplot2)
> ##D f <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
> ##D          data=d)
> ##D p <- Predict(f, cholesterol, sex)
> ##D g <- ggplot(p, aes(x=cholesterol, y=yhat, color=sex)) + geom_line() +
> ##D   xlab(xl2) + ylim(-1, 1)
> ##D g <- g + geom_ribbon(data=p, aes(ymin=lower, ymax=upper), alpha=0.2,
> ##D                 linetype=0, show_guide=FALSE)
> ##D g + histSpikeg(yhat ~ cholesterol + sex, p, d)
> ##D 
> ##D # colors <- c('red', 'blue')
> ##D # p <- plot_ly(x=x, y=y, color=g, colors=colors, mode='markers')
> ##D # histSpikep(p, x, y, z, color=g, colors=colors)
> ##D 
> ##D w <- data.frame(x1=rnorm(100), x2=exp(rnorm(100)))
> ##D g <- c(rep('a', 50), rep('b', 50))
> ##D ecdfpM(w, group=g, ncols=2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("score.binary")
> ### * score.binary
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: score.binary
> ### Title: Score a Series of Binary Variables
> ### Aliases: score.binary
> ### Keywords: manip
> 
> ### ** Examples
> 
> set.seed(1)
> age <- rnorm(25, 70, 15)
> previous.disease <- sample(0:1, 25, TRUE)
> #Hierarchical scale, highest of 1:age>70  2:previous.disease
> score.binary(age>70, previous.disease, retfactor=FALSE)
 [1] 2 1 2 2 1 0 2 2 2 0 1 2 2 2 2 2 2 1 2 2 2 2 1 0 1
> #Same as above but return factor variable with levels "none" "age>70" 
> # "previous.disease"
> score.binary(age>70, previous.disease)
 [1] previous.disease age > 70         previous.disease previous.disease
 [5] age > 70         none             previous.disease previous.disease
 [9] previous.disease none             age > 70         previous.disease
[13] previous.disease previous.disease previous.disease previous.disease
[17] previous.disease age > 70         previous.disease previous.disease
[21] previous.disease previous.disease age > 70         none            
[25] age > 70        
Levels: none age > 70 previous.disease
> 
> 
> #Additive scale with weights 1:age>70  2:previous.disease
> score.binary(age>70, previous.disease, fun=sum)
 [1] 2 1 2 3 1 0 3 3 3 0 1 3 2 2 3 2 2 1 3 3 3 3 1 0 1
> #Additive scale, equal weights
> score.binary(age>70, previous.disease, fun=sum, points=c(1,1))
 [1] 1 1 1 2 1 0 2 2 2 0 1 2 1 1 2 1 1 1 2 2 2 2 1 0 1
> #Same as saying points=1
> 
> 
> #Union of variables, to create a new binary variable
> score.binary(age>70, previous.disease, fun=any)
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[25]  TRUE
> 
> 
> 
> cleanEx()
> nameEx("sedit")
> ### * sedit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sedit
> ### Title: Character String Editing and Miscellaneous Character Handling
> ###   Functions
> ### Aliases: sedit substring.location substring2 substring2<-
> ###   replace.substring.wild numeric.string all.digits
> ### Keywords: manip character
> 
> ### ** Examples
> 
> x <- 'this string'
> substring2(x, 3, 4) <- 'IS'
> x
[1] "thIS string"
> substring2(x, 7) <- ''
> x
[1] "thIS s"
> 
> 
> substring.location('abcdefgabc', 'ab')
$first
[1] 1 8

$last
[1] 2 9

> substring.location('abcdefgabc', 'ab', restrict=c(3,999))
$first
[1] 8

$last
[1] 9

> 
> 
> replace.substring.wild('this is a cat','this*cat','that*dog')
[1] "that is a dog"
> replace.substring.wild('there is a cat','is a*', 'is not a*')
[1] "there is not a cat"
> replace.substring.wild('this is a cat','is a*', 'Z')
[1] "this Z"
> 
> 
> qualify <- function(x) x==' 1.5 ' | x==' 2.5 '
> replace.substring.wild('He won 1.5 million $','won*million',
+                        'lost*million', test=qualify)
[1] "He lost 1.5 million $"
> replace.substring.wild('He won 1 million $','won*million',
+                        'lost*million', test=qualify)
[1] "He won 1 million $"
> replace.substring.wild('He won 1.2 million $','won*million',
+                        'lost*million', test=numeric.string)
[1] "He lost 1.2 million $"
> 
> 
> x <- c('a = b','c < d','hello')
> sedit(x, c('=','he*o'),c('==','he*'))
[1] "a == b" "c < d"  "hell"  
> 
> 
> sedit('x23', '*$', '[*]', test=numeric.string)
[1] "x[23]"
> sedit('23xx', '^*', 'Y_{*} ', test=all.digits)
[1] "Y_{23} xx"
> 
> 
> replace.substring.wild("abcdefabcdef", "d*f", "xy")
[1] "abcxy"
> 
> 
> x <- "abcd"
> substring2(x, "bc") <- "BCX"
> x
[1] "aBCXd"
> substring2(x, "B*d") <- "B*D"
> x
[1] "aBCXD"
> 
> 
> 
> cleanEx()
> nameEx("show.pch")
> ### * show.pch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: show.pch
> ### Title: Display Colors, Plotting Symbols, and Symbol Numeric Equivalents
> ### Aliases: show.pch show.col character.table
> ### Keywords: aplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D show.pch()
> ##D show.col()
> ##D character.table()
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("simRegOrd")
> ### * simRegOrd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simRegOrd
> ### Title: Simulate Power for Adjusted Ordinal Regression Two-Sample Test
> ### Aliases: simRegOrd
> ### Keywords: htest category
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ## First use no ordinal high-end category overrides, and compare power
> ##D ## to t-test when there is no covariate
> ##D 
> ##D n <- 100
> ##D delta <- .5
> ##D sd <- 1
> ##D require(pwr)
> ##D power.t.test(n = n / 2, delta=delta, sd=sd, type='two.sample')  # 0.70
> ##D set.seed(1)
> ##D w <- simRegOrd(n, delta=delta, sigma=sd, pr=TRUE)     # 0.686
> ##D 
> ##D ## Now do ANCOVA with a quadratic effect of a covariate
> ##D n <- 100
> ##D x <- rnorm(n)
> ##D w <- simRegOrd(n, nsim=400, delta=delta, sigma=sd, x=x,
> ##D                X=cbind(x, x^2),
> ##D                Eyx=function(x) x + x^2, pr=TRUE)
> ##D w$power  # 0.68
> ##D 
> ##D ## Fit a cubic spline to some simulated pilot data and use the fitted
> ##D ## function as the true equation in the power simulation
> ##D require(rms)
> ##D N <- 1000
> ##D set.seed(2)
> ##D x <- rnorm(N)
> ##D y <- x + x^2 + rnorm(N, 0, sd=sd)
> ##D f <- ols(y ~ rcs(x, 4), x=TRUE)
> ##D 
> ##D n <- 100
> ##D j <- sample(1 : N, n, replace=n > N)
> ##D x <-   x[j]
> ##D X <- f$x[j,]
> ##D w <- simRegOrd(n, nsim=400, delta=delta, sigma=sd, x=x,
> ##D                X=X,
> ##D                Eyx=Function(f), pr=TRUE)
> ##D w$power  ## 0.70
> ##D 
> ##D ## Finally, add discrete ordinal category overrides and high end of y
> ##D ## Start with no effect of treatment on these ordinal event levels (OR=1.0)
> ##D 
> ##D w <- simRegOrd(n, nsim=400, delta=delta, odds.ratio=1, sigma=sd,
> ##D                x=x, X=X, Eyx=Function(f),
> ##D                p=c(.98, .01, .01),
> ##D                pr=TRUE)
> ##D w$power  ## 0.61   (0.3 if p=.8 .1 .1, 0.37 for .9 .05 .05, 0.50 for .95 .025 .025)
> ##D 
> ##D ## Now assume that odds ratio for treatment is 2.5
> ##D ## First compute power for clinical endpoint portion of Y alone
> ##D or <- 2.5
> ##D p <- c(.9, .05, .05)
> ##D popower(p, odds.ratio=or, n=100)   # 0.275
> ##D ## Compute power of t-test on continuous part of Y alone
> ##D power.t.test(n = 100 / 2, delta=delta, sd=sd, type='two.sample')  # 0.70
> ##D ## Note this is the same as the p.o. model power from simulation above
> ##D ## Solve for OR that gives the same power estimate from popower
> ##D popower(rep(.01, 100), odds.ratio=2.4, n=100)   # 0.706
> ##D ## Compute power for continuous Y with ordinal override
> ##D w <- simRegOrd(n, nsim=400, delta=delta, odds.ratio=or, sigma=sd,
> ##D                x=x, X=X, Eyx=Function(f),
> ##D                p=c(.9, .05, .05),
> ##D                pr=TRUE)
> ##D w$power  ## 0.72
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("simplifyDims")
> ### * simplifyDims
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simplifyDims
> ### Title: List Simplification
> ### Aliases: simplifyDims
> ### Keywords: print
> 
> ### ** Examples
> 
> a <- list(a = matrix(1:25, ncol=5), b = matrix(1:10, ncol=5), c = 1:5)
> 
> simplifyDims(a)
  [,1] [,2] [,3] [,4] [,5]
     1    6   11   16   21
     2    7   12   17   22
     3    8   13   18   23
     4    9   14   19   24
     5   10   15   20   25
     1    3    5    7    9
     2    4    6    8   10
c    1    2    3    4    5
> 
> 
> 
> cleanEx()
> nameEx("smean.sd")
> ### * smean.sd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smean.sd
> ### Title: Compute Summary Statistics on a Vector
> ### Aliases: smean.cl.normal smean.sd smean.sdl smean.cl.boot smedian.hilow
> ### Keywords: nonparametric htest
> 
> ### ** Examples
> 
> set.seed(1)
> x <- rnorm(100)
> smean.sd(x)
 Mean    SD 
0.109 0.898 
> smean.sdl(x)
  Mean  Lower  Upper 
 0.109 -1.688  1.905 
> smean.cl.normal(x)
   Mean   Lower   Upper 
 0.1089 -0.0693  0.2871 
> smean.cl.boot(x)
   Mean   Lower   Upper 
 0.1089 -0.0582  0.2770 
> smedian.hilow(x, conf.int=.5)  # 25th and 75th percentiles
Median  Lower  Upper 
 0.114 -0.494  0.692 
> 
> # Function to compute 0.95 confidence interval for the difference in two means
> # g is grouping variable
> bootdif <- function(y, g) {
+  g <- as.factor(g)
+  a <- attr(smean.cl.boot(y[g==levels(g)[1]], B=2000, reps=TRUE),'reps')
+  b <- attr(smean.cl.boot(y[g==levels(g)[2]], B=2000, reps=TRUE),'reps')
+  meandif <- diff(tapply(y, g, mean, na.rm=TRUE))
+  a.b <- quantile(b-a, c(.025,.975))
+  res <- c(meandif, a.b)
+  names(res) <- c('Mean Difference','.025','.975')
+  res
+ }
> 
> 
> 
> 
> cleanEx()
> nameEx("somers2")
> ### * somers2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: somers2
> ### Title: Somers' Dxy Rank Correlation
> ### Aliases: somers2
> ### Keywords: nonparametric
> 
> ### ** Examples
> 
> set.seed(1)
> predicted <- runif(200)
> dead      <- sample(0:1, 200, TRUE)
> roc.area <- somers2(predicted, dead)["C"]
> 
> # Check weights
> x <- 1:6
> y <- c(0,0,1,0,1,1)
> f <- c(3,2,2,3,2,1)
> somers2(x, y)
      C     Dxy       n Missing 
  0.889   0.778   6.000   0.000 
> somers2(rep(x, f), rep(y, f))
      C     Dxy       n Missing 
   0.85    0.70   13.00    0.00 
> somers2(x, y, f)
      C     Dxy       n Missing 
   0.85    0.70   13.00    0.00 
> 
> 
> 
> cleanEx()
> nameEx("spikecomp")
> ### * spikecomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spikecomp
> ### Title: spikecomp
> ### Aliases: spikecomp
> 
> ### ** Examples
> 
> spikecomp(1:1000)
$x
  [1]    0   10   20   30   40   50   60   70   80   90  100  110  120  130  140
 [16]  150  160  170  180  190  200  210  220  230  240  250  260  270  280  290
 [31]  300  310  320  330  340  350  360  370  380  390  400  410  420  430  440
 [46]  450  460  470  480  490  500  510  520  530  540  550  560  570  580  590
 [61]  600  610  620  630  640  650  660  670  680  690  700  710  720  730  740
 [76]  750  760  770  780  790  800  810  820  830  840  850  860  870  880  890
 [91]  900  910  920  930  940  950  960  970  980  990 1000

$y
  [1] 0.9 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
 [19] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
 [37] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
 [55] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
 [73] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
 [91] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.1

$roundedTo
[1] 10

> spikecomp(1:1000, method='grid')
$x
 [1] "Q<sub>0.01</sub>:10<br>Bin: [1, 20)<br>Observed:<br>1 - 19 (19 distinct)"         
 [2] "Bin: [20, 30)<br>Observed:<br>20; 21; 22; 23; 24; 25; 26; 27; 28; 29"             
 [3] "Bin: [30, 40)<br>Observed:<br>30; 31; 32; 33; 34; 35; 36; 37; 38; 39"             
 [4] "Bin: [40, 50)<br>Observed:<br>40; 41; 42; 43; 44; 45; 46; 47; 48; 49"             
 [5] "Bin: [50, 60)<br>Observed:<br>50; 51; 52; 53; 54; 55; 56; 57; 58; 59"             
 [6] "Bin: [60, 70)<br>Observed:<br>60; 61; 62; 63; 64; 65; 66; 67; 68; 69"             
 [7] "Bin: [70, 80)<br>Observed:<br>70; 71; 72; 73; 74; 75; 76; 77; 78; 79"             
 [8] "Bin: [80, 90)<br>Observed:<br>80; 81; 82; 83; 84; 85; 86; 87; 88; 89"             
 [9] "Bin: [90, 100)<br>Observed:<br>90; 91; 92; 93; 94; 95; 96; 97; 98; 99"            
[10] "Bin: [100, 110)<br>Observed:<br>100; 101; 102; 103; 104; 105; 106; 107; 108; 109" 
[11] "Bin: [110, 120)<br>Observed:<br>110; 111; 112; 113; 114; 115; 116; 117; 118; 119" 
[12] "Bin: [120, 130)<br>Observed:<br>120; 121; 122; 123; 124; 125; 126; 127; 128; 129" 
[13] "Bin: [130, 140)<br>Observed:<br>130; 131; 132; 133; 134; 135; 136; 137; 138; 139" 
[14] "Bin: [140, 150)<br>Observed:<br>140; 141; 142; 143; 144; 145; 146; 147; 148; 149" 
[15] "Bin: [150, 160)<br>Observed:<br>150; 151; 152; 153; 154; 155; 156; 157; 158; 159" 
[16] "Bin: [160, 170)<br>Observed:<br>160; 161; 162; 163; 164; 165; 166; 167; 168; 169" 
[17] "Bin: [170, 180)<br>Observed:<br>170; 171; 172; 173; 174; 175; 176; 177; 178; 179" 
[18] "Bin: [180, 190)<br>Observed:<br>180; 181; 182; 183; 184; 185; 186; 187; 188; 189" 
[19] "Bin: [190, 200)<br>Observed:<br>190; 191; 192; 193; 194; 195; 196; 197; 198; 199" 
[20] "Bin: [200, 210)<br>Observed:<br>200; 201; 202; 203; 204; 205; 206; 207; 208; 209" 
[21] "Bin: [210, 220)<br>Observed:<br>210; 211; 212; 213; 214; 215; 216; 217; 218; 219" 
[22] "Bin: [220, 230)<br>Observed:<br>220; 221; 222; 223; 224; 225; 226; 227; 228; 229" 
[23] "Bin: [230, 240)<br>Observed:<br>230; 231; 232; 233; 234; 235; 236; 237; 238; 239" 
[24] "Bin: [240, 250)<br>Observed:<br>240; 241; 242; 243; 244; 245; 246; 247; 248; 249" 
[25] "Bin: [250, 260)<br>Observed:<br>250; 251; 252; 253; 254; 255; 256; 257; 258; 259" 
[26] "Bin: [260, 270)<br>Observed:<br>260; 261; 262; 263; 264; 265; 266; 267; 268; 269" 
[27] "Bin: [270, 280)<br>Observed:<br>270; 271; 272; 273; 274; 275; 276; 277; 278; 279" 
[28] "Bin: [280, 290)<br>Observed:<br>280; 281; 282; 283; 284; 285; 286; 287; 288; 289" 
[29] "Bin: [290, 300)<br>Observed:<br>290; 291; 292; 293; 294; 295; 296; 297; 298; 299" 
[30] "Bin: [300, 310)<br>Observed:<br>300; 301; 302; 303; 304; 305; 306; 307; 308; 309" 
[31] "Bin: [310, 320)<br>Observed:<br>310; 311; 312; 313; 314; 315; 316; 317; 318; 319" 
[32] "Bin: [320, 330)<br>Observed:<br>320; 321; 322; 323; 324; 325; 326; 327; 328; 329" 
[33] "Bin: [330, 340)<br>Observed:<br>330; 331; 332; 333; 334; 335; 336; 337; 338; 339" 
[34] "Bin: [340, 350)<br>Observed:<br>340; 341; 342; 343; 344; 345; 346; 347; 348; 349" 
[35] "Bin: [350, 360)<br>Observed:<br>350; 351; 352; 353; 354; 355; 356; 357; 358; 359" 
[36] "Bin: [360, 370)<br>Observed:<br>360; 361; 362; 363; 364; 365; 366; 367; 368; 369" 
[37] "Bin: [370, 380)<br>Observed:<br>370; 371; 372; 373; 374; 375; 376; 377; 378; 379" 
[38] "Bin: [380, 390)<br>Observed:<br>380; 381; 382; 383; 384; 385; 386; 387; 388; 389" 
[39] "Bin: [390, 400)<br>Observed:<br>390; 391; 392; 393; 394; 395; 396; 397; 398; 399" 
[40] "Bin: [400, 410)<br>Observed:<br>400; 401; 402; 403; 404; 405; 406; 407; 408; 409" 
[41] "Bin: [410, 420)<br>Observed:<br>410; 411; 412; 413; 414; 415; 416; 417; 418; 419" 
[42] "Bin: [420, 430)<br>Observed:<br>420; 421; 422; 423; 424; 425; 426; 427; 428; 429" 
[43] "Bin: [430, 440)<br>Observed:<br>430; 431; 432; 433; 434; 435; 436; 437; 438; 439" 
[44] "Bin: [440, 450)<br>Observed:<br>440; 441; 442; 443; 444; 445; 446; 447; 448; 449" 
[45] "Bin: [450, 460)<br>Observed:<br>450; 451; 452; 453; 454; 455; 456; 457; 458; 459" 
[46] "Bin: [460, 470)<br>Observed:<br>460; 461; 462; 463; 464; 465; 466; 467; 468; 469" 
[47] "Bin: [470, 480)<br>Observed:<br>470; 471; 472; 473; 474; 475; 476; 477; 478; 479" 
[48] "Bin: [480, 490)<br>Observed:<br>480; 481; 482; 483; 484; 485; 486; 487; 488; 489" 
[49] "Bin: [490, 500)<br>Observed:<br>490; 491; 492; 493; 494; 495; 496; 497; 498; 499" 
[50] "Bin: [500, 510)<br>Observed:<br>500; 501; 502; 503; 504; 505; 506; 507; 508; 509" 
[51] "Bin: [510, 520)<br>Observed:<br>510; 511; 512; 513; 514; 515; 516; 517; 518; 519" 
[52] "Bin: [520, 530)<br>Observed:<br>520; 521; 522; 523; 524; 525; 526; 527; 528; 529" 
[53] "Bin: [530, 540)<br>Observed:<br>530; 531; 532; 533; 534; 535; 536; 537; 538; 539" 
[54] "Bin: [540, 550)<br>Observed:<br>540; 541; 542; 543; 544; 545; 546; 547; 548; 549" 
[55] "Bin: [550, 560)<br>Observed:<br>550; 551; 552; 553; 554; 555; 556; 557; 558; 559" 
[56] "Bin: [560, 570)<br>Observed:<br>560; 561; 562; 563; 564; 565; 566; 567; 568; 569" 
[57] "Bin: [570, 580)<br>Observed:<br>570; 571; 572; 573; 574; 575; 576; 577; 578; 579" 
[58] "Bin: [580, 590)<br>Observed:<br>580; 581; 582; 583; 584; 585; 586; 587; 588; 589" 
[59] "Bin: [590, 600)<br>Observed:<br>590; 591; 592; 593; 594; 595; 596; 597; 598; 599" 
[60] "Bin: [600, 610)<br>Observed:<br>600; 601; 602; 603; 604; 605; 606; 607; 608; 609" 
[61] "Bin: [610, 620)<br>Observed:<br>610; 611; 612; 613; 614; 615; 616; 617; 618; 619" 
[62] "Bin: [620, 630)<br>Observed:<br>620; 621; 622; 623; 624; 625; 626; 627; 628; 629" 
[63] "Bin: [630, 640)<br>Observed:<br>630; 631; 632; 633; 634; 635; 636; 637; 638; 639" 
[64] "Bin: [640, 650)<br>Observed:<br>640; 641; 642; 643; 644; 645; 646; 647; 648; 649" 
[65] "Bin: [650, 660)<br>Observed:<br>650; 651; 652; 653; 654; 655; 656; 657; 658; 659" 
[66] "Bin: [660, 670)<br>Observed:<br>660; 661; 662; 663; 664; 665; 666; 667; 668; 669" 
[67] "Bin: [670, 680)<br>Observed:<br>670; 671; 672; 673; 674; 675; 676; 677; 678; 679" 
[68] "Bin: [680, 690)<br>Observed:<br>680; 681; 682; 683; 684; 685; 686; 687; 688; 689" 
[69] "Bin: [690, 700)<br>Observed:<br>690; 691; 692; 693; 694; 695; 696; 697; 698; 699" 
[70] "Bin: [700, 710)<br>Observed:<br>700; 701; 702; 703; 704; 705; 706; 707; 708; 709" 
[71] "Bin: [710, 720)<br>Observed:<br>710; 711; 712; 713; 714; 715; 716; 717; 718; 719" 
[72] "Bin: [720, 730)<br>Observed:<br>720; 721; 722; 723; 724; 725; 726; 727; 728; 729" 
[73] "Bin: [730, 740)<br>Observed:<br>730; 731; 732; 733; 734; 735; 736; 737; 738; 739" 
[74] "Bin: [740, 750)<br>Observed:<br>740; 741; 742; 743; 744; 745; 746; 747; 748; 749" 
[75] "Bin: [750, 760)<br>Observed:<br>750; 751; 752; 753; 754; 755; 756; 757; 758; 759" 
[76] "Bin: [760, 770)<br>Observed:<br>760; 761; 762; 763; 764; 765; 766; 767; 768; 769" 
[77] "Bin: [770, 780)<br>Observed:<br>770; 771; 772; 773; 774; 775; 776; 777; 778; 779" 
[78] "Bin: [780, 790)<br>Observed:<br>780; 781; 782; 783; 784; 785; 786; 787; 788; 789" 
[79] "Bin: [790, 800)<br>Observed:<br>790; 791; 792; 793; 794; 795; 796; 797; 798; 799" 
[80] "Bin: [800, 810)<br>Observed:<br>800; 801; 802; 803; 804; 805; 806; 807; 808; 809" 
[81] "Bin: [810, 820)<br>Observed:<br>810; 811; 812; 813; 814; 815; 816; 817; 818; 819" 
[82] "Bin: [820, 830)<br>Observed:<br>820; 821; 822; 823; 824; 825; 826; 827; 828; 829" 
[83] "Bin: [830, 840)<br>Observed:<br>830; 831; 832; 833; 834; 835; 836; 837; 838; 839" 
[84] "Bin: [840, 850)<br>Observed:<br>840; 841; 842; 843; 844; 845; 846; 847; 848; 849" 
[85] "Bin: [850, 860)<br>Observed:<br>850; 851; 852; 853; 854; 855; 856; 857; 858; 859" 
[86] "Bin: [860, 870)<br>Observed:<br>860; 861; 862; 863; 864; 865; 866; 867; 868; 869" 
[87] "Bin: [870, 880)<br>Observed:<br>870; 871; 872; 873; 874; 875; 876; 877; 878; 879" 
[88] "Bin: [880, 890)<br>Observed:<br>880; 881; 882; 883; 884; 885; 886; 887; 888; 889" 
[89] "Bin: [890, 900)<br>Observed:<br>890; 891; 892; 893; 894; 895; 896; 897; 898; 899" 
[90] "Bin: [900, 910)<br>Observed:<br>900; 901; 902; 903; 904; 905; 906; 907; 908; 909" 
[91] "Bin: [910, 920)<br>Observed:<br>910; 911; 912; 913; 914; 915; 916; 917; 918; 919" 
[92] "Bin: [920, 930)<br>Observed:<br>920; 921; 922; 923; 924; 925; 926; 927; 928; 929" 
[93] "Bin: [930, 940)<br>Observed:<br>930; 931; 932; 933; 934; 935; 936; 937; 938; 939" 
[94] "Bin: [940, 950)<br>Observed:<br>940; 941; 942; 943; 944; 945; 946; 947; 948; 949" 
[95] "Bin: [950, 960)<br>Observed:<br>950; 951; 952; 953; 954; 955; 956; 957; 958; 959" 
[96] "Bin: [960, 970)<br>Observed:<br>960; 961; 962; 963; 964; 965; 966; 967; 968; 969" 
[97] "Bin: [970, 980]<br>Observed:<br>970 - 980 (11 distinct)"                          
[98] "Q<sub>0.99</sub>:990<br>Bin: (980, 1000]<br>Observed:<br>981 - 1000 (20 distinct)"

$y
 [1] 0.95 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[16] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[31] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[46] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[61] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[76] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50
[91] 0.50 0.50 0.50 0.50 0.50 0.50 0.55 1.00

$roundedTo
[1] 10

> ## Not run: 
> ##D On a data.table d use ggplot2 to make spike histograms by country and sex groups
> ##D s <- d[, spikecomp(x, tresult='segments'), by=.(country, sex)]
> ##D ggplot(s) + geom_segment(aes(x=x, y=y1, xend=x, yend=y2, alpha=I(0.3))) +
> ##D    scale_y_continuous(breaks=NULL, labels=NULL) + ylab('') +
> ##D    facet_grid(country ~ sex)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("spower")
> ### * spower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spower
> ### Title: Simulate Power of 2-Sample Test for Survival under Complex
> ###   Conditions
> ### Aliases: spower print.spower Quantile2 print.Quantile2 plot.Quantile2
> ###   logrank Gompertz2 Lognorm2 Weibull2
> ### Keywords: htest survival
> 
> ### ** Examples
> 
> # Simulate a simple 2-arm clinical trial with exponential survival so
> # we can compare power simulations of logrank-Cox test with cpower()
> # Hazard ratio is constant and patients enter the study uniformly
> # with follow-up ranging from 1 to 3 years
> # Drop-in probability is constant at .1 and drop-out probability is
> # constant at .175.  Two-year survival of control patients in absence
> # of drop-in is .8 (mortality=.2).  Note that hazard rate is -log(.8)/2
> # Total sample size (both groups combined) is 1000
> # % mortality reduction by intervention (if no dropin or dropout) is 25
> # This corresponds to a hazard ratio of 0.7283 (computed by cpower)
> 
> 
> cpower(2, 1000, .2, 25, accrual=2, tmin=1, 
+        noncomp.c=10, noncomp.i=17.5)

Accrual duration: 2 y  Minimum follow-up: 1 y

Total sample size: 1000 

Alpha= 0.05 

2-year Mortalities
     Control Intervention 
        0.20         0.15 

Hazard Rates
     Control Intervention 
      0.1116       0.0813 

Probabilities of an Event During Study
     Control Intervention 
       0.198        0.149 

Expected Number of Events
     Control Intervention 
        99.2         74.5 

Hazard ratio: 0.728 

Drop-in rate (controls):10%
Non-adherence rate (intervention):17.5%
Effective hazard ratio with non-compliance: 0.795 
Standard deviation of log hazard ratio: 0.153 
Power 
0.323 
> 
> 
> ranfun <- Quantile2(function(x)exp(log(.8)/2*x),
+                     hratio=function(x)0.7283156,
+                     dropin=function(x).1,
+                     dropout=function(x).175)

Interval of time for evaluating functions:[0, 61.9 ]

> 
> 
> rcontrol <- function(n) ranfun(n, what='control')
> rinterv  <- function(n) ranfun(n, what='int')
> rcens    <- function(n) runif(n, 1, 3)
> 
> 
> set.seed(11)   # So can reproduce results
> spower(rcontrol, rinterv, rcens, nc=500, ni=500, 
+        test=logrank, nsim=50)  # normally use nsim=500 or 1000
10 20 30 40 50 
[1] 0.24
> 
> ## Not run: 
> ##D # Run the same simulation but fit the Cox model for each one to
> ##D # get log hazard ratios for the purpose of assessing the tightness
> ##D # confidence intervals that are likely to result
> ##D 
> ##D set.seed(11)
> ##D u <- spower(rcontrol, rinterv, rcens, nc=500, ni=500, 
> ##D        test=logrank, nsim=50, cox=TRUE)
> ##D u
> ##D v <- print(u)
> ##D v[c('MOElower','MOEupper','SE')]
> ## End(Not run)
> 
> # Simulate a 2-arm 5-year follow-up study for which the control group's
> # survival distribution is Weibull with 1-year survival of .95 and
> # 3-year survival of .7.  All subjects are followed at least one year,
> # and patients enter the study with linearly increasing probability  after that
> # Assume there is no chance of dropin for the first 6 months, then the
> # probability increases linearly up to .15 at 5 years
> # Assume there is a linearly increasing chance of dropout up to .3 at 5 years
> # Assume that the treatment has no effect for the first 9 months, then
> # it has a constant effect (hazard ratio of .75)
> 
> 
> # First find the right Weibull distribution for compliant control patients
> sc <- Weibull2(c(1,3), c(.95,.7))
> sc
function (times = NULL, alpha = 0.0512932943875506, gamma = 1.76519490623438) 
{
    exp(-alpha * (times^gamma))
}
<environment: 0x5781db7664d0>
> 
> 
> # Inverse cumulative distribution for case where all subjects are followed
> # at least a years and then between a and b years the density rises
> # as (time - a) ^ d is a + (b-a) * u ^ (1/(d+1))
> 
> 
> rcens <- function(n) 1 + (5-1) * (runif(n) ^ .5)
> # To check this, type hist(rcens(10000), nclass=50)
> 
> 
> # Put it all together
> 
> 
> f <- Quantile2(sc, 
+       hratio=function(x)ifelse(x<=.75, 1, .75),
+       dropin=function(x)ifelse(x<=.5, 0, .15*(x-.5)/(5-.5)),
+       dropout=function(x).3*x/5)

Interval of time for evaluating functions:[0, 16.1 ]

> 
> 
> par(mfrow=c(2,2))
> # par(mfrow=c(1,1)) to make legends fit
> plot(f, 'all', label.curves=list(keys='lines'))
Warning in largest.empty(x, y, xlim = xlim, ylim = ylim, width = z[1], height = z[2],  :
  no empty rectangle was large enough
No empty area large enough for automatic key positioning.  Specify keyloc or cex.
Width and height of key as computed by key(), in data units: 9.995 0.286 
Warning in largest.empty(x, y, xlim = xlim, ylim = ylim, width = z[1], height = z[2],  :
  no empty rectangle was large enough
No empty area large enough for automatic key positioning.  Specify keyloc or cex.
Width and height of key as computed by key(), in data units: 14.356  0.088 
> 
> 
> rcontrol <- function(n) f(n, 'control')
> rinterv  <- function(n) f(n, 'intervention')
> 
> 
> set.seed(211)
> spower(rcontrol, rinterv, rcens, nc=350, ni=350, 
+        test=logrank, nsim=50)  # normally nsim=500 or more
10 20 30 40 50 
[1] 0.5
> par(mfrow=c(1,1))
> 
> # Compose a censoring time generator function such that at 1 year
> # 5% of subjects are accrued, at 3 years 70% are accured, and at 10
> # years 100% are accrued.  The trial proceeds two years past the last
> # accrual for a total of 12 years of follow-up for the first subject.
> # Use linear interporation between these 3 points
> 
> rcens <- function(n)
+ {
+   times <- c(0,1,3,10)
+   accrued <- c(0,.05,.7,1)
+   # Compute inverse of accrued function at U(0,1) random variables
+   accrual.times <- approx(accrued, times, xout=runif(n))$y
+   censor.times <- 12 - accrual.times
+   censor.times
+ }
> 
> censor.times <- rcens(500)
> # hist(censor.times, nclass=20)
> accrual.times <- 12 - censor.times
> # Ecdf(accrual.times)
> # lines(c(0,1,3,10), c(0,.05,.7,1), col='red')
> # spower(..., rcens=rcens, ...)
> 
> ## Not run: 
> ##D # To define a control survival curve from a fitted survival curve
> ##D # with coordinates (tt, surv) with tt[1]=0, surv[1]=1:
> ##D 
> ##D Scontrol <- function(times, tt, surv) approx(tt, surv, xout=times)$y
> ##D tt <- 0:6
> ##D surv <- c(1, .9, .8, .75, .7, .65, .64)
> ##D formals(Scontrol) <- list(times=NULL, tt=tt, surv=surv)
> ##D 
> ##D # To use a mixture of two survival curves, with e.g. mixing proportions
> ##D # of .2 and .8, use the following as a guide:
> ##D #
> ##D # Scontrol <- function(times, t1, s1, t2, s2)
> ##D #  .2*approx(t1, s1, xout=times)$y + .8*approx(t2, s2, xout=times)$y
> ##D # t1 <- ...; s1 <- ...; t2 <- ...; s2 <- ...;
> ##D # formals(Scontrol) <- list(times=NULL, t1=t1, s1=s1, t2=t2, s2=s2)
> ##D 
> ##D # Check that spower can detect a situation where generated censoring times
> ##D # are later than all failure times
> ##D 
> ##D rcens <- function(n) runif(n, 0, 7)
> ##D f <- Quantile2(scontrol=Scontrol, hratio=function(x).8, tmax=6)
> ##D cont <- function(n) f(n, what='control')
> ##D int  <- function(n) f(n, what='intervention')
> ##D spower(rcontrol=cont, rinterv=int, rcens=rcens, nc=300, ni=300, nsim=20)
> ##D 
> ##D # Do an unstratified logrank test
> ##D library(survival)
> ##D # From SAS/STAT PROC LIFETEST manual, p. 1801
> ##D days <- c(179,256,262,256,255,224,225,287,319,264,237,156,270,257,242,
> ##D           157,249,180,226,268,378,355,319,256,171,325,325,217,255,256,
> ##D           291,323,253,206,206,237,211,229,234,209)
> ##D status <- c(1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,
> ##D             0,rep(1,19))
> ##D treatment <- c(rep(1,10), rep(2,10), rep(1,10), rep(2,10))
> ##D sex <- Cs(F,F,M,F,M,F,F,M,M,M,F,F,M,M,M,F,M,F,F,M,
> ##D           M,M,M,M,F,M,M,F,F,F,M,M,M,F,F,M,F,F,F,F)
> ##D data.frame(days, status, treatment, sex)
> ##D table(treatment, status)
> ##D logrank(Surv(days, status), treatment)  # agrees with p. 1807
> ##D # For stratified tests the picture is puzzling.
> ##D # survdiff(Surv(days,status) ~ treatment + strata(sex))$chisq
> ##D # is 7.246562, which does not agree with SAS (7.1609)
> ##D # But summary(coxph(Surv(days,status) ~ treatment + strata(sex)))
> ##D # yields 7.16 whereas summary(coxph(Surv(days,status) ~ treatment))
> ##D # yields 5.21 as the score test, not agreeing with SAS or logrank() (5.6485)
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("spss.get")
> ### * spss.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spss.get
> ### Title: Enhanced Importing of SPSS Files
> ### Aliases: spss.get
> ### Keywords: interface manip file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D w <- spss.get('/tmp/my.sav', datevars=c('birthdate','deathdate'))
> ##D   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("src")
> ### * src
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: src
> ### Title: Source a File from the Current Working Directory
> ### Aliases: src
> ### Keywords: file programming utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D src(myfile)   # source("myfile.s")
> ##D src()         # re-source myfile.s
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("stat_plsmo")
> ### * stat_plsmo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: stat_plsmo
> ### Title: Add a lowess smoother without counfidence bands.
> ### Aliases: stat_plsmo
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("stata.get")
> ### * stata.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: stata.get
> ### Title: Enhanced Importing of STATA Files
> ### Aliases: stata.get
> ### Keywords: interface manip file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D w <- stata.get('/tmp/my.dta')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("string.bounding.box")
> ### * string.bounding.box
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: string.bounding.box
> ### Title: Determine Dimensions of Strings
> ### Aliases: string.bounding.box
> ### Keywords: print
> 
> ### ** Examples
> 
> a <- c("this is a single line string", "This is a\nmulti-line string")
> stringDims(a)
$height
[1] 1 2

$width
[1] 28 17

> 
> 
> 
> cleanEx()
> nameEx("string.break.line")
> ### * string.break.line
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: string.break.line
> ### Title: Break a String into Many Lines at Newlines
> ### Aliases: string.break.line
> ### Keywords: print character
> 
> ### ** Examples
> 
> a <- c('', 'this is a single line string',
+        'This is a\nmulti-line string.')
> 
> b <- string.break.line(a)
> 
> 
> 
> cleanEx()
> nameEx("stringDims")
> ### * stringDims
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: stringDims
> ### Title: String Dimentions
> ### Aliases: stringDims
> ### Keywords: print
> 
> ### ** Examples
> 
> a <- c("this is a single line string", "This is a\nmulty line string")
> stringDims(a)
$height
[1] 1 2

$width
[1] 28 17

> 
> 
> 
> cleanEx()
> nameEx("subplot")
> ### * subplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: subplot
> ### Title: Embed a new plot within an existing plot
> ### Aliases: subplot
> ### Keywords: aplot dplot
> 
> ### ** Examples
> 
> # make an original plot
> plot( 11:20, sample(51:60) )
> 
> # add some histograms
> 
> subplot( hist(rnorm(100)), 15, 55)
> subplot( hist(runif(100),main='',xlab='',ylab=''), 11, 51, hadj=0, vadj=0)
> subplot( hist(rexp(100, 1/3)), 20, 60, hadj=1, vadj=1, size=c(0.5,2) )
> subplot( hist(rt(100,3)), c(12,16), c(57,59), pars=list(lwd=3,ask=FALSE) )
> 
> tmp <- rnorm(25)
> qqnorm(tmp)
> qqline(tmp)
> tmp2 <- subplot( hist(tmp,xlab='',ylab='',main=''), 
+ 		cnvrt.coords(0.1,0.9,'plt')$usr, vadj=1, hadj=0 )
> abline(v=0, col='red') # wrong way to add a reference line to histogram
> 
> # right way to add a reference line to histogram
> op <- par(no.readonly=TRUE)
> par(tmp2)
> abline(v=0, col='green')
> par(op)
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("summarize")
> ### * summarize
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summarize
> ### Title: Summarize Scalars or Matrices by Cross-Classification
> ### Aliases: summarize asNumericMatrix matrix2dataFrame
> ### Keywords: category manip multivariate
> 
> ### ** Examples
> 
> ## Not run: 
> ##D s <- summarize(ap>1, llist(size=cut2(sz, g=4), bone), mean,
> ##D                stat.name='Proportion')
> ##D dotplot(Proportion ~ size | bone, data=s7)
> ## End(Not run)
> 
> set.seed(1)
> temperature <- rnorm(300, 70, 10)
> month <- sample(1:12, 300, TRUE)
> year  <- sample(2000:2001, 300, TRUE)
> g <- function(x)c(Mean=mean(x,na.rm=TRUE),Median=median(x,na.rm=TRUE))
> summarize(temperature, month, g)
   month temperature Median
1      1        68.6   69.4
5      2        69.0   68.6
6      3        68.9   68.1
7      4        73.3   73.9
8      5        69.5   71.8
9      6        69.6   71.6
10     7        72.4   69.4
11     8        72.0   69.5
12     9        69.5   68.3
2     10        71.5   67.2
3     11        70.7   70.7
4     12        70.1   70.0
> mApply(temperature, month, g)
   Mean Median
1  68.6   69.4
2  69.0   68.6
3  68.9   68.1
4  73.3   73.9
5  69.5   71.8
6  69.6   71.6
7  72.4   69.4
8  72.0   69.5
9  69.5   68.3
10 71.5   67.2
11 70.7   70.7
12 70.1   70.0
> 
> mApply(temperature, month, mean, na.rm=TRUE)
   1    2    3    4    5    6    7    8    9   10   11   12 
68.6 69.0 68.9 73.3 69.5 69.6 72.4 72.0 69.5 71.5 70.7 70.1 
> w <- summarize(temperature, month, mean, na.rm=TRUE)
> library(lattice)
> xyplot(temperature ~ month, data=w) # plot mean temperature by month
> 
> w <- summarize(temperature, llist(year,month), 
+                quantile, probs=c(.5,.25,.75), na.rm=TRUE, type='matrix')
> xYplot(Cbind(temperature[,1],temperature[,-1]) ~ month | year, data=w)
> mApply(temperature, llist(year,month),
+        quantile, probs=c(.5,.25,.75), na.rm=TRUE)
, ,  = 50%

     year
month 2000 2001
   1  69.7 66.1
   2  65.3 72.1
   3  67.4 68.1
   4  72.6 75.6
   5  69.4 73.8
   6  74.2 69.6
   7  69.3 69.6
   8  70.8 68.6
   9  67.4 69.5
   10 67.5 66.8
   11 70.8 70.7
   12 69.2 71.3

, ,  = 25%

     year
month 2000 2001
   1  56.6 63.6
   2  62.6 65.3
   3  61.5 64.4
   4  68.5 67.4
   5  61.5 64.1
   6  61.7 64.2
   7  65.9 66.8
   8  66.2 66.2
   9  62.9 63.0
   10 63.9 64.0
   11 67.5 67.6
   12 63.6 68.6

, ,  = 75%

     year
month 2000 2001
   1  74.6 74.5
   2  72.3 74.2
   3  73.6 77.4
   4  76.9 78.8
   5  73.9 79.1
   6  77.1 75.6
   7  84.3 73.4
   8  80.4 73.5
   9  77.4 78.1
   10 77.4 77.1
   11 72.4 80.2
   12 71.3 75.7

> 
> # Compute the median and outer quartiles.  The outer quartiles are
> # displayed using "error bars"
> set.seed(111)
> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> attach(dfr)
The following objects are masked _by_ .GlobalEnv:

    month, year

> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5)
> s
   month year     y Lower Upper
1      1 2000  9.03  8.92  9.35
2      1 2001 10.73  9.97 11.15
9      2 2000  8.89  8.69  9.17
10     2 2001  9.48  9.13  9.83
11     3 2000  7.15  6.81  8.02
12     3 2001  8.76  8.30  9.39
13     4 2000  6.29  5.77  7.02
14     4 2001  7.21  6.93  7.83
15     5 2000  5.30  5.06  5.75
16     5 2001  6.58  5.71  6.79
17     6 2000  4.65  4.42  4.81
18     6 2001  5.56  5.04  5.99
19     7 2000  5.02  4.28  5.19
20     7 2001  5.02  4.64  5.33
21     8 2000  5.66  5.54  5.80
22     8 2001  6.53  6.21  7.13
23     9 2000  6.73  6.04  7.16
24     9 2001  7.96  7.31  8.37
3     10 2000  7.67  7.12  8.19
4     10 2001  8.60  7.96  8.82
5     11 2000  9.00  8.65  9.27
6     11 2001  9.30  8.84  9.73
7     12 2000  9.03  8.65  9.60
8     12 2001 10.69 10.27 11.11
> mApply(y, llist(month,year), smedian.hilow, conf.int=.5)
, ,  = Median

      month
year       1    2    3    4    5    6    7    8    9   10  11    12
  2000  9.03 8.89 7.15 6.29 5.30 4.65 5.02 5.66 6.73 7.67 9.0  9.03
  2001 10.73 9.48 8.76 7.21 6.58 5.56 5.02 6.53 7.96 8.60 9.3 10.69

, ,  = Lower

      month
year      1    2    3    4    5    6    7    8    9   10   11    12
  2000 8.92 8.69 6.81 5.77 5.06 4.42 4.28 5.54 6.04 7.12 8.65  8.65
  2001 9.97 9.13 8.30 6.93 5.71 5.04 4.64 6.21 7.31 7.96 8.84 10.27

, ,  = Upper

      month
year       1    2    3    4    5    6    7    8    9   10   11   12
  2000  9.35 9.17 8.02 7.02 5.75 4.81 5.19 5.80 7.16 8.19 9.27  9.6
  2001 11.15 9.83 9.39 7.83 6.79 5.99 5.33 7.13 8.37 8.82 9.73 11.1

> 
> xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s, 
+        keys='lines', method='alt')
> # Can also do:
> s <- summarize(y, llist(month,year), quantile, probs=c(.5,.25,.75),
+                stat.name=c('y','Q1','Q3'))
> xYplot(Cbind(y, Q1, Q3) ~ month, groups=year, data=s, keys='lines')
> # To display means and bootstrapped nonparametric confidence intervals
> # use for example:
> s <- summarize(y, llist(month,year), smean.cl.boot)
> xYplot(Cbind(y, Lower, Upper) ~ month | year, data=s)
> 
> # For each subject use the trapezoidal rule to compute the area under
> # the (time,response) curve using the Hmisc trap.rule function
> x <- cbind(time=c(1,2,4,7, 1,3,5,10),response=c(1,3,2,4, 1,3,2,4))
> subject <- c(rep(1,4),rep(2,4))
> trap.rule(x[1:4,1],x[1:4,2])
[1] 16
> summarize(x, subject, function(y) trap.rule(y[,1],y[,2]))
  subject  x
1       1 16
2       2 24
> 
> ## Not run: 
> ##D # Another approach would be to properly re-shape the mm array below
> ##D # This assumes no missing cells.  There are many other approaches.
> ##D # mApply will do this well while allowing for missing cells.
> ##D m <- tapply(y, list(year,month), quantile, probs=c(.25,.5,.75))
> ##D mm <- array(unlist(m), dim=c(3,2,12), 
> ##D             dimnames=list(c('lower','median','upper'),c('1997','1998'),
> ##D                           as.character(1:12)))
> ##D # aggregate will help but it only allows you to compute one quantile
> ##D # at a time; see also the Hmisc mApply function
> ##D dframe <- aggregate(y, list(Year=year,Month=month), quantile, probs=.5)
> ##D 
> ##D # Compute expected life length by race assuming an exponential
> ##D # distribution - can also use summarize
> ##D g <- function(y) { # computations for one race group
> ##D   futime <- y[,1]; event <- y[,2]
> ##D   sum(futime)/sum(event)  # assume event=1 for death, 0=alive
> ##D }
> ##D mApply(cbind(followup.time, death), race, g)
> ##D 
> ##D # To run mApply on a data frame:
> ##D xn <- asNumericMatrix(x)
> ##D m <- mApply(xn, race, h)
> ##D # Here assume h is a function that returns a matrix similar to x
> ##D matrix2dataFrame(m)
> ##D 
> ##D 
> ##D # Get stratified weighted means
> ##D g <- function(y) wtd.mean(y[,1],y[,2])
> ##D summarize(cbind(y, wts), llist(sex,race), g, stat.name='y')
> ##D mApply(cbind(y,wts), llist(sex,race), g)
> ##D 
> ##D # Compare speed of mApply vs. by for computing 
> ##D d <- data.frame(sex=sample(c('female','male'),100000,TRUE),
> ##D                 country=sample(letters,100000,TRUE),
> ##D                 y1=runif(100000), y2=runif(100000))
> ##D g <- function(x) {
> ##D   y <- c(median(x[,'y1']-x[,'y2']),
> ##D          med.sum =median(x[,'y1']+x[,'y2']))
> ##D   names(y) <- c('med.diff','med.sum')
> ##D   y
> ##D }
> ##D 
> ##D system.time(by(d, llist(sex=d$sex,country=d$country), g))
> ##D system.time({
> ##D              x <- asNumericMatrix(d)
> ##D              a <- subsAttr(d)
> ##D              m <- mApply(x, llist(sex=d$sex,country=d$country), g)
> ##D             })
> ##D system.time({
> ##D              x <- asNumericMatrix(d)
> ##D              summarize(x, llist(sex=d$sex, country=d$country), g)
> ##D             })
> ##D 
> ##D # An example where each subject has one record per diagnosis but sex of
> ##D # subject is duplicated for all the rows a subject has.  Get the cross-
> ##D # classified frequencies of diagnosis (dx) by sex and plot the results
> ##D # with a dot plot
> ##D 
> ##D count <- rep(1,length(dx))
> ##D d <- summarize(count, llist(dx,sex), sum)
> ##D Dotplot(dx ~ count | sex, data=d)
> ## End(Not run)
> d <- list(x=1:10, a=factor(rep(c('a','b'), 5)),
+           b=structure(letters[1:10], label='label for b'),
+           d=c(rep(TRUE,9), FALSE), f=pi*(1 : 10))
> x <- asNumericMatrix(d)
> attr(x, 'origAttributes')
$x
$x$.type.
[1] "integer"


$a
$a$levels
[1] "a" "b"

$a$class
[1] "factor"

$a$.type.
[1] "integer"


$b
$b$label
[1] "label for b"

$b$levels
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"

$b$class
[1] "factor"

$b$.type.
[1] "character"


$d
$d$.type.
[1] "logical"


$f
$f$.type.
[1] "double"


> matrix2dataFrame(x)
    x a b     d     f
1   1 a a  TRUE  3.14
2   2 b b  TRUE  6.28
3   3 a c  TRUE  9.42
4   4 b d  TRUE 12.57
5   5 a e  TRUE 15.71
6   6 b f  TRUE 18.85
7   7 a g  TRUE 21.99
8   8 b h  TRUE 25.13
9   9 a i  TRUE 28.27
10 10 b j FALSE 31.42
> 
> detach('dfr')
> 
> # Run summarize on a matrix to get column means
> x <- c(1:19,NA)
> y <- 101:120
> z <- cbind(x, y)
> g <- c(rep(1, 10), rep(2, 10))
> summarize(z, g, colMeans, na.rm=TRUE, stat.name='x')
  g    x   y
1 1  5.5 106
2 2 15.0 116
> # Also works on an all numeric data frame
> summarize(as.data.frame(z), g, colMeans, na.rm=TRUE, stat.name='x')
  g    x   y
1 1  5.5 106
2 2 15.0 116
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’

> nameEx("summary.formula")
> ### * summary.formula
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.formula
> ### Title: Summarize Data for Making Tables and Plots
> ### Aliases: summary.formula stratify print.summary.formula.response
> ###   plot.summary.formula.response latex.summary.formula.response
> ###   print.summary.formula.reverse plot.summary.formula.reverse
> ###   latex.summary.formula.reverse [.summary.formula.response
> ###   print.summary.formula.cross latex.summary.formula.cross
> ###   formula.summary.formula.cross na.retain cumcategory conTestkw
> ###   catTestchisq ordTestpo
> ### Keywords: category interface hplot manip
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(173)
> sex <- factor(sample(c("m","f"), 500, rep=TRUE))
> age <- rnorm(500, 50, 5)
> treatment <- factor(sample(c("Drug","Placebo"), 500, rep=TRUE))
> 
> # Generate a 3-choice variable; each of 3 variables has 5 possible levels
> symp <- c('Headache','Stomach Ache','Hangnail',
+           'Muscle Ache','Depressed')
> symptom1 <- sample(symp, 500,TRUE)
> symptom2 <- sample(symp, 500,TRUE)
> symptom3 <- sample(symp, 500,TRUE)
> Symptoms <- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
> table(Symptoms)
Symptoms
    1   1;2 1;2;3 1;2;4 1;2;5   1;3 1;3;4 1;3;5   1;4 1;4;5   1;5     2   2;3 
    0     0     0     0     0     0     0     0     0     0     0     0     0 
2;3;4 2;3;5   2;4 2;4;5   2;5     3   3;4 3;4;5   3;5     4   4;5     5 
    0     0     0     0     0     0     0     0     0     0     0     0 
> 
> # Note: In this example, some subjects have the same symptom checked
> # multiple times; in practice these redundant selections would be NAs
> # mChoice will ignore these redundant selections
> 
> #Frequency table sex*treatment, sex*Symptoms
> summary(sex ~ treatment + Symptoms, fun=table)
sex      N= 500  

+---------+------------+---+---+---+
|         |            |  N|  f|  m|
+---------+------------+---+---+---+
|treatment|        Drug|246|121|125|
|         |     Placebo|254|120|134|
+---------+------------+---+---+---+
| Symptoms| Muscle Ache|229|108|121|
|         |Stomach Ache|248|121|127|
|         |    Hangnail|262|125|137|
|         |    Headache|253|119|134|
|         |   Depressed|245|127|118|
+---------+------------+---+---+---+
|  Overall|            |500|241|259|
+---------+------------+---+---+---+
> # could also do summary(sex ~ treatment +
> #  mChoice(symptom1,symptom2,symptom3), fun=table)
> 
> 
> #Compute mean age, separately by 3 variables
> summary(age ~ sex + treatment + Symptoms)
age      N= 500  

+---------+------------+---+----+
|         |            |  N| age|
+---------+------------+---+----+
|      sex|           f|241|49.7|
|         |           m|259|50.3|
+---------+------------+---+----+
|treatment|        Drug|246|50.0|
|         |     Placebo|254|49.9|
+---------+------------+---+----+
| Symptoms| Muscle Ache|229|50.6|
|         |Stomach Ache|248|50.0|
|         |    Hangnail|262|49.8|
|         |    Headache|253|50.0|
|         |   Depressed|245|49.9|
+---------+------------+---+----+
|  Overall|            |500|50.0|
+---------+------------+---+----+
> 
> 
> f <- summary(treatment ~ age + sex + Symptoms, method="reverse", test=TRUE)
> f


Descriptive Statistics by treatment

+------------------------------+----------------------+----------------------+------------------------------+
|                              |Drug                  |Placebo               |  Test                        |
|                              |(N=246)               |(N=254)               |Statistic                     |
+------------------------------+----------------------+----------------------+------------------------------+
|age                           |        46.9/50.1/53.3|        46.5/49.9/53.3|   F=0.13 d.f.=1,498 P=0.715  |
+------------------------------+----------------------+----------------------+------------------------------+
|sex : m                       |          51%  (125)  |          53%  (134)  |Chi-square=0.19 d.f.=1 P=0.664|
+------------------------------+----------------------+----------------------+------------------------------+
|Primary Symptoms : Muscle Ache|          48%  (118)  |          44%  (111)  |Chi-square=0.92 d.f.=1 P=0.338|
+------------------------------+----------------------+----------------------+------------------------------+
|    Stomach Ache              |          47%  (116)  |          52%  (132)  |Chi-square=1.16 d.f.=1 P=0.282|
+------------------------------+----------------------+----------------------+------------------------------+
|    Hangnail                  |          55%  (135)  |          50%  (127)  |Chi-square=1.19 d.f.=1 P=0.275|
+------------------------------+----------------------+----------------------+------------------------------+
|    Headache                  |          49%  (121)  |          52%  (132)  |Chi-square=0.39 d.f.=1 P=0.534|
+------------------------------+----------------------+----------------------+------------------------------+
|    Depressed                 |          48%  (118)  |          50%  (127)  |Chi-square=0.21 d.f.=1 P=0.649|
+------------------------------+----------------------+----------------------+------------------------------+
> # trio of numbers represent 25th, 50th, 75th percentile
> print(f, long=TRUE)


Descriptive Statistics by treatment

+----------------+----------------------+----------------------+------------------------------+
|                |Drug                  |Placebo               |  Test                        |
|                |(N=246)               |(N=254)               |Statistic                     |
+----------------+----------------------+----------------------+------------------------------+
|age             |        46.9/50.1/53.3|        46.5/49.9/53.3|   F=0.13 d.f.=1,498 P=0.715  |
+----------------+----------------------+----------------------+------------------------------+
|sex             |                      |                      |Chi-square=0.19 d.f.=1 P=0.664|
+----------------+----------------------+----------------------+------------------------------+
|    m           |          51%  (125)  |          53%  (134)  |                              |
+----------------+----------------------+----------------------+------------------------------+
|Primary Symptoms|                      |                      |                              |
+----------------+----------------------+----------------------+------------------------------+
|    Muscle Ache |          48%  (118)  |          44%  (111)  |Chi-square=0.92 d.f.=1 P=0.338|
+----------------+----------------------+----------------------+------------------------------+
|    Stomach Ache|          47%  (116)  |          52%  (132)  |Chi-square=1.16 d.f.=1 P=0.282|
+----------------+----------------------+----------------------+------------------------------+
|    Hangnail    |          55%  (135)  |          50%  (127)  |Chi-square=1.19 d.f.=1 P=0.275|
+----------------+----------------------+----------------------+------------------------------+
|    Headache    |          49%  (121)  |          52%  (132)  |Chi-square=0.39 d.f.=1 P=0.534|
+----------------+----------------------+----------------------+------------------------------+
|    Depressed   |          48%  (118)  |          50%  (127)  |Chi-square=0.21 d.f.=1 P=0.649|
+----------------+----------------------+----------------------+------------------------------+
> plot(f)
> plot(f, conType='bp', prtest='P')
> bpplt()    # annotated example showing layout of bp plot
> 
> #Compute predicted probability from a logistic regression model
> #For different stratifications compute receiver operating
> #characteristic curve areas (C-indexes)
> predicted <- plogis(.4*(sex=="m")+.15*(age-50))
> positive.diagnosis <- ifelse(runif(500)<=predicted, 1, 0)
> roc <- function(z) {
+    x <- z[,1];
+    y <- z[,2];
+    n <- length(x);
+    if(n<2)return(c(ROC=NA));
+    n1 <- sum(y==1);
+    c(ROC= (mean(rank(x)[y==1])-(n1+1)/2)/(n-n1) );
+  }
> y <- cbind(predicted, positive.diagnosis)
> options(digits=2)
> summary(y ~ age + sex, fun=roc)
y      N= 500  

+-------+-----------+---+----+
|       |           |  N| ROC|
+-------+-----------+---+----+
|    age|[36.8,46.7)|125|0.60|
|       |[46.7,50.0)|125|0.55|
|       |[50.0,53.3)|125|0.62|
|       |[53.3,67.5]|125|0.62|
+-------+-----------+---+----+
|    sex|          f|241|0.72|
|       |          m|259|0.70|
+-------+-----------+---+----+
|Overall|           |500|0.72|
+-------+-----------+---+----+
> 
> 
> options(digits=3)
> summary(y ~ age + sex, fun=roc, method="cross")

 roc by age, sex 

+-+
|N|
|y|
+-+
+-----------+-----+-----+-----+
|    age    |  f  |  m  | ALL |
+-----------+-----+-----+-----+
|[36.8,46.7)|   65|   60|  125|
|           |0.526|0.626|0.598|
+-----------+-----+-----+-----+
|[46.7,50.0)|   65|   60|  125|
|           |0.588|0.467|0.554|
+-----------+-----+-----+-----+
|[50.0,53.3)|   53|   72|  125|
|           |0.567|0.652|0.622|
+-----------+-----+-----+-----+
|[53.3,67.5]|   58|   67|  125|
|           |0.646|0.622|0.618|
+-----------+-----+-----+-----+
|        ALL|  241|  259|  500|
|           |0.718|0.704|0.716|
+-----------+-----+-----+-----+
> 
> #Use stratify() to produce a table in which time intervals go down the
> #page and going across 3 continuous variables are summarized using
> #quartiles, and are stratified by two treatments
> 
> set.seed(1)
> d <- expand.grid(visit=1:5, treat=c('A','B'), reps=1:100)
> d$sysbp <- rnorm(100*5*2, 120, 10)
> label(d$sysbp) <- 'Systolic BP'
> d$diasbp <- rnorm(100*5*2, 80,  7)
> d$diasbp[1] <- NA
> d$age    <- rnorm(100*5*2, 50, 12)
> g <- function(y) {
+   N <- apply(y, 2, function(w) sum(!is.na(w)))
+   h <- function(x) {
+     qu <- quantile(x, c(.25,.5,.75), na.rm=TRUE)
+     names(qu) <- c('Q1','Q2','Q3')
+     c(N=sum(!is.na(x)), qu)
+ }
+   w <- as.vector(apply(y, 2, h))
+   names(w) <- as.vector( outer(c('N','Q1','Q2','Q3'), dimnames(y)[[2]],
+                                 function(x,y) paste(y,x)))
+   w
+ }
> #Use na.rm=FALSE to count NAs separately by column
> s <- summary(cbind(age,sysbp,diasbp) ~ visit + stratify(treat),
+              na.rm=FALSE, fun=g, data=d)
> #The result is very wide.  Re-do, putting treatment vertically
> x <- with(d, factor(paste('Visit', visit, treat)))
> summary(cbind(age,sysbp,diasbp) ~ x, na.rm=FALSE, fun=g, data=d)
cbind(age, sysbp, diasbp)      N= 1000  

+-------+---------+----+-----+------+------+------+-------+--------+--------+--------+--------+---------+---------+---------+
|       |         |   N|age N|age Q1|age Q2|age Q3|sysbp N|sysbp Q1|sysbp Q2|sysbp Q3|diasbp N|diasbp Q1|diasbp Q2|diasbp Q3|
+-------+---------+----+-----+------+------+------+-------+--------+--------+--------+--------+---------+---------+---------+
|      x|Visit 1 A| 100|  100|  43.6|  49.8|  59.5|    100|     113|     118|     127|      99|     75.6|     79.3|     83.4|
|       |Visit 1 B| 100|  100|  42.1|  49.1|  56.5|    100|     111|     119|     127|     100|     75.8|     79.6|     84.1|
|       |Visit 2 A| 100|  100|  41.6|  48.1|  58.3|    100|     116|     122|     129|     100|     76.7|     79.9|     85.1|
|       |Visit 2 B| 100|  100|  43.3|  49.9|  58.0|    100|     113|     119|     127|     100|     73.4|     79.2|     86.0|
|       |Visit 3 A| 100|  100|  43.8|  51.0|  61.8|    100|     113|     120|     127|     100|     73.9|     78.9|     83.5|
|       |Visit 3 B| 100|  100|  42.3|  50.3|  58.9|    100|     115|     120|     127|     100|     75.8|     79.5|     85.3|
|       |Visit 4 A| 100|  100|  44.9|  50.1|  58.6|    100|     111|     117|     122|     100|     74.6|     81.4|     86.1|
|       |Visit 4 B| 100|  100|  38.6|  47.2|  56.2|    100|     113|     120|     126|     100|     74.3|     79.5|     84.7|
|       |Visit 5 A| 100|  100|  46.2|  52.1|  57.6|    100|     114|     120|     127|     100|     74.8|     80.6|     85.3|
|       |Visit 5 B| 100|  100|  39.2|  49.1|  59.5|    100|     115|     120|     127|     100|     76.9|     80.0|     84.5|
+-------+---------+----+-----+------+------+------+-------+--------+--------+--------+--------+---------+---------+---------+
|Overall|         |1000| 1000|  42.4|  49.9|  58.6|   1000|     113|     120|     127|     999|     75.1|     79.8|     85.2|
+-------+---------+----+-----+------+------+------+-------+--------+--------+--------+--------+---------+---------+---------+
> 
> #Compose LaTeX code directly
> g <- function(y) {
+   h <- function(x) {
+     qu <- format(round(quantile(x, c(.25,.5,.75), na.rm=TRUE),1),nsmall=1)
+     paste('{\\scriptsize(',sum(!is.na(x)),
+           ')} \\hfill{\\scriptsize ', qu[1], '} \\textbf{', qu[2],
+           '} {\\scriptsize ', qu[3],'}', sep='')
+   }
+   apply(y, 2, h)
+ }
> s <- summary(cbind(age,sysbp,diasbp) ~ visit + stratify(treat),
+              na.rm=FALSE, fun=g, data=d)
> # latex(s, prn=FALSE)
> ## need option in latex to not print n
> #Put treatment vertically
> s <- summary(cbind(age,sysbp,diasbp) ~ x, fun=g, data=d, na.rm=FALSE)
> # latex(s, prn=FALSE)
> 
> #Plot estimated mean life length (assuming an exponential distribution) 
> #separately by levels of 4 other variables.  Repeat the analysis
> #by levels of a stratification variable, drug.  Automatically break
> #continuous variables into tertiles.
> #We are using the default, method='response'
> ## Not run: 
> ##D life.expect <- function(y) c(Years=sum(y[,1])/sum(y[,2]))
> ##D attach(pbc)
> ##D require(survival)
> ##D S <- Surv(follow.up.time, death)
> ##D s2 <- summary(S ~ age + albumin + ascites + edema + stratify(drug),
> ##D                          fun=life.expect, g=3)
> ##D 
> ##D 
> ##D #Note: You can summarize other response variables using the same 
> ##D #independent variables using e.g. update(s2, response~.), or you 
> ##D #can change the list of independent variables using e.g. 
> ##D #update(s2, response ~.- ascites) or update(s2, .~.-ascites)
> ##D #You can also print, typeset, or plot subsets of s2, e.g.
> ##D #plot(s2[c('age','albumin'),]) or plot(s2[1:2,])
> ##D 
> ##D 
> ##D s2    # invokes print.summary.formula.response
> ##D 
> ##D 
> ##D #Plot results as a separate dot chart for each of the 3 strata levels
> ##D par(mfrow=c(2,2))
> ##D plot(s2, cex.labels=.6, xlim=c(0,40), superposeStrata=FALSE)
> ##D 
> ##D 
> ##D #Typeset table, creating s2.tex
> ##D w <- latex(s2, cdec=1)
> ##D #Typeset table but just print LaTeX code
> ##D latex(s2, file="")    # useful for Sweave
> ##D 
> ##D 
> ##D #Take control of groups used for age.  Compute 3 quartiles for
> ##D #both cholesterol and bilirubin (excluding observations that are missing
> ##D #on EITHER ONE)
> ##D 
> ##D 
> ##D age.groups <- cut2(age, c(45,60))
> ##D g <- function(y) apply(y, 2, quantile, c(.25,.5,.75))
> ##D y <- cbind(Chol=chol,Bili=bili)
> ##D label(y) <- 'Cholesterol and Bilirubin'
> ##D #You can give new column names that are not legal S names
> ##D #by enclosing them in quotes, e.g. 'Chol (mg/dl)'=chol
> ##D 
> ##D 
> ##D s <- summary(y ~ age.groups + ascites, fun=g)
> ##D 
> ##D 
> ##D par(mfrow=c(1,2), oma=c(3,0,3,0))   # allow outer margins for overall
> ##D for(ivar in 1:2) {                  # title 
> ##D   isub <- (1:3)+(ivar-1)*3          # *3=number of quantiles/var.
> ##D   plot(s3, which=isub, main='', 
> ##D        xlab=c('Cholesterol','Bilirubin')[ivar],
> ##D        pch=c(91,16,93))            # [, closed circle, ]
> ##D   }
> ##D mtext(paste('Quartiles of', label(y)), adj=.5, outer=TRUE, cex=1.75)  
> ##D #Overall (outer) title
> ##D 
> ##D 
> ##D prlatex(latex(s3, trios=TRUE)) 
> ##D # trios -> collapse 3 quartiles
> ##D 
> ##D 
> ##D #Summarize only bilirubin, but do it with two statistics:
> ##D #the mean and the median.  Make separate tables for the two randomized
> ##D #groups and make plots for the active arm.
> ##D 
> ##D 
> ##D g <- function(y) c(Mean=mean(y), Median=median(y))
> ##D 
> ##D 
> ##D for(sub in c("D-penicillamine", "placebo")) {
> ##D   ss <- summary(bili ~ age.groups + ascites + chol, fun=g,
> ##D                 subset=drug==sub)
> ##D   cat('\n',sub,'\n\n')
> ##D   print(ss)
> ##D 
> ##D 
> ##D   if(sub=='D-penicillamine') {
> ##D     par(mfrow=c(1,1))
> ##D     plot(s4, which=1:2, dotfont=c(1,-1), subtitles=FALSE, main='')
> ##D     #1=mean, 2=median     -1 font = open circle
> ##D     title(sub='Closed circle: mean;  Open circle: median', adj=0)
> ##D     title(sub=sub, adj=1)
> ##D   }
> ##D 
> ##D 
> ##D   w <- latex(ss, append=TRUE, fi='my.tex', 
> ##D              label=if(sub=='placebo') 's4b' else 's4a',
> ##D              caption=paste(label(bili),' {\\em (',sub,')}', sep=''))
> ##D   #Note symbolic labels for tables for two subsets: s4a, s4b
> ##D   prlatex(w)
> ##D }
> ##D 
> ##D 
> ##D #Now consider examples in 'reverse' format, where the lone dependent
> ##D #variable tells the summary function how to stratify all the 
> ##D #'independent' variables.  This is typically used to make tables 
> ##D #comparing baseline variables by treatment group, for example.
> ##D 
> ##D 
> ##D s5 <- summary(drug ~ bili + albumin + stage + protime + sex + 
> ##D                      age + spiders,
> ##D               method='reverse')
> ##D #To summarize all variables, use summary(drug ~., data=pbc)
> ##D #To summarize all variables with no stratification, use
> ##D #summary(~a+b+c) or summary(~.,data=\dots)
> ##D 
> ##D 
> ##D options(digits=1)
> ##D print(s5, npct='both')
> ##D #npct='both' : print both numerators and denominators
> ##D plot(s5, which='categorical')
> ##D Key(locator(1))  # draw legend at mouse click
> ##D par(oma=c(3,0,0,0))  # leave outer margin at bottom
> ##D plot(s5, which='continuous')
> ##D Key2()           # draw legend at lower left corner of plot
> ##D                  # oma= above makes this default key fit the page better
> ##D 
> ##D 
> ##D options(digits=3)
> ##D w <- latex(s5, npct='both', here=TRUE)     
> ##D # creates s5.tex
> ##D 
> ##D 
> ##D #Turn to a different dataset and do cross-classifications on possibly 
> ##D #more than one independent variable.  The summary function with 
> ##D #method='cross' produces a data frame containing the cross-
> ##D #classifications.  This data frame is suitable for multi-panel 
> ##D #trellis displays, although `summarize' works better for that.
> ##D 
> ##D 
> ##D attach(prostate)
> ##D size.quartile <- cut2(sz, g=4)
> ##D bone <- factor(bm,labels=c("no mets","bone mets"))
> ##D 
> ##D 
> ##D s7 <- summary(ap>1 ~ size.quartile + bone, method='cross')
> ##D #In this case, quartiles are the default so could have said sz + bone
> ##D 
> ##D 
> ##D options(digits=3)
> ##D print(s7, twoway=FALSE)
> ##D s7   # same as print(s7)
> ##D w <- latex(s7, here=TRUE)   # Make s7.tex
> ##D 
> ##D 
> ##D library(trellis,TRUE)
> ##D invisible(ps.options(reset=TRUE))
> ##D trellis.device(postscript, file='demo2.ps')
> ##D 
> ##D 
> ##D dotplot(S ~ size.quartile|bone, data=s7, #s7 is name of summary stats
> ##D                   xlab="Fraction ap>1", ylab="Quartile of Tumor Size")
> ##D #Can do this more quickly with summarize:
> ##D # s7 <- summarize(ap>1, llist(size=cut2(sz, g=4), bone), mean,
> ##D #                 stat.name='Proportion')
> ##D # dotplot(Proportion ~ size | bone, data=s7)
> ##D 
> ##D 
> ##D summary(age ~ stage, method='cross')
> ##D summary(age ~ stage, fun=quantile, method='cross')
> ##D summary(age ~ stage, fun=smean.sd, method='cross')
> ##D summary(age ~ stage, fun=smedian.hilow, method='cross')
> ##D summary(age ~ stage, fun=function(x) c(Mean=mean(x), Median=median(x)),
> ##D         method='cross')
> ##D #The next statements print real two-way tables
> ##D summary(cbind(age,ap) ~ stage + bone, 
> ##D         fun=function(y) apply(y, 2, quantile, c(.25,.75)),
> ##D         method='cross')
> ##D options(digits=2)
> ##D summary(log(ap) ~ sz + bone,
> ##D         fun=function(y) c(Mean=mean(y), quantile(y)),
> ##D         method='cross')
> ##D 
> ##D 
> ##D #Summarize an ordered categorical response by all of the needed
> ##D #cumulative proportions
> ##D summary(cumcategory(disease.severity) ~ age + sex)
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("summaryM")
> ### * summaryM
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryM
> ### Title: Summarize Mixed Data Types vs. Groups
> ### Aliases: summaryM print.summaryM plot.summaryM latex.summaryM
> ###   html.summaryM printsummaryM
> ### Keywords: category interface hplot manip
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(173)
> sex <- factor(sample(c("m","f"), 500, rep=TRUE))
> country <- factor(sample(c('US', 'Canada'), 500, rep=TRUE))
> age <- rnorm(500, 50, 5)
> sbp <- rnorm(500, 120, 12)
> label(sbp) <- 'Systolic BP'
> units(sbp) <- 'mmHg'
> treatment <- factor(sample(c("Drug","Placebo"), 500, rep=TRUE))
> treatment[1]
[1] Placebo
Levels: Drug Placebo
> sbp[1] <- NA
> 
> # Generate a 3-choice variable; each of 3 variables has 5 possible levels
> symp <- c('Headache','Stomach Ache','Hangnail',
+           'Muscle Ache','Depressed')
> symptom1 <- sample(symp, 500,TRUE)
> symptom2 <- sample(symp, 500,TRUE)
> symptom3 <- sample(symp, 500,TRUE)
> Symptoms <- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
> table(as.character(Symptoms))

                         Depressed              Depressed;Muscle Ache 
                                 5                                 24 
                          Hangnail                 Hangnail;Depressed 
                                 3                                 24 
    Hangnail;Depressed;Muscle Ache               Hangnail;Muscle Ache 
                                23                                 24 
             Hangnail;Stomach Ache    Hangnail;Stomach Ache;Depressed 
                                33                                 20 
 Hangnail;Stomach Ache;Muscle Ache                           Headache 
                                33                                  3 
                Headache;Depressed     Headache;Depressed;Muscle Ache 
                                20                                 16 
                 Headache;Hangnail        Headache;Hangnail;Depressed 
                                24                                 27 
     Headache;Hangnail;Muscle Ache     Headache;Hangnail;Stomach Ache 
                                29                                 17 
              Headache;Muscle Ache              Headache;Stomach Ache 
                                24                                 21 
   Headache;Stomach Ache;Depressed  Headache;Stomach Ache;Muscle Ache 
                                22                                 18 
                       Muscle Ache                       Stomach Ache 
                                 5                                  3 
            Stomach Ache;Depressed Stomach Ache;Depressed;Muscle Ache 
                                26                                 23 
          Stomach Ache;Muscle Ache 
                                33 
> 
> # Note: In this example, some subjects have the same symptom checked
> # multiple times; in practice these redundant selections would be NAs
> # mChoice will ignore these redundant selections
> 
> f <- summaryM(age + sex + sbp + Symptoms ~ treatment, test=TRUE)
> f


Descriptive Statistics  (N=500)

+--------------------+---+----------------------+----------------------+------------------------------+
|                    |N  |Drug                  |Placebo               |  Test                        |
|                    |   |(N=261)               |(N=239)               |Statistic                     |
+--------------------+---+----------------------+----------------------+------------------------------+
|age                 |500|        46.7/50.4/53.2|        46.5/49.7/53.2|   F=0.93 d.f.=1,498 P=0.335  |
+--------------------+---+----------------------+----------------------+------------------------------+
|sex : m             |500|          0.49  (129) |          0.54  (130) |Chi-square=1.23 d.f.=1 P=0.267|
+--------------------+---+----------------------+----------------------+------------------------------+
|Systolic BP [mmHg]  |499|          111/120/128 |          113/119/126 |   F=0.04 d.f.=1,497 P=0.833  |
+--------------------+---+----------------------+----------------------+------------------------------+
|Primary Symptoms : 1|  0|                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;3           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    5               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
> # trio of numbers represent 25th, 50th, 75th percentile
> print(f, long=TRUE)


Descriptive Statistics  (N=500)

+------------------+---+----------------------+----------------------+------------------------------+
|                  |N  |Drug                  |Placebo               |  Test                        |
|                  |   |(N=261)               |(N=239)               |Statistic                     |
+------------------+---+----------------------+----------------------+------------------------------+
|age               |500|        46.7/50.4/53.2|        46.5/49.7/53.2|   F=0.93 d.f.=1,498 P=0.335  |
+------------------+---+----------------------+----------------------+------------------------------+
|sex               |500|                      |                      |Chi-square=1.23 d.f.=1 P=0.267|
+------------------+---+----------------------+----------------------+------------------------------+
|    m             |   |          0.49  (129) |          0.54  (130) |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|Systolic BP [mmHg]|499|          111/120/128 |          113/119/126 |   F=0.04 d.f.=1,497 P=0.833  |
+------------------+---+----------------------+----------------------+------------------------------+
|Primary Symptoms  |  0|                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1             |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;2           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;2;3         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;2;4         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;2;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;3           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;3;4         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;3;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;4           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;4;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    1;5           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2             |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;3           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;3;4         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;3;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;4           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;4;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    2;5           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    3             |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    3;4           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    3;4;5         |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    3;5           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    4             |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    4;5           |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
|    5             |   |                      |                      |                              |
+------------------+---+----------------------+----------------------+------------------------------+
> plot(f)    # first specify options(grType='plotly') to use plotly
> plot(f, conType='dot', prtest='P')
> bpplt()    # annotated example showing layout of bp plot
> 
> # Produce separate tables by country
> f <- summaryM(age + sex + sbp + Symptoms ~ treatment + country,
+               groups='treatment', test=TRUE)
> f

Canada


Descriptive Statistics  (N=247)

+--------------------+---+----------------------+----------------------+------------------------------+
|                    |N  |Drug                  |Placebo               |  Test                        |
|                    |   |(N=124)               |(N=123)               |Statistic                     |
+--------------------+---+----------------------+----------------------+------------------------------+
|age                 |247|        48.1/51.0/53.3|        47.0/50.1/52.9|   F=3.14 d.f.=1,245 P=0.078  |
+--------------------+---+----------------------+----------------------+------------------------------+
|sex : m             |247|          0.52  (65)  |          0.56  (69)  |Chi-square=0.34 d.f.=1 P=0.562|
+--------------------+---+----------------------+----------------------+------------------------------+
|Systolic BP [mmHg]  |247|          111/120/128 |          113/120/127 |   F=0.03 d.f.=1,245 P=0.862  |
+--------------------+---+----------------------+----------------------+------------------------------+
|Primary Symptoms : 1|  0|                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;3           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    5               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+

US


Descriptive Statistics  (N=253)

+--------------------+---+----------------------+----------------------+------------------------------+
|                    |N  |Drug                  |Placebo               |  Test                        |
|                    |   |(N=137)               |(N=116)               |Statistic                     |
+--------------------+---+----------------------+----------------------+------------------------------+
|age                 |253|        45.6/49.3/53.1|        46.1/49.3/53.7|   F=0.08 d.f.=1,251 P=0.775  |
+--------------------+---+----------------------+----------------------+------------------------------+
|sex : m             |253|          0.47  (64)  |          0.53  (61)  |Chi-square=0.87 d.f.=1 P=0.352|
+--------------------+---+----------------------+----------------------+------------------------------+
|Systolic BP [mmHg]  |252|          111/120/128 |          113/119/126 |    F=0.02 d.f.=1,250 P=0.9   |
+--------------------+---+----------------------+----------------------+------------------------------+
|Primary Symptoms : 1|  0|                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;3           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;2;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    1;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;4           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;3;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    2;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;4;5           |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    3;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    4;5             |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
|    5               |   |                      |                      |                              |
+--------------------+---+----------------------+----------------------+------------------------------+
> 
> ## Not run: 
> ##D getHdata(pbc)
> ##D s5 <- summaryM(bili + albumin + stage + protime + sex + 
> ##D                age + spiders ~ drug, data=pbc)
> ##D 
> ##D print(s5, npct='both')
> ##D # npct='both' : print both numerators and denominators
> ##D plot(s5, which='categorical')
> ##D Key(locator(1))  # draw legend at mouse click
> ##D par(oma=c(3,0,0,0))  # leave outer margin at bottom
> ##D plot(s5, which='continuous')  # see also bpplotM
> ##D Key2()           # draw legend at lower left corner of plot
> ##D                  # oma= above makes this default key fit the page better
> ##D 
> ##D options(digits=3)
> ##D w <- latex(s5, npct='both', here=TRUE, file='')
> ##D 
> ##D options(grType='plotly')
> ##D pbc <- upData(pbc, moveUnits = TRUE)
> ##D s <- summaryM(bili + albumin + alk.phos + copper + spiders + sex ~
> ##D               drug, data=pbc, test=TRUE)
> ##D # Render html
> ##D options(prType='html')
> ##D s   # invokes print.summaryM
> ##D a <- plot(s)
> ##D a$Categorical
> ##D a$Continuous
> ##D plot(s, which='con')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("summaryP")
> ### * summaryP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryP
> ### Title: Multi-way Summary of Proportions
> ### Aliases: summaryP plot.summaryP ggplot.summaryP latex.summaryP
> ### Keywords: hplot category manip
> 
> ### ** Examples
> 
> n <- 100
> f <- function(na=FALSE) {
+   x <- sample(c('N', 'Y'), n, TRUE)
+   if(na) x[runif(100) < .1] <- NA
+   x
+ }
> set.seed(1)
> d <- data.frame(x1=f(), x2=f(), x3=f(), x4=f(), x5=f(), x6=f(), x7=f(TRUE),
+                 age=rnorm(n, 50, 10),
+                 race=sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
+                 sex=sample(c('Female', 'Male'), n, TRUE),
+                 treat=sample(c('A', 'B'), n, TRUE),
+                 region=sample(c('North America','Europe'), n, TRUE))
> d <- upData(d, labels=c(x1='MI', x2='Stroke', x3='AKI', x4='Migraines',
+                  x5='Pregnant', x6='Other event', x7='MD withdrawal',
+                  race='Race', sex='Sex'))
Input object size:	 13016 bytes;	 12 variables	 100 observations
New object size:	17800 bytes;	12 variables	100 observations
> dasna <- subset(d, region=='North America')
> with(dasna, table(race, treat))
          treat
race        A  B
  Asian     8 10
  Black/AA 13  6
  White     4 10
> s <- summaryP(race + sex + ynbind(x1, x2, x3, x4, x5, x6, x7, label='Exclusions') ~
+               region + treat, data=d)
> # add exclude1=FALSE below to include female category
> plot(s, groups='treat')
> require(ggplot2)
Loading required package: ggplot2
> ggplot(s, groups='treat')
> 
> plot(s, val ~ freq | region * var, groups='treat', outerlabels=FALSE)
> # Much better looking if omit outerlabels=FALSE; see output at
> # https://hbiostat.org/R/Hmisc/summaryFuns.pdf
> # See more examples under bpplotM
> 
> ## For plotly interactive graphic that does not handle variable size
> ## panels well:
> ## require(plotly)
> ## g <- ggplot(s, groups='treat')
> ## ggplotly(g, tooltip='text')
> 
> ## For nice plotly interactive graphic:
> ## options(grType='plotly')
> ## s <- summaryP(race + sex + ynbind(x1, x2, x3, x4, x5, x6, x7,
> ##                                   label='Exclusions') ~
> ##               treat, data=subset(d, region='Europe'))
> ##
> ## plot(s, groups='treat', refgroup='A')  # refgroup='A' does B-A differences
> 
> 
> # Make a chart where there is a block of variables that
> # are only analyzed for males.  Keep redundant sex in block for demo.
> # Leave extra space for numerators, denominators
> sb <- summaryP(race + sex +
+                pBlock(race, sex, label='Race: Males', subset=sex=='Male') ~
+                region, data=d)
> plot(sb, text.at=1.3)
> plot(sb, groups='region', layout=c(1,3), key=list(space='top'),
+      text.at=1.15)
> ggplot(sb, groups='region')
> ## Not run: 
> ##D plot(s, groups='treat')
> ##D # plot(s, groups='treat', outerlabels=FALSE) for standard lattice output
> ##D plot(s, groups='region', key=list(columns=2, space='bottom'))
> ##D require(ggplot2)
> ##D colorFacet(ggplot(s))
> ##D 
> ##D plot(summaryP(race + sex ~ region, data=d), exclude1=FALSE, col='green')
> ##D 
> ##D require(lattice)
> ##D # Make your own plot using data frame created by summaryP
> ##D useOuterStrips(dotplot(val ~ freq | region * var, groups=treat, data=s,
> ##D         xlim=c(0,1), scales=list(y='free', rot=0), xlab='Fraction',
> ##D         panel=function(x, y, subscripts, ...) {
> ##D           denom <- s$denom[subscripts]
> ##D           x <- x / denom
> ##D           panel.dotplot(x=x, y=y, subscripts=subscripts, ...) }))
> ##D 
> ##D # Show marginal summary for all regions combined
> ##D s <- summaryP(race + sex ~ region, data=addMarginal(d, region))
> ##D plot(s, groups='region', key=list(space='top'), layout=c(1,2))
> ##D 
> ##D # Show marginal summaries for both race and sex
> ##D s <- summaryP(ynbind(x1, x2, x3, x4, label='Exclusions', sort=FALSE) ~
> ##D               race + sex, data=addMarginal(d, race, sex))
> ##D plot(s, val ~ freq | sex*race)
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("summaryRc")
> ### * summaryRc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryRc
> ### Title: Graphical Summarization of Continuous Variables Against a
> ###   Response
> ### Aliases: summaryRc
> ### Keywords: hplot
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(177)
> sex <- factor(sample(c("m","f"), 500, rep=TRUE))
> age <- rnorm(500, 50, 5)
> bp  <- rnorm(500, 120, 7)
> units(age) <- 'Years'; units(bp) <- 'mmHg'
> label(bp) <- 'Systolic Blood Pressure'
> L <- .5*(sex == 'm') + 0.1 * (age - 50)
> y <- rbinom(500, 1, plogis(L))
> par(mfrow=c(1,2))
> summaryRc(y ~ age + bp)
> # For x limits use 1st and 99th percentiles to frame extended box plots
> summaryRc(y ~ age + bp, bpplot='top', datadensity=FALSE, trim=.01)
> summaryRc(y ~ age + bp + stratify(sex),
+           label.curves=list(keys='lines'), nloc=list(x=.1, y=.05))
> y2 <- rbinom(500, 1, plogis(L + .5))
> Y <- cbind(y, y2)
> summaryRc(Y ~ age + bp + stratify(sex),
+           label.curves=list(keys='lines'), nloc=list(x=.1, y=.05))
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("summaryS")
> ### * summaryS
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryS
> ### Title: Summarize Multiple Response Variables and Make Multipanel
> ###   Scatter or Dot Plot
> ### Aliases: summaryS plot.summaryS plotp.summaryS mbarclPanel medvPanel
> ###   mbarclpl medvpl
> ### Keywords: category hplot manip grouping
> 
> ### ** Examples
> 
> # See tests directory file summaryS.r for more examples, and summarySp.r
> # for plotp examples
> require(survival)
Loading required package: survival
> n <- 100
> set.seed(1)
> d <- data.frame(sbp=rnorm(n, 120, 10),
+                 dbp=rnorm(n, 80, 10),
+                 age=rnorm(n, 50, 10),
+                 days=sample(1:n, n, TRUE),
+                 S1=Surv(2*runif(n)), S2=Surv(runif(n)),
+                 race=sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
+                 sex=sample(c('Female', 'Male'), n, TRUE),
+                 treat=sample(c('A', 'B'), n, TRUE),
+                 region=sample(c('North America','Europe'), n, TRUE),
+                 meda=sample(0:1, n, TRUE), medb=sample(0:1, n, TRUE))
> 
> d <- upData(d, labels=c(sbp='Systolic BP', dbp='Diastolic BP',
+             race='Race', sex='Sex', treat='Treatment',
+             days='Time Since Randomization',
+             S1='Hospitalization', S2='Re-Operation',
+             meda='Medication A', medb='Medication B'),
+             units=c(sbp='mmHg', dbp='mmHg', age='Year', days='Days'))
Input object size:	 14568 bytes;	 12 variables	 100 observations
New object size:	20320 bytes;	12 variables	100 observations
> 
> s <- summaryS(age + sbp + dbp ~ days + region + treat,  data=d)
> # plot(s)   # 3 pages
> plot(s, groups='treat', datadensity=TRUE,
+      scat1d.opts=list(lwd=.5, nhistSpike=0))
> plot(s, groups='treat', panel=lattice::panel.loess,
+      key=list(space='bottom', columns=2),
+      datadensity=TRUE, scat1d.opts=list(lwd=.5))
> 
> # To make a plotly graph when the stratification variable region is not
> # present, run the following (showpts adds raw data points):
> # plotp(s, groups='treat', fitter=loess, showpts=TRUE)
> 
> # Make your own plot using data frame created by summaryP
> # xyplot(y ~ days | yvar * region, groups=treat, data=s,
> #        scales=list(y='free', rot=0))
> 
> # Use loess to estimate the probability of two different types of events as
> # a function of time
> s <- summaryS(meda + medb ~ days + treat + region, data=d)
> pan <- function(...)
+    panel.plsmo(..., type='l', label.curves=max(which.packet()) == 1,
+                datadensity=TRUE)
> plot(s, groups='treat', panel=pan, paneldoesgroups=TRUE,
+      scat1d.opts=list(lwd=.7), cex.strip=.8)
> 
> # Repeat using intervals instead of nonparametric smoother
> pan <- function(...)  # really need mobs > 96 to est. proportion
+   panel.plsmo(..., type='l', label.curves=max(which.packet()) == 1,
+               method='intervals', mobs=5)
> 
> plot(s, groups='treat', panel=pan, paneldoesgroups=TRUE, xlim=c(0, 150))
> 
> 
> # Demonstrate dot charts of summary statistics
> s <- summaryS(age + sbp + dbp ~ region + treat, data=d, fun=mean)
> plot(s)
> plot(s, groups='treat', funlabel=expression(bar(X)))
> # Compute parametric confidence limits for mean, and include sample
> # sizes by naming a column "n"
> 
> f <- function(x) {
+   x <- x[! is.na(x)]
+   c(smean.cl.normal(x, na.rm=FALSE), n=length(x))
+ }
> s <- summaryS(age + sbp + dbp ~ region + treat, data=d, fun=f)
> plot(s, funlabel=expression(bar(X) %+-% t[0.975] %*% s))
> plot(s, groups='treat', cex.values=.65,
+      key=list(space='bottom', columns=2,
+        text=c('Treatment A:','Treatment B:')))
> 
> # For discrete time, plot Harrell-Davis quantiles of y variables across
> # time using different line characteristics to distinguish quantiles
> d <- upData(d, days=round(days / 30) * 30)
Input object size:	 20320 bytes;	 12 variables	 100 observations
Modified variable	days
New object size:	20320 bytes;	12 variables	100 observations
> g <- function(y) {
+   probs <- c(0.05, 0.125, 0.25, 0.375)
+   probs <- sort(c(probs, 1 - probs))
+   y <- y[! is.na(y)]
+   w <- hdquantile(y, probs)
+   m <- hdquantile(y, 0.5, se=TRUE)
+   se <- as.numeric(attr(m, 'se'))
+   c(Median=as.numeric(m), w, se=se, n=length(y))
+ }
> s <- summaryS(sbp + dbp ~ days + region, fun=g, data=d)
> plot(s, panel=mbarclPanel)
> plot(s, groups='region', panel=mbarclPanel, paneldoesgroups=TRUE)
> 
> # For discrete time, plot median y vs x along with CL for difference,
> # using Harrell-Davis median estimator and its s.e., and use violin
> # plots
> 
> s <- summaryS(sbp + dbp ~ days + region, data=d)
> plot(s, groups='region', panel=medvPanel, paneldoesgroups=TRUE)
> 
> # Proportions and Wilson confidence limits, plus approx. Gaussian
> # based half/width confidence limits for difference in probabilities
> g <- function(y) {
+   y <- y[!is.na(y)]
+   n <- length(y)
+   p <- mean(y)
+   se <- sqrt(p * (1. - p) / n)
+   structure(c(binconf(sum(y), n), se=se, n=n),
+             names=c('Proportion', 'Lower', 'Upper', 'se', 'n'))
+ }
> s <- summaryS(meda + medb ~ days + region, fun=g, data=d)
> plot(s, groups='region', panel=mbarclPanel, paneldoesgroups=TRUE)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("symbol.freq")
> ### * symbol.freq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: symbol.freq
> ### Title: Graphic Representation of a Frequency Table
> ### Aliases: symbol.freq
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getHdata(titanic)
> ##D attach(titanic)
> ##D age.tertile <- cut2(titanic$age, g=3)
> ##D symbol.freq(age.tertile, pclass, marginals=T, srtx=45)
> ##D detach(2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("t.test.cluster")
> ### * t.test.cluster
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: t.test.cluster
> ### Title: t-test for Clustered Data
> ### Aliases: t.test.cluster print.t.test.cluster
> ### Keywords: htest
> 
> ### ** Examples
> 
> set.seed(1)
> y <- rnorm(800)
> group <- sample(1:2, 800, TRUE)
> cluster <- sample(1:40, 800, TRUE)
> table(cluster,group)
       group
cluster  1  2
     1   4  7
     2  10  8
     3   9 10
     4   8 11
     5  14  6
     6   8 10
     7   7 14
     8  10 10
     9   6 11
     10 11 12
     11 12  3
     12 10 13
     13 12 11
     14 13 11
     15  8  4
     16  9  8
     17 11 16
     18  8 14
     19 11 12
     20  7 12
     21 11 14
     22  8  7
     23 12 10
     24 14 11
     25  7 12
     26 10 14
     27 10  8
     28 18 13
     29 11 14
     30  7 11
     31  2 12
     32 14  6
     33  6 11
     34 12 12
     35 14  4
     36  6 10
     37 11  8
     38  8  9
     39  5 15
     40 15  7
> t.test(y ~ group)   # R only

	Welch Two Sample t-test

data:  y by group
t = 3, df = 788, p-value = 0.01
alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0
95 percent confidence interval:
 0.0436 0.3282
sample estimates:
mean in group 1 mean in group 2 
         0.0792         -0.1067 

> t.test.cluster(y, cluster, group)
                                    1       2      
N                                   389     411    
Clusters                            40      40     
Mean                                 0.0792 -0.1067
SS among clusters within groups     43.5    40.6   
SS within clusters within groups    388     363    
MS among clusters within groups     1.08           
d.f.                                78             
MS within clusters within groups    1.04           
d.f.                                720            
Na                                  9.85           
Intracluster correlation            0.00347        
Variance Correction Factor          1.03    1.04   
Variance of effect                  0.0054         
Variance without cluster adjustment 0.00522        
Design Effect                       1.03           
Effect (Difference in Means)        -0.186         
S.E. of Effect                      0.0735         
0.95 Confidence limits              -0.3300 -0.0419
Z Statistic                         -2.53          
2-sided P Value                     0.0114         
> # Note: negate estimates of differences from t.test to
> # compare with t.test.cluster
> 
> 
> 
> cleanEx()
> nameEx("tabulr")
> ### * tabulr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tabulr
> ### Title: Interface to Tabular Function
> ### Aliases: tabulr table_trio table_N table_freq table_pc table_latexdefs
> ###   table_formatpct nFm
> ### Keywords: utilities interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D n <- 400
> ##D set.seed(1)
> ##D d <- data.frame(country=factor(sample(c('US','Canada','Mexico'), n, TRUE)),
> ##D                 sex=factor(sample(c('Female','Male'), n, TRUE)),
> ##D                 age=rnorm(n, 50, 10),
> ##D                 sbp=rnorm(n, 120, 8))
> ##D d <- upData(d,
> ##D             preghx=ifelse(sex=='Female', sample(c('No','Yes'), n, TRUE), NA),
> ##D             labels=c(sbp='Systolic BP', age='Age', preghx='Pregnancy History'),
> ##D             units=c(sbp='mmHg', age='years'))
> ##D contents(d)
> ##D require(tables)
> ##D invisible(booktabs())  # use booktabs LaTeX style for tabular
> ##D g <- function(x) {
> ##D   x <- x[!is.na(x)]
> ##D   if(length(x) == 0) return('')
> ##D   paste(latexNumeric(nFm(mean(x), 3, 1)),
> ##D         ' \hfill{\smaller[2](', length(x), ')}', sep='')
> ##D }
> ##D tab <- tabulr((age + Heading('Females')*(sex == 'Female')*sbp)*
> ##D               Heading()*g + (age + sbp)*Heading()*trio ~ 
> ##D               Heading()*country*Heading()*sex, data=d)
> ##D # Formula after interpretation by tabulr:
> ##D # (Heading('Age\hfill {\smaller[2] years}') * age + Heading("Females")
> ##D # * (sex == "Female") * Heading('Systolic BP {\smaller[2] mmHg}') * sbp)
> ##D # * Heading() * g + (age + sbp) * Heading() * table_trio ~ Heading()
> ##D # * country * Heading() * sex
> ##D cat('\begin{landscape}\n')
> ##D cat('\begin{minipage}{\textwidth}\n')
> ##D cat('\keytrio\n')
> ##D latex(tab)
> ##D cat('\end{minipage}\end{landscape}\n')
> ##D 
> ##D getHdata(pbc)
> ##D pbc <- upData(pbc, moveUnits=TRUE)
> ##D # Convert to character to prevent tabular from stratifying
> ##D for(x in c('sex', 'stage', 'spiders')) {
> ##D   pbc[[x]] <- as.character(pbc[[x]])
> ##D   label(pbc[[x]]) <- paste(toupper(substring(x, 1, 1)), substring(x, 2), sep='')
> ##D }
> ##D table_options(pn=TRUE, showfreq='all')
> ##D tab <- tabulr((bili + albumin + protime + age) *
> ##D               Heading()*trio +
> ##D               (sex + stage + spiders)*Heading()*freq ~ drug, data=pbc)
> ##D latex(tab)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("testCharDateTime")
> ### * testCharDateTime
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: testCharDateTime
> ### Title: testCharDateTime
> ### Aliases: testCharDateTime
> 
> ### ** Examples
> 
> for(conv in c(FALSE, TRUE)) {
+   print(testCharDateTime(c('2023-03-11', '2023-04-11', 'a', 'b', 'c'), convert=conv))
+   print(testCharDateTime(c('2023-03-11', '2023-04-11', 'a', 'b'), convert=conv))
+   print(testCharDateTime(c('2023-03-11 11:12:13', '2023-04-11 11:13:14', 'a', 'b'), convert=conv))
+   print(testCharDateTime(c('2023-03-11 11:12', '2023-04-11 11:13', 'a', 'b'), convert=conv))
+   print(testCharDateTime(c('3/11/2023', '4/11/2023', 'a', 'b'), convert=conv))
+ }
[1] "character"
[1] "date"
[1] "datetime"
[1] "datetime"
[1] "date"
$type
[1] "character"

$x
[1] "2023-03-11" "2023-04-11" "a"          "b"          "c"         

$numna
[1] 0

$type
[1] "date"

$x
[1] 2023-03-11 2023-04-11 a          b         

$numna
[1] 2

$type
[1] "datetime"

$x
[1] 2023-03-11 11:12:13 2023-04-11 11:13:14 a                  
[4] b                  

$numna
[1] 2

$type
[1] "datetime"

$x
[1] 2023-03-11 11:12:00 2023-04-11 11:13:00 a                  
[4] b                  

$numna
[1] 2

$type
[1] "date"

$x
[1] 2023-03-11 2023-04-11 a          b         

$numna
[1] 2

> x <- c(paste0('2023-03-0', 1:9), 'a', 'a', 'a', 'b')
> y <- testCharDateTime(x, convert=TRUE)$x
> describe(y)  # note counts of special missing values a, b
y 
         n    missing          a          b   distinct       Info       Mean 
         9          4          3          1          9          1 2023-03-05 
       Gmd 
     3.333 
                                                                            
Value      2023-03-01 2023-03-02 2023-03-03 2023-03-04 2023-03-05 2023-03-06
Frequency           1          1          1          1          1          1
Proportion      0.111      0.111      0.111      0.111      0.111      0.111
                                           
Value      2023-03-07 2023-03-08 2023-03-09
Frequency           1          1          1
Proportion      0.111      0.111      0.111

For the frequency table, variable is rounded to the nearest 0
> 
> 
> 
> cleanEx()
> nameEx("tex")
> ### * tex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tex
> ### Title: function for use in graphs that are used with the psfrag package
> ###   in LaTeX
> ### Aliases: tex
> ### Keywords: hplot device
> 
> ### ** Examples
> 
> ## Not run: 
> ##D pdf('test.pdf')
> ##D x <- seq(0,15,length=100)
> ##D plot(x, dchisq(x, 5), xlab=tex('$x$'),
> ##D         ylab=tex('$f(x)$'), type='l')
> ##D title(tex('Density Function of the $\chi_{5}^{2}$ Distribution'))
> ##D dev.off()
> ##D # To process this file in LaTeX do something like
> ##D #\documentclass{article}
> ##D #\usepackage[scanall]{psfrag}
> ##D #\begin{document}
> ##D #\begin{figure}
> ##D #\includegraphics{test.ps}
> ##D #\caption{This is an example}
> ##D #\end{figure}
> ##D #\end{document}
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("transace")
> ### * transace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: transace
> ### Title: Additive Regression and Transformations using ace or avas
> ### Aliases: transace ggplot.transace print.transace areg.boot
> ###   print.areg.boot plot.areg.boot predict.areg.boot summary.areg.boot
> ###   print.summary.areg.boot Function.areg.boot Mean Mean.areg.boot
> ###   Quantile Quantile.areg.boot monotone smearingEst
> ### Keywords: nonparametric smooth multivariate nonlinear regression
> 
> ### ** Examples
> 
> # xtrans <- transace(~ monotone(age) + sex + blood.pressure + categorical(race.code))
> # print(xtrans)  # show R^2s and a few other things
> # ggplot(xtrans) # show transformations
> 
> # Generate random data from the model y = exp(x1 + epsilon/3) where
> # x1 and epsilon are Gaussian(0,1)
> set.seed(171)  # to be able to reproduce example
> x1 <- rnorm(200)
> x2 <- runif(200)  # a variable that is really unrelated to y]
> x3 <- factor(sample(c('cat','dog','cow'), 200,TRUE))  # also unrelated to y
> y  <- exp(x1 + rnorm(200)/3)
> f  <- areg.boot(y ~ x1 + x2 + x3, B=40)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
> f

areg Additive Regression Model

areg.boot(x = y ~ x1 + x2 + x3, B = 40)


Predictor Types

   type d.f.
x1    s    3
x2    s    3
x3    c    2

y type: s	d.f.: 3

n= 200   p= 3 

Apparent R2 on transformed Y scale: 0.905
Bootstrap validated R2            : 0.862

Coefficients of standardized transformations:

Intercept        x1        x2        x3 
 6.55e-16  9.51e-01  9.51e-01  9.51e-01 


Residuals on transformed scale:

      Min        1Q    Median        3Q       Max      Mean      S.D. 
-7.12e-01 -2.08e-01 -7.07e-03  1.70e-01  9.52e-01 -5.40e-18  3.09e-01 

> plot(f)
> # Note that the fitted transformation of y is very nearly log(y)
> # (the appropriate one), the transformation of x1 is nearly linear,
> # and the transformations of x2 and x3 are essentially flat 
> # (specifying monotone(x2) if method='avas' would have resulted
> # in a smaller confidence band for x2)
> 
> 
> summary(f)
summary.areg.boot(object = f)

Estimates based on 40 resamples



Values to which predictors are set when estimating
effects of other predictors:

     y     x1     x2     x3 
1.0004 0.0528 0.5117 2.0000 

Estimates of differences of effects on Median Y (from first X
value), and bootstrap standard errors of these differences.
Settings for X are shown as row headings.


Predictor: x1 
         
x         Differences    S.E Lower 0.95 Upper 0.95    Z  Pr(|Z|)
  -0.6289       0.000     NA         NA         NA   NA       NA
   0.0528       0.511 0.0453      0.422       0.60 11.3 1.49e-29
   0.6979       1.400 0.1282      1.148       1.65 10.9 9.75e-28


Predictor: x2 
       
x       Differences    S.E Lower 0.95 Upper 0.95     Z Pr(|Z|)
  0.223      0.0000     NA         NA         NA    NA      NA
  0.512      0.0362 0.0435    -0.0491      0.121 0.832   0.405
  0.768      0.0427 0.0699    -0.0942      0.180 0.612   0.541


Predictor: x3 
     
x     Differences    S.E Lower 0.95 Upper 0.95      Z Pr(|Z|)
  cat      0.0000     NA         NA         NA     NA      NA
  cow     -0.0135 0.0605     -0.132     0.1050 -0.223 0.82357
  dog     -0.1184 0.0409     -0.198    -0.0383 -2.898 0.00376
> 
> 
> # use summary(f, values=list(x2=c(.2,.5,.8))) for example if you
> # want to use nice round values for judging effects
> 
> 
> # Plot Y hat vs. Y (this doesn't work if there were NAs)
> plot(fitted(f), y)  # or: plot(predict(f,statistic='fitted'), y)
> 
> 
> # Show fit of model by varying x1 on the x-axis and creating separate
> # panels for x2 and x3.  For x2 using only a few discrete values
> newdat <- expand.grid(x1=seq(-2,2,length=100),x2=c(.25,.75),
+                       x3=c('cat','dog','cow'))
> yhat <- predict(f, newdat, statistic='fitted')  
> # statistic='mean' to get estimated mean rather than simple inverse trans.
> xYplot(yhat ~ x1 | x2, groups=x3, type='l', data=newdat)
> 
> 
> ## Not run: 
> ##D # Another example, on hypothetical data
> ##D f <- areg.boot(response ~ I(age) + monotone(blood.pressure) + race)
> ##D # use I(response) to not transform the response variable
> ##D plot(f, conf.int=.9)
> ##D # Check distribution of residuals
> ##D plot(fitted(f), resid(f))
> ##D qqnorm(resid(f))
> ##D # Refit this model using ols so that we can draw a nomogram of it.
> ##D # The nomogram will show the linear predictor, median, mean.
> ##D # The last two are smearing estimators.
> ##D Function(f, type='individual')  # create transformation functions
> ##D f.ols <- ols(.response(response) ~ age + 
> ##D              .blood.pressure(blood.pressure) + .race(race))
> ##D # Note: This model is almost exactly the same as f but there
> ##D # will be very small differences due to interpolation of
> ##D # transformations
> ##D meanr <- Mean(f)      # create function of lp computing mean response
> ##D medr  <- Quantile(f)  # default quantile is .5
> ##D nomogram(f.ols, fun=list(Mean=meanr,Median=medr))
> ##D 
> ##D 
> ##D # Create S functions that will do the transformations
> ##D # This is a table look-up with linear interpolation
> ##D g <- Function(f)
> ##D plot(blood.pressure, g$blood.pressure(blood.pressure))
> ##D # produces the central curve in the last plot done by plot(f)
> ## End(Not run)
> 
> 
> # Another simulated example, where y has a log-normal distribution
> # with mean x and variance 1.  Untransformed y thus has median
> # exp(x) and mean exp(x + .5sigma^2) = exp(x + .5)
> # First generate data from the model y = exp(x + epsilon),
> # epsilon ~ Gaussian(0, 1)
> 
> 
> set.seed(139)
> n <- 1000
> x <- rnorm(n)
> y <- exp(x + rnorm(n))
> f <- areg.boot(y ~ x, B=20)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
> plot(f)       # note log shape for y, linear for x.  Good!
> xs <- c(-2, 0, 2)
> d <- data.frame(x=xs)
> predict(f, d, 'fitted')
Inverse Transformation 
[1] 0.217 0.945 6.427
> predict(f, d, 'median')   # almost same; median residual=-.001
Median 
[1] 0.216 0.944 6.412
> exp(xs)                   # population medians
[1] 0.135 1.000 7.389
> predict(f, d, 'mean')
Mean 
[1]  0.261  1.675 10.113
> exp(xs + .5)              # population means
[1]  0.223  1.649 12.182
> 
> 
> # Show how smearingEst works
> res <- c(-1,0,1)          # define residuals
> y <- 1:5
> ytrans <- log(y)
> ys <- seq(.1,15,length=50)
> trans.approx <- list(x=log(ys), y=ys)
> options(digits=4)
> smearingEst(ytrans, exp, res, 'fitted')          # ignores res
Inverse Transformation 
[1] 1 2 3 4 5
> smearingEst(ytrans, trans.approx, res, 'fitted') # ignores res 
Inverse Transformation 
[1] 1.002 2.004 3.004 4.002 5.001
> smearingEst(ytrans, exp, res, 'median')          # median res=0
Median 
[1] 1 2 3 4 5
> smearingEst(ytrans, exp, res+.1, 'median')       # median res=.1
Median 
[1] 1.105 2.210 3.316 4.421 5.526
> smearingEst(ytrans, trans.approx, res, 'median')
Median 
[1] 1.002 2.004 3.004 4.002 5.001
> smearingEst(ytrans, exp, res, 'mean')
Mean 
[1] 1.362 2.724 4.086 5.448 6.810
> mean(exp(ytrans[2] + res))                       # should equal 2nd # above
[1] 2.724
> smearingEst(ytrans, trans.approx, res, 'mean')
Mean 
[1] 1.369 2.728 4.091 5.452 6.813
> smearingEst(ytrans, trans.approx, res, mean)
mean 
[1] 1.369 2.728 4.091 5.452 6.813
> # Last argument can be any statistical function operating
> # on a vector that returns a single value
> 
> 
> 
> cleanEx()
> nameEx("transcan")
> ### * transcan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: transcan
> ### Title: Transformations/Imputations using Canonical Variates
> ### Aliases: transcan summary.transcan print.transcan plot.transcan
> ###   ggplot.transcan impute.transcan predict.transcan Function
> ###   Function.transcan fit.mult.impute vcov.default vcov.fit.mult.impute
> ###   [.transcan invertTabulated
> ### Keywords: smooth regression multivariate methods models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- cbind(age, disease, blood.pressure, pH)  
> ##D #cbind will convert factor object `disease' to integer
> ##D par(mfrow=c(2,2))
> ##D x.trans <- transcan(x, categorical="disease", asis="pH",
> ##D                     transformed=TRUE, imputed=TRUE)
> ##D summary(x.trans)  #Summary distribution of imputed values, and R-squares
> ##D f <- lm(y ~ x.trans$transformed)   #use transformed values in a regression
> ##D #Now replace NAs in original variables with imputed values, if not
> ##D #using transformations
> ##D age            <- impute(x.trans, age)
> ##D disease        <- impute(x.trans, disease)
> ##D blood.pressure <- impute(x.trans, blood.pressure)
> ##D pH             <- impute(x.trans, pH)
> ##D #Do impute(x.trans) to impute all variables, storing new variables under
> ##D #the old names
> ##D summary(pH)       #uses summary.impute to tell about imputations
> ##D                   #and summary.default to tell about pH overall
> ##D # Get transformed and imputed values on some new data frame xnew
> ##D newx.trans     <- predict(x.trans, xnew)
> ##D w              <- predict(x.trans, xnew, type="original")
> ##D age            <- w[,"age"]            #inserts imputed values
> ##D blood.pressure <- w[,"blood.pressure"]
> ##D Function(x.trans)  #creates .age, .disease, .blood.pressure, .pH()
> ##D #Repeat first fit using a formula
> ##D x.trans <- transcan(~ age + disease + blood.pressure + I(pH), 
> ##D                     imputed=TRUE)
> ##D age <- impute(x.trans, age)
> ##D predict(x.trans, expand.grid(age=50, disease="pneumonia",
> ##D         blood.pressure=60:260, pH=7.4))
> ##D z <- transcan(~ age + factor(disease.code),  # disease.code categorical
> ##D               transformed=TRUE, trantab=TRUE, imputed=TRUE, pl=FALSE)
> ##D ggplot(z, scale=TRUE)
> ##D plot(z$transformed)
> ## End(Not run)
> 
> 
> # Multiple imputation and estimation of variances and covariances of
> # regression coefficient estimates accounting for imputation
> set.seed(1)
> x1 <- factor(sample(c('a','b','c'),100,TRUE))
> x2 <- (x1=='b') + 3*(x1=='c') + rnorm(100)
> y  <- x2 + 1*(x1=='c') + rnorm(100)
> x1[1:20] <- NA
> x2[18:23] <- NA
> d <- data.frame(x1,x2,y)
> n <- naclus(d)
> plot(n); naplot(n)  # Show patterns of NAs
> f  <- transcan(~y + x1 + x2, n.impute=10, shrink=FALSE, data=d)
Warning in transcan(~y + x1 + x2, n.impute = 10, shrink = FALSE, data = d) :
  transcan provides only an approximation to true multiple imputation.
A better approximation is provided by the aregImpute function.
The MICE and other S libraries provide imputations from Bayesian posterior distributions.
Convergence criterion:0.666 0.173 0.068 0.022 
Convergence in 5 iterations
R-squared achieved in predicting each variable:

    y    x1    x2 
0.838 0.748 0.815 

Adjusted R-squared:

    y    x1    x2 
0.826 0.727 0.800 
> options(digits=3)
> summary(f)
transcan(x = ~y + x1 + x2, n.impute = 10, shrink = FALSE, data = d)

Iterations: 5 

R-squared achieved in predicting each variable:

    y    x1    x2 
0.838 0.748 0.815 

Adjusted R-squared:

    y    x1    x2 
0.826 0.727 0.800 

Coefficients of canonical variates for predicting each (row) variable

   y     x1    x2   
y        -0.39  0.58
x1 -0.65       -0.49
x2  0.67 -0.35      

Summary of imputed values

x1 
       n  missing distinct     Info     Mean      Gmd 
     200        0        3    0.803     1.75   0.9196 
                         
Value         1    2    3
Frequency   110   30   60
Proportion 0.55 0.15 0.30

For the frequency table, variable is rounded to the nearest 0
x2 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
      60        0       53    0.999    1.128    1.922  -1.4251  -0.8569 
     .25      .50      .75      .90      .95 
 -0.3847   1.0902   2.5492   3.2485   3.7237 

lowest : -1.4251   -1.04073  -0.83646  -0.784439 -0.711954
highest: 3.48258   3.70141   4.1473    4.58272   4.82811  

Starting estimates for imputed values:

   y   x1   x2 
1.33 2.00 1.26 
> 
> 
> f  <- transcan(~y + x1 + x2, n.impute=10, shrink=TRUE, data=d)
Warning in transcan(~y + x1 + x2, n.impute = 10, shrink = TRUE, data = d) :
  transcan provides only an approximation to true multiple imputation.
A better approximation is provided by the aregImpute function.
The MICE and other S libraries provide imputations from Bayesian posterior distributions.
Convergence criterion:0.654 0.14 0.059 0.018 
Convergence in 5 iterations
R-squared achieved in predicting each variable:

    y    x1    x2 
0.839 0.746 0.815 

Adjusted R-squared:

    y    x1    x2 
0.826 0.726 0.800 

Shrinkage factors:

    y    x1    x2 
0.943 0.951 0.942 
> summary(f)
transcan(x = ~y + x1 + x2, n.impute = 10, shrink = TRUE, data = d)

Iterations: 5 

R-squared achieved in predicting each variable:

    y    x1    x2 
0.839 0.746 0.815 

Adjusted R-squared:

    y    x1    x2 
0.826 0.726 0.800 

Shrinkage factors:

    y    x1    x2 
0.943 0.951 0.942 

Coefficients of canonical variates for predicting each (row) variable

   y     x1    x2   
y        -0.37  0.55
x1 -0.62       -0.46
x2  0.64 -0.33      

Summary of imputed values

x1 
       n  missing distinct     Info     Mean      Gmd 
     200        0        3     0.84      1.8   0.9246 
                      
Value        1   2   3
Frequency  100  40  60
Proportion 0.5 0.2 0.3

For the frequency table, variable is rounded to the nearest 0
x2 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
      60        0       55        1    1.438    1.745  -1.0444  -0.3443 
     .25      .50      .75      .90      .95 
  0.3446   1.2551   2.7124   3.2616   4.0600 

lowest : -1.4251   -1.02433  -0.690399 -0.430158 -0.33478 
highest: 3.69022   4.05863   4.08554   4.39528   4.4897   

Starting estimates for imputed values:

   y   x1   x2 
1.33 2.00 1.26 
> 
> 
> h <- fit.mult.impute(y ~ x1 + x2, lm, f, data=d)
Warning in fit.mult.impute(y ~ x1 + x2, lm, f, data = d) :
  If you use print, summary, or anova on the result, lm methods use the
sum of squared residuals rather than the Rubin formula for computing
residual variance and standard errors.  It is suggested to use ols
instead of lm.

Wald Statistic Information

Variance Inflation Factors Due to Imputation:

(Intercept)         x1b         x1c          x2 
       1.07        1.03        1.04        1.06 

Fraction of Missing Information:

(Intercept)         x1b         x1c          x2 
       0.06        0.03        0.03        0.05 

d.f. for t-distribution for Tests of Single Coefficients:

(Intercept)         x1b         x1c          x2 
       2281        9238        7466        3137 

The following fit components were averaged over the 10 model fits:

  fitted.values 

> # Add ,fit.reps=TRUE to save all fit objects in h, then do something like:
> # for(i in 1:length(h$fits)) print(summary(h$fits[[i]]))
> 
> 
> diag(vcov(h))
(Intercept)         x1b         x1c          x2 
     0.0308      0.0756      0.1912      0.0135 
> 
> 
> h.complete <- lm(y ~ x1 + x2, na.action=na.omit)
> h.complete

Call:
lm(formula = y ~ x1 + x2, na.action = na.omit)

Coefficients:
(Intercept)          x1b          x1c           x2  
     0.0691       0.1052       1.3126       0.9245  

> diag(vcov(h.complete))
(Intercept)         x1b         x1c          x2 
     0.0467      0.0989      0.2346      0.0153 
> 
> 
> # Note: had the rms ols function been used in place of lm, any
> # function run on h (anova, summary, etc.) would have automatically
> # used imputation-corrected variances and covariances
> 
> 
> # Example demonstrating how using the multinomial logistic model
> # to impute a categorical variable results in a frequency
> # distribution of imputed values that matches the distribution
> # of non-missing values of the categorical variable
> 
> 
> ## Not run: 
> ##D set.seed(11)
> ##D x1 <- factor(sample(letters[1:4], 1000,TRUE))
> ##D x1[1:200] <- NA
> ##D table(x1)/sum(table(x1))
> ##D x2 <- runif(1000)
> ##D z  <- transcan(~ x1 + I(x2), n.impute=20, impcat='multinom')
> ##D table(z$imputed$x1)/sum(table(z$imputed$x1))
> ##D 
> ##D # Here is how to create a completed dataset
> ##D d <- data.frame(x1, x2)
> ##D z <- transcan(~x1 + I(x2), n.impute=5, data=d)
> ##D imputed <- impute(z, imputation=1, data=d,
> ##D                   list.out=TRUE, pr=FALSE, check=FALSE)
> ##D sapply(imputed, function(x)sum(is.imputed(x)))
> ##D sapply(imputed, function(x)sum(is.na(x)))
> ## End(Not run)
> 
> # Do single imputation and create a filled-in data frame
> z <- transcan(~x1 + I(x2), data=d, imputed=TRUE)
Convergence criterion:0.026 0.008 
Convergence in 3 iterations
R-squared achieved in predicting each variable:

   x1    x2 
0.618 0.658 

Adjusted R-squared:

   x1    x2 
0.608 0.650 
> imputed <- as.data.frame(impute(z, data=d, list.out=TRUE))


Imputed missing values with the following frequencies
 and stored them in variables with their original names:

x1 x2 
20  6 
> 
> # Example where multiple imputations are for basic variables and
> # modeling is done on variables derived from these
> 
> 
> set.seed(137)
> n <- 400
> x1 <- runif(n)
> x2 <- runif(n)
> y  <- x1*x2 + x1/(1+x2) + rnorm(n)/3
> x1[1:5] <- NA
> d <- data.frame(x1,x2,y)
> w <- transcan(~ x1 + x2 + y, n.impute=5, data=d)
Warning in transcan(~x1 + x2 + y, n.impute = 5, data = d) :
  transcan provides only an approximation to true multiple imputation.
A better approximation is provided by the aregImpute function.
The MICE and other S libraries provide imputations from Bayesian posterior distributions.
Convergence criterion:0.167 0.025 0.002 
Convergence in 4 iterations
R-squared achieved in predicting each variable:

   x1    x2     y 
0.507 0.114 0.542 

Adjusted R-squared:

   x1    x2     y 
0.497 0.096 0.533 
> # Add ,show.imputed.actual for graphical diagnostics
> ## Not run: 
> ##D g <- fit.mult.impute(y ~ product + ratio, ols, w,
> ##D                      data=data.frame(x1,x2,y),
> ##D                      derived=expression({
> ##D                        product <- x1*x2
> ##D                        ratio   <- x1/(1+x2)
> ##D                        print(cbind(x1,x2,x1*x2,product)[1:6,])}))
> ## End(Not run)
> 
> 
> # Here's a method for creating a permanent data frame containing
> # one set of imputed values for each variable specified to transcan
> # that had at least one NA, and also containing all the variables
> # in an original data frame.  The following is based on the fact
> # that the default output location for impute.transcan is
> # given by the global environment
> 
> 
> ## Not run: 
> ##D xt <- transcan(~. , data=mine,
> ##D                imputed=TRUE, shrink=TRUE, n.impute=10, trantab=TRUE)
> ##D attach(mine, use.names=FALSE)
> ##D impute(xt, imputation=1) # use first imputation
> ##D # omit imputation= if using single imputation
> ##D detach(1, 'mine2')
> ## End(Not run)
> 
> 
> # Example of using invertTabulated outside transcan
> x    <- c(1,2,3,4,5,6,7,8,9,10)
> y    <- c(1,2,3,4,5,5,5,5,9,10)
> freq <- c(1,1,1,1,1,2,3,4,1,1)
> # x=5,6,7,8 with prob. .1 .2 .3 .4 when y=5
> # Within a tolerance of .05*(10-1) all y's match exactly
> # so the distance measure does not play a role
> set.seed(1)      # so can reproduce
> for(inverse in c('linearInterp','sample'))
+  print(table(invertTabulated(x, y, freq, rep(5,1000), inverse=inverse)))

 6.5 
1000 

  5   6   7   8 
110 194 291 405 
> 
> 
> # Test inverse='sample' when the estimated transformation is
> # flat on the right.  First show default imputations
> set.seed(3)
> x <- rnorm(1000)
> y <- pmin(x, 0)
> x[1:500] <- NA
> for(inverse in c('linearInterp','sample')) {
+ par(mfrow=c(2,2))
+   w <- transcan(~ x + y, imputed.actual='hist',
+                 inverse=inverse, curtail=FALSE,
+                 data=data.frame(x,y))
+   if(inverse=='sample') next
+ # cat('Click mouse on graph to proceed\n')
+ # locator(1)
+ }
Convergence criterion:0.032 0.016 
Convergence in 3 iterations
R-squared achieved in predicting each variable:

x y 
1 1 

Adjusted R-squared:

x y 
1 1 
Convergence criterion:0.032 Warning in invertTabulated(x[!j, i], newy[!j], aty = newy[j], name = nam[i],  :
  No actual x has y value within 0.05* range(y) (7.35) of the following y values:5.4.
Consider increasing tolInverse. Used linear interpolation instead.
0.016 
Convergence in 3 iterations
R-squared achieved in predicting each variable:

x y 
1 1 

Adjusted R-squared:

x y 
1 1 
> 
> ## Not run: 
> ##D # While running multiple imputation for a logistic regression model
> ##D # Run the rms package validate and calibrate functions and save the
> ##D # results in w$funresults
> ##D a <- aregImpute(~ x1 + x2 + y, data=d, n.impute=10)
> ##D require(rms)
> ##D g <- function(fit)
> ##D   list(validate=validate(fit, B=50), calibrate=calibrate(fit, B=75))
> ##D w <- fit.mult.impute(y ~ x1 + x2, lrm, a, data=d, fun=g,
> ##D                      fitargs=list(x=TRUE, y=TRUE))
> ##D # Get all validate results in it's own list of length 10
> ##D r <- w$funresults
> ##D val <- lapply(r, function(x) x$validate)
> ##D cal <- lapply(r, function(x) x$calibrate)
> ##D # See rms processMI and https://hbiostat.org/rmsc/validate.html#sec-val-mival
> ## End(Not run)
> 
> ## Not run: 
> ##D # Account for within-subject correlation using the robust cluster sandwich
> ##D # covariance estimate in conjunction with Rubin's rule for multiple imputation
> ##D # rms package must be installed
> ##D a <- aregImpute(..., data=d)
> ##D f <- fit.mult.impute(y ~ x1 + x2, lrm, a, n.impute=30, data=d, cluster=d$id)
> ##D # Get likelihood ratio chi-square tests accounting for missingness
> ##D a <- aregImpute(..., data=d)
> ##D h <- fit.mult.impute(y ~ x1 + x2, lrm, a, n.impute=40, data=d, lrt=TRUE)
> ##D processMI(h, which='anova')   # processMI is in rms
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("translate")
> ### * translate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: translate
> ### Title: Translate Vector or Matrix of Text Strings
> ### Aliases: translate
> ### Keywords: character
> 
> ### ** Examples
> 
> translate(c("ABC","DEF"),"ABCDEFG", "abcdefg")
[1] "abc" "def"
> translate("23.12","[.]","\\cdot ") # change . to \cdot
[1] "23\004ot 12"
> translate(c("dog","cat","tiger"),c("dog","cat"),c("DOG","CAT"))
[1] "DOG"   "CAT"   "tiger"
> # S-Plus gives  [1] "DOG"   "CAT"   "tiger" - check discrepency
> translate(c("dog","cat2","snake"),c("dog","cat"),"animal")
[1] "animal"  "animal2" "snake"  
> # S-Plus gives  [1] "animal"  "animal2" "snake" 
> 
> 
> 
> cleanEx()
> nameEx("trunc.POSIXt")
> ### * trunc.POSIXt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trunc.POSIXt
> ### Title: Return the floor, ceiling, or rounded value of date or time to
> ###   specified unit.
> ### Aliases: truncPOSIXt ceil.POSIXt ceil ceil.default roundPOSIXt
> ### Keywords: manip utilities chron
> 
> ### ** Examples
> 
> date <- ISOdate(1832, 7, 12)
> ceil(date, units='months')  # '1832-8-1'
[1] "1832-08-01 GMT"
> truncPOSIXt(date, units='years')     # '1832-1-1'
[1] "1832-01-01 GMT"
> roundPOSIXt(date, digits='months')    # '1832-7-1'
[1] "1832-07-01 GMT"
> 
> 
> 
> cleanEx()
> nameEx("units")
> ### * units
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: units
> ### Title: Units Attribute of a Vector
> ### Aliases: units units.default units.Surv units<-.default
> ### Keywords: utilities interface
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> fail.time <- c(10,20)
> units(fail.time) <- "Day"
> describe(fail.time)
fail.time [Day] 
       n  missing distinct     Info     Mean      Gmd 
       2        0        2        1       15       10 
                  
Value       10  20
Frequency    1   1
Proportion 0.5 0.5
> S <- Surv(fail.time)
> units(S)
[1] "Day"
> 
> label(fail.time) <- 'Failure Time'
> fail.time
Failure Time [Day] 
[1] 10 20
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("upData")
> ### * upData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: upData
> ### Title: Update a Data Frame or Cleanup a Data Frame after Importing
> ### Aliases: cleanup.import upData dataframeReduce
> ### Keywords: data manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat <- read.table('myfile.asc')
> ##D dat <- cleanup.import(dat)
> ## End(Not run)
> dat <- data.frame(a=1:3, d=c('01/02/2004',' 1/3/04',''))
> cleanup.import(dat, datevars='d', dateformat='%m/%d/%y', fixdates='year')
  a          d
1 1 2004-01-02
2 2 2004-01-03
3 3       <NA>
> 
> dat <- data.frame(a=(1:3)/7, y=c('a','b1','b2'), z=1:3)
> dat2 <- upData(dat, x=x^2, x=x-5, m=x/10, 
+                rename=c(a='x'), drop='z',
+                labels=c(x='X', y='test'),
+                levels=list(y=list(a='a',b=c('b1','b2'))))
Input object size:	 1232 bytes;	 3 variables	 3 observations
Renamed variable	 a 	to x 
Modified variable	x
Modified variable	x
Added variable		m
Dropped variable	z
New object size:	2392 bytes;	3 variables	3 observations
> dat2
      x y      m
1 -4.98 a -0.498
2 -4.92 b -0.492
3 -4.82 b -0.482
> describe(dat2)
dat2 

 3  Variables      3  Observations
--------------------------------------------------------------------------------
x : X 
       n  missing distinct     Info     Mean      Gmd 
       3        0        3        1   -4.905   0.1088 
                            
Value      -4.98 -4.92 -4.82
Frequency      1     1     1
Proportion 0.333 0.333 0.333

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
y : test 
       n  missing distinct 
       3        0        2 
                      
Value          a     b
Frequency      1     2
Proportion 0.333 0.667
--------------------------------------------------------------------------------
m 
       n  missing distinct     Info     Mean      Gmd 
       3        0        3        1  -0.4905  0.01088 
                               
Value      -0.498 -0.492 -0.482
Frequency       1      1      1
Proportion  0.333  0.333  0.333

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
> dat <- dat2    # copy to original name and delete dat2 if OK
> rm(dat2)
> dat3 <- upData(dat, X=X^2, subset = x < (3/7)^2 - 5, rename=c(x='X'))
Input object size:	 2392 bytes;	 3 variables	 3 observations
Renamed variable	 x 	to X 
Modified variable	X
New object size:	2352 bytes;	3 variables	2 observations
> 
> # Remove hard to analyze variables from a redundancy analysis of all
> # variables in the data frame
> d <- dataframeReduce(dat, fracmiss=.1, minprev=.05, maxlevels=5)
> # Could run redun(~., data=d) at this point or include dataframeReduce
> # arguments in the call to redun
> 
> # If you import a SAS dataset created by PROC CONTENTS CNTLOUT=x.datadict,
> # the LABELs from this dataset can be added to the data.  Let's also
> # convert names to lower case for the main data file
> ## Not run: 
> ##D mydata2 <- cleanup.import(mydata2, lowernames=TRUE, sasdict=datadict)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("upFirst")
> ### * upFirst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: upFirst
> ### Title: Change First Letters to Upper Case
> ### Aliases: upFirst
> 
> ### ** Examples
> 
> upFirst(c('this and that','that is Beyond question'))
[1] "This and That"           "That Is Beyond Question"
> 
> 
> 
> cleanEx()
> nameEx("valueTags")
> ### * valueTags
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: valueTags
> ### Title: Store Descriptive Information About an Object
> ### Aliases: valueTags valueTags<- valueLabel valueLabel<- valueUnit
> ###   valueUnit<- valueName valueName<-
> ### Keywords: attribute misc utilities
> 
> ### ** Examples
> 
> age <- c(21,65,43)
> y   <- 1:3
> valueLabel(age) <- "Age in Years"
> plot(age, y, xlab=valueLabel(age))
> 
> 
> x1 <- 1:10
> x2 <- 10:1
> valueLabel(x2) <- 'Label for x2'
> valueUnit(x2) <- 'mmHg'
> x2
Label for x2 [mmHg] 
 [1] 10  9  8  7  6  5  4  3  2  1
> x2[1:5]
Label for x2 [mmHg] 
[1] 10  9  8  7  6
> dframe <- data.frame(x1, x2)
> Label(dframe)
label(x1)	<- ''
label(x2)	<- 'Label for x2'
> 
> 
> ##In these examples of llist, note that labels are printed after
> ##variable names, because of print.labelled
> a <- 1:3
> b <- 4:6
> valueLabel(b) <- 'B Label'
> 
> 
> 
> cleanEx()
> nameEx("varclus")
> ### * varclus
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varclus
> ### Title: Variable Clustering
> ### Aliases: varclus print.varclus plot.varclus naclus naplot plotMultSim
> ###   na.pattern
> ### Keywords: cluster multivariate category manip
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(200)
> x2 <- rnorm(200)
> x3 <- x1 + x2 + rnorm(200)
> x4 <- x2 + rnorm(200)
> x <- cbind(x1,x2,x3,x4)
> v <- varclus(x, similarity="spear")  # spearman is the default anyway
> v    # invokes print.varclus
varclus(x = x, similarity = "spear")


Similarity matrix (Spearman rho^2)

     x1   x2   x3   x4
x1 1.00 0.00 0.26 0.00
x2 0.00 1.00 0.26 0.42
x3 0.26 0.26 1.00 0.12
x4 0.00 0.42 0.12 1.00

No. of observations used for each pair:

    x1  x2  x3  x4
x1 200 200 200 200
x2 200 200 200 200
x3 200 200 200 200
x4 200 200 200 200

hclust results (method=complete)


Call:
hclust(d = as.dist(1 - x), method = method)

Cluster method   : complete 
Number of objects: 4 

> print(round(v$sim,2))
     x1   x2   x3   x4
x1 1.00 0.00 0.26 0.00
x2 0.00 1.00 0.26 0.42
x3 0.26 0.26 1.00 0.12
x4 0.00 0.42 0.12 1.00
> plot(v)
> 
> 
> # plot(varclus(~ age + sys.bp + dias.bp + country - 1), abbrev=TRUE)
> # the -1 causes k dummies to be generated for k countries
> # plot(varclus(~ age + factor(disease.code) - 1))
> #
> #
> # use varclus(~., data= fracmiss= maxlevels= minprev=) to analyze all
> # "useful" variables - see dataframeReduce for details about arguments
> 
> 
> df <- data.frame(a=c(1,2,3),b=c(1,2,3),c=c(1,2,NA),d=c(1,NA,3),
+                  e=c(1,NA,3),f=c(NA,NA,NA),g=c(NA,2,3),h=c(NA,NA,3))
> par(mfrow=c(2,2))
> for(m in c("ward","complete","median")) {
+   plot(naclus(df, method=m))
+   title(m)
+ }
The "ward" method has been renamed to "ward.D"; note new "ward.D2"
> naplot(naclus(df))
> n <- naclus(df)
> plot(n); naplot(n)
> na.pattern(df)
pattern
00000111 00011101 00100100 
       1        1        1 
> 
> # plotMultSim example: Plot proportion of observations
> # for which two variables are both positive (diagonals
> # show the proportion of observations for which the
> # one variable is positive).  Chance-correct the
> # off-diagonals by subtracting the product of the
> # marginal proportions.  On each subplot the x-axis
> # shows month (0, 4, 8, 12) and there is a separate
> # curve for females and males
> d <- data.frame(sex=sample(c('female','male'),1000,TRUE),
+                 month=sample(c(0,4,8,12),1000,TRUE),
+                 x1=sample(0:1,1000,TRUE),
+                 x2=sample(0:1,1000,TRUE),
+                 x3=sample(0:1,1000,TRUE))
> s <- array(NA, c(3,3,4))
> opar <- par(mar=c(0,0,4.1,0))  # waste less space
> for(sx in c('female','male')) {
+   for(i in 1:4) {
+     mon <- (i-1)*4
+     s[,,i] <- varclus(~x1 + x2 + x3, sim='ccbothpos', data=d,
+                       subset=d$month==mon & d$sex==sx)$sim
+     }
+   plotMultSim(s, c(0,4,8,12), vname=c('x1','x2','x3'),
+               add=sx=='male', slimds=TRUE,
+               lty=1+(sx=='male'))
+   # slimds=TRUE causes separate  scaling for diagonals and
+   # off-diagonals
+ }
> par(opar)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("wtd.stats")
> ### * wtd.stats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wtd.stats
> ### Title: Weighted Statistical Estimates
> ### Aliases: wtd.mean wtd.var wtd.quantile wtd.Ecdf wtd.table wtd.rank
> ###   wtd.loess.noiter num.denom.setup
> ### Keywords: nonparametric category distribution robust loess smooth manip
> 
> ### ** Examples
> 
> set.seed(1)
> x <- runif(500)
> wts <- sample(1:6, 500, TRUE)
> std.dev <- sqrt(wtd.var(x, wts))
> wtd.quantile(x, wts)
     0%     25%     50%     75%    100% 
0.00184 0.26024 0.46155 0.73964 0.99608 
> death <- sample(0:1, 500, TRUE)
> plot(wtd.loess.noiter(x, death, wts, type='evaluate'))
> describe(~x, weights=wts)
x 

 2  Variables      500  Observations
--------------------------------------------------------------------------------
x 
       n  missing distinct     Info     Mean      .05      .10      .25 
    1780        0      500        1   0.4949  0.07068  0.12169  0.26024 
     .50      .75      .90      .95 
 0.46155  0.73964  0.91037  0.95373 

lowest : 0.00183686 0.00193283 0.0111495  0.0130776  0.0133903 
highest: 0.991839   0.991906   0.992684   0.993749   0.996077  
--------------------------------------------------------------------------------
(weights) 
       n  missing distinct     Info     Mean 
    1780        0        6    0.949    4.385 
                                              
Value          1     2     3     4     5     6
Frequency     79   174   219   328   470   510
Proportion 0.044 0.098 0.123 0.184 0.264 0.287

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
> # describe uses wtd.mean, wtd.quantile, wtd.table
> xg <- cut2(x,g=4)
> table(xg)
xg
[0.00184,0.258) [0.25817,0.476) [0.47635,0.740) [0.73964,0.996] 
            125             125             125             125 
> wtd.table(xg, wts, type='table')
[0.00184,0.258) [0.25817,0.476) [0.47635,0.740) [0.73964,0.996] 
            444             467             421             448 
> 
> # Here is a method for getting stratified weighted means
> y <- runif(500)
> g <- function(y) wtd.mean(y[,1],y[,2])
> summarize(cbind(y, wts), llist(xg), g, stat.name='y')
               xg     y
1 [0.00184,0.258) 0.444
2 [0.25817,0.476) 0.500
3 [0.47635,0.740) 0.512
4 [0.73964,0.996] 0.496
> 
> # Empirically determine how methods used by wtd.quantile match with
> # methods used by quantile, when all weights are unity
> set.seed(1)
> u <-  eval(formals(wtd.quantile)$type)
> v <- as.character(1:9)
> r <- matrix(0, nrow=length(u), ncol=9, dimnames=list(u,v))
> 
> for(n in c(8, 13, 22, 29))
+   {
+     x <- rnorm(n)
+     for(i in 1:5) {
+       probs <- sort( runif(9))
+       for(wtype in u) {
+         wq <- wtd.quantile(x, type=wtype, weights=rep(1,length(x)), probs=probs)
+         for(qtype in 1:9) {
+           rq <- quantile(x, type=qtype, probs=probs)
+           r[wtype, qtype] <- max(r[wtype,qtype], max(abs(wq-rq)))
+         }
+       }
+     }
+   }
> 
> r
                1     2     3        4     5        6        7     8     9
quantile    0.573 0.573 0.755 5.02e-01 0.373 6.42e-01 0.00e+00 0.472 0.442
(i-1)/(n-1) 0.573 0.573 0.755 5.02e-01 0.373 6.42e-01 8.88e-16 0.472 0.442
i/(n+1)     0.709 0.709 0.857 7.20e-01 0.321 4.44e-16 6.42e-01 0.214 0.241
i/n         0.777 0.777 0.409 4.44e-16 0.428 7.20e-01 5.02e-01 0.546 0.517
> 
> # Restructure data to generate a dichotomous response variable
> # from records containing numbers of events and numbers of trials
> num   <- c(10,NA,20,0,15)   # data are 10/12 NA/999 20/20 0/25 15/35
> denom <- c(12,999,20,25,35)
> w     <- num.denom.setup(num, denom)
> w
$subs
[1] 1 3 5 1 4 5

$weights
[1] 10 20 15  2 25 20

$y
[1] 1 1 1 0 0 0

> # attach(my.data.frame[w$subs,])
> 
> 
> 
> cleanEx()
> nameEx("xYplot")
> ### * xYplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xYplot
> ### Title: xyplot and dotplot with Matrix Variables to Plot Error Bars and
> ###   Bands
> ### Aliases: xYplot panel.xYplot prepanel.xYplot Dotplot panel.Dotplot
> ###   prepanel.Dotplot Cbind [.Cbind setTrellis numericScale
> ### Keywords: hplot
> 
> ### ** Examples
> 
> # Plot 6 smooth functions.  Superpose 3, panel 2.
> # Label curves with p=1,2,3 where most separated 
> d <- expand.grid(x=seq(0,2*pi,length=150), p=1:3, shift=c(0,pi)) 
> xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, type='l') 
> # Use a key instead, use 3 line widths instead of 3 colors 
> # Put key in most empty portion of each panel
> xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, 
+        type='l', keys='lines', lwd=1:3, col=1) 
> # Instead of implicitly using labcurve(), put a 
> # single key outside of panels at lower left corner
> xYplot(sin(x+shift)^p ~ x | shift, groups=p, data=d, 
+        type='l', label.curves=FALSE, lwd=1:3, col=1, lty=1:3) 
> Key()
> 
> # Bubble plots
> x <- y <- 1:8
> x[2] <- NA
> units(x) <- 'cm^2'
> z <- 101:108
> p <- factor(rep(c('a','b'),4))
> g <- c(rep(1,7),2)
> data.frame(p, x, y, z, g)
  p  x y   z g
1 a  1 1 101 1
2 b NA 2 102 1
3 a  3 3 103 1
4 b  4 4 104 1
5 a  5 5 105 1
6 b  6 6 106 1
7 a  7 7 107 1
8 b  8 8 108 2
> xYplot(y ~ x | p, groups=g, size=z)
>  Key(other=list(title='g', cex.title=1.2))  # draw key for colors
> sKey(.2,.85,other=list(title='Z Values', cex.title=1.2))
> # draw key for character sizes
> 
> # Show the median and quartiles of height given age, stratified 
> # by sex and race.  Draws 2 sets (male, female) of 3 lines per panel.
> # xYplot(height ~ age | race, groups=sex, method='quantiles')
> 
> 
> # Examples of plotting raw data
> dfr <- expand.grid(month=1:12, continent=c('Europe','USA'), 
+                    sex=c('female','male'))
> set.seed(1)
> dfr <- upData(dfr,
+               y=month/10 + 1*(sex=='female') + 2*(continent=='Europe') + 
+                 runif(48,-.15,.15),
+               lower=y - runif(48,.05,.15),
+               upper=y + runif(48,.05,.15))
Input object size:	 5288 bytes;	 3 variables	 48 observations
Added variable		y
Added variable		lower
Added variable		upper
New object size:	6784 bytes;	6 variables	48 observations
> 
> 
> xYplot(Cbind(y,lower,upper) ~ month,subset=sex=='male' & continent=='USA',
+        data=dfr)
> xYplot(Cbind(y,lower,upper) ~ month|continent, subset=sex=='male',data=dfr)
> xYplot(Cbind(y,lower,upper) ~ month|continent, groups=sex, data=dfr); Key() 
> # add ,label.curves=FALSE to suppress use of labcurve to label curves where
> # farthest apart
> 
> 
> xYplot(Cbind(y,lower,upper) ~ month,groups=sex,
+                               subset=continent=='Europe', data=dfr) 
> xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b',
+                               subset=continent=='Europe', keys='lines',
+                               data=dfr)
> # keys='lines' causes labcurve to draw a legend where the panel is most empty
> 
> 
> xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b', data=dfr,
+                               subset=continent=='Europe',method='bands') 
> xYplot(Cbind(y,lower,upper) ~ month,groups=sex, type='b', data=dfr,
+                               subset=continent=='Europe',method='upper')
> 
> 
> label(dfr$y) <- 'Quality of Life Score'   
> # label is in Hmisc library = attr(y,'label') <- 'Quality\dots'; will be
> # y-axis label 
> # can also specify Cbind('Quality of Life Score'=y,lower,upper) 
> xYplot(Cbind(y,lower,upper) ~ month, groups=sex,
+        subset=continent=='Europe', method='alt bars',
+         offset=grid::unit(.1,'inches'), type='b', data=dfr)   
> # offset passed to labcurve to label .4 y units away from curve
> # for R (using grid/lattice), offset is specified using the grid
> # unit function, e.g., offset=grid::unit(.4,'native') or
> # offset=grid::unit(.1,'inches') or grid::unit(.05,'npc')
> 
> 
> # The following example uses the summarize function in Hmisc to 
> # compute the median and outer quartiles.  The outer quartiles are 
> # displayed using "error bars"
> set.seed(111)
> dfr <- expand.grid(month=1:12, year=c(1997,1998), reps=1:100)
> month <- dfr$month; year <- dfr$year
> y <- abs(month-6.5) + 2*runif(length(month)) + year-1997
> s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 
> xYplot(Cbind(y,Lower,Upper) ~ month, groups=year, data=s, 
+        keys='lines', method='alt', type='b')
> # Can also do:
> s <- summarize(y, llist(month,year), quantile, probs=c(.5,.25,.75),
+                stat.name=c('y','Q1','Q3')) 
> xYplot(Cbind(y, Q1, Q3) ~ month, groups=year, data=s, 
+        type='b', keys='lines') 
> # Or:
> xYplot(y ~ month, groups=year, keys='lines', nx=FALSE, method='quantile',
+        type='b') 
> # nx=FALSE means to treat month as a discrete variable
> 
> 
> # To display means and bootstrapped nonparametric confidence intervals 
> # use:
> s <- summarize(y, llist(month,year), smean.cl.boot) 
> s
   month year    y Lower Upper
1      1 1997 6.47  6.36  6.59
2      1 1998 7.53  7.41  7.64
9      2 1997 5.51  5.40  5.62
10     2 1998 6.45  6.35  6.56
11     3 1997 4.56  4.45  4.66
12     3 1998 5.47  5.36  5.57
13     4 1997 3.46  3.36  3.56
14     4 1998 4.42  4.31  4.51
15     5 1997 2.54  2.44  2.66
16     5 1998 3.47  3.36  3.57
17     6 1997 1.44  1.33  1.56
18     6 1998 2.53  2.42  2.64
19     7 1997 1.52  1.40  1.64
20     7 1998 2.55  2.43  2.66
21     8 1997 2.52  2.41  2.63
22     8 1998 3.53  3.41  3.65
23     9 1997 3.46  3.36  3.57
24     9 1998 4.52  4.41  4.63
3     10 1997 4.46  4.36  4.57
4     10 1998 5.50  5.39  5.60
5     11 1997 5.42  5.30  5.53
6     11 1998 6.56  6.44  6.67
7     12 1997 6.57  6.46  6.69
8     12 1998 7.48  7.37  7.59
> xYplot(Cbind(y, Lower, Upper) ~ month | year, data=s, type='b')
> # Can also use Y <- cbind(y, Lower, Upper); xYplot(Cbind(Y) ~ ...) 
> # Or:
> xYplot(y ~ month | year, nx=FALSE, method=smean.cl.boot, type='b')
> 
> 
> # This example uses the summarize function in Hmisc to 
> # compute the median and outer quartiles.  The outer quartiles are 
> # displayed using "filled bands"
> 
> 
> s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 
> 
> 
> # filled bands: default fill = pastel colors matching solid colors
> # in superpose.line (this works differently in R)
> xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
+      method="filled bands" , data=s, type="l")
> 
> 
> # note colors based on levels of selected subgroups, not first two colors
> xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
+      method="filled bands" , data=s, type="l",
+      subset=(year == 1998 | year == 2000), label.curves=FALSE )
> 
> 
> # filled bands using black lines with selected solid colors for fill
> xYplot ( Cbind ( y, Lower, Upper ) ~ month, groups=year, 
+      method="filled bands" , data=s, label.curves=FALSE,
+      type="l", col=1, col.fill = 2:3)
> Key(.5,.8,col = 2:3) #use fill colors in key
> 
> 
> # A good way to check for stable variance of residuals from ols 
> # xYplot(resid(fit) ~ fitted(fit), method=smean.sdl) 
> # smean.sdl is defined with summary.formula in Hmisc
> 
> 
> # Plot y vs. a special variable x
> # xYplot(y ~ numericScale(x, label='Label for X') | country) 
> # For this example could omit label= and specify 
> #    y ~ numericScale(x) | country, xlab='Label for X'
> 
> 
> # Here is an example of using xYplot with several options
> # to change various Trellis parameters,
> # xYplot(y ~ x | z, groups=v, pch=c('1','2','3'),
> #        layout=c(3,1),     # 3 panels side by side
> #        ylab='Y Label', xlab='X Label',
> #        main=list('Main Title', cex=1.5),
> #        par.strip.text=list(cex=1.2),
> #        strip=function(\dots) strip.default(\dots, style=1),
> #        scales=list(alternating=FALSE))
> 
> 
> #
> # Dotplot examples
> #
> 
> 
> s <- summarize(y, llist(month,year), smedian.hilow, conf.int=.5) 
> 
> 
> setTrellis()            # blank conditioning panel backgrounds 
> Dotplot(month ~ Cbind(y, Lower, Upper) | year, data=s) 
> # or Cbind(\dots), groups=year, data=s
> 
> 
> # Display a 5-number (5-quantile) summary (2 intervals, dot=median) 
> # Note that summarize produces a matrix for y, and Cbind(y) trusts the 
> # first column to be the point estimate (here the median) 
> s <- summarize(y, llist(month,year), quantile,
+                probs=c(.5,.05,.25,.75,.95), type='matrix') 
> Dotplot(month ~ Cbind(y) | year, data=s) 
> # Use factor(year) to make actual years appear in conditioning title strips
> 
> # Plot proportions and their Wilson confidence limits
> set.seed(3)
> d <- expand.grid(continent=c('USA','Europe'), year=1999:2001,
+                  reps=1:100)
> # Generate binary events from a population probability of 0.2
> # of the event, same for all years and continents
> d$y <- ifelse(runif(6*100) <= .2, 1, 0)
> s <- with(d,
+           summarize(y, llist(continent,year),
+                     function(y) {
+                      n <- sum(!is.na(y))
+                      s <- sum(y, na.rm=TRUE)
+                      binconf(s, n)
+                     }, type='matrix')
+ )
> 
> Dotplot(year ~ Cbind(y) | continent,  data=s, ylab='Year',
+         xlab='Probability')
> 
> 
> # Dotplot(z ~ x | g1*g2)                 
> # 2-way conditioning 
> # Dotplot(z ~ x | g1, groups=g2); Key()  
> # Key defines symbols for g2
> 
> 
> # If the data are organized so that the mean, lower, and upper 
> # confidence limits are in separate records, the Hmisc reShape 
> # function is useful for assembling these 3 values as 3 variables 
> # a single observation, e.g., assuming type has values such as 
> # c('Mean','Lower','Upper'):
> # a <- reShape(y, id=month, colvar=type) 
> # This will make a matrix with 3 columns named Mean Lower Upper 
> # and with 1/3 as many rows as the original data 
> 
> 
> 
> cleanEx()
> nameEx("xy.group")
> ### * xy.group
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xy.group
> ### Title: Mean x vs. function of y in groups of x
> ### Aliases: xy.group
> ### Keywords: category nonparametric
> 
> ### ** Examples
> 
> # plot(xy.group(x, y, g=10))	#Plot mean y by deciles of x
> # xy.group(x, y, m=100, result="matrix")	#Print table, 100 obs/group
> 
> 
> 
> cleanEx()
> nameEx("ynbind")
> ### * ynbind
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ynbind
> ### Title: Combine Variables in a Matrix
> ### Aliases: ynbind [.ynbind pBlock [.pBlock
> ### Keywords: misc utilities
> 
> ### ** Examples
> 
> x1 <- c('yEs', 'no', 'UNKNOWN', NA)
> x2 <- c('y', 'n', 'no', 'present')
> label(x2) <- 'X2'
> X <- ynbind(x1, x2, label='x1-2')
> X[1:3,]
        x1    x2
[1,]  TRUE  TRUE
[2,] FALSE FALSE
[3,]    NA FALSE
attr(,"label")
[1] "x1-2"
attr(,"labels")
[1] "x1" "X2"
attr(,"class")
[1] "ynbind"
> 
> pBlock(x1, x2, subset=2:3, label='x1-2')
     x1        x2  
[1,] NA        NA  
[2,] "no"      "n" 
[3,] "UNKNOWN" "no"
[4,] NA        NA  
attr(,"label")
[1] "x1-2"
attr(,"labels")
[1] "x1" "X2"
attr(,"class")
[1] "pBlock" "matrix"
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  28.559 1.644 29.026 0.008 0.013 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
