
R version 4.5.1 (2025-06-13) -- "Great Square Root"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "rms"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('rms')
Loading required package: Hmisc

Attaching package: ‘Hmisc’

The following objects are masked from ‘package:base’:

    format.pval, units

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("ExProb")
> ### * ExProb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ExProb
> ### Title: Function Generators For Exceedance and Survival Probabilities
> ### Aliases: ExProb ExProb.orm Survival.orm plot.ExProb
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- runif(200)
> yvar <- x1 + runif(200)
> f <- orm(yvar ~ x1)
> d <- ExProb(f)
> lp <- predict(f, newdata=data.frame(x1=c(.2,.8)))
> w <- d(lp)
> s1 <- abs(x1 - .2) < .1
> s2 <- abs(x1 - .8) < .1
> plot(w, data=data.frame(x1=c(rep(.2, sum(s1)), rep(.8, sum(s2))),
+                         yvar=c(yvar[s1], yvar[s2])))
> 
> qu <- Quantile(f)
> abline(h=c(.1,.5), col='gray80')
> abline(v=qu(.5, lp), col='gray80')
> abline(v=qu(.9, lp), col='green')
> ## Not run: 
> ##D   Y <- Ocens(dtime, ifelse(censored, Inf, dtime))
> ##D   f <- orm(Y ~ x, family='loglog')
> ##D   s <- Survival(f)
> ##D   s()  # all times
> ##D   s(times=c(1, 3))
> ##D   d <- data.frame(x=2:4)
> ##D   s(X=predict(f, d, conf.int=0.95)  # all times
> ##D   s(lp=predict(f, d))  # same surv estimates, no CLs
> ##D   # use s(..., forcedf=TRUE) to force output to be a data.frame
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Function")
> ### * Function
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Function
> ### Title: Compose an S Function to Compute X beta from a Fit
> ### Aliases: Function.rms Function.cph sascode perlcode
> ### Keywords: regression methods interface models survival math
> 
> ### ** Examples
> 
> suppressWarnings(RNGversion("3.5.0"))
> set.seed(1331)
> x1 <- exp(rnorm(100))
> x2 <- factor(sample(c('a','b'),100,rep=TRUE))
> dd <- datadist(x1, x2)
> options(datadist='dd')
> y  <- log(x1)^2+log(x1)*(x2=='b')+rnorm(100)/4
> f  <- ols(y ~ pol(log(x1),2)*x2)
> f$coef
   Intercept           x1         x1^2         x2=b    x1 * x2=b  x1^2 * x2=b 
 0.050809859  0.012335335  0.991077315 -0.049218199  1.001128354 -0.003440629 
> g  <- Function(f, digits=5)
> g
function (x1 = 1.0906, x2 = "a") 
{
    x1 <- log(x1)
    0.05081 + 0.012335 * x1 + 0.99108 * x1^2 - 0.049218 * (x2 == 
        "b") + (x2 == "b") * (1.0011 * x1 - 0.0034406 * x1^2)
}
<environment: 0x564d6f40bf58>
> sascode(g)
    x1 = log(x1)
    0.05081 + 0.012335 * x1 + 0.99108 * x1**2 - 0.049218 * (x2 = 
        "b") + (x2 = "b") * (1.0011 * x1 - 0.0034406 * x1**2);
> cat(perlcode(g), '\n')
use List::Util 'max', 'min';
sub g {
  my $x1 = $_[0];
  my $x2 = $_[1];
  $x1 = log($x1);
  0.05081 + 0.012335 * $x1 + 0.99108 * $x1 ** 2 - 0.049218 * (($x2 eq "b") ? 1 : 0) + (($x2 eq "b") ? 1 : 0) * (1.0011 * $x1 - 0.0034406 * $x1 ** 2)
} 
> g()
[1] 0.05933444
> g(x1=c(2,3), x2='b')   #could omit x2 since b is default category
[1] 1.178566 2.306994
> predict(f, expand.grid(x1=c(2,3),x2='b'))
       1        2 
1.178584 2.307022 
> g8 <- Function(f)   # default is 8 sig. digits
> g8(x1=c(2,3), x2='b')
[1] 1.178584 2.307022
> options(datadist=NULL)
> 
> 
> ## Not run: 
> ##D require(survival)
> ##D # Make self-contained functions for computing survival probabilities
> ##D # using a log-normal regression
> ##D f <- psm(Surv(d.time, death) ~ rcs(age,4)*sex, dist='gaussian')
> ##D g <- Function(f)
> ##D surv <- Survival(f)
> ##D # Compute 2 and 5-year survival estimates for 50 year old male
> ##D surv(c(2,5), g(age=50, sex='male'))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Glm")
> ### * Glm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Glm
> ### Title: rms Version of glm
> ### Aliases: Glm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> ## Dobson (1990) Page 93: Randomized Controlled Trial :
> counts <- c(18,17,15,20,10,20,25,13,12)
> outcome <- gl(3,1,9)
> treatment <- gl(3,3)
> f <- glm(counts ~ outcome + treatment, family=poisson())
> f

Call:  glm(formula = counts ~ outcome + treatment, family = poisson())

Coefficients:
(Intercept)     outcome2     outcome3   treatment2   treatment3  
  3.045e+00   -4.543e-01   -2.930e-01    1.011e-15    7.105e-16  

Degrees of Freedom: 8 Total (i.e. Null);  4 Residual
Null Deviance:	    10.58 
Residual Deviance: 5.129 	AIC: 56.76
> anova(f)
Analysis of Deviance Table

Model: poisson, link: log

Response: counts

Terms added sequentially (first to last)


          Df Deviance Resid. Df Resid. Dev Pr(>Chi)  
NULL                          8    10.5814           
outcome    2   5.4523         6     5.1291  0.06547 .
treatment  2   0.0000         4     5.1291  1.00000  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> summary(f)

Call:
glm(formula = counts ~ outcome + treatment, family = poisson())

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.045e+00  1.709e-01  17.815   <2e-16 ***
outcome2    -4.543e-01  2.022e-01  -2.247   0.0246 *  
outcome3    -2.930e-01  1.927e-01  -1.520   0.1285    
treatment2   1.011e-15  2.000e-01   0.000   1.0000    
treatment3   7.105e-16  2.000e-01   0.000   1.0000    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 10.5814  on 8  degrees of freedom
Residual deviance:  5.1291  on 4  degrees of freedom
AIC: 56.761

Number of Fisher Scoring iterations: 4

> f <- Glm(counts ~ outcome + treatment, family=poisson())
> # could have had rcs( ) etc. if there were continuous predictors
> f
General Linear Model

Glm(formula = counts ~ outcome + treatment, family = poisson())

                   Model Likelihood    
                         Ratio Test    
       Obs   9    LR chi2      5.45    
Residual d.f.4    d.f.            4    
       g 0.227    Pr(> chi2) 0.2440    

            Coef    S.E.   Wald Z Pr(>|Z|)
Intercept    3.0445 0.1709 17.81  <0.0001 
outcome=2   -0.4543 0.2022 -2.25  0.0246  
outcome=3   -0.2930 0.1927 -1.52  0.1285  
treatment=2  0.0000 0.2000  0.00  1.0000  
treatment=3  0.0000 0.2000  0.00  1.0000  

> anova(f)
                Wald Statistics          Response: counts 

 Factor     Chi-Square d.f. P     
 outcome    5.49       2    0.0643
 treatment  0.00       2    1.0000
 TOTAL      5.49       4    0.2409
> summary(f, outcome=c('1','2','3'), treatment=c('1','2','3'))
             Effects              Response : counts 

 Factor          Low High Diff. Effect      S.E.    Lower 0.95 Upper 0.95
 outcome - 1:2   2   1    NA     4.5426e-01 0.20217 -0.10706   1.01560   
 outcome - 3:2   2   3    NA     1.6127e-01 0.21512 -0.43600   0.75854   
 treatment - 1:2 2   1    NA    -1.0115e-15 0.20000 -0.55529   0.55529   
 treatment - 3:2 2   3    NA    -3.0094e-16 0.20000 -0.55529   0.55529   

> 
> 
> 
> 
> cleanEx()
> nameEx("Gls")
> ### * Gls
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gls
> ### Title: Fit Linear Model Using Generalized Least Squares
> ### Aliases: Gls print.Gls
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D require(ggplot2)
> ##D ns  <- 20  # no. subjects
> ##D nt  <- 10  # no. time points/subject
> ##D B   <- 10  # no. bootstrap resamples
> ##D            # usually do 100 for variances, 1000 for nonparametric CLs
> ##D rho <- .5  # AR(1) correlation parameter
> ##D V <- matrix(0, nrow=nt, ncol=nt)
> ##D V <- rho^abs(row(V)-col(V))   # per-subject correlation/covariance matrix
> ##D 
> ##D d <- expand.grid(tim=1:nt, id=1:ns)
> ##D d$trt <- factor(ifelse(d$id <= ns/2, 'a', 'b'))
> ##D true.beta <- c(Intercept=0,tim=.1,'tim^2'=0,'trt=b'=1)
> ##D d$ey  <- true.beta['Intercept'] + true.beta['tim']*d$tim +
> ##D   true.beta['tim^2']*(d$tim^2) +  true.beta['trt=b']*(d$trt=='b')
> ##D set.seed(13)
> ##D library(MASS)   # needed for mvrnorm
> ##D d$y <- d$ey + as.vector(t(mvrnorm(n=ns, mu=rep(0,nt), Sigma=V)))
> ##D 
> ##D dd <- datadist(d); options(datadist='dd')
> ##D f <- Gls(y ~ pol(tim,2) + trt, correlation=corCAR1(form= ~tim | id),
> ##D          data=d, B=B)
> ##D f
> ##D AIC(f)
> ##D f$var      # bootstrap variances
> ##D f$varBeta  # original variances
> ##D summary(f)
> ##D anova(f)
> ##D ggplot(Predict(f, tim, trt))
> ##D # v <- Variogram(f, form=~tim|id, data=d)
> ##D nlme:::summary.gls(f)$tTable   # print matrix of estimates etc.
> ##D 
> ##D options(datadist=NULL)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("LRupdate")
> ### * LRupdate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: LRupdate
> ### Title: LRupdate
> ### Aliases: LRupdate
> 
> ### ** Examples
> 
> ## Not run: 
> ##D a <- aregImpute(~ y + x1 + x2, n.impute=30, data=d)
> ##D f <- fit.mult.impute(y ~ x1 + x2, lrm, a, data=d, lrt=TRUE)
> ##D a <- processMI(f, 'anova')
> ##D f <- LRupdate(f, a)
> ##D print(f, r2=1:4)   # print all imputation-corrected R2 measures
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Ocens2Surv")
> ### * Ocens2Surv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Ocens2Surv
> ### Title: Ocens2Surv
> ### Aliases: Ocens2Surv
> 
> ### ** Examples
> 
> Y <- Ocens(1:3, c(1, Inf, 3))
> Ocens2Surv(Y)
[1] 1  2+ 3 
> 
> 
> 
> cleanEx()
> nameEx("Olinks")
> ### * Olinks
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Olinks
> ### Title: Likehood-Based Statistics for Other Links for orm Fits
> ### Aliases: Olinks
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- orm(y ~ x1 + x2, family='loglog', x=TRUE, y=TRUE)
> ##D Olinks(f)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Predict")
> ### * Predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict
> ### Title: Compute Predicted Values and Confidence Limits
> ### Aliases: Predict print.Predict rbind.Predict
> ### Keywords: models
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)))
> Predict(fit, age, cholesterol, np=4)
   blood.pressure    sex      age cholesterol         yhat       lower
1        119.3427 female 27.02467    140.5738 -0.489704089 -1.44380472
2        119.3427 female 42.94044    140.5738  0.009444021 -0.86177082
3        119.3427 female 58.85622    140.5738  0.508592131 -0.36642995
4        119.3427 female 74.77199    140.5738  1.007740242  0.04324397
5        119.3427 female 27.02467    178.5845 -0.811728269 -1.32879988
6        119.3427 female 42.94044    178.5845 -0.312580159 -0.63783120
7        119.3427 female 58.85622    178.5845  0.186567952 -0.13304821
8        119.3427 female 74.77199    178.5845  0.685716062  0.17929550
9        119.3427 female 27.02467    216.5951 -0.713351908 -1.22443989
10       119.3427 female 42.94044    216.5951 -0.214203797 -0.54326027
11       119.3427 female 58.85622    216.5951  0.284944313 -0.05163622
12       119.3427 female 74.77199    216.5951  0.784092423  0.25851103
13       119.3427 female 27.02467    254.6058 -1.165706237 -1.97778739
14       119.3427 female 42.94044    254.6058 -0.666558126 -1.38339415
15       119.3427 female 58.85622    254.6058 -0.167410016 -0.89278786
16       119.3427 female 74.77199    254.6058  0.331738094 -0.50278765
         upper
1   0.46439654
2   0.88065887
3   1.38361422
4   1.97223651
5  -0.29465666
6   0.01267089
7   0.50618411
8   1.19213663
9  -0.20226392
10  0.11485268
11  0.62152485
12  1.30967381
13 -0.35362508
14  0.05027790
15  0.55796783
16  1.16626383

Response variable (y): log odds 

Adjust to: blood.pressure=119.3 sex=female  

Limits are 0.95 confidence limits
> Predict(fit, age=seq(20,80,by=10), sex, conf.int=FALSE)
   blood.pressure    sex age cholesterol        yhat
1        119.3427 female  20    200.4846 -0.97757337
2        119.3427   male  20    200.4846 -1.06626523
3        119.3427 female  30    200.4846 -0.66395487
4        119.3427   male  30    200.4846 -0.58100169
5        119.3427 female  40    200.4846 -0.35033638
6        119.3427   male  40    200.4846 -0.09573815
7        119.3427 female  50    200.4846 -0.03671788
8        119.3427   male  50    200.4846  0.38952539
9        119.3427 female  60    200.4846  0.27690061
10       119.3427   male  60    200.4846  0.87478893
11       119.3427 female  70    200.4846  0.59051911
12       119.3427   male  70    200.4846  1.36005247
13       119.3427 female  80    200.4846  0.90413760
14       119.3427   male  80    200.4846  1.84531601

Response variable (y): log odds 

Adjust to: blood.pressure=119.3 cholesterol=200.5  
> Predict(fit, age=seq(20,80,by=10), sex='male')  # works if datadist not used
  blood.pressure  sex age cholesterol        yhat      lower      upper
1       119.3427 male  20    200.4846 -1.06626523 -1.6921173 -0.4404131
2       119.3427 male  30    200.4846 -0.58100169 -1.0301414 -0.1318620
3       119.3427 male  40    200.4846 -0.09573815 -0.3984671  0.2069908
4       119.3427 male  50    200.4846  0.38952539  0.1416426  0.6374082
5       119.3427 male  60    200.4846  0.87478893  0.5414277  1.2081501
6       119.3427 male  70    200.4846  1.36005247  0.8694445  1.8506605
7       119.3427 male  80    200.4846  1.84531601  1.1743872  2.5162448

Response variable (y): log odds 

Adjust to: blood.pressure=119.3 cholesterol=200.5  

Limits are 0.95 confidence limits
> # Get simultaneous confidence limits accounting for making 7 estimates
> # Predict(fit, age=seq(20,80,by=10), sex='male', conf.type='simult')
> # (this needs the multcomp package)
> 
> ddist$limits$age[2] <- 30    # make 30 the reference value for age
> # Could also do: ddist$limits["Adjust to","age"] <- 30
> fit <- update(fit)   # make new reference value take effect
> Predict(fit, age, ref.zero=TRUE, fun=exp)
    blood.pressure    sex      age cholesterol      yhat     lower     upper
1         119.3427 female 27.02467    200.4846 0.9109094 0.8642910 0.9600423
2         119.3427 female 27.26461    200.4846 0.9177897 0.8745162 0.9632045
3         119.3427 female 27.50454    200.4846 0.9247220 0.8848623 0.9663771
4         119.3427 female 27.74448    200.4846 0.9317066 0.8953309 0.9695602
5         119.3427 female 27.98441    200.4846 0.9387440 0.9059233 0.9727538
6         119.3427 female 28.22435    200.4846 0.9458345 0.9166410 0.9759579
7         119.3427 female 28.46429    200.4846 0.9529786 0.9274855 0.9791725
8         119.3427 female 28.70422    200.4846 0.9601767 0.9384583 0.9823977
9         119.3427 female 28.94416    200.4846 0.9674291 0.9495610 0.9856335
10        119.3427 female 29.18410    200.4846 0.9747364 0.9607950 0.9888800
11        119.3427 female 29.42403    200.4846 0.9820988 0.9721619 0.9921372
12        119.3427 female 29.66397    200.4846 0.9895168 0.9836632 0.9954052
13        119.3427 female 29.90391    200.4846 0.9969908 0.9953007 0.9986839
14        119.3427 female 30.14384    200.4846 1.0045213 1.0019733 1.0070758
15        119.3427 female 30.38378    200.4846 1.0121087 1.0052737 1.0189902
16        119.3427 female 30.62371    200.4846 1.0197534 1.0085849 1.0310456
17        119.3427 female 30.86365    200.4846 1.0274558 1.0119070 1.0432436
18        119.3427 female 31.10359    200.4846 1.0352164 1.0152400 1.0555859
19        119.3427 female 31.34352    200.4846 1.0430357 1.0185840 1.0680743
20        119.3427 female 31.58346    200.4846 1.0509139 1.0219391 1.0807104
21        119.3427 female 31.82340    200.4846 1.0588517 1.0253051 1.0934959
22        119.3427 female 32.06333    200.4846 1.0668495 1.0286823 1.1064328
23        119.3427 female 32.30327    200.4846 1.0749077 1.0320706 1.1195227
24        119.3427 female 32.54320    200.4846 1.0830267 1.0354701 1.1327674
25        119.3427 female 32.78314    200.4846 1.0912070 1.0388807 1.1461689
26        119.3427 female 33.02308    200.4846 1.0994492 1.0423026 1.1597289
27        119.3427 female 33.26301    200.4846 1.1077535 1.0457358 1.1734493
28        119.3427 female 33.50295    200.4846 1.1161207 1.0491802 1.1873321
29        119.3427 female 33.74289    200.4846 1.1245510 1.0526360 1.2013791
30        119.3427 female 33.98282    200.4846 1.1330450 1.0561032 1.2155922
31        119.3427 female 34.22276    200.4846 1.1416031 1.0595819 1.2299736
32        119.3427 female 34.46269    200.4846 1.1502259 1.0630719 1.2445250
33        119.3427 female 34.70263    200.4846 1.1589138 1.0665735 1.2592487
34        119.3427 female 34.94257    200.4846 1.1676674 1.0700866 1.2741465
35        119.3427 female 35.18250    200.4846 1.1764870 1.0736113 1.2892205
36        119.3427 female 35.42244    200.4846 1.1853733 1.0771476 1.3044730
37        119.3427 female 35.66238    200.4846 1.1943267 1.0806955 1.3199058
38        119.3427 female 35.90231    200.4846 1.2033477 1.0842551 1.3355212
39        119.3427 female 36.14225    200.4846 1.2124369 1.0878265 1.3513214
40        119.3427 female 36.38219    200.4846 1.2215947 1.0914096 1.3673085
41        119.3427 female 36.62212    200.4846 1.2308217 1.0950045 1.3834848
42        119.3427 female 36.86206    200.4846 1.2401184 1.0986112 1.3998524
43        119.3427 female 37.10199    200.4846 1.2494853 1.1022299 1.4164136
44        119.3427 female 37.34193    200.4846 1.2589229 1.1058604 1.4331708
45        119.3427 female 37.58187    200.4846 1.2684318 1.1095029 1.4501263
46        119.3427 female 37.82180    200.4846 1.2780126 1.1131574 1.4672823
47        119.3427 female 38.06174    200.4846 1.2876657 1.1168240 1.4846413
48        119.3427 female 38.30168    200.4846 1.2973917 1.1205026 1.5022057
49        119.3427 female 38.54161    200.4846 1.3071912 1.1241933 1.5199778
50        119.3427 female 38.78155    200.4846 1.3170648 1.1278962 1.5379603
51        119.3427 female 39.02148    200.4846 1.3270128 1.1316113 1.5561555
52        119.3427 female 39.26142    200.4846 1.3370361 1.1353386 1.5745659
53        119.3427 female 39.50136    200.4846 1.3471350 1.1390782 1.5931941
54        119.3427 female 39.74129    200.4846 1.3573102 1.1428302 1.6120428
55        119.3427 female 39.98123    200.4846 1.3675623 1.1465944 1.6311144
56        119.3427 female 40.22117    200.4846 1.3778918 1.1503711 1.6504116
57        119.3427 female 40.46110    200.4846 1.3882994 1.1541602 1.6699372
58        119.3427 female 40.70104    200.4846 1.3987855 1.1579618 1.6896938
59        119.3427 female 40.94097    200.4846 1.4093508 1.1617759 1.7096840
60        119.3427 female 41.18091    200.4846 1.4199960 1.1656026 1.7299108
61        119.3427 female 41.42085    200.4846 1.4307215 1.1694419 1.7503769
62        119.3427 female 41.66078    200.4846 1.4415281 1.1732939 1.7710851
63        119.3427 female 41.90072    200.4846 1.4524163 1.1771585 1.7920383
64        119.3427 female 42.14066    200.4846 1.4633867 1.1810358 1.8132394
65        119.3427 female 42.38059    200.4846 1.4744400 1.1849259 1.8346914
66        119.3427 female 42.62053    200.4846 1.4855768 1.1888289 1.8563971
67        119.3427 female 42.86047    200.4846 1.4967977 1.1927447 1.8783596
68        119.3427 female 43.10040    200.4846 1.5081034 1.1966734 1.9005819
69        119.3427 female 43.34034    200.4846 1.5194944 1.2006150 1.9230672
70        119.3427 female 43.58027    200.4846 1.5309715 1.2045696 1.9458185
71        119.3427 female 43.82021    200.4846 1.5425353 1.2085372 1.9688389
72        119.3427 female 44.06015    200.4846 1.5541864 1.2125179 1.9921317
73        119.3427 female 44.30008    200.4846 1.5659255 1.2165118 2.0157000
74        119.3427 female 44.54002    200.4846 1.5777533 1.2205187 2.0395472
75        119.3427 female 44.77996    200.4846 1.5896705 1.2245389 2.0636765
76        119.3427 female 45.01989    200.4846 1.6016776 1.2285723 2.0880913
77        119.3427 female 45.25983    200.4846 1.6137754 1.2326190 2.1127949
78        119.3427 female 45.49976    200.4846 1.6259647 1.2366791 2.1377908
79        119.3427 female 45.73970    200.4846 1.6382459 1.2407525 2.1630824
80        119.3427 female 45.97964    200.4846 1.6506200 1.2448393 2.1886732
81        119.3427 female 46.21957    200.4846 1.6630875 1.2489395 2.2145668
82        119.3427 female 46.45951    200.4846 1.6756492 1.2530533 2.2407667
83        119.3427 female 46.69945    200.4846 1.6883057 1.2571807 2.2672766
84        119.3427 female 46.93938    200.4846 1.7010579 1.2613216 2.2941001
85        119.3427 female 47.17932    200.4846 1.7139064 1.2654762 2.3212409
86        119.3427 female 47.41925    200.4846 1.7268519 1.2696444 2.3487029
87        119.3427 female 47.65919    200.4846 1.7398952 1.2738264 2.3764897
88        119.3427 female 47.89913    200.4846 1.7530370 1.2780222 2.4046053
89        119.3427 female 48.13906    200.4846 1.7662781 1.2822317 2.4330535
90        119.3427 female 48.37900    200.4846 1.7796192 1.2864552 2.4618383
91        119.3427 female 48.61894    200.4846 1.7930611 1.2906925 2.4909636
92        119.3427 female 48.85887    200.4846 1.8066045 1.2949438 2.5204335
93        119.3427 female 49.09881    200.4846 1.8202502 1.2992092 2.5502520
94        119.3427 female 49.33874    200.4846 1.8339990 1.3034885 2.5804233
95        119.3427 female 49.57868    200.4846 1.8478516 1.3077820 2.6109516
96        119.3427 female 49.81862    200.4846 1.8618088 1.3120896 2.6418410
97        119.3427 female 50.05855    200.4846 1.8758715 1.3164114 2.6730959
98        119.3427 female 50.29849    200.4846 1.8900404 1.3207474 2.7047205
99        119.3427 female 50.53843    200.4846 1.9043163 1.3250977 2.7367193
100       119.3427 female 50.77836    200.4846 1.9187000 1.3294623 2.7690967
101       119.3427 female 51.01830    200.4846 1.9331924 1.3338414 2.8018571
102       119.3427 female 51.25824    200.4846 1.9477942 1.3382348 2.8350051
103       119.3427 female 51.49817    200.4846 1.9625064 1.3426427 2.8685452
104       119.3427 female 51.73811    200.4846 1.9773296 1.3470651 2.9024822
105       119.3427 female 51.97804    200.4846 1.9922649 1.3515021 2.9368206
106       119.3427 female 52.21798    200.4846 2.0073129 1.3559537 2.9715653
107       119.3427 female 52.45792    200.4846 2.0224746 1.3604200 3.0067211
108       119.3427 female 52.69785    200.4846 2.0377508 1.3649010 3.0422927
109       119.3427 female 52.93779    200.4846 2.0531424 1.3693967 3.0782852
110       119.3427 female 53.17773    200.4846 2.0686503 1.3739073 3.1147036
111       119.3427 female 53.41766    200.4846 2.0842752 1.3784327 3.1515527
112       119.3427 female 53.65760    200.4846 2.1000183 1.3829730 3.1888379
113       119.3427 female 53.89753    200.4846 2.1158802 1.3875283 3.2265641
114       119.3427 female 54.13747    200.4846 2.1318619 1.3920985 3.2647367
115       119.3427 female 54.37741    200.4846 2.1479643 1.3966839 3.3033609
116       119.3427 female 54.61734    200.4846 2.1641884 1.4012843 3.3424420
117       119.3427 female 54.85728    200.4846 2.1805350 1.4058999 3.3819855
118       119.3427 female 55.09722    200.4846 2.1970051 1.4105306 3.4219968
119       119.3427 female 55.33715    200.4846 2.2135996 1.4151767 3.4624815
120       119.3427 female 55.57709    200.4846 2.2303194 1.4198380 3.5034451
121       119.3427 female 55.81702    200.4846 2.2471655 1.4245147 3.5448934
122       119.3427 female 56.05696    200.4846 2.2641389 1.4292068 3.5868320
123       119.3427 female 56.29690    200.4846 2.2812404 1.4339144 3.6292668
124       119.3427 female 56.53683    200.4846 2.2984712 1.4386374 3.6722037
125       119.3427 female 56.77677    200.4846 2.3158320 1.4433760 3.7156485
126       119.3427 female 57.01671    200.4846 2.3333240 1.4481303 3.7596073
127       119.3427 female 57.25664    200.4846 2.3509482 1.4529002 3.8040861
128       119.3427 female 57.49658    200.4846 2.3687054 1.4576857 3.8490912
129       119.3427 female 57.73652    200.4846 2.3865968 1.4624871 3.8946287
130       119.3427 female 57.97645    200.4846 2.4046233 1.4673043 3.9407050
131       119.3427 female 58.21639    200.4846 2.4227860 1.4721373 3.9873263
132       119.3427 female 58.45632    200.4846 2.4410858 1.4769863 4.0344993
133       119.3427 female 58.69626    200.4846 2.4595239 1.4818512 4.0822303
134       119.3427 female 58.93620    200.4846 2.4781013 1.4867322 4.1305260
135       119.3427 female 59.17613    200.4846 2.4968189 1.4916292 4.1793931
136       119.3427 female 59.41607    200.4846 2.5156780 1.4965424 4.2288383
137       119.3427 female 59.65601    200.4846 2.5346795 1.5014717 4.2788685
138       119.3427 female 59.89594    200.4846 2.5538245 1.5064173 4.3294906
139       119.3427 female 60.13588    200.4846 2.5731141 1.5113791 4.3807116
140       119.3427 female 60.37581    200.4846 2.5925494 1.5163574 4.4325385
141       119.3427 female 60.61575    200.4846 2.6121315 1.5213520 4.4849786
142       119.3427 female 60.85569    200.4846 2.6318616 1.5263630 4.5380392
143       119.3427 female 61.09562    200.4846 2.6517406 1.5313906 4.5917274
144       119.3427 female 61.33556    200.4846 2.6717698 1.5364347 4.6460509
145       119.3427 female 61.57550    200.4846 2.6919503 1.5414955 4.7010170
146       119.3427 female 61.81543    200.4846 2.7122832 1.5465729 4.7566334
147       119.3427 female 62.05537    200.4846 2.7327697 1.5516670 4.8129078
148       119.3427 female 62.29530    200.4846 2.7534109 1.5567779 4.8698479
149       119.3427 female 62.53524    200.4846 2.7742081 1.5619057 4.9274617
150       119.3427 female 62.77518    200.4846 2.7951623 1.5670503 4.9857571
151       119.3427 female 63.01511    200.4846 2.8162748 1.5722119 5.0447422
152       119.3427 female 63.25505    200.4846 2.8375468 1.5773905 5.1044252
153       119.3427 female 63.49499    200.4846 2.8589794 1.5825861 5.1648142
154       119.3427 female 63.73492    200.4846 2.8805739 1.5877989 5.2259176
155       119.3427 female 63.97486    200.4846 2.9023316 1.5930288 5.2877440
156       119.3427 female 64.21480    200.4846 2.9242536 1.5982760 5.3503018
157       119.3427 female 64.45473    200.4846 2.9463411 1.6035404 5.4135997
158       119.3427 female 64.69467    200.4846 2.9685955 1.6088222 5.4776465
159       119.3427 female 64.93460    200.4846 2.9910180 1.6141214 5.5424510
160       119.3427 female 65.17454    200.4846 3.0136098 1.6194380 5.6080221
161       119.3427 female 65.41448    200.4846 3.0363723 1.6247721 5.6743691
162       119.3427 female 65.65441    200.4846 3.0593067 1.6301239 5.7415009
163       119.3427 female 65.89435    200.4846 3.0824144 1.6354932 5.8094270
164       119.3427 female 66.13429    200.4846 3.1056965 1.6408802 5.8781567
165       119.3427 female 66.37422    200.4846 3.1291546 1.6462850 5.9476995
166       119.3427 female 66.61416    200.4846 3.1527898 1.6517076 6.0180651
167       119.3427 female 66.85409    200.4846 3.1766035 1.6571480 6.0892631
168       119.3427 female 67.09403    200.4846 3.2005972 1.6626063 6.1613035
169       119.3427 female 67.33397    200.4846 3.2247720 1.6680827 6.2341961
170       119.3427 female 67.57390    200.4846 3.2491294 1.6735770 6.3079511
171       119.3427 female 67.81384    200.4846 3.2736709 1.6790895 6.3825787
172       119.3427 female 68.05378    200.4846 3.2983976 1.6846201 6.4580892
173       119.3427 female 68.29371    200.4846 3.3233112 1.6901690 6.5344930
174       119.3427 female 68.53365    200.4846 3.3484129 1.6957361 6.6118008
175       119.3427 female 68.77358    200.4846 3.3737043 1.7013215 6.6900231
176       119.3427 female 69.01352    200.4846 3.3991866 1.7069254 6.7691709
177       119.3427 female 69.25346    200.4846 3.4248614 1.7125477 6.8492551
178       119.3427 female 69.49339    200.4846 3.4507302 1.7181885 6.9302867
179       119.3427 female 69.73333    200.4846 3.4767944 1.7238479 7.0122770
180       119.3427 female 69.97327    200.4846 3.5030554 1.7295260 7.0952372
181       119.3427 female 70.21320    200.4846 3.5295148 1.7352227 7.1791790
182       119.3427 female 70.45314    200.4846 3.5561740 1.7409382 7.2641138
183       119.3427 female 70.69307    200.4846 3.5830346 1.7466726 7.3500535
184       119.3427 female 70.93301    200.4846 3.6100981 1.7524258 7.4370099
185       119.3427 female 71.17295    200.4846 3.6373660 1.7581980 7.5249951
186       119.3427 female 71.41288    200.4846 3.6648398 1.7639892 7.6140212
187       119.3427 female 71.65282    200.4846 3.6925212 1.7697994 7.7041005
188       119.3427 female 71.89276    200.4846 3.7204117 1.7756288 7.7952456
189       119.3427 female 72.13269    200.4846 3.7485128 1.7814775 7.8874689
190       119.3427 female 72.37263    200.4846 3.7768262 1.7873453 7.9807833
191       119.3427 female 72.61257    200.4846 3.8053534 1.7932325 8.0752017
192       119.3427 female 72.85250    200.4846 3.8340961 1.7991391 8.1707372
193       119.3427 female 73.09244    200.4846 3.8630559 1.8050652 8.2674029
194       119.3427 female 73.33237    200.4846 3.8922344 1.8110107 8.3652122
195       119.3427 female 73.57231    200.4846 3.9216334 1.8169759 8.4641786
196       119.3427 female 73.81225    200.4846 3.9512544 1.8229607 8.5643159
197       119.3427 female 74.05218    200.4846 3.9810991 1.8289652 8.6656380
198       119.3427 female 74.29212    200.4846 4.0111692 1.8349895 8.7681587
199       119.3427 female 74.53206    200.4846 4.0414665 1.8410336 8.8718923
200       119.3427 female 74.77199    200.4846 4.0719926 1.8470976 8.9768531

Response variable (y):  

Adjust to: blood.pressure=119.3 sex=female cholesterol=200.5  

Limits are 0.95 confidence limits
> 
> # Make two curves, and plot the predicted curves as two trellis panels
> w <- Predict(fit, age, sex)
> require(lattice)
Loading required package: lattice
> xyplot(yhat ~ age | sex, data=w, type='l')
> # To add confidence bands we need to use the Hmisc xYplot function in
> # place of xyplot
> xYplot(Cbind(yhat,lower,upper) ~ age | sex, data=w, 
+        method='filled bands', type='l', col.fill=gray(.95))
> # If non-displayed variables were in the model, add a subtitle to show
> # their settings using title(sub=paste('Adjusted to',attr(w,'info')$adjust),adj=0)
> # Easier: feed w into plot.Predict, ggplot.Predict, plotp.Predict
> ## Not run: 
> ##D # Predictions form a parametric survival model
> ##D require(survival)
> ##D n <- 1000
> ##D set.seed(731)
> ##D age <- 50 + 12*rnorm(n)
> ##D label(age) <- "Age"
> ##D sex <- factor(sample(c('Male','Female'), n, 
> ##D               rep=TRUE, prob=c(.6, .4)))
> ##D cens <- 15*runif(n)
> ##D h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> ##D t <- -log(runif(n))/h
> ##D label(t) <- 'Follow-up Time'
> ##D e <- ifelse(t<=cens,1,0)
> ##D t <- pmin(t, cens)
> ##D units(t) <- "Year"
> ##D ddist <- datadist(age, sex)
> ##D Srv <- Surv(t,e)
> ##D 
> ##D # Fit log-normal survival model and plot median survival time vs. age
> ##D f <- psm(Srv ~ rcs(age), dist='lognormal')
> ##D med <- Quantile(f)       # Creates function to compute quantiles
> ##D                          # (median by default)
> ##D Predict(f, age, fun=function(x)med(lp=x))
> ##D # Note: This works because med() expects the linear predictor (X*beta)
> ##D #       as an argument.  Would not work if use 
> ##D #       ref.zero=TRUE or adj.zero=TRUE.
> ##D # Also, confidence intervals from this method are approximate since
> ##D # they don't take into account estimation of scale parameter
> ##D 
> ##D # Fit an ols model to log(y) and plot the relationship between x1
> ##D # and the predicted mean(y) on the original scale without assuming
> ##D # normality of residuals; use the smearing estimator.  Before doing
> ##D # that, show confidence intervals for mean and individual log(y),
> ##D # and for the latter, also show bootstrap percentile nonparametric
> ##D # pointwise confidence limits
> ##D set.seed(1)
> ##D x1 <- runif(300)
> ##D x2 <- runif(300)
> ##D ddist <- datadist(x1,x2); options(datadist='ddist')
> ##D y  <- exp(x1+ x2 - 1 + rnorm(300))
> ##D f  <- ols(log(y) ~ pol(x1,2) + x2, x=TRUE, y=TRUE)  # x y for bootcov
> ##D fb <- bootcov(f, B=100)
> ##D pb <- Predict(fb, x1, x2=c(.25,.75))
> ##D p1 <- Predict(f,  x1, x2=c(.25,.75))
> ##D p <- rbind(normal=p1, boot=pb)
> ##D plot(p)
> ##D 
> ##D p1 <- Predict(f, x1, conf.type='mean')
> ##D p2 <- Predict(f, x1, conf.type='individual')
> ##D p  <- rbind(mean=p1, individual=p2)
> ##D plot(p, label.curve=FALSE)   # uses superposition
> ##D plot(p, ~x1 | .set.)         # 2 panels
> ##D 
> ##D r <- resid(f)
> ##D smean <- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
> ##D formals(smean) <- list(yhat=numeric(0), res=r[!is.na(r)])
> ##D #smean$res <- r[!is.na(r)]   # define default res argument to function
> ##D Predict(f, x1, fun=smean)
> ##D 
> ##D ## Example using offset
> ##D g <- Glm(Y ~ offset(log(N)) + x1 + x2, family=poisson)
> ##D Predict(g, offset=list(N=100))
> ## End(Not run)
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’

> nameEx("Punits")
> ### * Punits
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Punits
> ### Title: Prepare units for Printing and Plotting
> ### Aliases: Punits
> 
> ### ** Examples
> 
> ## Not run: 
> ##D Punits('Years')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Rq")
> ### * Rq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rq
> ### Title: rms Package Interface to quantreg Package
> ### Aliases: Rq RqFit print.Rq latex.Rq predict.Rq
> ### Keywords: models nonparametric
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D n <- 100
> ##D x1 <- rnorm(n)
> ##D y <- exp(x1 + rnorm(n)/4)
> ##D dd <- datadist(x1); options(datadist='dd')
> ##D fq2 <- Rq(y ~ pol(x1,2))
> ##D anova(fq2)
> ##D fq3 <- Rq(y ~ pol(x1,2), tau=.75)
> ##D anova(fq3)
> ##D pq2 <- Predict(fq2, x1)
> ##D pq3 <- Predict(fq3, x1)
> ##D p <- rbind(Median=pq2, Q3=pq3)
> ##D plot(p, ~ x1 | .set.)
> ##D # For superpositioning, with true curves superimposed
> ##D a <- function(x, y, ...) {
> ##D  x <- unique(x)
> ##D  col <- trellis.par.get('superpose.line')$col
> ##D  llines(x, exp(x), col=col[1], lty=2)
> ##D  llines(x, exp(x + qnorm(.75)/4), col=col[2], lty=2)
> ##D }
> ##D plot(p, addpanel=a)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("anova.rms")
> ### * anova.rms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: anova.rms
> ### Title: Analysis of Variance (Wald, LR, and F Statistics)
> ### Aliases: anova.rms print.anova.rms plot.anova.rms latex.anova.rms
> ###   html.anova.rms
> ### Keywords: models regression htest aplot
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> treat <- factor(sample(c('a','b','c'), n,TRUE))
> num.diseases <- sample(0:4, n,TRUE)
> age <- rnorm(n, 50, 10)
> cholesterol <- rnorm(n, 200, 25)
> weight <- rnorm(n, 150, 20)
> sex <- factor(sample(c('female','male'), n,TRUE))
> label(age) <- 'Age'      # label is in Hmisc
> label(num.diseases) <- 'Number of Comorbid Diseases'
> label(cholesterol) <- 'Total Cholesterol'
> label(weight) <- 'Weight, lbs.'
> label(sex) <- 'Sex'
> units(cholesterol) <- 'mg/dl'   # uses units.default in Hmisc
> 
> 
> # Specify population model for log odds that Y=1
> L <- .1*(num.diseases-2) + .045*(age-50) +
+      (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
+      3.5*(treat=='b')+2*(treat=='c'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> 
> fit <- lrm(y ~ treat + scored(num.diseases) + rcs(age) +
+                log(cholesterol+10) + treat:log(cholesterol+10),
+            x=TRUE, y=TRUE)   # x, y needed for test='LR'
number of knots in rcs defaulting to 5
> a <- anova(fit)                       # Test all factors
> b <- anova(fit, treat, cholesterol)   # Test these 2 by themselves
>                                       # to get their pooled effects
> a
                Wald Statistics          Response: y 

 Factor                                             Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)               38.91       4   <.0001
  All Interactions                                  30.00       2   <.0001
 num.diseases                                        8.79       4   0.0666
  Nonlinear                                          3.28       3   0.3505
 age                                                46.03       4   <.0001
  Nonlinear                                          3.63       3   0.3043
 cholesterol  (Factor+Higher Order Factors)         41.35       3   <.0001
  All Interactions                                  30.00       2   <.0001
 treat * cholesterol  (Factor+Higher Order Factors) 30.00       2   <.0001
 TOTAL NONLINEAR                                     6.70       6   0.3491
 TOTAL NONLINEAR + INTERACTION                      35.78       8   <.0001
 TOTAL                                              93.30      13   <.0001
> b
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)       38.91      4    <.0001
  All Interactions                          30.00      2    <.0001
 cholesterol  (Factor+Higher Order Factors) 41.35      3    <.0001
  All Interactions                          30.00      2    <.0001
 TOTAL                                      50.44      5    <.0001
> a2 <- anova(fit, test='LR')
> b2 <- anova(fit, treat, cholesterol, test='LR')
> a2
                Likelihood Ratio Statistics          Response: y 

 Factor                                             Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)                41.13      4   <.0001
  All Interactions                                   31.59      2   <.0001
 num.diseases                                         8.85      4   0.0648
  Nonlinear                                           3.29      3   0.3486
 age                                                 52.41      4   <.0001
  Nonlinear                                           3.72      3   0.2936
 cholesterol  (Factor+Higher Order Factors)          45.69      3   <.0001
  All Interactions                                   31.59      2   <.0001
 treat * cholesterol  (Factor+Higher Order Factors)  31.59      2   <.0001
 TOTAL NONLINEAR                                      6.83      6   0.3370
 TOTAL NONLINEAR + INTERACTION                       38.10      8   <.0001
 TOTAL                                              111.80     13   <.0001
> b2
                Likelihood Ratio Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)       41.13      4    <.0001
  All Interactions                          31.59      2    <.0001
 cholesterol  (Factor+Higher Order Factors) 45.69      3    <.0001
  All Interactions                          31.59      2    <.0001
 TOTAL                                      55.28      5    <.0001
> 
> # Add a new line to the plot with combined effects
> s <- rbind(a2, 'treat+cholesterol'=b2['TOTAL',])
> 
> class(s) <- 'anova.rms'
> plot(s, margin=c('chisq', 'proportion chisq'))
> 
> g <- lrm(y ~ treat*rcs(age))
number of knots in rcs defaulting to 5
> dd <- datadist(treat, num.diseases, age, cholesterol)
> options(datadist='dd')
> p <- Predict(g, age, treat="b")
> s <- anova(g)
> tx <- paste(capture.output(s), collapse='\n')
> ggplot(p) + annotate('text', x=27, y=3.2, family='mono', label=tx,
+                       hjust=0, vjust=1, size=1.5)
> 
> plot(s, margin=c('chisq', 'proportion chisq'))
> # new plot - dot chart of chisq-d.f. with 2 other stats in right margin
> # latex(s)                       # nice printout - creates anova.g.tex
> options(datadist=NULL)
> 
> 
> # Simulate data with from a given model, and display exactly which
> # hypotheses are being tested
> 
> 
> set.seed(123)
> age <- rnorm(500, 50, 15)
> treat <- factor(sample(c('a','b','c'), 500, TRUE))
> bp  <- rnorm(500, 120, 10)
> y   <- ifelse(treat=='a', (age-50)*.05, abs(age-50)*.08) + 3*(treat=='c') +
+        pmax(bp, 100)*.09 + rnorm(500)
> f   <- ols(y ~ treat*lsp(age,50) + rcs(bp,4))
> print(names(coef(f)), quote=FALSE)
 [1] Intercept      treat=b        treat=c        age            age'          
 [6] bp             bp'            bp''           treat=b * age  treat=c * age 
[11] treat=b * age' treat=c * age'
> specs(f)
ols(formula = y ~ treat * lsp(age, 50) + rcs(bp, 4))

            Assumption  Parameters                 d.f.
treat       category     a b c                     2   
age         lspline      50                        2   
bp          rcspline     103.5 116.66 123.72 137.7 3   
treat * age interaction linear x nonlinear - Ag(B) 4   
> anova(f)
                Analysis of Variance          Response: y 

 Factor                                     d.f. Partial SS   MS         
 treat  (Factor+Higher Order Factors)         6  1459.3919411 243.2319902
  All Interactions                            4    92.8011403  23.2002851
 age  (Factor+Higher Order Factors)           6   232.5534824  38.7589137
  All Interactions                            4    92.8011403  23.2002851
  Nonlinear (Factor+Higher Order Factors)     3   145.3404947  48.4468316
 bp                                           3   311.8671677 103.9557226
  Nonlinear                                   2     0.3791187   0.1895593
 treat * age  (Factor+Higher Order Factors)   4    92.8011403  23.2002851
  Nonlinear                                   2    55.0236651  27.5118325
  Nonlinear Interaction : f(A,B) vs. AB       2    55.0236651  27.5118325
 TOTAL NONLINEAR                              5   145.5493643  29.1098729
 TOTAL NONLINEAR + INTERACTION                7   175.4233130  25.0604733
 REGRESSION                                  11  1908.0623468 173.4602133
 ERROR                                      488   474.2072823   0.9717362
 F      P     
 250.31 <.0001
  23.88 <.0001
  39.89 <.0001
  23.88 <.0001
  49.86 <.0001
 106.98 <.0001
   0.20 0.8228
  23.88 <.0001
  28.31 <.0001
  28.31 <.0001
  29.96 <.0001
  25.79 <.0001
 178.51 <.0001
              
> an <- anova(f)
> options(digits=3)
> print(an, 'subscripts')
                Analysis of Variance          Response: y 

 Factor                                     d.f. Partial SS MS      F    
 treat  (Factor+Higher Order Factors)         6  1459.392   243.232 250.3
  All Interactions                            4    92.801    23.200  23.9
 age  (Factor+Higher Order Factors)           6   232.553    38.759  39.9
  All Interactions                            4    92.801    23.200  23.9
  Nonlinear (Factor+Higher Order Factors)     3   145.340    48.447  49.9
 bp                                           3   311.867   103.956 107.0
  Nonlinear                                   2     0.379     0.190   0.2
 treat * age  (Factor+Higher Order Factors)   4    92.801    23.200  23.9
  Nonlinear                                   2    55.024    27.512  28.3
  Nonlinear Interaction : f(A,B) vs. AB       2    55.024    27.512  28.3
 TOTAL NONLINEAR                              5   145.549    29.110  30.0
 TOTAL NONLINEAR + INTERACTION                7   175.423    25.060  25.8
 REGRESSION                                  11  1908.062   173.460 178.5
 ERROR                                      488   474.207     0.972      
 P      Tested     
 <.0001 1-2,8-11   
 <.0001 8-11       
 <.0001 3-4,8-11   
 <.0001 8-11       
 <.0001 4,10-11    
 <.0001 5-7        
 0.823  6-7        
 <.0001 8-11       
 <.0001 10-11      
 <.0001 10-11      
 <.0001 4,6-7,10-11
 <.0001 4,6-11     
 <.0001 1-11       
                   

Subscripts correspond to:
 [1] treat=b        treat=c        age            age'           bp            
 [6] bp'            bp''           treat=b * age  treat=c * age  treat=b * age'
[11] treat=c * age'
> print(an, 'dots')
                Analysis of Variance          Response: y 

 Factor                                     d.f. Partial SS MS      F    
 treat  (Factor+Higher Order Factors)         6  1459.392   243.232 250.3
  All Interactions                            4    92.801    23.200  23.9
 age  (Factor+Higher Order Factors)           6   232.553    38.759  39.9
  All Interactions                            4    92.801    23.200  23.9
  Nonlinear (Factor+Higher Order Factors)     3   145.340    48.447  49.9
 bp                                           3   311.867   103.956 107.0
  Nonlinear                                   2     0.379     0.190   0.2
 treat * age  (Factor+Higher Order Factors)   4    92.801    23.200  23.9
  Nonlinear                                   2    55.024    27.512  28.3
  Nonlinear Interaction : f(A,B) vs. AB       2    55.024    27.512  28.3
 TOTAL NONLINEAR                              5   145.549    29.110  30.0
 TOTAL NONLINEAR + INTERACTION                7   175.423    25.060  25.8
 REGRESSION                                  11  1908.062   173.460 178.5
 ERROR                                      488   474.207     0.972      
 P      Tested     
 <.0001 ..     ....
 <.0001        ....
 <.0001   ..   ....
 <.0001        ....
 <.0001    .     ..
 <.0001     ...    
 0.823       ..    
 <.0001        ....
 <.0001          ..
 <.0001          ..
 <.0001    . ..  ..
 <.0001    . ......
 <.0001 ...........
                   

Subscripts correspond to:
 [1] treat=b        treat=c        age            age'           bp            
 [6] bp'            bp''           treat=b * age  treat=c * age  treat=b * age'
[11] treat=c * age'
> 
> 
> an <- anova(f, test='Chisq', ss=FALSE)
> # plot(0:1)                        # make some plot
> # tab <- pantext(an, 1.2, .6, lattice=FALSE, fontfamily='Helvetica')
> # create function to write table; usually omit fontfamily
> # tab()                            # execute it; could do tab(cex=.65)
> plot(an)                         # new plot - dot chart of chisq-d.f.
> # Specify plot(an, trans=sqrt) to use a square root scale for this plot
> # latex(an)                      # nice printout - creates anova.f.tex
> 
> 
> ## Example to save partial R^2 for all predictors, along with overall
> ## R^2, from two separate fits, and to combine them with ggplot2
> 
> require(ggplot2)
> set.seed(1)
> n <- 100
> x1 <- runif(n)
> x2 <- runif(n)
> y  <- (x1-.5)^2 + x2 + runif(n)
> group <- c(rep('a', n/2), rep('b', n/2))
> A <- NULL
> for(g in c('a','b')) {
+     f <- ols(y ~ pol(x1,2) + pol(x2,2) + pol(x1,2) %ia% pol(x2,2),
+              subset=group==g)
+     a <- plot(anova(f),
+               what='partial R2', pl=FALSE, rm.totals=FALSE, sort='none')
+     a <- a[-grep('NONLINEAR', names(a))]
+     d <- data.frame(group=g, Variable=factor(names(a), names(a)),
+                     partialR2=unname(a))
+     A <- rbind(A, d)
+   }
> ggplot(A, aes(x=partialR2, y=Variable)) + geom_point() +
+        facet_wrap(~ group) + xlab(ex <- expression(partial~R^2)) +
+        scale_y_discrete(limits=rev)
> ggplot(A, aes(x=partialR2, y=Variable, color=group)) + geom_point() +
+        xlab(ex <- expression(partial~R^2)) +
+        scale_y_discrete(limits=rev)
> 
> # Suppose that a researcher wants to make a big deal about a variable
> # because it has the highest adjusted chi-square.  We use the
> # bootstrap to derive 0.95 confidence intervals for the ranks of all
> # the effects in the model.  We use the plot method for anova, with
> # pl=FALSE to suppress actual plotting of chi-square - d.f. for each
> # bootstrap repetition.
> # It is important to tell plot.anova.rms not to sort the results, or
> # every bootstrap replication would have ranks of 1,2,3,... for the stats.
> 
> n <- 300
> set.seed(1)
> d <- data.frame(x1=runif(n), x2=runif(n),  x3=runif(n),
+    x4=runif(n), x5=runif(n), x6=runif(n),  x7=runif(n),
+    x8=runif(n), x9=runif(n), x10=runif(n), x11=runif(n),
+    x12=runif(n))
> d$y <- with(d, 1*x1 + 2*x2 + 3*x3 +  4*x4  + 5*x5 + 6*x6 +
+                7*x7 + 8*x8 + 9*x9 + 10*x10 + 11*x11 +
+               12*x12 + 9*rnorm(n))
> 
> f <- ols(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12, data=d)
> B <- 20   # actually use B=1000
> ranks <- matrix(NA, nrow=B, ncol=12)
> rankvars <- function(fit)
+   rank(plot(anova(fit), sort='none', pl=FALSE))
> Rank <- rankvars(f)
> for(i in 1:B) {
+   j <- sample(1:n, n, TRUE)
+   bootfit <- update(f, data=d, subset=j)
+   ranks[i,] <- rankvars(bootfit)
+   }
> lim <- t(apply(ranks, 2, quantile, probs=c(.025,.975)))
> predictor <- factor(names(Rank), names(Rank))
> w <- data.frame(predictor, Rank, lower=lim[,1], upper=lim[,2])
> ggplot(w, aes(x=predictor, y=Rank)) + geom_point() + coord_flip() +
+   scale_y_continuous(breaks=1:12) +
+   geom_errorbar(aes(ymin=lim[,1], ymax=lim[,2]), width=0)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("bj")
> ### * bj
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bj
> ### Title: Buckley-James Multiple Regression Model
> ### Aliases: bj bj.fit residuals.bj print.bj validate.bj bjplot
> ### Keywords: models survival
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> suppressWarnings(RNGversion("3.5.0"))
> set.seed(1)
> ftime  <- 10*rexp(200)
> stroke <- ifelse(ftime > 10, 0, 1)
> ftime  <- pmin(ftime, 10)
> units(ftime) <- "Month"
> age <- rnorm(200, 70, 10)
> hospital <- factor(sample(c('a','b'),200,TRUE))
> dd <- datadist(age, hospital)
> options(datadist="dd")
> 
> # Prior to rms 6.0 and R 4.0 the following worked with 5 knots
> f <- bj(Surv(ftime, stroke) ~ rcs(age,3) + hospital, x=TRUE, y=TRUE)
> # add link="identity" to use a censored normal regression model instead
> # of a lognormal one
> anova(f)
                Wald Statistics          Response: Surv(ftime, stroke) 

 Factor     Chi-Square d.f. P     
 age        1.84       2    0.3988
  Nonlinear 1.81       1    0.1790
 hospital   2.78       1    0.0953
 TOTAL      4.36       3    0.2253
> fastbw(f)

 Deleted  Chi-Sq d.f. P     Residual d.f. P     AIC  
 age      1.84   2    0.399 1.84     2    0.399 -2.16
 hospital 2.52   1    0.113 4.36     3    0.225 -1.64

Approximate Estimates after Deleting Factors

      Coef    S.E. Wald Z P
[1,] 1.741 0.07982  21.81 0

Factors in Final Model

None
> validate(f, B=15)

No convergence in 45 steps
Warning in fit(x[xtrain, , drop = FALSE], y[train, , drop = FALSE], strata = stra[train],  :
  bj.fit failed
    index.orig training   test optimism index.corrected  n
Dxy     0.0858    0.139 0.0805   0.0586          0.0272 14
> plot(Predict(f, age, hospital))
> # needs datadist since no explicit age,hosp.
> coef(f)               # look at regression coefficients
 Intercept        age       age' hospital=b 
    2.7459    -0.0186     0.0277     0.2685 
> coef(psm(Surv(ftime, stroke) ~ rcs(age,3) + hospital, dist='lognormal'))
(Intercept)         age        age'  hospital=b 
     3.1852     -0.0204      0.0251      0.2490 
>                       # compare with coefficients from likelihood-based
>                       # log-normal regression model
>                       # use dist='gau' not under R 
> 
> 
> r <- resid(f, 'censored.normalized')
> survplot(npsurv(r ~ 1), conf='none') 
>                       # plot Kaplan-Meier estimate of 
>                       # survival function of standardized residuals
> survplot(npsurv(r ~ cut2(age, g=2)), conf='none')  
>                       # may desire both strata to be n(0,1)
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("bootBCa")
> ### * bootBCa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootBCa
> ### Title: BCa Bootstrap on Existing Bootstrap Replicates
> ### Aliases: bootBCa
> ### Keywords: bootstrap
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x1 <- runif(100); x2 <- runif(100); y <- sample(0:1, 100, TRUE)
> ##D f <- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
> ##D seed <- .Random.seed
> ##D b <- bootcov(f)
> ##D # Get estimated log odds at x1=.4, x2=.6
> ##D X <- cbind(c(1,1), x1=c(.4,2), x2=c(.6,3))
> ##D est <- X ##D 
> ##D ests <- t(X ##D 
> ##D bootBCa(est, ests, n=100, seed=seed)
> ##D bootBCa(est, ests, type='bca', n=100, seed=seed)
> ##D bootBCa(est, ests, type='basic', n=100, seed=seed)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("bootcov")
> ### * bootcov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootcov
> ### Title: Bootstrap Covariance and Distribution for Regression
> ###   Coefficients
> ### Aliases: bootcov bootplot bootplot.bootcov confplot confplot.bootcov
> ###   histdensity
> ### Keywords: models regression htest methods hplot
> 
> ### ** Examples
> 
> set.seed(191)
> x <- exp(rnorm(200))
> logit <- 1 + x/2
> y <- ifelse(runif(200) <= plogis(logit), 1, 0)
> f <- lrm(y ~ pol(x,2), x=TRUE, y=TRUE)
> g <- bootcov(f, B=50, pr=TRUE, seed=3)
Iteration: 20 of 50Iteration: 40 of 50
> anova(g)    # using bootstrap covariance estimates
                Wald Statistics          Response: y 

 Factor     Chi-Square d.f. P    
 x          1.50       2    0.473
  Nonlinear 0.01       1    0.917
 TOTAL      1.50       2    0.473
> fastbw(g)   # using bootstrap covariance estimates

 Deleted Chi-Sq d.f. P     Residual d.f. P     AIC 
 x       1.5    2    0.473 1.5      2    0.473 -2.5

Approximate Estimates after Deleting Factors

      Coef   S.E. Wald Z         P
[1,] 1.239 0.2359  5.252 1.506e-07

Factors in Final Model

None
> beta <- g$boot.Coef[,1]
> hist(beta, nclass=15)     #look at normality of parameter estimates
> qqnorm(beta)
> # bootplot would be better than these last two commands
> 
> 
> # A dataset contains a variable number of observations per subject,
> # and all observations are laid out in separate rows. The responses
> # represent whether or not a given segment of the coronary arteries
> # is occluded. Segments of arteries may not operate independently
> # in the same patient.  We assume a "working independence model" to
> # get estimates of the coefficients, i.e., that estimates assuming
> # independence are reasonably efficient.  The job is then to get
> # unbiased estimates of variances and covariances of these estimates.
> 
> 
> set.seed(2)
> n.subjects <- 30
> ages <- rnorm(n.subjects, 50, 15)
> sexes  <- factor(sample(c('female','male'), n.subjects, TRUE))
> logit <- (ages-50)/5
> prob <- plogis(logit)  # true prob not related to sex
> id <- sample(1:n.subjects, 300, TRUE) # subjects sampled multiple times
> table(table(id))  # frequencies of number of obs/subject

 5  6  7  8  9 10 11 12 13 14 15 
 1  2  2  3  3  5  7  3  2  1  1 
> age <- ages[id]
> sex <- sexes[id]
> # In truth, observations within subject are independent:
> y   <- ifelse(runif(300) <= prob[id], 1, 0)
> f <- lrm(y ~ lsp(age,50)*sex, x=TRUE, y=TRUE)
> g <- bootcov(f, id, B=50, seed=3)  # usually do B=200 or more
optimization did not converge for lrm.fit
singular Hessian matrix
convergence code 2
Warning in bootcov(f, id, B = 50, seed = 3) :
  fit failure in 1 resamples.  Consider specifying tol, maxit, opt_method, or other optimization criteria.
> diag(g$var)/diag(f$var)
<0 x 0 matrix>
> # add ,group=w to re-sample from within each level of w
> anova(g)            # cluster-adjusted Wald statistics
                Wald Statistics          Response: y 

 Factor                                   Chi-Square d.f. P     
 age  (Factor+Higher Order Factors)       27.60      4    <.0001
  All Interactions                         0.60      2    0.742 
  Nonlinear (Factor+Higher Order Factors)  0.32      2    0.852 
 sex  (Factor+Higher Order Factors)        1.23      3    0.745 
  All Interactions                         0.60      2    0.742 
 age * sex  (Factor+Higher Order Factors)  0.60      2    0.742 
  Nonlinear                                0.10      1    0.754 
  Nonlinear Interaction : f(A,B) vs. AB    0.10      1    0.754 
 TOTAL NONLINEAR                           0.32      2    0.852 
 TOTAL NONLINEAR + INTERACTION             0.64      3    0.888 
 TOTAL                                    42.06      5    <.0001
> # fastbw(g)         # cluster-adjusted backward elimination
> plot(Predict(g, age=30:70, sex='female'))  # cluster-adjusted confidence bands
> 
> 
> # Get design effects based on inflation of the variances when compared
> # with bootstrap estimates which ignore clustering
> g2 <- bootcov(f, B=50, seed=3)
> diag(g$var)/diag(g2$var)
      Intercept             age            age'        sex=male  age * sex=male 
         3.6958          3.4142          2.1613          0.0672          0.0669 
age' * sex=male 
         0.0650 
> 
> 
> # Get design effects based on pooled tests of factors in model
> anova(g2)[,1] / anova(g)[,1]
      age  (Factor+Higher Order Factors) 
                                   1.655 
                        All Interactions 
                                   2.407 
 Nonlinear (Factor+Higher Order Factors) 
                                   1.043 
      sex  (Factor+Higher Order Factors) 
                                   1.349 
                        All Interactions 
                                   2.407 
age * sex  (Factor+Higher Order Factors) 
                                   2.407 
                               Nonlinear 
                                   0.065 
   Nonlinear Interaction : f(A,B) vs. AB 
                                   0.065 
                         TOTAL NONLINEAR 
                                   1.043 
           TOTAL NONLINEAR + INTERACTION 
                                   2.789 
                                   TOTAL 
                                   1.282 
> 
> 
> # Simulate binary data where there is a strong 
> # age x sex interaction with linear age effects 
> # for both sexes, but where not knowing that
> # we fit a quadratic model.  Use the bootstrap
> # to get bootstrap distributions of various
> # effects, and to get pointwise and simultaneous
> # confidence limits
> 
> 
> set.seed(71)
> n   <- 500
> age <- rnorm(n, 50, 10)
> sex <- factor(sample(c('female','male'), n, rep=TRUE))
> L   <- ifelse(sex=='male', 0, .1*(age-50))
> y   <- ifelse(runif(n)<=plogis(L), 1, 0)
> 
> 
> f <- lrm(y ~ sex*pol(age,2), x=TRUE, y=TRUE)
> b <- bootcov(f, B=50, loglik=TRUE, pr=TRUE, seed=3)   # better: B=500
Iteration: 20 of 50Iteration: 40 of 50
> 
> 
> par(mfrow=c(2,3))
> # Assess normality of regression estimates
> bootplot(b, which=1:6, what='qq')
> # They appear somewhat non-normal
> 
> 
> # Plot histograms and estimated densities 
> # for 6 coefficients
> w <- bootplot(b, which=1:6)
> # Print bootstrap quantiles
> w$quantiles
                                    0.050     0.025     0.005     0.950
Intercept                       -14.95710 -16.99837 -17.75768 -4.685547
Coefficient of sex=male          -1.88387  -4.85495  -6.25372 10.873791
Coefficient of age                0.06345   0.05562   0.03631  0.452007
Coefficient of age^2             -0.00295  -0.00327  -0.00391  0.000576
Coefficient of sex=male * age    -0.31521  -0.32055  -0.34281  0.172410
Coefficient of sex=male * age^2  -0.00273  -0.00370  -0.00408  0.001970
                                   0.975     0.995
Intercept                       -4.25376 -4.100404
Coefficient of sex=male         11.23676 12.100729
Coefficient of age               0.50567  0.554573
Coefficient of age^2             0.00069  0.000809
Coefficient of sex=male * age    0.27660  0.331240
Coefficient of sex=male * age^2  0.00206  0.002092
> 
> # Show box plots for bootstrap reps for all coefficients
> bootplot(b, what='box')
> 
> 
> # Estimate regression function for females
> # for a sequence of ages
> ages <- seq(25, 75, length=100)
> label(ages) <- 'Age'
> 
> 
> # Plot fitted function and pointwise normal-
> # theory confidence bands
> par(mfrow=c(1,1))
> p <- Predict(f, age=ages, sex='female')
> plot(p)
> # Save curve coordinates for later automatic
> # labeling using labcurve in the Hmisc library
> curves <- vector('list',8)
> curves[[1]] <- with(p, list(x=age, y=lower))
> curves[[2]] <- with(p, list(x=age, y=upper))
> 
> 
> # Add pointwise normal-distribution confidence 
> # bands using unconditional variance-covariance
> # matrix from the 500 bootstrap reps
> p <- Predict(b, age=ages, sex='female')
> curves[[3]] <- with(p, list(x=age, y=lower))
> curves[[4]] <- with(p, list(x=age, y=upper))
> 
> 
> dframe <- expand.grid(sex='female', age=ages)
> X <- predict(f, dframe, type='x')  # Full design matrix
> 
> 
> # Add pointwise bootstrap nonparametric 
> # confidence limits
> p <- confplot(b, X=X, against=ages, method='pointwise',
+               add=TRUE, lty.conf=4)
> curves[[5]] <- list(x=ages, y=p$lower)
> curves[[6]] <- list(x=ages, y=p$upper)
> 
> 
> # Add simultaneous bootstrap confidence band
> p <- confplot(b, X=X, against=ages, add=TRUE, lty.conf=5)
> curves[[7]] <- list(x=ages, y=p$lower)
> curves[[8]] <- list(x=ages, y=p$upper)
> lab <- c('a','a','b','b','c','c','d','d')
> labcurve(curves, lab, pl=TRUE)
> 
> 
> # Now get bootstrap simultaneous confidence set for
> # female:male odds ratios for a variety of ages
> 
> 
> dframe <- expand.grid(age=ages, sex=c('female','male'))
> X <- predict(f, dframe, type='x')  # design matrix
> f.minus.m <- X[1:100,] - X[101:200,]
> # First 100 rows are for females.  By subtracting
> # design matrices are able to get Xf*Beta - Xm*Beta
> # = (Xf - Xm)*Beta
> 
> 
> confplot(b, X=f.minus.m, against=ages,
+          method='pointwise', ylab='F:M Log Odds Ratio')
> confplot(b, X=f.minus.m, against=ages,
+          lty.conf=3, add=TRUE)
> 
> 
> # contrast.rms makes it easier to compute the design matrix for use
> # in bootstrapping contrasts:
> 
> 
> f.minus.m <- contrast(f, list(sex='female',age=ages),
+                          list(sex='male',  age=ages))$X
> confplot(b, X=f.minus.m)
$fitted
      1       2       3       4       5       6       7       8       9      10 
-2.7813 -2.7298 -2.6782 -2.6264 -2.5745 -2.5224 -2.4702 -2.4179 -2.3654 -2.3128 
     11      12      13      14      15      16      17      18      19      20 
-2.2601 -2.2072 -2.1542 -2.1010 -2.0477 -1.9942 -1.9407 -1.8869 -1.8331 -1.7791 
     21      22      23      24      25      26      27      28      29      30 
-1.7250 -1.6707 -1.6163 -1.5617 -1.5070 -1.4522 -1.3973 -1.3422 -1.2869 -1.2315 
     31      32      33      34      35      36      37      38      39      40 
-1.1760 -1.1204 -1.0646 -1.0086 -0.9526 -0.8964 -0.8400 -0.7835 -0.7269 -0.6702 
     41      42      43      44      45      46      47      48      49      50 
-0.6133 -0.5562 -0.4990 -0.4417 -0.3843 -0.3267 -0.2690 -0.2111 -0.1531 -0.0949 
     51      52      53      54      55      56      57      58      59      60 
-0.0367  0.0218  0.0803  0.1390  0.1979  0.2568  0.3159  0.3752  0.4346  0.4941 
     61      62      63      64      65      66      67      68      69      70 
 0.5538  0.6136  0.6735  0.7336  0.7938  0.8542  0.9147  0.9753  1.0361  1.0970 
     71      72      73      74      75      76      77      78      79      80 
 1.1580  1.2192  1.2805  1.3420  1.4036  1.4653  1.5272  1.5892  1.6514  1.7137 
     81      82      83      84      85      86      87      88      89      90 
 1.7761  1.8387  1.9014  1.9642  2.0272  2.0903  2.1536  2.2170  2.2805  2.3442 
     91      92      93      94      95      96      97      98      99     100 
 2.4080  2.4720  2.5361  2.6003  2.6647  2.7292  2.7938  2.8586  2.9235  2.9886 

$upper
      1       2       3       4       5       6       7       8       9      10 
 0.5645  0.4963  0.4302  0.3662  0.3044  0.2447  0.1871  0.1317  0.0784  0.0272 
     11      12      13      14      15      16      17      18      19      20 
-0.0219 -0.0688 -0.1136 -0.1563 -0.1969 -0.2353 -0.2716 -0.3057 -0.3378 -0.3677 
     21      22      23      24      25      26      27      28      29      30 
-0.3954 -0.4211 -0.4446 -0.4660 -0.4853 -0.5024 -0.5174 -0.5303 -0.5366 -0.5231 
     31      32      33      34      35      36      37      38      39      40 
-0.5079 -0.4808 -0.4466 -0.4124 -0.3781 -0.3438 -0.3094 -0.2750 -0.2406 -0.2061 
     41      42      43      44      45      46      47      48      49      50 
-0.1715 -0.1369 -0.1023 -0.0676 -0.0243  0.0476  0.1192  0.1904  0.2612  0.3317 
     51      52      53      54      55      56      57      58      59      60 
 0.4017  0.4715  0.5408  0.6097  0.6783  0.7465  0.8144  0.8819  0.9490  1.0157 
     61      62      63      64      65      66      67      68      69      70 
 1.0820  1.1480  1.2136  1.2788  1.3437  1.4082  1.4723  1.5360  1.6040  1.6937 
     71      72      73      74      75      76      77      78      79      80 
 1.7840  1.8747  1.9661  2.0580  2.1504  2.2434  2.3369  2.4310  2.5256  2.6208 
     81      82      83      84      85      86      87      88      89      90 
 2.7165  2.8128  2.9097  3.0071  3.1050  3.2035  3.3025  3.4021  3.5022  3.6029 
     91      92      93      94      95      96      97      98      99     100 
 3.7042  3.8059  3.9083  4.0112  4.1146  4.2268  4.3573  4.4895  4.6235  4.7592 

$lower
      1       2       3       4       5       6       7       8       9      10 
-4.9263 -4.8031 -4.6811 -4.5601 -4.4402 -4.3213 -4.2035 -4.0868 -3.9712 -3.8567 
     11      12      13      14      15      16      17      18      19      20 
-3.7432 -3.6308 -3.5194 -3.4091 -3.2999 -3.1918 -3.0848 -2.9788 -2.8971 -2.8171 
     21      22      23      24      25      26      27      28      29      30 
-2.7371 -2.6570 -2.5769 -2.4968 -2.4165 -2.3363 -2.2560 -2.1757 -2.0953 -2.0148 
     31      32      33      34      35      36      37      38      39      40 
-1.9344 -1.8538 -1.7733 -1.6927 -1.6120 -1.5313 -1.4505 -1.3697 -1.2889 -1.2080 
     41      42      43      44      45      46      47      48      49      50 
-1.1271 -1.0461 -0.9651 -0.8863 -0.8180 -0.7493 -0.6801 -0.6176 -0.5695 -0.5205 
     51      52      53      54      55      56      57      58      59      60 
-0.4707 -0.4201 -0.3686 -0.3163 -0.2632 -0.2093 -0.1546 -0.0990 -0.0426  0.0146 
     61      62      63      64      65      66      67      68      69      70 
 0.0727  0.1315  0.1912  0.2518  0.3131  0.3753  0.4383  0.5021  0.5667  0.6098 
     71      72      73      74      75      76      77      78      79      80 
 0.6485  0.6868  0.7249  0.7626  0.8000  0.8371  0.8739  0.9104  0.9465  0.9823 
     81      82      83      84      85      86      87      88      89      90 
 1.0069  1.0231  1.0382  1.0523  1.0653  1.0773  1.0882  1.0981  1.1070  1.1148 
     91      92      93      94      95      96      97      98      99     100 
 1.1216  1.1273  1.1319  1.1356  1.1381  1.1397  1.1402  1.1396  1.1380  1.1354 

> 
> 
> # For a quadratic binary logistic regression model use bootstrap
> # bumping to estimate coefficients under a monotonicity constraint
> set.seed(177)
> n <- 400
> x <- runif(n)
> logit <- 3*(x^2-1)
> y <- rbinom(n, size=1, prob=plogis(logit))
> f <- lrm(y ~ pol(x,2), x=TRUE, y=TRUE)
> k <- coef(f)
> k
Intercept         x       x^2 
    -3.78      1.41      2.49 
> vertex <- -k[2]/(2*k[3])
> vertex
     x 
-0.283 
> 
> 
> # Outside [0,1] so fit satisfies monotonicity constraint within
> # x in [0,1], i.e., original fit is the constrained MLE
> 
> 
> g <- bootcov(f, B=50, coef.reps=TRUE, loglik=TRUE, seed=3)
> bootcoef <- g$boot.Coef    # 100x3 matrix
> vertex <- -bootcoef[,2]/(2*bootcoef[,3])
> table(cut2(vertex, c(0,1)))

[-7.31, 0.00) [ 0.00, 1.00) [ 1.00,12.19] 
           23            16            11 
> mono <- !(vertex >= 0 & vertex <= 1)
> mean(mono)    # estimate of Prob{monotonicity in [0,1]}
[1] 0.68
> 
> 
> var(bootcoef)   # var-cov matrix for unconstrained estimates
          Intercept     x   x^2
Intercept     0.837 -2.40  1.68
x            -2.396  8.52 -6.82
x^2           1.679 -6.82  5.87
> var(bootcoef[mono,])   # for constrained estimates
          Intercept     x   x^2
Intercept     0.755 -1.85  1.18
x            -1.852  5.41 -3.97
x^2           1.180 -3.97  3.23
> 
> 
> # Find second-best vector of coefficient estimates, i.e., best
> # from among bootstrap estimates
> g$boot.Coef[order(g$boot.loglik[-length(g$boot.loglik)])[1],]
Intercept         x       x^2 
   -3.555     0.254     3.528 
> # Note closeness to MLE
> 
> ## Not run: 
> ##D # Get the bootstrap distribution of the difference in two ROC areas for
> ##D # two binary logistic models fitted on the same dataset.  This analysis
> ##D # does not adjust for the bias ROC area (C-index) due to overfitting.
> ##D # The same random number seed is used in two runs to enforce pairing.
> ##D 
> ##D set.seed(17)
> ##D x1 <- rnorm(100)
> ##D x2 <- rnorm(100)
> ##D y <- sample(0:1, 100, TRUE)
> ##D f <- lrm(y ~ x1, x=TRUE, y=TRUE)
> ##D g <- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
> ##D f <- bootcov(f, stat='C', seed=4)
> ##D g <- bootcov(g, stat='C', seed=4)
> ##D dif <- g$boot.stats - f$boot.stats
> ##D hist(dif)
> ##D quantile(dif, c(.025,.25,.5,.75,.975))
> ##D # Compute a z-test statistic.  Note that comparing ROC areas is far less
> ##D # powerful than likelihood or Brier score-based methods
> ##D z <- (g$stats['C'] - f$stats['C'])/sd(dif)
> ##D names(z) <- NULL
> ##D c(z=z, P=2*pnorm(-abs(z)))
> ##D 
> ##D # For an ordinal y with some distinct values of y not very popular, let
> ##D # bootcov use linear extrapolation to fill in intercepts for non-sampled levels
> ##D 
> ##D f <- orm(y ~ x1 + x2, x=TRUE, y=TRUE)
> ##D bootcov(f, B=200)
> ##D 
> ##D # Instead of filling in missing intercepts, perform minimum binning so that
> ##D # there is a 0.9999 probability that all distinct Y values will be represented
> ##D # in bootstrap samples
> ##D y <- ordGroupBoot(y)
> ##D f <- orm(y ~ x1 + x2, x=TRUE, y=TRUE)
> ##D bootcov(f, B=200)
> ##D 
> ##D # Instead just keep one intercept for all bootstrap fits - the intercept
> ##D # that pertains to y=10
> ##D 
> ##D bootcov(f, B=200, ytarget=10)   # use ytarget=NA for the median
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("bplot")
> ### * bplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bplot
> ### Title: 3-D Plots Showing Effects of Two Continuous Predictors in a
> ###   Regression Model Fit
> ### Aliases: bplot perimeter
> ### Keywords: models hplot htest
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
+                x=TRUE, y=TRUE)
> p <- Predict(fit, age, cholesterol, sex, np=50) # vary sex last
> require(lattice)
Loading required package: lattice
> bplot(p)                 # image plot for age, cholesterol with color
>                          # coming from yhat; use default ranges for
>                          # both continuous predictors; two panels (for sex)
> bplot(p, lfun=wireframe) # same as bplot(p,,wireframe)
> # View from different angle, change y label orientation accordingly
> # Default is z=40, x=-60
> bplot(p,, wireframe, screen=list(z=40, x=-75), ylabrot=-25)
> bplot(p,, contourplot)   # contour plot
> bounds  <- perimeter(age, cholesterol, lowess=TRUE)
> plot(age, cholesterol)     # show bivariate data density and perimeter
> lines(bounds[,c('x','ymin')]); lines(bounds[,c('x','ymax')])
> p <- Predict(fit, age, cholesterol)  # use only one sex
> bplot(p, perim=bounds)   # draws image() plot
>                          # don't show estimates where data are sparse
>                          # doesn't make sense here since vars don't interact
> bplot(p, plogis(yhat) ~ age*cholesterol) # Probability scale
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’

> nameEx("calibrate")
> ### * calibrate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate
> ### Title: Resampling Model Calibration
> ### Aliases: calibrate calibrate.default calibrate.cph calibrate.orm
> ###   calibrate.psm print.calibrate print.calibrate.default plot.calibrate
> ###   plot.calibrate.default
> ### Keywords: methods models regression survival hplot
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> set.seed(1)
> n <- 200
> d.time <- rexp(n)
> x1 <- runif(n)
> x2 <- factor(sample(c('a', 'b', 'c'), n, TRUE))
> f <- cph(Surv(d.time) ~ pol(x1,2) * x2, x=TRUE, y=TRUE, surv=TRUE, time.inc=1.5)
> #or f <- psm(S ~ \dots)
> pa <- requireNamespace('polspline')
> if(pa) {
+  cal <- calibrate(f, u=1.5, B=20)  # cmethod='hare'
+  plot(cal)
+ }
Using Cox survival estimates at 1.5 Days
> cal <- calibrate(f, u=1.5, cmethod='KM', m=50, B=20)  # usually B=200 or 300
Using Cox survival estimates at 1.5 Days
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts) :
  one interval had < 2 observations
> plot(cal, add=pa)
> 
> set.seed(1)
> y <- sample(0:2, n, TRUE)
> x1 <- runif(n)
> x2 <- runif(n)
> x3 <- runif(n)
> x4 <- runif(n)
> f <- lrm(y ~ x1 + x2 + x3 * x4, x=TRUE, y=TRUE)
> cal <- calibrate(f, kint=2, predy=seq(.2, .8, length=60), 
+                  group=y)
> # group= does k-sample validation: make resamples have same 
> # numbers of subjects in each level of y as original sample
> 
> plot(cal)

n=200   Mean absolute error=0.031   Mean squared error=0.00265
0.9 Quantile of absolute error=0.082

> #See the example for the validate function for a method of validating
> #continuation ratio ordinal logistic models.  You can do the same
> #thing for calibrate
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("contrast")
> ### * contrast
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: contrast.rms
> ### Title: General Contrasts of Regression Coefficients
> ### Aliases: contrast contrast.rms print.contrast.rms
> ### Keywords: htest models regression
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> set.seed(1)
> age <- rnorm(200,40,12)
> sex <- factor(sample(c('female','male'),200,TRUE))
> logit <- (sex=='male') + (age-40)/5
> y <- ifelse(runif(200) <= plogis(logit), 1, 0)
> f <- lrm(y ~ pol(age,2)*sex)
> anova(f)
                Wald Statistics          Response: y 

 Factor                                   Chi-Square d.f. P     
 age  (Factor+Higher Order Factors)       45.73      4    <.0001
  All Interactions                         1.58      2    0.453 
  Nonlinear (Factor+Higher Order Factors)  0.26      2    0.876 
 sex  (Factor+Higher Order Factors)        6.11      3    0.106 
  All Interactions                         1.58      2    0.453 
 age * sex  (Factor+Higher Order Factors)  1.58      2    0.453 
  Nonlinear                                0.03      1    0.860 
  Nonlinear Interaction : f(A,B) vs. AB    0.03      1    0.860 
 TOTAL NONLINEAR                           0.26      2    0.876 
 TOTAL NONLINEAR + INTERACTION             2.34      3    0.504 
 TOTAL                                    46.81      5    <.0001
> # Compare a 30 year old female to a 40 year old male
> # (with or without age x sex interaction in the model)
> contrast(f, list(sex='female', age=30), list(sex='male', age=40))
   Contrast  S.E. Lower Upper    Z Pr(>|z|)
11    -3.05 0.636  -4.3  -1.8 -4.8        0

Confidence intervals are 0.95 individual intervals
> # Test for interaction between age and sex, duplicating anova
> contrast(f, list(sex='female', age=30),
+             list(sex='male',   age=30),
+             list(sex='female', age=c(40,50)),
+             list(sex='male',   age=c(40,50)), type='joint')
   Contrast  S.E.  Lower Upper    Z Pr(>|z|)
11    0.848 0.919 -0.954  2.65 0.92    0.356
22    1.962 1.578 -1.131  5.05 1.24    0.214

Joint test for all contrasts=0:

Chi-square=1.58 with 2 d.f.  P=0.453

Confidence intervals are 0.95 individual intervals
> # Duplicate overall sex effect in anova with 3 d.f.
> contrast(f, list(sex='female', age=c(30,40,50)),
+             list(sex='male',   age=c(30,40,50)), type='joint')
  age Contrast  S.E. Lower   Upper     Z Pr(>|z|)
1  30   -0.198 0.796 -1.76  1.3629 -0.25   0.8040
2  40   -1.045 0.487 -2.00 -0.0907 -2.15   0.0319
3  50   -2.159 1.273 -4.65  0.3352 -1.70   0.0898

Joint test for all contrasts=0:

Chi-square=6.11 with 3 d.f.  P=0.106

Confidence intervals are 0.95 individual intervals
> # For females get an array of odds ratios against age=40
> k <- contrast(f, list(sex='female', age=30:50),
+                  list(sex='female', age=40))
> print(k, fun=exp)
        sex Contrast  Lower  Upper     Z Pr(>|z|)
 1   female    0.135 0.0461  0.394 -3.66    3e-04
 2   female    0.167 0.0663  0.421 -3.79    1e-04
 3*  female    0.207 0.0941  0.453 -3.93    1e-04
 4*  female    0.254 0.1318  0.491 -4.08    0e+00
 5*  female    0.312 0.1822  0.536 -4.23    0e+00
 6*  female    0.382 0.2486  0.588 -4.38    0e+00
 7*  female    0.467 0.3352  0.649 -4.52    0e+00
 8*  female    0.567 0.4469  0.720 -4.66    0e+00
 9*  female    0.687 0.5897  0.801 -4.79    0e+00
10*  female    0.830 0.7710  0.895 -4.90    0e+00
11*  female    1.000 1.0000  1.000   NaN      NaN
12*  female    1.200 1.1178  1.289  5.03    0e+00
13*  female    1.436 1.2476  1.652  5.05    0e+00
14*  female    1.712 1.3880  2.111  5.02    0e+00
15*  female    2.035 1.5368  2.694  4.96    0e+00
16*  female    2.410 1.6905  3.437  4.86    0e+00
17*  female    2.846 1.8446  4.391  4.73    0e+00
18*  female    3.350 1.9940  5.627  4.57    0e+00
19*  female    3.929 2.1326  7.240  4.39    0e+00
20*  female    4.594 2.2543  9.364  4.20    0e+00
21*  female    5.355 2.3533 12.184  4.00    1e-04

Redundant contrasts are denoted by *

Confidence intervals are 0.95 individual intervals
> # Plot odds ratios with pointwise 0.95 confidence bands using log scale
> k <- as.data.frame(k[c('Contrast','Lower','Upper')])
> ggplot(k, aes(x=30:50, y=exp(Contrast))) + geom_line() +
+    geom_ribbon(aes(ymin=exp(Lower), ymax=exp(Upper)),
+                alpha=0.15, linetype=0) +
+    scale_y_continuous(trans='log10', n.breaks=10,
+                minor_breaks=c(seq(0.1, 1, by=.1), seq(1, 10, by=.5))) +
+   xlab('Age') + ylab('OR against age 40')
> 
> # For an ordinal model with 3 variables (x1 is quadratic, x2 & x3 linear)
> # Get a 1 d.f. likelihood ratio (LR) test for x1=1 vs x1=0.25
> # For the other variables get contrasts and LR tests that are the
> # ordinary ones for their original coefficients.
> # Get 0.95 profile likelihood confidence intervals for the x1 contrast
> # and for the x2 and x3 coefficients
> set.seed(7)
> x1 <- runif(50)
> x2 <- runif(50)
> x3 <- runif(50)
> dd <- datadist(x1, x2, x3); options(datadist='dd')
> y <- x1 + runif(50)   # need x=TRUE,y=TRUE for profile likelihood
> f <- orm(y ~ pol(x1, 2) + x2 + x3, x=TRUE, y=TRUE)
> a <- list(x1=c(   1,0,0), x2=c(0,1,0), x3=c(0,0,1))
> b <- list(x1=c(0.25,0,0), x2=c(0,0,0), x3=c(0,0,0))
> k <- contrast(f, a, b, expand=FALSE)      # Wald intervals and tests
> k; k$X[1,]
   Contrast S.E. Lower Upper     Z Pr(>|z|)
11    3.676 0.88  1.95  5.40  4.18    0.000
22   -0.939 1.12 -3.13  1.26 -0.84    0.402
33   -1.021 1.10 -3.17  1.13 -0.93    0.351

Confidence intervals are 0.95 individual intervals
                 pol(x1, 2)x1 pol(x1, 2)x1^2             x2             x3 
         0.000          0.750          0.938          0.000          0.000 
> summary(f, x1=c(.25, 1), x2=0:1, x3=0:1)  # Wald intervals
             Effects              Response : y 

 Factor      Low  High Diff. Effect S.E. Lower 0.95 Upper 0.95
 x1          0.25 1    0.75   3.676 0.88  1.9521      5.40    
  Odds Ratio 0.25 1    0.75  39.497   NA  7.0437    221.48    
 x2          0.00 1    1.00  -0.939 1.12 -3.1329      1.26    
  Odds Ratio 0.00 1    1.00   0.391   NA  0.0436      3.51    
 x3          0.00 1    1.00  -1.021 1.10 -3.1672      1.13    
  Odds Ratio 0.00 1    1.00   0.360   NA  0.0421      3.08    

> anova(f, test='LR')                       # LR tests
                Likelihood Ratio Statistics          Response: y 

 Factor     Chi-Square d.f. P     
 x1         29.76      2    <.0001
  Nonlinear  0.24      1    0.623 
 x2          0.70      1    0.402 
 x3          0.88      1    0.349 
 TOTAL      33.89      4    <.0001
> contrast(f, a, b, expand=FALSE, conf.type='profile', plot_profile=TRUE)
   Contrast Lower Upper    Χ² Pr(>Χ²)
11    3.676  2.02  5.49 19.95   0.000
22   -0.939 -3.15  1.26  0.70   0.402
33   -1.021 -3.20  1.11  0.88   0.349

Confidence intervals are 0.95 profile likelihood intervals
> options(datadist=NULL)
> 
> 
> # For a model containing two treatments, centers, and treatment
> # x center interaction, get 0.95 confidence intervals separately
> # by center
> center <- factor(sample(letters[1 : 8], 500, TRUE))
> treat  <- factor(sample(c('a','b'), 500, TRUE))
> y      <- 8*(treat == 'b') + rnorm(500, 100, 20)
> f <- ols(y ~ treat*center)
> 
> 
> lc <- levels(center)
> contrast(f, list(treat='b', center=lc),
+             list(treat='a', center=lc))
  center Contrast S.E.  Lower Upper     t Pr(>|t|)
1      a     2.70 4.60  -6.33 11.73  0.59   0.5576
2      b     1.61 4.60  -7.42 10.64  0.35   0.7259
3      c     0.72 5.38  -9.86 11.29  0.13   0.8937
4      d     2.65 4.77  -6.73 12.04  0.56   0.5786
5      e    16.10 4.68   6.90 25.30  3.44   0.0006
6      f     8.38 5.07  -1.58 18.35  1.65   0.0990
7      g    12.13 4.96   2.39 21.86  2.45   0.0148
8      h    -2.99 5.29 -13.39  7.41 -0.56   0.5723

Error d.f.= 484 

Confidence intervals are 0.95 individual intervals
> 
> 
> # Get 'Type III' contrast: average b - a treatment effect over
> # centers, weighting centers equally (which is almost always
> # an unreasonable thing to do)
> contrast(f, list(treat='b', center=lc),
+             list(treat='a', center=lc),
+          type='average')
  Contrast S.E. Lower Upper    t Pr(>|t|)  var
1     5.16 1.74  1.74  8.59 2.96   0.0032 3.03

Error d.f.= 484 

Confidence intervals are 0.95 individual intervals
> 
> 
> # Get 'Type II' contrast, weighting centers by the number of
> # subjects per center.  Print the design contrast matrix used.
> k <- contrast(f, list(treat='b', center=lc),
+                  list(treat='a', center=lc),
+               type='average', weights=table(center))
> print(k, X=TRUE)
  Contrast S.E. Lower Upper    t Pr(>|t|) var
1     5.39 1.73  1.99  8.79 3.11    0.002   3

Error d.f.= 484 

Confidence intervals are 0.95 individual intervals

Design Matrix for Contrasts

   treatb centerb centerc centerd centere centerf centerg centerh
 0      1       0       0       0       0       0       0       0
 treatb:centerb treatb:centerc treatb:centerd treatb:centere treatb:centerf
           0.14          0.108          0.128          0.138           0.12
 treatb:centerg treatb:centerh
           0.12          0.106
> # Note: If other variables had interacted with either treat
> # or center, we may want to list settings for these variables
> # inside the list()'s, so as to not use default settings
> 
> 
> # For a 4-treatment study, get all comparisons with treatment 'a'
> treat  <- factor(sample(c('a','b','c','d'),  500, TRUE))
> y      <- 8*(treat == 'b') + rnorm(500, 100, 20)
> dd     <- datadist(treat, center); options(datadist='dd')
> f <- ols(y ~ treat*center)
> lt <- levels(treat)
> contrast(f, list(treat=lt[-1]),
+             list(treat=lt[ 1]),
+          cnames=paste(lt[-1], lt[1], sep=':'), conf.int=1 - .05 / 3)
     treat Contrast S.E.  Lower Upper     t Pr(>|t|)
1b:a     b   11.582 8.69  -9.29  32.5  1.33    0.183
2c:a     c   -0.219 6.85 -16.68  16.2 -0.03    0.975
3d:a     d   -1.467 6.23 -16.43  13.5 -0.24    0.814

Error d.f.= 468 

Confidence intervals are 0.983 individual intervals
> 
> 
> # Compare each treatment with average of all others
> for(i in 1 : length(lt)) {
+   cat('Comparing with', lt[i], '\n\n')
+   print(contrast(f, list(treat=lt[-i]),
+                     list(treat=lt[ i]), type='average'))
+ }
Comparing with a 

  Contrast S.E. Lower Upper    t Pr(>|t|)  var
1      3.3 5.68 -7.87  14.5 0.58    0.562 32.3

Error d.f.= 468 

Confidence intervals are 0.95 individual intervals
Comparing with b 

  Contrast S.E. Lower Upper     t Pr(>|t|) var
1    -12.1 7.81 -27.5  3.21 -1.55    0.121  61

Error d.f.= 468 

Confidence intervals are 0.95 individual intervals
Comparing with c 

  Contrast S.E. Lower Upper   t Pr(>|t|)  var
1     3.59 5.97 -8.14  15.3 0.6    0.548 35.7

Error d.f.= 468 

Confidence intervals are 0.95 individual intervals
Comparing with d 

  Contrast S.E. Lower Upper    t Pr(>|t|)  var
1     5.25 5.33 -5.23  15.7 0.99    0.325 28.4

Error d.f.= 468 

Confidence intervals are 0.95 individual intervals
> options(datadist=NULL)
> 
> # Six ways to get the same thing, for a variable that
> # appears linearly in a model and does not interact with
> # any other variables.  We estimate the change in y per
> # unit change in a predictor x1.  Methods 4, 5 also
> # provide confidence limits.  Method 6 computes nonparametric
> # bootstrap confidence limits.  Methods 2-6 can work
> # for models that are nonlinear or non-additive in x1.
> # For that case more care is needed in choice of settings
> # for x1 and the variables that interact with x1.
> 
> 
> ## Not run: 
> ##D coef(fit)['x1']                            # method 1
> ##D diff(predict(fit, gendata(x1=c(0,1))))     # method 2
> ##D g <- Function(fit)                         # method 3
> ##D g(x1=1) - g(x1=0)
> ##D summary(fit, x1=c(0,1))                    # method 4
> ##D k <- contrast(fit, list(x1=1), list(x1=0)) # method 5
> ##D print(k, X=TRUE)
> ##D fit <- update(fit, x=TRUE, y=TRUE)         # method 6
> ##D b <- bootcov(fit, B=500)
> ##D contrast(fit, list(x1=1), list(x1=0))
> ##D 
> ##D 
> ##D # In a model containing age, race, and sex,
> ##D # compute an estimate of the mean response for a
> ##D # 50 year old male, averaged over the races using
> ##D # observed frequencies for the races as weights
> ##D 
> ##D 
> ##D f <- ols(y ~ age + race + sex)
> ##D contrast(f, list(age=50, sex='male', race=levels(race)),
> ##D          type='average', weights=table(race))
> ##D 
> ##D # For a Bayesian model get the highest posterior interval for the
> ##D # difference in two nonlinear functions of predicted values
> ##D # Start with the mean from a proportional odds model
> ##D g <- blrm(y ~ x)
> ##D M <- Mean(g)
> ##D contrast(g, list(x=1), list(x=0), fun=M)
> ##D 
> ##D # For the median we have to make sure that contrast can pass the
> ##D # per-posterior-draw vector of intercepts through
> ##D qu <- Quantile(g)
> ##D med <- function(lp, intercepts) qu(0.5, lp, intercepts=intercepts)
> ##D contrast(g, list(x=1), list(x=0), fun=med)
> ## End(Not run)
> 
> 
> # Plot the treatment effect (drug - placebo) as a function of age
> # and sex in a model in which age nonlinearly interacts with treatment
> # for females only
> 
> set.seed(1)
> n <- 800
> treat <- factor(sample(c('drug','placebo'), n,TRUE))
> sex   <- factor(sample(c('female','male'),  n,TRUE))
> age   <- rnorm(n, 50, 10)
> y     <- .05*age + (sex=='female')*(treat=='drug')*.05*abs(age-50) + rnorm(n)
> f     <- ols(y ~ rcs(age,4)*treat*sex)
> d     <- datadist(age, treat, sex); options(datadist='d')
> 
> # show separate estimates by treatment and sex
> 
> require(ggplot2)
> ggplot(Predict(f, age, treat, sex='female'))
> ggplot(Predict(f, age, treat, sex='male'))
> ages  <- seq(35,65,by=5); sexes <- c('female','male')
> w     <- contrast(f, list(treat='drug',    age=ages, sex=sexes),
+                      list(treat='placebo', age=ages, sex=sexes))
> # add conf.type="simultaneous" to adjust for having done 14 contrasts
> xYplot(Cbind(Contrast, Lower, Upper) ~ age | sex, data=w,
+        ylab='Drug - Placebo')
> w <- as.data.frame(w[c('age','sex','Contrast','Lower','Upper')])
> ggplot(w, aes(x=age, y=Contrast)) + geom_point() + facet_grid(sex ~ .) +
+    geom_errorbar(aes(ymin=Lower, ymax=Upper), width=0)
> ggplot(w, aes(x=age, y=Contrast)) + geom_line() + facet_grid(sex ~ .) +
+    geom_ribbon(aes(ymin=Lower, ymax=Upper), width=0, alpha=0.15, linetype=0)
Warning in geom_ribbon(aes(ymin = Lower, ymax = Upper), width = 0, alpha = 0.15,  :
  Ignoring unknown parameters: `width`
> xYplot(Cbind(Contrast, Lower, Upper) ~ age, groups=sex, data=w,
+        ylab='Drug - Placebo', method='alt bars')
> options(datadist=NULL)
> 
> 
> # Examples of type='joint' contrast tests
> 
> set.seed(1)
> x1 <- rnorm(100)
> x2 <- factor(sample(c('a','b','c'), 100, TRUE))
> dd <- datadist(x1, x2); options(datadist='dd')
> y  <- x1 + (x2=='b') + rnorm(100)
> 
> # First replicate a test statistic from anova()
> 
> f <- ols(y ~ x2)
> anova(f)
                Analysis of Variance          Response: y 

 Factor     d.f. Partial SS MS    F    P     
 x2          2    25        12.49 6.02 0.0034
 REGRESSION  2    25        12.49 6.02 0.0034
 ERROR      97   201         2.08            
> contrast(f, list(x2=c('b','c')), list(x2='a'), type='joint')
   Contrast  S.E.  Lower Upper    t Pr(>|t|)
11    1.166 0.336  0.499  1.83 3.47   0.0008
22    0.547 0.363 -0.173  1.27 1.51   0.1350

Joint test for all contrasts=0:

F(2,97)=6.02, P=0.0034

Error d.f.= 97 

Confidence intervals are 0.95 individual intervals
> 
> # Repeat with a redundancy; compare a vs b, a vs c, b vs c
> 
> contrast(f, list(x2=c('a','a','b')), list(x2=c('b','c','c')), type='joint')
     Contrast  S.E.  Lower  Upper     t Pr(>|t|)
1  1   -1.166 0.336 -1.833 -0.499 -3.47   0.0008
2  2   -0.547 0.363 -1.268  0.173 -1.51   0.1350
3* 3    0.619 0.375 -0.126  1.364  1.65   0.1025

Redundant contrasts are denoted by *

Joint test for all contrasts=0:

F(2,97)=6.02, P=0.0034

Error d.f.= 97 

Confidence intervals are 0.95 individual intervals
> 
> # Get a test of association of a continuous predictor with y
> # First assume linearity, then cubic
> 
> f <- lrm(y>0 ~ x1 + x2)
> anova(f)
                Wald Statistics          Response: y > 0 

 Factor     Chi-Square d.f. P     
 x1         21.03      1    <.0001
 x2          5.97      2    0.0506
 TOTAL      22.41      3    0.0001
> contrast(f, list(x1=1), list(x1=0), type='joint')  # a minimum set of contrasts
  x2 Contrast  S.E. Lower Upper    Z Pr(>|z|)
1  a     2.18 0.474  1.25  3.11 4.59        0

Joint test for all contrasts=0:

Chi-square=21 with 1 d.f.  P=0

Confidence intervals are 0.95 individual intervals
> xs <- seq(-2, 2, length=20)
> contrast(f, list(x1=0), list(x1=xs), type='joint')
     x2 Contrast   S.E.  Lower  Upper     Z Pr(>|z|)
 1    a    4.351 0.9488  2.491  6.210  4.59        0
 2*   a    3.893 0.8489  2.229  5.557  4.59        0
 3*   a    3.435 0.7490  1.967  4.903  4.59        0
 4*   a    2.977 0.6492  1.705  4.249  4.59        0
 5*   a    2.519 0.5493  1.442  3.596  4.59        0
 6*   a    2.061 0.4494  1.180  2.942  4.59        0
 7*   a    1.603 0.3495  0.918  2.288  4.59        0
 8*   a    1.145 0.2497  0.656  1.634  4.59        0
 9*   a    0.687 0.1498  0.393  0.981  4.59        0
10*   a    0.229 0.0499  0.131  0.327  4.59        0
11*   a   -0.229 0.0499 -0.327 -0.131 -4.59        0
12*   a   -0.687 0.1498 -0.981 -0.393 -4.59        0
13*   a   -1.145 0.2497 -1.634 -0.656 -4.59        0
14*   a   -1.603 0.3495 -2.288 -0.918 -4.59        0
15*   a   -2.061 0.4494 -2.942 -1.180 -4.59        0
16*   a   -2.519 0.5493 -3.596 -1.442 -4.59        0
17*   a   -2.977 0.6492 -4.249 -1.705 -4.59        0
18*   a   -3.435 0.7490 -4.903 -1.967 -4.59        0
19*   a   -3.893 0.8489 -5.557 -2.229 -4.59        0
20*   a   -4.351 0.9488 -6.210 -2.491 -4.59        0

Redundant contrasts are denoted by *

Joint test for all contrasts=0:

Chi-square=21 with 1 d.f.  P=0

Confidence intervals are 0.95 individual intervals
> 
> # All contrasts were redundant except for the first, because of
> # linearity assumption
> 
> f <- lrm(y>0 ~ pol(x1,3) + x2, x=TRUE, y=TRUE)
> anova(f)
                Wald Statistics          Response: y > 0 

 Factor     Chi-Square d.f. P     
 x1         16.76      3    0.0008
  Nonlinear  1.85      2    0.3963
 x2          6.03      2    0.0490
 TOTAL      17.98      5    0.0030
> anova(f, test='LR')   # discrepancy with Wald statistics points out a problem w/them
                Likelihood Ratio Statistics          Response: y > 0 

 Factor     Chi-Square d.f. P     
 x1         42.33      3    <.0001
  Nonlinear  2.81      2    0.2454
 x2          7.43      2    0.0244
 TOTAL      47.55      5    <.0001
> 
> contrast(f, list(x1=0), list(x1=xs), type='joint')
     x2 Contrast   S.E.    Lower   Upper     Z Pr(>|z|)
 1    a   13.674 7.9155  -1.8403 29.1878  1.73   0.0841
 2    a   10.721 5.7546  -0.5579 21.9996  1.86   0.0625
 3    a    8.232 4.0353   0.3234 16.1413  2.04   0.0413
 4*   a    6.165 2.7117   0.8500 11.4798  2.27   0.0230
 5*   a    4.475 1.7385   1.0679  7.8827  2.57   0.0100
 6*   a    3.120 1.0690   1.0250  5.2153  2.92   0.0035
 7*   a    2.056 0.6496   0.7831  3.3293  3.17   0.0015
 8*   a    1.240 0.4053   0.4457  2.0345  3.06   0.0022
 9*   a    0.629 0.2408   0.1567  1.1004  2.61   0.0090
10*   a    0.178 0.0839   0.0137  0.3428  2.12   0.0337
11*   a   -0.154 0.0871  -0.3249  0.0165 -1.77   0.0767
12*   a   -0.412 0.2619  -0.9253  0.1015 -1.57   0.1158
13*   a   -0.638 0.4214  -1.4644  0.1876 -1.51   0.1298
14*   a   -0.877 0.5573  -1.9691  0.2153 -1.57   0.1156
15*   a   -1.171 0.6975  -2.5378  0.1965 -1.68   0.0933
16*   a   -1.563 0.9312  -3.3882  0.2620 -1.68   0.0932
17*   a   -2.097 1.3784  -4.7990  0.6041 -1.52   0.1281
18*   a   -2.817 2.1231  -6.9782  1.3441 -1.33   0.1845
19*   a   -3.765 3.2124 -10.0614  2.5309 -1.17   0.2412
20*   a   -4.985 4.6852 -14.1681  4.1975 -1.06   0.2873

Redundant contrasts are denoted by *

Joint test for all contrasts=0:

Chi-square=16.8 with 3 d.f.  P=8e-04

Confidence intervals are 0.95 individual intervals
> print(contrast(f, list(x1=0), list(x1=xs), type='joint'), jointonly=TRUE)

Joint test for all contrasts=0:

Chi-square=16.8 with 3 d.f.  P=8e-04

Confidence intervals are 0.95 individual intervals
> 
> # All contrasts were redundant except for the first 3, because of
> # cubic regression assumption
> # These Wald tests and intervals are not very accurate.  Although joint
> # testing is not implemented in contrast(), individual profile likelihood
> # confidence intervals and associted likelihood ratio tests are helpful:
> # contrast(f, list(x1=0), list(x1=xs), conf.type='profile', plot_profile=TRUE)
> 
> # Now do something that is difficult to do without cryptic contrast
> # matrix operations: Allow each of the three x2 groups to have a different
> # shape for the x1 effect where x1 is quadratic.  Test whether there is
> # a difference in mean levels of y for x2='b' vs. 'c' or whether
> # the shape or slope of x1 is different between x2='b' and x2='c' regardless
> # of how they differ when x2='a'.  In other words, test whether the mean
> # response differs between group b and c at any value of x1.
> # This is a 3 d.f. test (intercept, linear, quadratic effects) and is
> # a better approach than subsetting the data to remove x2='a' then
> # fitting a simpler model, as it uses a better estimate of sigma from
> # all the data.
> 
> f <- ols(y ~ pol(x1,2) * x2)
> anova(f)
                Analysis of Variance          Response: y 

 Factor                                   d.f. Partial SS MS      F     P     
 x1  (Factor+Higher Order Factors)         6   103.708    17.2847 16.10 <.0001
  All Interactions                         4     0.942     0.2356  0.22 0.9269
  Nonlinear (Factor+Higher Order Factors)  3     0.302     0.1008  0.09 0.9632
 x2  (Factor+Higher Order Factors)         6    22.170     3.6949  3.44 0.0041
  All Interactions                         4     0.942     0.2356  0.22 0.9269
 x1 * x2  (Factor+Higher Order Factors)    4     0.942     0.2356  0.22 0.9269
  Nonlinear                                2     0.133     0.0667  0.06 0.9398
  Nonlinear Interaction : f(A,B) vs. AB    2     0.133     0.0667  0.06 0.9398
 TOTAL NONLINEAR                           3     0.302     0.1008  0.09 0.9632
 TOTAL NONLINEAR + INTERACTION             5     0.976     0.1953  0.18 0.9688
 REGRESSION                                8   128.694    16.0867 14.99 <.0001
 ERROR                                    91    97.672     1.0733             
> contrast(f, list(x1=xs, x2='b'),
+             list(x1=xs, x2='c'), type='joint')
         x1 Contrast  S.E.   Lower Upper     t Pr(>|t|)
 1   -2.000   1.0142 1.265 -1.4991  3.53  0.80   0.4249
 2   -1.789   1.0223 1.049 -1.0606  3.11  0.97   0.3322
 3   -1.579   1.0229 0.860 -0.6860  2.73  1.19   0.2375
 4*  -1.368   1.0160 0.702 -0.3782  2.41  1.45   0.1512
 5*  -1.158   1.0016 0.575 -0.1400  2.14  1.74   0.0847
 6*  -0.947   0.9796 0.480  0.0267  1.93  2.04   0.0440
 7*  -0.737   0.9502 0.416  0.1241  1.78  2.28   0.0246
 8*  -0.526   0.9132 0.378  0.1621  1.66  2.42   0.0177
 9*  -0.316   0.8688 0.358  0.1575  1.58  2.43   0.0172
10*  -0.105   0.8168 0.347  0.1278  1.51  2.35   0.0207
11*   0.105   0.7573 0.338  0.0852  1.43  2.24   0.0277
12*   0.316   0.6903 0.331  0.0324  1.35  2.08   0.0399
13*   0.526   0.6157 0.329 -0.0384  1.27  1.87   0.0647
14*   0.737   0.5337 0.342 -0.1457  1.21  1.56   0.1221
15*   0.947   0.4441 0.381 -0.3125  1.20  1.17   0.2467
16*   1.158   0.3471 0.454 -0.5553  1.25  0.76   0.4468
17*   1.368   0.2425 0.564 -0.8774  1.36  0.43   0.6681
18*   1.579   0.1304 0.707 -1.2747  1.54  0.18   0.8542
19*   1.789   0.0108 0.882 -1.7417  1.76  0.01   0.9903
20*   2.000  -0.1163 1.086 -2.2740  2.04 -0.11   0.9149

Redundant contrasts are denoted by *

Joint test for all contrasts=0:

F(3,91)=2.35, P=0.0775

Error d.f.= 91 

Confidence intervals are 0.95 individual intervals
> 
> # Note: If using a spline fit, there should be at least one value of
> # x1 between any two knots and beyond the outer knots.
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("cph")
> ### * cph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cph
> ### Title: Cox Proportional Hazards Model and Extensions
> ### Aliases: cph Survival.cph Quantile.cph Mean.cph
> ### Keywords: survival models nonparametric
> 
> ### ** Examples
> 
> # Simulate data from a population model in which the log hazard
> # function is linear in age and there is no age x sex interaction
> 
> require(survival)
Loading required package: survival
> require(ggplot2)
Loading required package: ggplot2
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('Male','Female'), n, 
+               rep=TRUE, prob=c(.6, .4)))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> dt <- -log(runif(n))/h
> label(dt) <- 'Follow-up Time'
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> dd <- datadist(age, sex)
> options(datadist='dd')
> S <- Surv(dt,e)
> 
> f <- cph(S ~ rcs(age,4) + sex, x=TRUE, y=TRUE)
> cox.zph(f, "rank")             # tests of PH
            chisq df     p
rcs(age, 4) 0.744  3 0.863
sex         2.859  1 0.091
GLOBAL      3.563  4 0.468
> anova(f)
                Wald Statistics          Response: S 

 Factor     Chi-Square d.f. P     
 age        57.75      3    <.0001
  Nonlinear  8.17      2    0.0168
 sex        18.75      1    <.0001
 TOTAL      75.63      4    <.0001
> ggplot(Predict(f, age, sex)) # plot age effect, 2 curves for 2 sexes
> survplot(f, sex)             # time on x-axis, curves for x2
> res <- resid(f, "scaledsch")
> time <- as.numeric(dimnames(res)[[1]])
> z <- loess(res[,4] ~ time, span=0.50)   # residuals for sex
> plot(time, fitted(z))
> lines(supsmu(time, res[,4]),lty=2)
> plot(cox.zph(f,"identity"))    #Easier approach for last few lines
> # latex(f)
> 
> 
> f <- cph(S ~ age + strat(sex), surv=TRUE)
> g <- Survival(f)   # g is a function
> g(seq(.1,1,by=.1), stratum="sex=Male", type="poly") #could use stratum=2
 [1] 0.999 0.996 0.994 0.993 0.992 0.992 0.992 0.992 0.992 0.991
> med <- Quantile(f)
> plot(Predict(f, age, fun=function(x) med(lp=x)))  #plot median survival
> 
> # Fit a model that is quadratic in age, interacting with sex as strata
> # Compare standard errors of linear predictor values with those from
> # coxph
> # Use more stringent convergence criteria to match with coxph
> 
> f <- cph(S ~ pol(age,2)*strat(sex), x=TRUE, eps=1e-9, iter.max=20)
Warning in coxph.control(eps = eps, toler.chol = tol, toler.inf = 1, iter.max = iter.max) :
  For numerical accuracy, tolerance should be < eps
> coef(f)
             age            age^2   age * sex=Male age^2 * sex=Male 
        0.100744        -0.000548        -0.020831         0.000276 
> se <- predict(f, se.fit=TRUE)$se.fit
> require(lattice)
Loading required package: lattice
> xyplot(se ~ age | sex, main='From cph')
> a <- c(30,50,70)
> comb <- data.frame(age=rep(a, each=2),
+                    sex=rep(levels(sex), 3))
> 
> p <- predict(f, comb, se.fit=TRUE)
> comb$yhat  <- p$linear.predictors
> comb$se    <- p$se.fit
> z <- qnorm(.975)
> comb$lower <- p$linear.predictors - z*p$se.fit
> comb$upper <- p$linear.predictors + z*p$se.fit
> comb
  age    sex    yhat    se  lower upper
1  30 Female -0.8237 1.247 -3.268 1.621
2  30   Male -1.2000 0.677 -2.527 0.127
3  50 Female  0.3142 1.459 -2.545 3.174
4  50   Male -0.0368 0.943 -1.886 1.812
5  70 Female  1.0135 1.482 -1.891 3.918
6  70   Male  0.9088 0.911 -0.876 2.694
> 
> age2 <- age^2
> f2 <- coxph(S ~ (age + age2)*strata(sex))
> coef(f2)
                 age                 age2  age:strata(sex)Male 
            0.100744            -0.000548            -0.020831 
age2:strata(sex)Male 
            0.000276 
> se <- predict(f2, se.fit=TRUE)$se.fit
> xyplot(se ~ age | sex, main='From coxph')
> comb <- data.frame(age=rep(a, each=2), age2=rep(a, each=2)^2,
+                    sex=rep(levels(sex), 3))
> p <- predict(f2, newdata=comb, se.fit=TRUE)
> comb$yhat <- p$fit
> comb$se   <- p$se.fit
> comb$lower <- p$fit - z*p$se.fit
> comb$upper <- p$fit + z*p$se.fit
> comb
  age age2    sex    yhat     se   lower  upper
1  30  900 Female -1.0414 0.3084 -1.6458 -0.437
2  30  900   Male -1.0665 0.3111 -1.6762 -0.457
3  50 2500 Female  0.0964 0.0683 -0.0375  0.230
4  50 2500   Male  0.0966 0.0979 -0.0953  0.289
5  70 4900 Female  0.7957 0.1780  0.4469  1.145
6  70 4900   Male  1.0423 0.1880  0.6738  1.411
> 
> 
> # g <- cph(Surv(hospital.charges) ~ age, surv=TRUE)
> # Cox model very useful for analyzing highly skewed data, censored or not
> # m <- Mean(g)
> # m(0)                           # Predicted mean charge for reference age
> 
> 
> #Fit a time-dependent covariable representing the instantaneous effect
> #of an intervening non-fatal event
> rm(age)
> set.seed(121)
> dframe <- data.frame(failure.time=1:10, event=rep(0:1,5),
+                      ie.time=c(NA,1.5,2.5,NA,3,4,NA,5,5,5), 
+                      age=sample(40:80,10,rep=TRUE))
> z <- ie.setup(dframe$failure.time, dframe$event, dframe$ie.time)
> S <- z$S
> ie.status <- z$ie.status
> attach(dframe[z$subs,])    # replicates all variables
> 
> f <- cph(S ~ age + ie.status, x=TRUE, y=TRUE)  
> #Must use x=TRUE,y=TRUE to get survival curves with time-dep. covariables
> 
> 
> #Get estimated survival curve for a 50-year old who has an intervening
> #non-fatal event at 5 days
> new <- data.frame(S=Surv(c(0,5), c(5,999), c(FALSE,FALSE)), age=rep(50,2),
+                   ie.status=c(0,1))
> g <- survfit(f, new)
> plot(c(0,g$time), c(1,g$surv[,2]), type='s', 
+      xlab='Days', ylab='Survival Prob.')
> # Not certain about what columns represent in g$surv for survival5
> # but appears to be for different ie.status
> #or:
> #g <- survest(f, new)
> #plot(g$time, g$surv, type='s', xlab='Days', ylab='Survival Prob.')
> 
> 
> #Compare with estimates when there is no intervening event
> new2 <- data.frame(S=Surv(c(0,5), c(5, 999), c(FALSE,FALSE)), age=rep(50,2),
+                    ie.status=c(0,0))
> g2 <- survfit(f, new2)
> lines(c(0,g2$time), c(1,g2$surv[,2]), type='s', lty=2)
> #or:
> #g2 <- survest(f, new2)
> #lines(g2$time, g2$surv, type='s', lty=2)
> detach("dframe[z$subs, ]")
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:lattice’, ‘package:ggplot2’, ‘package:survival’

> nameEx("cr.setup")
> ### * cr.setup
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cr.setup
> ### Title: Continuation Ratio Ordinal Logistic Setup
> ### Aliases: cr.setup
> ### Keywords: category models regression
> 
> ### ** Examples
> 
> y <- c(NA, 10, 21, 32, 32)
> cr.setup(y)
$y
[1] NA  1  0  1  0  0  0  0

$cohort
[1] <NA>  all   all   y>=21 all   y>=21 all   y>=21
Levels: all y>=21

$subs
[1] 1 2 3 3 4 4 5 5

$reps
[1] 1 1 2 2 2

> 
> 
> set.seed(171)
> y <- sample(0:2, 100, rep=TRUE)
> sex <- sample(c("f","m"),100,rep=TRUE)
> sex <- factor(sex)
> table(sex, y)
   y
sex  0  1  2
  f 15 20 14
  m 14 15 22
> options(digits=5)
> tapply(y==0, sex, mean)
      f       m 
0.30612 0.27451 
> tapply(y==1, sex, mean)
      f       m 
0.40816 0.29412 
> tapply(y==2, sex, mean)
      f       m 
0.28571 0.43137 
> cohort <- y>=1
> tapply(y[cohort]==1, sex[cohort], mean)
      f       m 
0.58824 0.40541 
> 
> u <- cr.setup(y)
> Y <- u$y
> cohort <- u$cohort
> sex <- sex[u$subs]
> 
> lrm(Y ~ cohort + sex)
Logistic Regression Model

lrm(formula = Y ~ cohort + sex)

                      Model Likelihood      Discrimination    Rank Discrim.    
                            Ratio Test             Indexes          Indexes    
Obs           171    LR chi2      8.98      R2       0.070    C       0.628    
 0            107    d.f.            2      R2(2,171)0.040    Dxy     0.256    
 1             64    Pr(> chi2) 0.0112    R2(2,120.1)0.056    gamma   0.336    
max |deriv| 3e-07                           Brier    0.222    tau-a   0.120    

            Coef    S.E.   Wald Z Pr(>|Z|)
Intercept   -0.6893 0.2682 -2.57  0.0102  
cohort=y>=1  0.8810 0.3260  2.70  0.0069  
sex=m       -0.4225 0.3252 -1.30  0.1939  

>  
> f <- lrm(Y ~ cohort*sex)   # saturated model - has to fit all data cells
> f
Logistic Regression Model

lrm(formula = Y ~ cohort * sex)

                      Model Likelihood      Discrimination    Rank Discrim.    
                            Ratio Test             Indexes          Indexes    
Obs           171    LR chi2      9.79      R2       0.076    C       0.628    
 0            107    d.f.            3      R2(3,171)0.039    Dxy     0.256    
 1             64    Pr(> chi2) 0.0205    R2(3,120.1)0.055    gamma   0.336    
max |deriv| 1e-07                           Brier    0.221    tau-a   0.120    

                    Coef    S.E.   Wald Z Pr(>|Z|)
Intercept           -0.8183 0.3100 -2.64  0.0083  
cohort=y>=1          1.1750 0.4664  2.52  0.0118  
sex=m               -0.1536 0.4411 -0.35  0.7277  
cohort=y>=1 * sex=m -0.5861 0.6543 -0.90  0.3703  

> 
> #Prob(y=0|female):
> # plogis(-.50078)
> #Prob(y=0|male):
> # plogis(-.50078+.11301)
> #Prob(y=1|y>=1, female):
> plogis(-.50078+.31845)
[1] 0.45454
> #Prob(y=1|y>=1, male):
> plogis(-.50078+.31845+.11301-.07379)
[1] 0.46428
> 
> combinations <- expand.grid(cohort=levels(cohort), sex=levels(sex))
> combinations
  cohort sex
1    all   f
2   y>=1   f
3    all   m
4   y>=1   m
> p <- predict(f, combinations, type="fitted")
> p
      1       2       3       4 
0.30612 0.58824 0.27451 0.40541 
> p0 <- p[c(1,3)]
> p1 <- p[c(2,4)]
> p1.unconditional <- (1 - p0) *p1
> p1.unconditional
      1       3 
0.40816 0.29412 
> p2.unconditional <- 1 - p0 - p1.unconditional
> p2.unconditional
      1       3 
0.28571 0.43137 
> 
> 
> ## Not run: 
> ##D dd <- datadist(inputdata)   # do this on non-replicated data
> ##D options(datadist='dd')
> ##D pain.severity <- inputdata$pain.severity
> ##D u <- cr.setup(pain.severity)
> ##D # inputdata frame has age, sex with pain.severity
> ##D attach(inputdata[u$subs,])  # replicate age, sex
> ##D # If age, sex already available, could do age <- age[u$subs] etc., or
> ##D # age <- rep(age, u$reps), etc.
> ##D y      <- u$y
> ##D cohort <- u$cohort
> ##D dd     <- datadist(dd, cohort)       # add to dd
> ##D f <- lrm(y ~ cohort + age*sex)       # ordinary cont. ratio model
> ##D g <- lrm(y ~ cohort*sex + age, x=TRUE,y=TRUE) # allow unequal slopes for
> ##D                                      # sex across cutoffs
> ##D cal <- calibrate(g, cluster=u$subs, subset=cohort=='all')  
> ##D # subs makes bootstrap sample the correct units, subset causes
> ##D # Predicted Prob(pain.severity=0) to be checked for calibration
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("datadist")
> ### * datadist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: datadist
> ### Title: Distribution Summaries for Predictor Variables
> ### Aliases: datadist print.datadist
> ### Keywords: models nonparametric regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D d <- datadist(data=1)         # use all variables in search pos. 1
> ##D d <- datadist(x1, x2, x3)
> ##D page(d)                       # if your options(pager) leaves up a pop-up
> ##D                               # window, this is a useful guide in analyses
> ##D d <- datadist(data=2)         # all variables in search pos. 2
> ##D d <- datadist(data=my.data.frame)
> ##D d <- datadist(my.data.frame)  # same as previous.  Run for all potential vars.
> ##D d <- datadist(x2, x3, data=my.data.frame)   # combine variables
> ##D d <- datadist(x2, x3, q.effect=c(.1,.9), q.display=c(0,1))
> ##D # uses inter-decile range odds ratios,
> ##D # total range of variables for regression function plots
> ##D d <- datadist(d, z)           # add a new variable to an existing datadist
> ##D options(datadist="d")         #often a good idea, to store info with fit
> ##D f <- ols(y ~ x1*x2*x3)
> ##D 
> ##D 
> ##D options(datadist=NULL)        #default at start of session
> ##D f <- ols(y ~ x1*x2)
> ##D d <- datadist(f)              #info not stored in `f'
> ##D d$limits["Adjust to","x1"] <- .5   #reset adjustment level to .5
> ##D options(datadist="d")
> ##D 
> ##D 
> ##D f <- lrm(y ~ x1*x2, data=mydata)
> ##D d <- datadist(f, data=mydata)
> ##D options(datadist="d")
> ##D 
> ##D 
> ##D f <- lrm(y ~ x1*x2)           #datadist not used - specify all values for
> ##D summary(f, x1=c(200,500,800), x2=c(1,3,5))         # obtaining predictions
> ##D plot(Predict(f, x1=200:800, x2=3))  # or ggplot()
> ##D 
> ##D 
> ##D # Change reference value to get a relative odds plot for a logistic model
> ##D d$limits$age[2] <- 30    # make 30 the reference value for age
> ##D # Could also do: d$limits["Adjust to","age"] <- 30
> ##D fit <- update(fit)   # make new reference value take effect
> ##D plot(Predict(fit, age, ref.zero=TRUE, fun=exp),
> ##D      ylab='Age=x:Age=30 Odds Ratio')   # or ggplot()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("fastbw")
> ### * fastbw
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fastbw
> ### Title: Fast Backward Variable Selection
> ### Aliases: fastbw print.fastbw
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D fastbw(fit, optional.arguments)     # print results
> ##D z <- fastbw(fit, optional.args)     # typically used in simulations
> ##D lm.fit(X[,z$parms.kept], Y)         # least squares fit of reduced model
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gIndex")
> ### * gIndex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gIndex
> ### Title: Calculate Total and Partial g-indexes for an rms Fit
> ### Aliases: gIndex print.gIndex plot.gIndex
> ### Keywords: predictive accuracy robust univar
> 
> ### ** Examples
> 
> set.seed(1)
> n <- 40
> x <- 1:n
> w <- factor(sample(c('a','b'), n, TRUE))
> u <- factor(sample(c('A','B'), n, TRUE))
> y <- .01*x + .2*(w=='b') + .3*(u=='B') + .2*(w=='b' & u=='B') + rnorm(n)/5
> dd <- datadist(x,w,u); options(datadist='dd')
> f <- ols(y ~ x*w*u, x=TRUE, y=TRUE)
> f
Linear Regression Model

ols(formula = y ~ x * w * u, x = TRUE, y = TRUE)

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs      40    LR chi2     42.69    R2       0.656    
sigma0.1885    d.f.            7    R2 adj   0.581    
d.f.     32    Pr(> chi2) 0.0000    g        0.263    

Residuals

      Min        1Q    Median        3Q       Max 
-0.318329 -0.116336  0.001699  0.079795  0.472002 


              Coef    S.E.   t     Pr(>|t|)
Intercept      0.2011 0.1443  1.39 0.1732  
x              0.0057 0.0057  0.99 0.3278  
w=b            0.2414 0.2217  1.09 0.2843  
u=B            0.1124 0.1719  0.65 0.5177  
x * w=b       -0.0034 0.0088 -0.38 0.7031  
x * u=B        0.0043 0.0075  0.58 0.5662  
w=b * u=B      0.0558 0.2717  0.21 0.8386  
x * w=b * u=B  0.0051 0.0111  0.46 0.6480  

> anova(f)
Warning in anova.rms(f) :
  tests of nonlinear interaction with respect to single component 
variables ignore 3-way interactions
                Analysis of Variance          Response: y 

 Factor                                   d.f. Partial SS MS        F    P     
 x  (Factor+Higher Order Factors)          4   0.4044281  0.1011070 2.85 0.0399
  All Interactions                         3   0.0583654  0.0194551 0.55 0.6533
 w  (Factor+Higher Order Factors)          4   0.7343790  0.1835948 5.17 0.0025
  All Interactions                         3   0.0642051  0.0214017 0.60 0.6182
 u  (Factor+Higher Order Factors)          4   0.7553646  0.1888412 5.32 0.0021
  All Interactions                         3   0.1240289  0.0413430 1.16 0.3388
 x * w  (Factor+Higher Order Factors)      2   0.0075815  0.0037907 0.11 0.8991
 x * u  (Factor+Higher Order Factors)      2   0.0582252  0.0291126 0.82 0.4497
 w * u  (Factor+Higher Order Factors)      2   0.0635351  0.0317676 0.89 0.4189
 x * w * u  (Factor+Higher Order Factors)  1   0.0075462  0.0075462 0.21 0.6480
 TOTAL INTERACTION                         4   0.1240951  0.0310238 0.87 0.4907
 REGRESSION                                7   2.1681606  0.3097372 8.72 <.0001
 ERROR                                    32   1.1368328  0.0355260            
> z <- list()
> for(type in c('terms','cterms','ccterms'))
+   {
+     zc <- predict(f, type=type)
+     cat('type:', type, '\n')
+     print(zc)
+     z[[type]] <- zc
+   }
type: terms 
            x       w        u      x * w      x * u    w * u x * w * u
1  -0.1105472 0.00000  0.00000  0.0000000 -0.0842429 0.000000  0.000000
2  -0.1048781 0.24144  0.00000 -0.0067938 -0.0799227 0.055783  0.010271
3  -0.0992090 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
4  -0.0935399 0.00000  0.00000  0.0000000 -0.0712825 0.000000  0.000000
5  -0.0878709 0.24144  0.00000 -0.0169845 -0.0669623 0.055783  0.025677
6  -0.0822018 0.00000  0.00000  0.0000000 -0.0626422 0.000000  0.000000
7  -0.0765327 0.00000  0.00000  0.0000000 -0.0583220 0.000000  0.000000
8  -0.0708636 0.00000  0.00000  0.0000000 -0.0540019 0.000000  0.000000
9  -0.0651945 0.24144 -0.11241 -0.0305720 -0.0885630 0.000000  0.000000
10 -0.0595254 0.24144 -0.11241 -0.0339689 -0.0885630 0.000000  0.000000
11 -0.0538563 0.00000  0.00000  0.0000000 -0.0410414 0.000000  0.000000
12 -0.0481872 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
13 -0.0425182 0.00000  0.00000  0.0000000 -0.0324011 0.000000  0.000000
14 -0.0368491 0.00000  0.00000  0.0000000 -0.0280810 0.000000  0.000000
15 -0.0311800 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
16 -0.0255109 0.24144 -0.11241 -0.0543502 -0.0885630 0.000000  0.000000
17 -0.0198418 0.24144  0.00000 -0.0577471 -0.0151205 0.055783  0.087302
18 -0.0141727 0.24144  0.00000 -0.0611440 -0.0108004 0.055783  0.092437
19 -0.0085036 0.24144  0.00000 -0.0645409 -0.0064802 0.055783  0.097573
20 -0.0028345 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
21  0.0028345 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
22  0.0085036 0.00000  0.00000  0.0000000  0.0064802 0.000000  0.000000
23  0.0141727 0.00000  0.00000  0.0000000  0.0108004 0.000000  0.000000
24  0.0198418 0.00000  0.00000  0.0000000  0.0151205 0.000000  0.000000
25  0.0255109 0.00000  0.00000  0.0000000  0.0194407 0.000000  0.000000
26  0.0311800 0.00000  0.00000  0.0000000  0.0237608 0.000000  0.000000
27  0.0368491 0.24144  0.00000 -0.0917160  0.0280810 0.055783  0.138656
28  0.0425182 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
29  0.0481872 0.00000  0.00000  0.0000000  0.0367213 0.000000  0.000000
30  0.0538563 0.24144  0.00000 -0.1019067  0.0410414 0.055783  0.154062
31  0.0595254 0.24144  0.00000 -0.1053036  0.0453616 0.055783  0.159197
32  0.0651945 0.24144  0.00000 -0.1087005  0.0496817 0.055783  0.164333
33  0.0708636 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
34  0.0765327 0.24144 -0.11241 -0.1154943 -0.0885630 0.000000  0.000000
35  0.0822018 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
36  0.0878709 0.00000  0.00000  0.0000000  0.0669623 0.000000  0.000000
37  0.0935399 0.24144  0.00000 -0.1256849  0.0712825 0.055783  0.190010
38  0.0992090 0.00000 -0.11241  0.0000000 -0.0885630 0.000000  0.000000
39  0.1048781 0.24144 -0.11241 -0.1324787 -0.0885630 0.000000  0.000000
40  0.1105472 0.24144  0.00000 -0.1358756  0.0842429 0.055783  0.205416
type: cterms 
           x       w       u
1  0.0099892 0.00000 0.11673
2  0.0234555 0.30070 0.18710
3  0.0170073 0.00000 0.00000
4  0.0399569 0.00000 0.12969
5  0.0586387 0.30592 0.21547
6  0.0599354 0.00000 0.13833
7  0.0699247 0.00000 0.14265
8  0.0799139 0.00000 0.14697
9  0.0204498 0.21087 0.00000
10 0.0227220 0.20747 0.00000
11 0.1098816 0.00000 0.15993
12 0.0680290 0.00000 0.00000
13 0.1298601 0.00000 0.16857
14 0.1398493 0.00000 0.17289
15 0.0850363 0.00000 0.00000
16 0.0363552 0.18709 0.00000
17 0.1993717 0.32678 0.32894
18 0.2110994 0.32852 0.33839
19 0.2228272 0.33026 0.34785
20 0.1133817 0.00000 0.00000
21 0.1190508 0.00000 0.00000
22 0.2197632 0.00000 0.20745
23 0.2297524 0.00000 0.21177
24 0.2397417 0.00000 0.21609
25 0.2497309 0.00000 0.22041
26 0.2597201 0.00000 0.22473
27 0.3166491 0.34416 0.42349
28 0.1587344 0.00000 0.00000
29 0.2896878 0.00000 0.23769
30 0.3518324 0.34938 0.45186
31 0.3635601 0.35112 0.46131
32 0.3752879 0.35286 0.47077
33 0.1870799 0.00000 0.00000
34 0.0772547 0.12595 0.00000
35 0.1984181 0.00000 0.00000
36 0.3596125 0.00000 0.26793
37 0.4339266 0.36155 0.51805
38 0.2154253 0.00000 0.00000
39 0.0886157 0.10896 0.00000
40 0.4691098 0.36677 0.54641
type: ccterms 
       x, u, w
 [1,] 0.122398
 [2,] 0.433089
 [3,] 0.017007
 [4,] 0.152365
 [5,] 0.468272
 [6,] 0.172344
 [7,] 0.182333
 [8,] 0.192322
 [9,] 0.261892
[10,] 0.264164
[11,] 0.222290
[12,] 0.068029
[13,] 0.242269
[14,] 0.252258
[15,] 0.085036
[16,] 0.277797
[17,] 0.609005
[18,] 0.620733
[19,] 0.632461
[20,] 0.113382
[21,] 0.119051
[22,] 0.332172
[23,] 0.342161
[24,] 0.352150
[25,] 0.362139
[26,] 0.372129
[27,] 0.726283
[28,] 0.158734
[29,] 0.402096
[30,] 0.761466
[31,] 0.773194
[32,] 0.784921
[33,] 0.187080
[34,] 0.318697
[35,] 0.198418
[36,] 0.472021
[37,] 0.843560
[38,] 0.215425
[39,] 0.330058
[40,] 0.878743
> 
> zc <- z$cterms
> GiniMd(zc[, 1])
[1] 0.14287
> GiniMd(zc[, 2])
[1] 0.15475
> GiniMd(zc[, 3])
[1] 0.18792
> GiniMd(f$linear.predictors)
[1] 0.26271
> g <- gIndex(f)
> g

g Index:  y ~ x * w * u 

             y
x, u, w 0.2627
Total   0.2627

> g['Total',]
[1] 0.26271
> gIndex(f, partials=FALSE)

g Index:  y ~ x * w * u 

           y
Total 0.2627

> gIndex(f, type='cterms')

g Index:  y ~ x * w * u 

           y
x     0.1429
w     0.1548
u     0.1879
Total 0.2627

> gIndex(f, type='terms')

g Index:  y ~ x * w * u 

                y
x         0.07748
w         0.11886
u         0.05246
x * w     0.04593
x * u     0.06386
w * u     0.02281
x * w * u 0.05453
Total     0.26271

> 
> y <- y > .8
> f <- lrm(y ~ x * w * u, x=TRUE, y=TRUE, reltol=1e-5)
> 
> gIndex(f, fun=plogis, funlabel='Prob[y=1]')

g Index:  y ~ x * w * u 

        log odds Odds Ratio
x, u, w    8.069       3193
Total      8.069       3193

g Index on transformed linear predictors (Prob[y=1]): 0.2561

> 
> # Manual calculation of combined main effect + interaction effort of
> # sex in a 2x2 design with treatments A B, sexes F M,
> # model -.1 + .3*(treat=='B') + .5*(sex=='M') + .4*(treat=='B' & sex=='M')
> 
> set.seed(1)
> X <- expand.grid(treat=c('A','B'), sex=c('F', 'M'))
> a <- 3; b <- 7; c <- 13; d <- 5
> X <- rbind(X[rep(1, a),], X[rep(2, b),], X[rep(3, c),], X[rep(4, d),])
> y <- with(X, -.1 + .3*(treat=='B') + .5*(sex=='M') + .4*(treat=='B' & sex=='M'))
> f <- ols(y ~ treat*sex, data=X, x=TRUE)
> gIndex(f, type='cterms')

g Index:  y ~ treat * sex 

           y
treat 0.2741
sex   0.3598
Total 0.3677

> k <- coef(f)
> b1 <- k[2]; b2 <- k[3]; b3 <- k[4]
> n <- nrow(X)
> ( (a+b)*c*abs(b2) + (a+b)*d*abs(b2+b3) + c*d*abs(b3))/(n*(n-1)/2 )
  sex=M 
0.35979 
> 
> # Manual calculation for combined age effect in a model with sex,
> # age, and age*sex interaction
> 
> a <- 13; b <- 7
> sex <- c(rep('female',a), rep('male',b))
> agef <- round(runif(a, 20, 30))
> agem <- round(runif(b, 20, 40))
> age  <- c(agef, agem)
> y <- (sex=='male') + age/10 - (sex=='male')*age/20
> f <- ols(y ~ sex*age, x=TRUE)
> f
Linear Regression Model

ols(formula = y ~ sex * age, x = TRUE)

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs      20    LR chi2       Inf    R2       1.000    
sigma0.0000    d.f.            3    R2 adj   1.000    
d.f.     16    Pr(> chi2) 0.0000    g        0.323    

Residuals

       Min         1Q     Median         3Q        Max 
-2.241e-15 -1.223e-16  1.292e-16  3.284e-16  5.129e-16 


               Coef    S.E.   t           Pr(>|t|)
Intercept       0.0000 0.0000 -7.7000e-01 0.4503  
sex=male        1.0000 0.0000  4.1111e+14 <0.0001 
age             0.1000 0.0000  1.6441e+15 <0.0001 
sex=male * age -0.0500 0.0000 -6.0174e+14 <0.0001 

> gIndex(f, type='cterms')

g Index:  y ~ sex * age 

           y
sex   0.3413
age   0.5876
Total 0.3234

> k <- coef(f)
> b1 <- k[2]; b2 <- k[3]; b3 <- k[4]
> n <- a + b
> sp <- function(w, z=w) sum(outer(w, z, function(u, v) abs(u-v)))
> 
> ( abs(b2)*sp(agef) + abs(b2+b3)*sp(agem) + 2*sp(b2*agef, (b2+b3)*agem) ) / (n*(n-1))
    age 
0.58763 
> 
> ( abs(b2)*GiniMd(agef)*a*(a-1) + abs(b2+b3)*GiniMd(agem)*b*(b-1) +
+   2*sp(b2*agef, (b2+b3)*agem) ) / (n*(n-1))
    age 
0.58763 
> 
> ## Not run: 
> ##D # Compare partial and total g-indexes over many random fits
> ##D plot(NA, NA, xlim=c(0,3), ylim=c(0,3), xlab='Global',
> ##D      ylab='x1 (black)  x2 (red)  x3 (green)  x4 (blue)')
> ##D abline(a=0, b=1, col=gray(.9))
> ##D big <- integer(3)
> ##D n <- 50   # try with n=7 - see lots of exceptions esp. for interacting var
> ##D for(i in 1:100) {
> ##D    x1 <- runif(n)
> ##D    x2 <- runif(n)
> ##D    x3 <- runif(n)
> ##D    x4 <- runif(n)
> ##D    y  <- x1 + x2 + x3 + x4 + 2*runif(n)
> ##D    f <- ols(y ~ x1*x2+x3+x4, x=TRUE)
> ##D    # f <- ols(y ~ x1+x2+x3+x4, x=TRUE)   # also try this
> ##D    w <- gIndex(f)[,1]
> ##D    gt <- w['Total']
> ##D    points(gt, w['x1, x2'])
> ##D    points(gt, w['x3'], col='green')
> ##D    points(gt, w['x4'], col='blue')
> ##D    big[1] <- big[1] + (w['x1, x2'] > gt)
> ##D    big[2] <- big[2] + (w['x3'] > gt)
> ##D    big[3] <- big[3] + (w['x4'] > gt)
> ##D    }
> ##D print(big)
> ## End(Not run)
> 
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("gendata")
> ### * gendata
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gendata
> ### Title: Generate Data Frame with Predictor Combinations
> ### Aliases: gendata
> ### Keywords: methods models regression manip
> 
> ### ** Examples
> 
> set.seed(1)
> age <- rnorm(200, 50, 10)
> sex <- factor(sample(c('female','male'),200,TRUE))
> race <- factor(sample(c('a','b','c','d'),200,TRUE))
> y <- sample(0:1, 200, TRUE)
> dd <- datadist(age,sex,race)
> options(datadist="dd")
> f <- lrm(y ~ age*sex + race)
> gendata(f)
     age    sex race
1 49.506 female    b
> gendata(f, age=50)
  age    sex race
1  50 female    b
> d <- gendata(f, age=50, sex="female")  # leave race=reference category
> d <- gendata(f, age=c(50,60), race=c("b","a"))  # 4 obs.
> d$Predicted <- predict(f, d, type="fitted")
> d      # Predicted column prints at the far right
  age    sex race Predicted
1  50 female    b   0.55666
2  60 female    b   0.59418
3  50 female    a   0.58455
4  60 female    a   0.62131
> options(datadist=NULL)
> ## Not run: 
> ##D d <- gendata(f, nobs=5, view=TRUE)        # 5 interactively defined obs.
> ##D d[,attr(d,"names.subset")]             # print variables which varied
> ##D predict(f, d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ggplot.Predict")
> ### * ggplot.Predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ggplot.Predict
> ### Title: Plot Effects of Variables Estimated by a Regression Model Fit
> ###   Using ggplot2
> ### Aliases: ggplot.Predict
> ### Keywords: models hplot htest
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> n <- 350     # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
+   .01 * (blood.pressure - 120)
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
+                x=TRUE, y=TRUE)
> an <- anova(fit)
> # Plot effects in two vertical sub-panels with continuous predictors on top
> # ggplot(Predict(fit), sepdiscrete='vertical')
> # Plot effects of all 4 predictors with test statistics from anova, and P
> ggplot(Predict(fit), anova=an, pval=TRUE)
> # ggplot(Predict(fit), rdata=llist(blood.pressure, age))
> # spike histogram plot for two of the predictors
> 
> # p <- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
> # ggplot(p)
> 
> # p <- Predict(fit, age=seq(20,80,length=100), sex, conf.int=FALSE)
> #                        # Plot relationship between age and log
>                          # odds, separate curve for each sex,
> # ggplot(p, subset=sex=='female' | age > 30)
> # No confidence interval, suppress estimates for males <= 30
> 
> # p <- Predict(fit, age, sex)
> # ggplot(p, rdata=llist(age,sex))
>                          # rdata= allows rug plots (1-dimensional scatterplots)
>                          # on each sex's curve, with sex-
>                          # specific density of age
>                          # If data were in data frame could have used that
> # p <- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
>                          # works if datadist not used
> # ggplot(p, ylab=expression(hat(P)))
>                          # plot predicted probability in place of log odds
> # per <- function(x, y) x >= 30
> # ggplot(p, perim=per)       # suppress output for age < 30 but leave scale alone
> 
> # Do ggplot2 faceting a few different ways
> p <- Predict(fit, age, sex, blood.pressure=c(120,140,160),
+              cholesterol=c(180,200,215))
> # ggplot(p)
> ggplot(p, cholesterol ~ blood.pressure)
> # ggplot(p, ~ cholesterol + blood.pressure)
> # color for sex, line type for blood.pressure:
> ggplot(p, groups=c('sex', 'blood.pressure'))
> # Add legend.position='top' to allow wider plot
> # Map blood.pressure to line thickness instead of line type:
> # ggplot(p, groups=c('sex', 'blood.pressure'), aestype=c('color', 'size'))
> 
> # Plot the age effect as an odds ratio
> # comparing the age shown on the x-axis to age=30 years
> 
> # ddist$limits$age[2] <- 30    # make 30 the reference value for age
> # Could also do: ddist$limits["Adjust to","age"] <- 30
> # fit <- update(fit)   # make new reference value take effect
> # p <- Predict(fit, age, ref.zero=TRUE, fun=exp)
> # ggplot(p, ylab='Age=x:Age=30 Odds Ratio',
> #        addlayer=geom_hline(yintercept=1, col=gray(.8)) +
> #                 geom_vline(xintercept=30, col=gray(.8)) +
> #                 scale_y_continuous(trans='log',
> #                       breaks=c(.5, 1, 2, 4, 8))))
> 
> # Compute predictions for three predictors, with superpositioning or
> # conditioning on sex, combined into one graph
> 
> p1 <- Predict(fit, age, sex)
> p2 <- Predict(fit, cholesterol, sex)
> p3 <- Predict(fit, blood.pressure, sex)
> p <- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
> ggplot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE)
> # ggplot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE, sepdiscrete='vert')
> 
> ## Not run: 
> ##D # For males at the median blood pressure and cholesterol, plot 3 types
> ##D # of confidence intervals for the probability on one plot, for varying age
> ##D ages <- seq(20, 80, length=100)
> ##D p1 <- Predict(fit, age=ages, sex='male', fun=plogis)  # standard pointwise
> ##D p2 <- Predict(fit, age=ages, sex='male', fun=plogis,
> ##D               conf.type='simultaneous')               # simultaneous
> ##D p3 <- Predict(fit, age=c(60,65,70), sex='male', fun=plogis,
> ##D               conf.type='simultaneous')               # simultaneous 3 pts
> ##D # The previous only adjusts for a multiplicity of 3 points instead of 100
> ##D f <- update(fit, x=TRUE, y=TRUE)
> ##D g <- bootcov(f, B=500, coef.reps=TRUE)
> ##D p4 <- Predict(g, age=ages, sex='male', fun=plogis)    # bootstrap percentile
> ##D p <- rbind(Pointwise=p1, 'Simultaneous 100 ages'=p2,
> ##D            'Simultaneous     3 ages'=p3, 'Bootstrap nonparametric'=p4)
> ##D # as.data.frame so will call built-in ggplot
> ##D ggplot(as.data.frame(p), aes(x=age, y=yhat)) + geom_line() +
> ##D  geom_ribbon(data=p, aes(ymin=lower, ymax=upper), alpha=0.2, linetype=0)+
> ##D  facet_wrap(~ .set., ncol=2)
> ##D 
> ##D # Plots for a parametric survival model
> ##D n <- 1000
> ##D set.seed(731)
> ##D age <- 50 + 12*rnorm(n)
> ##D label(age) <- "Age"
> ##D sex <- factor(sample(c('Male','Female'), n, 
> ##D               rep=TRUE, prob=c(.6, .4)))
> ##D cens <- 15*runif(n)
> ##D h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> ##D t <- -log(runif(n))/h
> ##D label(t) <- 'Follow-up Time'
> ##D e <- ifelse(t<=cens,1,0)
> ##D t <- pmin(t, cens)
> ##D units(t) <- "Year"
> ##D ddist <- datadist(age, sex)
> ##D require(survival)
> ##D Srv <- Surv(t,e)
> ##D 
> ##D # Fit log-normal survival model and plot median survival time vs. age
> ##D f <- psm(Srv ~ rcs(age), dist='lognormal')
> ##D med <- Quantile(f)       # Creates function to compute quantiles
> ##D                          # (median by default)
> ##D p <- Predict(f, age, fun=function(x) med(lp=x))
> ##D ggplot(p, ylab="Median Survival Time")
> ##D # Note: confidence intervals from this method are approximate since
> ##D # they don't take into account estimation of scale parameter
> ##D 
> ##D 
> ##D # Fit an ols model to log(y) and plot the relationship between x1
> ##D # and the predicted mean(y) on the original scale without assuming
> ##D # normality of residuals; use the smearing estimator
> ##D # See help file for rbind.Predict for a method of showing two
> ##D # types of confidence intervals simultaneously.
> ##D # Add raw data scatterplot to graph
> ##D set.seed(1)
> ##D x1 <- runif(300)
> ##D x2 <- runif(300)
> ##D ddist <- datadist(x1, x2); options(datadist='ddist')
> ##D y  <- exp(x1 + x2 - 1 + rnorm(300))
> ##D f <- ols(log(y) ~ pol(x1,2) + x2)
> ##D r <- resid(f)
> ##D smean <- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
> ##D formals(smean) <- list(yhat=numeric(0), res=r[! is.na(r)])
> ##D #smean$res <- r[! is.na(r)]   # define default res argument to function
> ##D ggplot(Predict(f, x1, fun=smean), ylab='Predicted Mean on y-scale', 
> ##D    addlayer=geom_point(aes(x=x1, y=y), data.frame(x1, y)))
> ##D # Had ggplot not added a subtitle (i.e., if x2 were not present), you
> ##D # could have done ggplot(Predict(), ylab=...) + geom_point(...) 
> ## End(Not run)
> 
> # Make an 'interaction plot', forcing the x-axis variable to be
> # plotted at integer values but labeled with category levels
> n <- 100
> set.seed(1)
> gender <- c(rep('male', n), rep('female',n))
> m <- sample(c('a','b'), 2*n, TRUE)
> d <-  datadist(gender, m); options(datadist='d')
> anxiety <- runif(2*n) + .2*(gender=='female') + .4*(gender=='female' & m=='b')
> tapply(anxiety, llist(gender,m), mean)
        m
gender         a       b
  female 0.74423 1.05654
  male   0.47459 0.39699
> f <- ols(anxiety ~ gender*m)
> p <- Predict(f, gender, m)
> # ggplot(p)     # horizontal dot chart; usually preferred for categorical predictors
> # ggplot(p, flipxdiscrete=FALSE)  # back to vertical
> ggplot(p, groups='gender')
> ggplot(p, ~ m, groups=FALSE, flipxdiscrete=FALSE)
> 
> options(datadist=NULL)
> 
> ## Not run: 
> ##D # Example in which separate curves are shown for 4 income values
> ##D # For each curve the estimated percentage of voters voting for
> ##D # the democratic party is plotted against the percent of voters
> ##D # who graduated from college.  Data are county-level percents.
> ##D 
> ##D incomes <- seq(22900, 32800, length=4)  
> ##D # equally spaced to outer quintiles
> ##D p <- Predict(f, college, income=incomes, conf.int=FALSE)
> ##D ggplot(p, xlim=c(0,35), ylim=c(30,55))
> ##D 
> ##D # Erase end portions of each curve where there are fewer than 10 counties having
> ##D # percent of college graduates to the left of the x-coordinate being plotted,
> ##D # for the subset of counties having median family income with 1650
> ##D # of the target income for the curve
> ##D 
> ##D show.pts <- function(college.pts, income.pt) {
> ##D   s <- abs(income - income.pt) < 1650  #assumes income known to top frame
> ##D   x <- college[s]
> ##D   x <- sort(x[!is.na(x)])
> ##D   n <- length(x)
> ##D   low <- x[10]; high <- x[n-9]
> ##D   college.pts >= low & college.pts <= high
> ##D }
> ##D 
> ##D ggplot(p, xlim=c(0,35), ylim=c(30,55), perim=show.pts)
> ##D 
> ##D # Rename variables for better plotting of a long list of predictors
> ##D f <- ...
> ##D p <- Predict(f)
> ##D re <- c(trt='treatment', diabet='diabetes', sbp='systolic blood pressure')
> ##D 
> ##D for(n in names(re)) {
> ##D   names(p)[names(p)==n] <- re[n]
> ##D   p$.predictor.[p$.predictor.==n] <- re[n]
> ##D   }
> ##D ggplot(p)
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("ggplot.npsurv")
> ### * ggplot.npsurv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ggplot.npsurv
> ### Title: Title Plot npsurv Nonparametric Survival Curves Using ggplot2
> ### Aliases: ggplot.npsurv
> 
> ### ** Examples
> 
> set.seed(1)
> g <- c(rep('a', 500), rep('b', 500))
> y <- exp(-1 + 2 * (g == 'b') + rlogis(1000) / 3)
> f <- npsurv(Surv(y) ~ g)
> ggplot(f, trans='logit', logt=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("groupkm")
> ### * groupkm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: groupkm
> ### Title: Kaplan-Meier Estimates vs. a Continuous Variable
> ### Aliases: groupkm
> ### Keywords: survival nonparametric
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50))
> d.time <- -log(runif(n))/h
> label(d.time) <- 'Follow-up Time'
> e <- ifelse(d.time <= cens,1,0)
> d.time <- pmin(d.time, cens)
> units(d.time) <- "Year"
> groupkm(age, Surv(d.time, e), g=10, u=5, pl=TRUE)
           x   n events      KM  std.err
 [1,] 27.917 100      4 0.94651 0.027727
 [2,] 36.560 100      6 0.95640 0.022420
 [3,] 40.794 100     13 0.89594 0.036899
 [4,] 44.157 100     11 0.91388 0.034117
 [5,] 47.342 100     18 0.86132 0.045553
 [6,] 50.884 100     10 0.92282 0.030492
 [7,] 53.931 100     18 0.87666 0.041909
 [8,] 57.650 100     18 0.86837 0.045158
 [9,] 62.185 100     12 0.93819 0.028810
[10,] 70.899 100     29 0.72407 0.070133
> #Plot 5-year K-M survival estimates and 0.95 confidence bars by 
> #decile of age.  If omit g=10, will have >= 50 obs./group.
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("hazard.ratio.plot")
> ### * hazard.ratio.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hazard.ratio.plot
> ### Title: Hazard Ratio Plot
> ### Aliases: hazard.ratio.plot
> ### Keywords: survival
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> n <- 500
> set.seed(1)
> age <- 50 + 12*rnorm(n)
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50))
> d.time <- -log(runif(n))/h
> label(d.time) <- 'Follow-up Time'
> e <- ifelse(d.time <= cens,1,0)
> d.time <- pmin(d.time, cens)
> units(d.time) <- "Year"
> hazard.ratio.plot(age, Surv(d.time,e), e=20, legendloc='ll')
$time
[1]  0.72199  2.99413  5.74525 10.85871

$log.hazard.ratio
       [,1]      [,2]     [,3]     [,4]
x1 0.018155 0.0064516 0.063675 0.017749

$se
        [,1]    [,2]     [,3]     [,4]
[1,] 0.01774 0.01836 0.019553 0.018855

> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("ie.setup")
> ### * ie.setup
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ie.setup
> ### Title: Intervening Event Setup
> ### Aliases: ie.setup
> ### Keywords: survival
> 
> ### ** Examples
> 
> failure.time <- c(1 ,   2,   3)
> event        <- c(1 ,   1,   0)
> ie.time      <- c(NA, 1.5, 2.5)
> 
> z <- ie.setup(failure.time, event, ie.time)
> S <- z$S
> S
[1] (0.0,1.0]  (0.0,1.5+] (1.5,2.0]  (0.0,2.5+] (2.5,3.0+]
> ie.status <- z$ie.status
> ie.status
[1] 0 0 1 0 1
> z$subs
[1] 1 2 2 3 3
> z$reps
[1] 1 2 2
> ## Not run: 
> ##D attach(input.data.frame[z$subs,])   #replicates all variables
> ##D f <- cph(S ~ age + sex + ie.status)
> ##D # Instead of duplicating rows of data frame, could do this:
> ##D attach(input.data.frame)
> ##D z <- ie.setup(failure.time, event, ie.time)
> ##D s <- z$subs
> ##D age <- age[s]
> ##D sex <- sex[s]
> ##D f <- cph(S ~ age + sex + ie.status)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("impactPO")
> ### * impactPO
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: impactPO
> ### Title: Impact of Proportional Odds Assumpton
> ### Aliases: impactPO
> ### Keywords: category models regression
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D set.seed(1)
> ##D age <- rnorm(500, 50, 10)
> ##D sex <- sample(c('female', 'male'), 500, TRUE)
> ##D y   <- sample(0:4, 500, TRUE)
> ##D d   <- expand.grid(age=50, sex=c('female', 'male'))
> ##D w   <- impactPO(y ~ age + sex, nonpo = ~ sex, newdata=d)
> ##D w
> ##D # Note that PO model is a better model than multinomial (lower AIC)
> ##D # since multinomial model's improvement in fit is low in comparison
> ##D # with number of additional parameters estimated.  Same for PO model
> ##D # in comparison with partial PO model.
> ##D 
> ##D # Reverse levels of y so stacked bars have higher y located higher
> ##D revo <- function(z) {
> ##D   z <- as.factor(z)
> ##D   factor(z, levels=rev(levels(as.factor(z))))
> ##D }
> ##D 
> ##D require(ggplot2)
> ##D ggplot(w$estimates, aes(x=method, y=Probability, fill=revo(y))) +
> ##D   facet_wrap(~ sex) + geom_col() +
> ##D   xlab('') + guides(fill=guide_legend(title=''))
> ##D 
> ##D # Now vary 2 predictors
> ##D 
> ##D d <- expand.grid(sex=c('female', 'male'), age=c(40, 60))
> ##D w <- impactPO(y ~ age + sex, nonpo = ~ sex, newdata=d)
> ##D w
> ##D ggplot(w$estimates, aes(x=method, y=Probability, fill=revo(y))) +
> ##D   facet_grid(age ~ sex) + geom_col() +
> ##D  xlab('') + guides(fill=guide_legend(title=''))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("importexport")
> ### * importexport
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: importedexported
> ### Title: Exported Functions That Were Imported From Other Packages
> ### Aliases: Surv ggplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- psm(Surv(dtime, death) ~ x1 + x2 + sex + race, dist='gau')
> ##D ggplot(Predict(f))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("infoMxop")
> ### * infoMxop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: infoMxop
> ### Title: Operate on Information Matrices
> ### Aliases: infoMxop
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- orm(y ~ x)
> ##D infoMxop(f$info.matrix)   # assembles 3 pieces
> ##D infoMxop(v, i=c(2,4))     # returns a submatrix of v inverse
> ##D infoMxop(f$info.matrix, i='x')  # sub-covariance matrix for just the betas
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("intCalibration")
> ### * intCalibration
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: intCalibration
> ### Title: Check Parallelism Assumption of Ordinal Semiparametric Models
> ### Aliases: intCalibration
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getHdata(nhgh)
> ##D f <- orm(gh ~ rcs(age, 4), data=nhgh, family='loglog', x=TRUE, y=TRUE)
> ##D intCalibration(f, ycuts=c(5, 5.5, 6, 6.5))
> ##D f <- update(f, family='cloglog')
> ##D intCalibration(f, ycuts=c(5, 5.5, 6, 6.5))
> ##D intCalibration(f, ycuts=c(5, 6, 7), x=nhgh$age)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("is.na.Ocens")
> ### * is.na.Ocens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.na.Ocens
> ### Title: is.na Method for Ocens Objects
> ### Aliases: is.na.Ocens
> 
> ### ** Examples
> 
> Y <- Ocens(c(1, 2, NA, 4))
> Y
[1]  1  2 NA  4
attr(,"label")
[1] ""
attr(,"units")
[1] ""
> is.na(Y)
[1] FALSE FALSE  TRUE FALSE
> 
> 
> 
> cleanEx()
> nameEx("latex.cph")
> ### * latex.cph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latex.cph
> ### Title: LaTeX Representation of a Fitted Cox Model
> ### Aliases: latex.cph latex.lrm latex.ols latex.orm latex.pphsm latex.psm
> ### Keywords: regression character survival interface models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D require(survival)
> ##D units(ftime) <- "Day"
> ##D f <- cph(Surv(ftime, death) ~ rcs(age)+sex, surv=TRUE, time.inc=60)
> ##D w <- latex(f, file='f.tex')  #Interprets fitted model and makes table of S0(t)
> ##D                #for t=0,60,120,180,...
> ##D w              #displays image, if viewer installed and file given above
> ##D latex(f)   # send LaTeX code to the console for knitr
> ##D options(prType='html')
> ##D latex(f)       # for use with knitr and R Markdown/Quarto using MathJax
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("latexrms")
> ### * latexrms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexrms
> ### Title: LaTeX Representation of a Fitted Model
> ### Aliases: latexrms latex.bj latex.Glm latex.Gls
> ### Keywords: models regression character methods interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- lrm(death ~ rcs(age)+sex)
> ##D w <- latex(f, file='f.tex')
> ##D w     # displays, using e.g. xdvi
> ##D latex(f)    # send LaTeX code to console, as for knitr
> ##D options(prType='html')
> ##D latex(f)    # emit html and latex for knitr html and html notebooks
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lrm")
> ### * lrm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lrm
> ### Title: Logistic Regression Model
> ### Aliases: lrm print.lrm
> ### Keywords: category models
> 
> ### ** Examples
> 
> #Fit a logistic model containing predictors age, blood.pressure, sex
> #and cholesterol, with age fitted with a smooth 5-knot restricted cubic
> #spline function and a different shape of the age relationship for males
> #and females.  As an intermediate step, predict mean cholesterol from
> #age using a proportional odds ordinal logistic model
> #
> require(ggplot2)
Loading required package: ggplot2
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> # Group cholesterol unnecessarily into 40-tiles
> ch <- cut2(cholesterol, g=40, levels.mean=TRUE) # use mean values in intervals
> table(ch)
ch
143.16 155.16 162.68 166.81 170.68 173.56 176.95 179.04 181.17 182.83 184.91 
    25     25     25     25     25     25     25     25     25     25     25 
186.74 188.48 190.09 191.54 193.23 195.04 196.62 198.16 199.93 201.07 202.78 
    25     25     25     25     25     25     25     25     25     25     25 
204.92 206.49 208.08 209.53 211.16 212.71 214.12 215.80 217.42 219.50 221.76 
    25     25     25     25     25     25     25     25     25     25     25 
224.28 227.32 230.02 233.76 238.77 245.69 255.30 
    25     25     25     25     25     25     25 
> f <- lrm(ch ~ age)
> # options(prType='latex')
> print(f)  # write latex code to console if prType='latex' is in effect
Logistic Regression Model

lrm(formula = ch ~ age)

                      Model Likelihood      Discrimination    Rank Discrim.    
                            Ratio Test             Indexes          Indexes    
Obs          1000    LR chi2      0.19      R2       0.000    C       0.511    
max |deriv| 4e-06    d.f.            1     R2(1,1000)0.000    Dxy     0.021    
                     Pr(> chi2) 0.6651    R2(1,999.4)0.000    gamma   0.021    
                                            Brier    0.249    tau-a   0.021    

    Coef    S.E.   Wald Z Pr(>|Z|)
age -0.0023 0.0054 -0.43  0.6651  

> m <- Mean(f)    # see help file for Mean.lrm
> d <- data.frame(age=seq(0,90,by=10))
> m(predict(f, d))
     1      2      3      4      5      6      7      8      9     10 
202.06 201.73 201.41 201.09 200.76 200.44 200.12 199.79 199.47 199.14 
> # Repeat using ols
> f <- ols(cholesterol ~ age)
> predict(f, d)
     1      2      3      4      5      6      7      8      9     10 
201.46 201.26 201.05 200.85 200.64 200.44 200.23 200.03 199.82 199.61 
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+      (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> cholesterol[1:3] <- NA   # 3 missings, at random
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
+                x=TRUE, y=TRUE)
> #      x=TRUE, y=TRUE allows use of resid(), which.influence below
> #      could define d <- datadist(fit) after lrm(), but data distribution
> #      summary would not be stored with fit, so later uses of Predict
> #      or summary.rms would require access to the original dataset or
> #      d or specifying all variable values to summary, Predict, nomogram
> anova(fit)
                Wald Statistics          Response: y 

 Factor                                           Chi-Square d.f. P     
 blood.pressure                                    0.02       1   0.8904
 sex  (Factor+Higher Order Factors)               27.40       5   <.0001
  All Interactions                                11.56       4   0.0209
 age  (Factor+Higher Order Factors)               35.03       2   <.0001
  All Interactions                                 1.71       1   0.1916
 cholesterol  (Factor+Higher Order Factors)       11.21       6   0.0822
  All Interactions                                10.01       3   0.0185
  Nonlinear (Factor+Higher Order Factors)          4.20       4   0.3799
 sex * age  (Factor+Higher Order Factors)          1.71       1   0.1916
 sex * cholesterol  (Factor+Higher Order Factors) 10.01       3   0.0185
  Nonlinear                                        3.91       2   0.1413
  Nonlinear Interaction : f(A,B) vs. AB            3.91       2   0.1413
 TOTAL NONLINEAR                                   4.20       4   0.3799
 TOTAL INTERACTION                                11.56       4   0.0209
 TOTAL NONLINEAR + INTERACTION                    11.66       6   0.0699
 TOTAL                                            57.52      10   <.0001
> p <- Predict(fit, age, sex)
> ggplot(p)   # or plot()
> ggplot(Predict(fit, age=20:70, sex="male"))   # need if datadist not used
> print(cbind(resid(fit,"dfbetas"), resid(fit,"dffits"))[1:20,])
     Intercept blood.pressure    sex=male         age cholesterol cholesterol'
1           NA             NA          NA          NA          NA           NA
2           NA             NA          NA          NA          NA           NA
3           NA             NA          NA          NA          NA           NA
1   0.01954055    -0.00577380 -0.01260677  3.5745e-02 -2.7714e-02   2.0969e-02
2   0.03268089    -0.01133196 -0.02081679 -3.2795e-02 -2.5895e-02   1.2432e-02
3  -0.00051908     0.00267191 -0.00872982 -1.5183e-04 -5.8138e-05   1.0197e-04
4  -0.00908372    -0.01375395  0.00848580 -4.8068e-02  2.3990e-02  -5.2870e-02
5  -0.01738870     0.07036084  0.00080159 -1.0119e-01  2.3775e-02  -5.5178e-02
6   0.00283888    -0.01461268  0.01313192  8.3035e-04  3.1796e-04  -5.5770e-04
7  -0.00414798     0.02135108  0.03413085 -1.2133e-03 -4.6458e-04   8.1487e-04
8   0.05287842     0.02181224 -0.04009411 -5.5251e-02 -5.2707e-02   5.9274e-02
9  -0.01688180     0.00920435  0.01021808 -3.0946e-02  2.3169e-02  -4.9639e-02
10 -0.00034829    -0.04818508  0.00793689 -6.5137e-02  2.6507e-02  -5.0500e-02
11 -0.00142294     0.00732439  0.00407410 -4.1620e-04 -1.5937e-04   2.7954e-04
12  0.00013803    -0.00071051 -0.00459221  4.0374e-05  1.5460e-05  -2.7117e-05
13  0.00850926    -0.04380012  0.01672743  2.4889e-03  9.5304e-04  -1.6716e-03
14  0.01362389    -0.04822770 -0.00172995  3.8045e-02 -1.1283e-02   1.9445e-02
15 -0.03112080     0.03229563  0.01638850 -6.0438e-04  2.9545e-02  -4.6685e-02
16 -0.00495284     0.02549399 -0.02328376 -1.4487e-03 -5.5472e-04   9.7299e-04
17  0.00350768    -0.01805526  0.04278389  1.0260e-03  3.9286e-04  -6.8908e-04
   cholesterol'' sex=male * age sex=male * cholesterol sex=male * cholesterol'
1             NA             NA                     NA                      NA
2             NA             NA                     NA                      NA
3             NA             NA                     NA                      NA
1    -1.3685e-02    -0.02368333              0.0188339              -0.0139983
2    -2.6534e-03     0.02155711              0.0175906              -0.0083806
3    -1.0778e-04     0.00045140              0.0094030              -0.0135410
4     6.0372e-02     0.03162628             -0.0163256               0.0350018
5     6.4143e-02     0.06760449             -0.0160803               0.0374050
6     5.8947e-04    -0.01721599             -0.0152184               0.0389696
7    -8.6129e-04    -0.02249130             -0.0323605               0.0264868
8    -5.4222e-02     0.03673978              0.0358575              -0.0391750
9     5.5868e-02     0.02054675             -0.0157401               0.0330909
10    5.1813e-02     0.04255150             -0.0180774               0.0330703
11   -2.9546e-04     0.00978836             -0.0066974               0.0209400
12    2.8661e-05     0.01394735              0.0020284              -0.0045482
13    1.7669e-03    -0.00498389             -0.0168988               0.0045239
14   -1.3577e-02    -0.02564126              0.0076137              -0.0134241
15    4.7027e-02     0.00073251             -0.0200475               0.0313662
16   -1.0284e-03     0.02002865              0.0211345              -0.0204209
17    7.2834e-04    -0.03908213             -0.0382741               0.0344115
   sex=male * cholesterol''         
1                        NA       NA
2                        NA       NA
3                        NA       NA
1                 0.0089055 -0.12653
2                 0.0018686 -0.14939
3                -0.0038382 -0.33987
4                -0.0387399 -0.20675
5                -0.0423415 -0.32201
6                -0.0425337 -0.21673
7                -0.0179692 -0.19072
8                 0.0346624 -0.20453
9                -0.0361550 -0.18131
10               -0.0327423 -0.24366
11               -0.0267170  0.12364
12                0.0096503  0.12751
13                0.0048291 -0.17425
14                0.0094255  0.18938
15               -0.0307748  0.16532
16                0.0164903  0.12403
17               -0.0259876 -0.22246
> which.influence(fit, .3)
$Intercept
[1] 885

$cholesterol
[1] 885

> # latex(fit)                       #print nice statement of fitted model
> #
> #Repeat this fit using penalized MLE, penalizing complex terms
> #(for nonlinear or interaction effects)
> #
> fitp <- update(fit, penalty=list(simple=0,nonlinear=10), x=TRUE, y=TRUE)
> effective.df(fitp)

Original and Effective Degrees of Freedom

                         Original Penalized
All                            10      6.77
Simple Terms                    4      4.00
Interaction or Nonlinear        6      2.77
Nonlinear                       4      2.23
Interaction                     4      1.72
Nonlinear Interaction           2      1.18
> # or lrm(y ~ \dots, penalty=\dots)
> 
> 
> #Get fits for a variety of penalties and assess predictive accuracy
> #in a new data set.  Program efficiently so that complex design
> #matrices are only created once.
> 
> 
> set.seed(201)
> x1 <- rnorm(500)
> x2 <- rnorm(500)
> x3 <- sample(0:1,500,rep=TRUE)
> L  <- x1+abs(x2)+x3
> y  <- ifelse(runif(500)<=plogis(L), 1, 0)
> new.data <- data.frame(x1,x2,x3,y)[301:500,]
> #
> for(penlty in seq(0,.15,by=.005)) {
+   if(penlty==0) {
+     f <- lrm(y ~ rcs(x1,4)+rcs(x2,6)*x3, subset=1:300, x=TRUE, y=TRUE)
+     # True model is linear in x1 and has no interaction
+     X <- f$x    # saves time for future runs - don't have to use rcs etc.
+     Y <- f$y    # this also deletes rows with NAs (if there were any)
+     penalty.matrix <- diag(diag(var(X)))
+     Xnew <- predict(f, new.data, type="x")
+     # expand design matrix for new data
+     Ynew <- new.data$y
+   } else f <- lrm.fit(X,Y, penalty.matrix=penlty*penalty.matrix)
+ #
+   cat("\nPenalty :",penlty,"\n")
+   pred.logit <- f$coef[1] + (Xnew %*% f$coef[-1])
+   pred <- plogis(pred.logit)
+   C.index <- somers2(pred, Ynew)["C"]
+   Brier   <- mean((pred-Ynew)^2)
+   Deviance<- -2*sum( Ynew*log(pred) + (1-Ynew)*log(1-pred) )
+   cat("ROC area:",format(C.index),"   Brier score:",format(Brier),
+       "   -2 Log L:",format(Deviance),"\n")
+ }

Penalty : 0 
ROC area: 0.73954    Brier score: 0.16019    -2 Log L: 197.21 

Penalty : 0.005 
ROC area: 0.74065    Brier score: 0.1605    -2 Log L: 197.66 

Penalty : 0.01 
ROC area: 0.7412    Brier score: 0.16018    -2 Log L: 197.37 

Penalty : 0.015 
ROC area: 0.74023    Brier score: 0.15995    -2 Log L: 197.14 

Penalty : 0.02 
ROC area: 0.74009    Brier score: 0.15978    -2 Log L: 196.95 

Penalty : 0.025 
ROC area: 0.74065    Brier score: 0.15965    -2 Log L: 196.8 

Penalty : 0.03 
ROC area: 0.74079    Brier score: 0.15954    -2 Log L: 196.67 

Penalty : 0.035 
ROC area: 0.74107    Brier score: 0.15945    -2 Log L: 196.56 

Penalty : 0.04 
ROC area: 0.74093    Brier score: 0.15937    -2 Log L: 196.47 

Penalty : 0.045 
ROC area: 0.7412    Brier score: 0.15931    -2 Log L: 196.39 

Penalty : 0.05 
ROC area: 0.74176    Brier score: 0.15925    -2 Log L: 196.31 

Penalty : 0.055 
ROC area: 0.7419    Brier score: 0.1592    -2 Log L: 196.25 

Penalty : 0.06 
ROC area: 0.74134    Brier score: 0.15916    -2 Log L: 196.19 

Penalty : 0.065 
ROC area: 0.74204    Brier score: 0.15912    -2 Log L: 196.14 

Penalty : 0.07 
ROC area: 0.74232    Brier score: 0.15909    -2 Log L: 196.09 

Penalty : 0.075 
ROC area: 0.74204    Brier score: 0.15906    -2 Log L: 196.05 

Penalty : 0.08 
ROC area: 0.74204    Brier score: 0.15904    -2 Log L: 196.01 

Penalty : 0.085 
ROC area: 0.74176    Brier score: 0.15901    -2 Log L: 195.98 

Penalty : 0.09 
ROC area: 0.7419    Brier score: 0.15899    -2 Log L: 195.94 

Penalty : 0.095 
ROC area: 0.7419    Brier score: 0.15897    -2 Log L: 195.91 

Penalty : 0.1 
ROC area: 0.74218    Brier score: 0.15896    -2 Log L: 195.88 

Penalty : 0.105 
ROC area: 0.74204    Brier score: 0.15894    -2 Log L: 195.86 

Penalty : 0.11 
ROC area: 0.74176    Brier score: 0.15893    -2 Log L: 195.83 

Penalty : 0.115 
ROC area: 0.74162    Brier score: 0.15892    -2 Log L: 195.81 

Penalty : 0.12 
ROC area: 0.74176    Brier score: 0.15891    -2 Log L: 195.79 

Penalty : 0.125 
ROC area: 0.7419    Brier score: 0.15889    -2 Log L: 195.77 

Penalty : 0.13 
ROC area: 0.7419    Brier score: 0.15889    -2 Log L: 195.75 

Penalty : 0.135 
ROC area: 0.74176    Brier score: 0.15888    -2 Log L: 195.73 

Penalty : 0.14 
ROC area: 0.74176    Brier score: 0.15887    -2 Log L: 195.72 

Penalty : 0.145 
ROC area: 0.74148    Brier score: 0.15886    -2 Log L: 195.7 

Penalty : 0.15 
ROC area: 0.74162    Brier score: 0.15886    -2 Log L: 195.69 
> #penalty=0.045 gave lowest -2 Log L, Brier, ROC in test sample for S+
> #
> #Use bootstrap validation to estimate predictive accuracy of
> #logistic models with various penalties
> #To see how noisy cross-validation estimates can be, change the
> #validate(f, \dots) to validate(f, method="cross", B=10) for example.
> #You will see tremendous variation in accuracy with minute changes in
> #the penalty.  This comes from the error inherent in using 10-fold
> #cross validation but also because we are not fixing the splits.
> #20-fold cross validation was even worse for some
> #indexes because of the small test sample size.  Stability would be
> #obtained by using the same sample splits for all penalty values
> #(see above), but then we wouldn't be sure that the choice of the
> #best penalty is not specific to how the sample was split.  This
> #problem is addressed in the last example.
> #
> penalties <- seq(0,.7,length=3)   # really use by=.02
> index <- matrix(NA, nrow=length(penalties), ncol=11,
+ 	        dimnames=list(format(penalties),
+           c("Dxy","R2","Intercept","Slope","Emax","D","U","Q","B","g","gp")))
> i <- 0
> for(penlty in penalties)
+ {
+   cat(penlty, "")
+   i <- i+1
+   if(penlty==0)
+     {
+     f <- lrm(y ~ rcs(x1,4)+rcs(x2,6)*x3, x=TRUE, y=TRUE)  # fit whole sample
+     X <- f$x
+     Y <- f$y
+     penalty.matrix <- diag(diag(var(X)))   # save time - only do once
+     }
+   else
+    f <- lrm(Y ~ X, penalty=penlty,
+             penalty.matrix=penalty.matrix, x=TRUE,y=TRUE)
+   val <- validate(f, method="boot", B=20)  # use larger B in practice
+   index[i,] <- val[,"index.corrected"]
+ }
0 0.35 0.7 > par(mfrow=c(3,3))
> for(i in 1:9)
+ {
+   plot(penalties, index[,i],
+        xlab="Penalty", ylab=dimnames(index)[[2]][i])
+   lines(lowess(penalties, index[,i]))
+ }
> options(datadist=NULL)
> 
> # Example of weighted analysis
> x    <- 1:5
> y    <- c(0,1,0,1,0)
> reps <- c(1,2,3,2,1)
> lrm(y ~ x, weights=reps)
Warning in lrm(y ~ x, weights = reps) :
  currently weights are ignored in model validation and bootstrapping lrm fits
Logistic Regression Model

lrm(formula = y ~ x, weights = reps)


Sum of Weights by Response Category

0 1 
5 4 

                    Model Likelihood    Discrimination    Rank Discrim.    
                          Ratio Test           Indexes          Indexes    
  Obs         5    LR chi2      0.00    R2       0.000    C       0.500    
   0          3    d.f.            1    R2(1,9)  0.000    Dxy     0.000    
   1          2    Pr(> chi2) 1.0000    R2(1,6.7)0.000    tau-a   0.000    
Sum of weights9                         Brier    0.247                     
  max |deriv| 0                                                            

          Coef    S.E.   Wald Z Pr(>|Z|)
Intercept -0.2231 1.8675 -0.12  0.9049  
x          0.0000 0.5809  0.00  1.0000  

> x <- rep(x, reps)
> y <- rep(y, reps)
> lrm(y ~ x)   # same as above
Logistic Regression Model

lrm(formula = y ~ x)

                      Model Likelihood    Discrimination    Rank Discrim.    
                            Ratio Test           Indexes          Indexes    
Obs             9    LR chi2      0.00    R2       0.000    C       0.500    
 0              5    d.f.            1    R2(1,9)  0.000    Dxy     0.000    
 1              4    Pr(> chi2) 1.0000    R2(1,6.7)0.000    gamma   0.000    
max |deriv| 9e-16                         Brier    0.247    tau-a   0.000    

          Coef    S.E.   Wald Z Pr(>|Z|)
Intercept -0.2231 1.8675 -0.12  0.9049  
x          0.0000 0.5809  0.00  1.0000  

> 
> #
> #Study performance of a modified AIC which uses the effective d.f.
> #See Verweij and Van Houwelingen (1994) Eq. (6).  Here AIC=chisq-2*df.
> #Also try as effective d.f. equation (4) of the previous reference.
> #Also study performance of Shao's cross-validation technique (which was
> #designed to pick the "right" set of variables, and uses a much smaller
> #training sample than most methods).  Compare cross-validated deviance
> #vs. penalty to the gold standard accuracy on a 7500 observation dataset.
> #Note that if you only want to get AIC or Schwarz Bayesian information
> #criterion, all you need is to invoke the pentrace function.
> #NOTE: the effective.df( ) function is used in practice
> #
> ## Not run: 
> ##D for(seed in c(339,777,22,111,3)){
> ##D # study performance for several datasets
> ##D   set.seed(seed)
> ##D   n <- 175; p <- 8
> ##D   X <- matrix(rnorm(n*p), ncol=p) # p normal(0,1) predictors
> ##D   Coef <- c(-.1,.2,-.3,.4,-.5,.6,-.65,.7)  # true population coefficients
> ##D   L <- X %*% Coef                 # intercept is zero
> ##D   Y <- ifelse(runif(n)<=plogis(L), 1, 0)
> ##D   pm <- diag(diag(var(X)))
> ##D   #Generate a large validation sample to use as a gold standard
> ##D   n.val <- 7500
> ##D   X.val <- matrix(rnorm(n.val*p), ncol=p)
> ##D   L.val <- X.val %*% Coef
> ##D   Y.val <- ifelse(runif(n.val)<=plogis(L.val), 1, 0)
> ##D   #
> ##D   Penalty <- seq(0,30,by=1)
> ##D   reps <- length(Penalty)
> ##D   effective.df <- effective.df2 <- aic <- aic2 <- deviance.val <-
> ##D     Lpenalty <- single(reps)
> ##D   n.t <- round(n^.75)
> ##D   ncv <- c(10,20,30,40)     # try various no. of reps in cross-val.
> ##D   deviance <- matrix(NA,nrow=reps,ncol=length(ncv))
> ##D   #If model were complex, could have started things off by getting X, Y
> ##D   #penalty.matrix from an initial lrm fit to save time
> ##D   #
> ##D   for(i in 1:reps) {
> ##D     pen <- Penalty[i]
> ##D     cat(format(pen),"")
> ##D     f.full <- lrm.fit(X, Y, penalty.matrix=pen*pm)
> ##D     Lpenalty[i] <- pen* t(f.full$coef[-1]) %*% pm %*% f.full$coef[-1]
> ##D     f.full.nopenalty <- lrm.fit(X, Y, initial=f.full$coef, maxit=1)
> ##D     info.matrix.unpenalized <- solve(f.full.nopenalty$var)
> ##D     effective.df[i] <- sum(diag(info.matrix.unpenalized %*% f.full$var)) - 1
> ##D     lrchisq <- f.full.nopenalty$stats["Model L.R."]
> ##D     # lrm does all this penalty adjustment automatically (for var, d.f.,
> ##D     # chi-square)
> ##D     aic[i] <- lrchisq - 2*effective.df[i]
> ##D     #
> ##D     pred <- plogis(f.full$linear.predictors)
> ##D     score.matrix <- cbind(1,X) * (Y - pred)
> ##D     sum.u.uprime <- t(score.matrix) %*% score.matrix
> ##D     effective.df2[i] <- sum(diag(f.full$var %*% sum.u.uprime))
> ##D     aic2[i] <- lrchisq - 2*effective.df2[i]
> ##D     #
> ##D     #Shao suggested averaging 2*n cross-validations, but let's do only 40
> ##D     #and stop along the way to see if fewer is OK
> ##D     dev <- 0
> ##D     for(j in 1:max(ncv)) {
> ##D       s    <- sample(1:n, n.t)
> ##D       cof  <- lrm.fit(X[s,],Y[s],
> ##D                       penalty.matrix=pen*pm)$coef
> ##D       pred <- cof[1] + (X[-s,] %*% cof[-1])
> ##D       dev <- dev -2*sum(Y[-s]*pred + log(1-plogis(pred)))
> ##D       for(k in 1:length(ncv)) if(j==ncv[k]) deviance[i,k] <- dev/j
> ##D     }
> ##D     #
> ##D     pred.val <- f.full$coef[1] + (X.val %*% f.full$coef[-1])
> ##D     prob.val <- plogis(pred.val)
> ##D     deviance.val[i] <- -2*sum(Y.val*pred.val + log(1-prob.val))
> ##D   }
> ##D   postscript(hor=TRUE)   # along with graphics.off() below, allow plots
> ##D   par(mfrow=c(2,4))   # to be printed as they are finished
> ##D   plot(Penalty, effective.df, type="l")
> ##D   lines(Penalty, effective.df2, lty=2)
> ##D   plot(Penalty, Lpenalty, type="l")
> ##D   title("Penalty on -2 log L")
> ##D   plot(Penalty, aic, type="l")
> ##D   lines(Penalty, aic2, lty=2)
> ##D   for(k in 1:length(ncv)) {
> ##D     plot(Penalty, deviance[,k], ylab="deviance")
> ##D     title(paste(ncv[k],"reps"))
> ##D     lines(supsmu(Penalty, deviance[,k]))
> ##D   }
> ##D   plot(Penalty, deviance.val, type="l")
> ##D   title("Gold Standard (n=7500)")
> ##D   title(sub=format(seed),adj=1,cex=.5)
> ##D   graphics.off()
> ##D }
> ## End(Not run)
> #The results showed that to obtain a clear picture of the penalty-
> #accuracy relationship one needs 30 or 40 reps in the cross-validation.
> #For 4 of 5 samples, though, the super smoother was able to detect
> #an accurate penalty giving the best (lowest) deviance using 10-fold
> #cross-validation.  Cross-validation would have worked better had
> #the same splits been used for all penalties.
> #The AIC methods worked just as well and are much quicker to compute.
> #The first AIC based on the effective d.f. in Gray's Eq. 2.9
> #(Verweij and Van Houwelingen (1994) Eq. 5 (note typo)) worked best.
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("lrm.fit")
> ### * lrm.fit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lrm.fit
> ### Title: lrm.fit
> ### Aliases: lrm.fit
> ### Keywords: logistic models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Fit an additive logistic model containing numeric predictors age,
> ##D # blood.pressure, and sex, assumed to be already properly coded and
> ##D # transformed
> ##D 
> ##D fit <- lrm.fit(cbind(age,blood.pressure,sex=='male'), death)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("matinv")
> ### * matinv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: matinv
> ### Title: Total and Partial Matrix Inversion using Gauss-Jordan Sweep
> ###   Operator
> ### Aliases: matinv
> ### Keywords: array
> 
> ### ** Examples
> 
> a      <- diag(1:3)
> a.inv1 <- matinv(a, 1, negate=FALSE)	     #Invert with respect to a[1,1]
> a.inv1
     [,1] [,2] [,3]
[1,]   -1    0    0
[2,]    0    2    0
[3,]    0    0    3
attr(,"rank")
[1] 1
attr(,"swept")
[1]  TRUE FALSE FALSE
> a.inv  <- -matinv(a.inv1, 2:3, negate=FALSE) #Finish the job
> a.inv
     [,1] [,2]    [,3]
[1,]    1  0.0 0.00000
[2,]    0  0.5 0.00000
[3,]    0  0.0 0.33333
attr(,"rank")
[1] 2
attr(,"swept")
[1] TRUE TRUE TRUE
> solve(a)
     [,1] [,2]    [,3]
[1,]    1  0.0 0.00000
[2,]    0  0.5 0.00000
[3,]    0  0.0 0.33333
> 
> 
> 
> cleanEx()
> nameEx("nomogram")
> ### * nomogram
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nomogram
> ### Title: Draw a Nomogram Representing a Regression Fit
> ### Aliases: nomogram print.nomogram plot.nomogram legend.nomabbrev
> ### Keywords: models regression hplot
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> d <- data.frame(age = rnorm(n, 50, 10),
+                 blood.pressure = rnorm(n, 120, 15),
+                 cholesterol = rnorm(n, 200, 25),
+                 sex = factor(sample(c('female','male'), n,TRUE)))
> 
> # Specify population model for log odds that Y=1
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> d <- upData(d,
+   L = .4*(sex=='male') + .045*(age-50) +
+        (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')),
+   y = ifelse(runif(n) < plogis(L), 1, 0))
Input object size:	 29616 bytes;	 4 variables	 1000 observations
Added variable		L
Added variable		y
New object size:	41856 bytes;	6 variables	1000 observations
> 
> ddist <- datadist(d); options(datadist='ddist')
> 
> 
> f <- lrm(y ~ lsp(age,50) + sex * rcs(cholesterol, 4) + blood.pressure,
+          data=d)
> nom <- nomogram(f, fun=function(x)1/(1+exp(-x)),  # or fun=plogis
+     fun.at=c(.001,.01,.05,seq(.1,.9,by=.1),.95,.99,.999),
+     funlabel="Risk of Death")
> #Instead of fun.at, could have specified fun.lp.at=logit of
> #sequence above - faster and slightly more accurate
> plot(nom, xfrac=.45)
> print(nom)
Points per unit of linear predictor: 25.394 
Linear predictor units per point   : 0.03938 


 age Points
 10   0    
 20  14    
 30  27    
 40  41    
 50  54    
 60  61    
 70  67    
 80  74    
 90  80    


 cholesterol (sex=female) Points
 120                      30    
 140                      25    
 160                      21    
 180                      17    
 200                      19    
 220                      20    
 240                      14    
 260                       7    
 280                       0    


 cholesterol (sex=male) Points
 120                     12   
 140                     18   
 160                     25   
 180                     30   
 200                     30   
 220                     35   
 240                     55   
 260                     77   
 280                    100   


 blood.pressure Points
  70            2     
  80            2     
  90            1     
 100            1     
 110            1     
 120            1     
 130            1     
 140            1     
 150            0     
 160            0     
 170            0     


 Total Points Risk of Death
           37           0.2
           51           0.3
           62           0.4
           72           0.5
           83           0.6
           94           0.7
          108           0.8
          128           0.9

> nom <- nomogram(f, age=seq(10,90,by=10))
> plot(nom, xfrac=.45)
> g <- lrm(y ~ sex + rcs(age, 3) * rcs(cholesterol, 3), data=d)
> nom <- nomogram(g, interact=list(age=c(20,40,60)), 
+                 conf.int=c(.7,.9,.95))
> plot(nom, col.conf=c(1,.5,.2), naxes=7)
> 
> require(survival)
Loading required package: survival
> w <- upData(d,
+             cens = 15 * runif(n),
+             h = .02 * exp(.04 * (age - 50) + .8 * (sex == 'Female')),
+             d.time = -log(runif(n)) / h,
+             death = ifelse(d.time <= cens, 1, 0),
+             d.time = pmin(d.time, cens))
Input object size:	 41856 bytes;	 6 variables	 1000 observations
Added variable		cens
Added variable		h
Added variable		d.time
Added variable		death
Modified variable	d.time
New object size:	70432 bytes;	10 variables	1000 observations
> 
> 
> f <- psm(Surv(d.time,death) ~ sex * age, data=w, dist='lognormal')
> med  <- Quantile(f)
> surv <- Survival(f)  # This would also work if f was from cph
> plot(nomogram(f, fun=function(x) med(lp=x), funlabel="Median Survival Time"))
> nom <- nomogram(f, fun=list(function(x) surv(3, x),
+                             function(x) surv(6, x)),
+             funlabel=c("3-Month Survival Probability", 
+                        "6-month Survival Probability"))
> plot(nom, xfrac=.7)
> 
> ## Not run: 
> ##D nom <- nomogram(fit.with.categorical.predictors, abbrev=TRUE, minlength=1)
> ##D nom$x1$points   # print points assigned to each level of x1 for its axis
> ##D #Add legend for abbreviations for category levels
> ##D abb <- attr(nom, 'info')$abbrev$treatment
> ##D legend(locator(1), abb$full, pch=paste(abb$abbrev,collapse=''), 
> ##D        ncol=2, bty='n')  # this only works for 1-letter abbreviations
> ##D #Or use the legend.nomabbrev function:
> ##D legend.nomabbrev(nom, 'treatment', locator(1), ncol=2, bty='n')
> ## End(Not run)
> 
> 
> #Make a nomogram with axes predicting probabilities Y>=j for all j=1-3
> #in an ordinal logistic model, where Y=0,1,2,3
> w <- upData(w, Y = ifelse(y==0, 0, sample(1:3, length(y), TRUE)))
Input object size:	 70432 bytes;	 10 variables	 1000 observations
Added variable		Y
New object size:	74536 bytes;	11 variables	1000 observations
> g <- lrm(Y ~ age+rcs(cholesterol,4) * sex, data=w)
> fun2 <- function(x) plogis(x-g$coef[1]+g$coef[2])
> fun3 <- function(x) plogis(x-g$coef[1]+g$coef[3])
> f <- Newlabels(g, c(age='Age in Years'))  
> #see Design.Misc, which also has Newlevels to change 
> #labels for levels of categorical variables
> g <- nomogram(f, fun=list('Prob Y>=1'=plogis, 'Prob Y>=2'=fun2, 
+                      'Prob Y=3'=fun3), 
+          fun.at=c(.01,.05,seq(.1,.9,by=.1),.95,.99))
> plot(g, lmgp=.2, cex.axis=.6)
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("npsurv")
> ### * npsurv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: npsurv
> ### Title: Nonparametric Survival Estimates for Censored Data
> ### Aliases: npsurv
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> # fit a Kaplan-Meier and plot it
> fit <- npsurv(Surv(time, status) ~ x, data = aml)
> plot(fit, lty = 2:3)
> legend(100, .8, c("Maintained", "Nonmaintained"), lty = 2:3)
> ggplot(fit)   # prettier than plot()
> 
> # Here is the data set from Turnbull
> #  There are no interval censored subjects, only left-censored (status=3),
> #  right-censored (status 0) and observed events (status 1)
> #
> #                             Time
> #                         1    2   3   4
> # Type of observation
> #           death        12    6   2   3
> #          losses         3    2   0   3
> #      late entry         2    4   2   5
> #
> tdata <- data.frame(time   = c(1,1,1,2,2,2,3,3,3,4,4,4),
+                     status = rep(c(1,0,2),4),
+                     n      = c(12,3,2,6,2,4,2,0,2,3,3,5))
> fit  <- npsurv(Surv(time, time, status, type='interval') ~ 1,
+                data=tdata, weights=n)
> 
> #
> # Time to progression/death for patients with monoclonal gammopathy
> # Competing risk curves (cumulative incidence)
> # status variable must be a factor with first level denoting right censoring
> m <- upData(mgus1, stop = stop / 365.25, units=c(stop='years'),
+             labels=c(stop='Follow-up Time'), subset=start == 0)
Input object size:	 35552 bytes;	 14 variables	 305 observations
Modified variable	stop
New object size:	25104 bytes;	14 variables	241 observations
> f <- npsurv(Surv(stop, event) ~ 1, data=m)
> 
> # CI curves are always plotted from 0 upwards, rather than 1 down
> plot(f, fun='event', xmax=20, mark.time=FALSE,
+      col=2:3, xlab="Years post diagnosis of MGUS")
> text(10, .4, "Competing Risk: death", col=3)
> text(16, .15,"Competing Risk: progression", col=2)
> 
> # Use survplot for enhanced displays of cumulative incidence curves for
> # competing risks
> 
> survplot(f, state='pcm', n.risk=TRUE, xlim=c(0, 20), ylim=c(0, .5), col=2)
> survplot(f, state='death', add=TRUE, col=3)
> 
> f <- npsurv(Surv(stop, event) ~ sex, data=m)
> survplot(f, state='death', n.risk=TRUE, conf='diffbands')
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("ols")
> ### * ols
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ols
> ### Title: Linear Model Estimation Using Ordinary Least Squares
> ### Aliases: ols
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- runif(200)
> x2 <- sample(0:3, 200, TRUE)
> distance <- (x1 + x2/3 + rnorm(200))^2
> d <- datadist(x1,x2)
> options(datadist="d")   # No d -> no summary, plot without giving all details
> 
> 
> f <- ols(sqrt(distance) ~ rcs(x1,4) + scored(x2), x=TRUE)
> # could use d <- datadist(f); options(datadist="d") at this point,
> # but predictor summaries would not be stored in the fit object for
> # use with Predict, summary.rms.  In that case, the original
> # dataset or d would need to be accessed later, or all variable values
> # would have to be specified to summary, plot
> anova(f)
                Analysis of Variance          Response: sqrt(distance) 

 Factor          d.f. Partial SS MS         F     P     
 x1                3  7.5902e+00 2.53005526  4.32 0.0057
  Nonlinear        2  1.2803e-03 0.00064014  0.00 0.9989
 x2                3  2.0655e+01 6.88495669 11.75 <.0001
  Nonlinear        2  1.0131e+00 0.50655659  0.86 0.4227
 TOTAL NONLINEAR   4  1.0150e+00 0.25376021  0.43 0.7845
 REGRESSION        6  2.8613e+01 4.76881124  8.14 <.0001
 ERROR           193  1.1304e+02 0.58570564             
> which.influence(f)
$Intercept
[1] "27"  "47"  "74"  "116" "197"

$x1
[1] "2"  "18" "26" "27" "47" "74"

$x2
[1] "2"   "6"   "150"

> summary(f)
             Effects              Response : sqrt(distance) 

 Factor   Low    High    Diff.   Effect    S.E.    Lower 0.95 Upper 0.95
 x1       0.2937 0.74248 0.44879  0.322970 0.15484  0.017572  0.62837   
 x2 - 0:1 2.0000 1.00000      NA -0.085345 0.15029 -0.381760  0.21107   
 x2 - 2:1 2.0000 3.00000      NA  0.423560 0.16308  0.101920  0.74521   
 x2 - 3:1 2.0000 4.00000      NA  0.689120 0.15287  0.387600  0.99063   

> summary.lm(f)    # will only work if penalty and penalty.matrix not used

Call:
ols(formula = sqrt(distance) ~ rcs(x1, 4) + scored(x2), x = TRUE)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7001 -0.5587 -0.0981  0.5102  2.4239 

Coefficients:
          Estimate Std. Error t value Pr(>|t|)  
Intercept   0.5633     0.2383    2.36    0.019 *
x1          0.7335     0.9420    0.78    0.437  
x1'        -0.0555     2.6752   -0.02    0.983  
x1''        0.2510     8.7509    0.03    0.977  
x2          0.0853     0.1503    0.57    0.571  
x2=2        0.3382     0.2713    1.25    0.214  
x2=3        0.5184     0.4032    1.29    0.200  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.765 on 193 degrees of freedom
Multiple R-squared:  0.202,	Adjusted R-squared:  0.177 
F-statistic: 8.14 on 6 and 193 DF,  p-value: 7.43e-08

> 
> 
> # Fit a complex model and approximate it with a simple one
> x1 <- runif(200)
> x2 <- runif(200)
> x3 <- runif(200)
> x4 <- runif(200)
> y <- x1 + x2 + rnorm(200)
> f    <- ols(y ~ rcs(x1,4) + x2 + x3 + x4)
> pred <- fitted(f)   # or predict(f) or f$linear.predictors
> f2   <- ols(pred ~ rcs(x1,4) + x2 + x3 + x4, sigma=1)
> # sigma=1 prevents numerical problems resulting from R2=1
> fastbw(f2, aics=100000)

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC   R2   
 x4       1.16  1    0.2815  1.16    1    0.2815 -0.84 0.981
 x3       3.54  1    0.0599  4.70    2    0.0954  0.70 0.922
 x2      14.08  1    0.0002 18.78    3    0.0003 12.78 0.687
 x1      41.27  3    0.0000 60.05    6    0.0000 48.05 0.000

Approximate Estimates after Deleting Factors

       Coef    S.E. Wald Z P
[1,] 0.9716 0.07071  13.74 0

Factors in Final Model

None
> # This will find the best 1-variable model, best 2-variable model, etc.
> # in predicting the predicted values from the original model
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("ordESS")
> ### * ordESS
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ordESS
> ### Title: ordESS
> ### Aliases: ordESS
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- orm(Ocens(y1, y2) ~ x, y=TRUE, lpe=TRUE)
> ##D ordESS(f)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ordParallel")
> ### * ordParallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ordParallel
> ### Title: Check Parallelism Assumption of Ordinal Semiparametric Models
> ### Aliases: ordParallel
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- orm(..., x=TRUE, y=TRUE)
> ##D ordParallel(f, which=1:5)  # first 5 betas
> ##D 
> ##D getHdata(nhgh)
> ##D set.seed(1)
> ##D nhgh$ran <- runif(nrow(nhgh))
> ##D f <- orm(gh ~ rcs(age, 4) + ran, data=nhgh, x=TRUE, y=TRUE)
> ##D ordParallel(f)  # one panel per parameter (multiple parameters per predictor)
> ##D dd <- datadist(nhgh); options(datadist='dd')
> ##D ordParallel(f, terms=TRUE)
> ##D d <- ordParallel(f, maxcuts=30, onlydata=TRUE)
> ##D dd2 <- datadist(d); options(datadist='dd2')  # needed for plotting
> ##D g <- orm(Yge_cut ~ (age + ran) * rcs(Ycut, 4), data=d, x=TRUE, y=TRUE)
> ##D h <- robcov(g, d$obs)
> ##D anova(h)
> ##D qu <- quantile(d$age, c(1, 3)/4)
> ##D qu
> ##D cuts <- sort(unique(d$Ycut))
> ##D cuts
> ##D z <- contrast(h, list(age=qu[2], Ycut=cuts),
> ##D                  list(age=qu[1], Ycut=cuts))
> ##D z <- as.data.frame(z[.q(Ycut, Contrast, Lower, Upper)])
> ##D ggplot(z, aes(x=Ycut, y=Contrast)) + geom_line() +
> ##D   geom_ribbon(aes(ymin=Lower, ymax=Upper), alpha=0.2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("orm")
> ### * orm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: orm
> ### Title: Ordinal Regression Model
> ### Aliases: orm print.orm Quantile.orm
> ### Keywords: category models
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> set.seed(1)
> n <- 100
> y <- round(runif(n), 2)
> x1 <- sample(c(-1,0,1), n, TRUE)
> x2 <- sample(c(-1,0,1), n, TRUE)
> f <- lrm(y ~ x1 + x2, eps=1e-5)
> g <- orm(y ~ x1 + x2, eps=1e-5)
> max(abs(coef(g) - coef(f)))
[1] 7.1054e-15
> w <- vcov(g, intercepts='all') / vcov(f) - 1
> max(abs(w))
[1] 6.3449e-13
> 
> set.seed(1)
> n <- 300
> x1 <- c(rep(0,150), rep(1,150))
> y <- rnorm(n) + 3*x1
> g <- orm(y ~ x1)
> g
Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = y ~ x1)

                        Model Likelihood               Discrimination    Rank Discrim.    
                              Ratio Test                      Indexes          Indexes    
Obs            300    LR chi2     322.59    R2                  0.659    rho     0.841    
ESS            300    d.f.             1    R2(1,300)           0.658    Dxy     0.487    
Distinct Y     300    Pr(> chi2) <0.0001    R2(1,300)           0.658                     
Median Y    1.5226    Score chi2  271.61    |Pr(Y>=median)-0.5| 0.442                     
max |deriv|  5e-11    Pr(> chi2) <0.0001                                                  

   Coef   S.E.   Wald Z Pr(>|Z|)
x1 5.6036 0.4369 12.83  <0.0001 

> k <- coef(g)
> i <- num.intercepts(g)
> h <- orm(y ~ x1, family='probit')
> ll <- orm(y ~ x1, family='loglog')
> cll <- orm(y ~ x1, family='cloglog')
> cau <- orm(y ~ x1, family='cauchit')
> x <- 1:i
> z <- list(logistic=list(x=x, y=coef(g)[1:i]),
+           probit  =list(x=x, y=coef(h)[1:i]),
+           loglog  =list(x=x, y=coef(ll)[1:i]),
+           cloglog =list(x=x, y=coef(cll)[1:i]))
> labcurve(z, pl=TRUE, col=1:4, ylab='Intercept')
> 
> tapply(y, x1, mean)
       0        1 
0.021763 3.045405 
> m <- Mean(g)
> m(w <- k[1] + k['x1']*c(0,1))
[1] 4.2053 5.6204
> mh <- Mean(h)
> wh <- coef(h)[1] + coef(h)['x1']*c(0,1)
> mh(wh)
[1] 3.9247 5.6176
> 
> qu <- Quantile(g)
> # Compare model estimated and empirical quantiles
> cq <- function(y) {
+    cat(qu(.1, w), tapply(y, x1, quantile, probs=.1), '\n')
+    cat(qu(.5, w), tapply(y, x1, quantile, probs=.5), '\n')
+    cat(qu(.9, w), tapply(y, x1, quantile, probs=.9), '\n')
+    }
> cq(y)
2.9411 5.6492 -1.1389 1.7665 
4.1721 5.6492 -0.049369 2.984 
5.4832 5.6492 1.1811 4.2195 
> 
> # Try on log-normal model
> g <- orm(exp(y) ~ x1)
> g
Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = exp(y) ~ x1)

                        Model Likelihood               Discrimination    Rank Discrim.    
                              Ratio Test                      Indexes          Indexes    
Obs            300    LR chi2     322.59    R2                  0.659    rho     0.841    
ESS            300    d.f.             1    R2(1,300)           0.658    Dxy     0.487    
Distinct Y     300    Pr(> chi2) <0.0001    R2(1,300)           0.658                     
Median Y    4.5846    Score chi2  271.61    |Pr(Y>=median)-0.5| 0.442                     
max |deriv|  5e-11    Pr(> chi2) <0.0001                                                  

   Coef   S.E.   Wald Z Pr(>|Z|)
x1 5.6036 0.4369 12.83  <0.0001 

> k <- coef(g)
> plot(k[1:i])
> m <- Mean(g)
> m(w <- k[1] + k['x1']*c(0,1))
[1]  95.874 278.486
> tapply(exp(y), x1, mean)
      0       1 
 1.5448 35.0218 
> 
> qu <- Quantile(g)
> cq(exp(y))
18.937 284.05 0.3203 5.8504 
64.872 284.05 0.95184 19.767 
240.91 284.05 3.258 68.045 
> 
> # Compare predicted mean with ols for a continuous x
> set.seed(3)
> n <- 200
> x1 <- rnorm(n)
> y <- x1 + rnorm(n)
> dd <- datadist(x1); options(datadist='dd')
> f <- ols(y ~ x1)
> g <- orm(y ~ x1, family='probit')
> h <- orm(y ~ x1, family='logistic')
> w <- orm(y ~ x1, family='cloglog')
> mg <- Mean(g); mh <- Mean(h); mw <- Mean(w)
> r <- rbind(ols      = Predict(f, conf.int=FALSE),
+            probit   = Predict(g, conf.int=FALSE, fun=mg),
+            logistic = Predict(h, conf.int=FALSE, fun=mh),
+            cloglog  = Predict(w, conf.int=FALSE, fun=mw))
> plot(r, groups='.set.')
> 
> # Compare predicted 0.8 quantile with quantile regression
> qu <- Quantile(g)
> qu80 <- function(lp) qu(.8, lp)
> f <- Rq(y ~ x1, tau=.8)
> r <- rbind(probit   = Predict(g, conf.int=FALSE, fun=qu80),
+            quantreg = Predict(f, conf.int=FALSE))
> plot(r, groups='.set.')
> 
> # Verify transformation invariance of ordinal regression
> ga <- orm(exp(y) ~ x1, family='probit')
> qua <- Quantile(ga)
> qua80 <- function(lp) log(qua(.8, lp))
> r <- rbind(logprobit = Predict(ga, conf.int=FALSE, fun=qua80),
+            probit    = Predict(g,  conf.int=FALSE, fun=qu80))
> plot(r, groups='.set.')
> 
> # Try the same with quantile regression.  Need to transform x1
> fa <- Rq(exp(y) ~ rcs(x1,5), tau=.8)
> r <- rbind(qr    = Predict(f, conf.int=FALSE),
+            logqr = Predict(fa, conf.int=FALSE, fun=log))
> plot(r, groups='.set.')
> 
> # Make a plot of Pr(Y >= y) vs. a continuous covariate for 3 levels
> # of y and also against a binary covariate
> set.seed(1)
> n <- 1000
> age <- rnorm(n, 50, 15)
> sex <- sample(c('m', 'f'), 1000, TRUE)
> Y   <- runif(n)
> dd  <- datadist(age, sex); options(datadist='dd')
> f <- orm(Y ~ age + sex)
> # Use ExProb function to derive an R function to compute
> # P(Y >= y | X)
> ex  <- ExProb(f)
> ex1 <- function(x) ex(x, y=0.25)
> ex2 <- function(x) ex(x, y=0.5)
> ex3 <- function(x) ex(x, y=0.75)
> p1  <- Predict(f, age, sex, fun=ex1)
> p2  <- Predict(f, age, sex, fun=ex2)
> p3  <- Predict(f, age, sex, fun=ex3)
> p   <- rbind('P(Y >= 0.25)' = p1,
+              'P(Y >= 0.5)'  = p2,
+              'P(Y >= 0.75)' = p3)
> ggplot(p)
Don't know how to automatically pick scale for object of type <ExProb>.
Defaulting to continuous.
> 
> # Make plot with two curves (by sex) with y on the x-axis, and
> # estimated P(Y >= y | sex, age=median) on the y-axis
> ys <- seq(min(Y), max(Y), length=100)
> g <- function(sx) as.vector(ex(y=ys, Predict(f, sex=sx)$yhat)$prob)
> 
> d  <- rbind(data.frame(sex='m', y=ys, p=g('m')),
+             data.frame(sex='f', y=ys, p=g('f')))
> ggplot(d, aes(x=y, y=p, color=sex)) + geom_line() +
+   ylab(expression(P(Y >= y))) +
+   guides(color=guide_legend(title='Sex')) +
+   theme(legend.position='bottom')
> 
> options(datadist=NULL)
> ## Not run: 
> ##D ## Simulate power and type I error for orm logistic and probit regression
> ##D ## for likelihood ratio, Wald, and score chi-square tests, and compare
> ##D ## with t-test
> ##D require(rms)
> ##D set.seed(5)
> ##D nsim <- 2000
> ##D r <- NULL
> ##D for(beta in c(0, .4)) {
> ##D   for(n in c(10, 50, 300)) {
> ##D     cat('beta=', beta, '  n=', n, '\n\n')
> ##D     plogistic <- pprobit <- plogistics <- pprobits <- plogisticw <-
> ##D       pprobitw <- ptt <- numeric(nsim)
> ##D     x <- c(rep(0, n/2), rep(1, n/2))
> ##D     pb <- setPb(nsim, every=25, label=paste('beta=', beta, '  n=', n))
> ##D     for(j in 1:nsim) {
> ##D       pb(j)
> ##D       y <- beta*x + rnorm(n)
> ##D       tt <- t.test(y ~ x)
> ##D       ptt[j] <- tt$p.value
> ##D       f <- orm(y ~ x)
> ##D       plogistic[j]  <- f$stats['P']
> ##D       plogistics[j] <- f$stats['Score P']
> ##D       plogisticw[j] <- 1 - pchisq(coef(f)['x']^2 / vcov(f)[2,2], 1)
> ##D       f <- orm(y ~ x, family'='probit')
> ##D       pprobit[j]  <- f$stats['P']
> ##D       pprobits[j] <- f$stats['Score P']
> ##D       pprobitw[j] <- 1 - pchisq(coef(f)['x']^2 / vcov(f)[2,2], 1)
> ##D     }
> ##D     if(beta == 0) plot(ecdf(plogistic))
> ##D     r <- rbind(r, data.frame(beta         = beta, n=n,
> ##D                              ttest        = mean(ptt        < 0.05),
> ##D                              logisticlr   = mean(plogistic  < 0.05),
> ##D                              logisticscore= mean(plogistics < 0.05),
> ##D                              logisticwald = mean(plogisticw < 0.05),
> ##D                              probit       = mean(pprobit    < 0.05),
> ##D                              probitscore  = mean(pprobits   < 0.05),
> ##D                              probitwald   = mean(pprobitw   < 0.05)))
> ##D   }
> ##D }
> ##D print(r)
> ##D #  beta   n  ttest logisticlr logisticscore logisticwald probit probitscore probitwald
> ##D #1  0.0  10 0.0435     0.1060        0.0655        0.043 0.0920      0.0920     0.0820
> ##D #2  0.0  50 0.0515     0.0635        0.0615        0.060 0.0620      0.0620     0.0620
> ##D #3  0.0 300 0.0595     0.0595        0.0590        0.059 0.0605      0.0605     0.0605
> ##D #4  0.4  10 0.0755     0.1595        0.1070        0.074 0.1430      0.1430     0.1285
> ##D #5  0.4  50 0.2950     0.2960        0.2935        0.288 0.3120      0.3120     0.3120
> ##D #6  0.4 300 0.9240     0.9215        0.9205        0.920 0.9230      0.9230     0.9230
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("orm.fit")
> ### * orm.fit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: orm.fit
> ### Title: Ordinal Regression Model Fitter
> ### Aliases: orm.fit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> #Fit an additive logistic model containing numeric predictors age,
> #blood.pressure, and sex, assumed to be already properly coded and
> #transformed
> #
> # fit <- orm.fit(cbind(age,blood.pressure,sex), death)
> 
> 
> 
> cleanEx()
> nameEx("pentrace")
> ### * pentrace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pentrace
> ### Title: Trace AIC and BIC vs. Penalty
> ### Aliases: pentrace plot.pentrace print.pentrace effective.df
> ### Keywords: models regression
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> 
> f <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
+          x=TRUE, y=TRUE)
> p <- pentrace(f, seq(.2,1,by=.05))
> plot(p)
> p$diag      # may learn something about fractional effective d.f. 
               Intercept           blood.pressure                 sex=male 
                 1.00000                  0.99570                  0.45037 
                     age              cholesterol             cholesterol' 
                 0.99233                  0.95829                  0.73826 
           cholesterol''           sex=male * age   sex=male * cholesterol 
                 0.84801                  0.90124                  0.63340 
 sex=male * cholesterol' sex=male * cholesterol'' 
                 0.81625                  0.87116 
>             # for each original parameter
> pentrace(f, list(simple=c(0,.2,.4), nonlinear=c(0,.2,.4,.8,1)))

Best penalty:

 simple nonlinear     df
    0.4         1 8.3129

 simple nonlinear      df    aic     bic  aic.c
    0.0       0.0 10.0000 46.592 -2.4856 46.369
    0.0       0.2  9.2977 47.582  1.9510 47.388
    0.2       0.2  9.0756 47.663  3.1222 47.478
    0.0       0.4  8.9763 47.917  3.8629 47.736
    0.2       0.4  8.8318 47.974  4.6299 47.799
    0.4       0.4  8.7490 47.994  5.0556 47.821
    0.0       0.8  8.5922 48.375  6.2065 48.208
    0.2       0.8  8.4979 48.442  6.7362 48.279
    0.4       0.8  8.4346 48.482  7.0875 48.322
    0.0       1.0  8.4540 48.561  7.0707 48.399
    0.2       1.0  8.3711 48.630  7.5469 48.472
    0.4       1.0  8.3129 48.676  7.8783 48.520
> 
> 
> # Bootstrap pentrace 5 times, making a plot of corrected AIC plot with 5 reps
> n <- nrow(f$x)
> plot(pentrace(f, seq(.2,1,by=.05)), which='aic.c', 
+      col=1, ylim=c(30,120)) #original in black
> for(j in 1:5)
+   plot(pentrace(f, seq(.2,1,by=.05), subset=sample(n,n,TRUE)), 
+        which='aic.c', col=j+1, add=TRUE)
> 
> 
> # Find penalty giving optimum corrected AIC.  Initial guess is 1.0
> # Not implemented yet
> # pentrace(f, 1, method='optimize')
> 
> 
> # Find penalty reducing total regression d.f. effectively to 5
> # pentrace(f, 1, target.df=5)
> 
> 
> # Re-fit with penalty giving best aic.c without differential penalization
> f <- update(f, penalty=p$penalty)
> effective.df(f)

Original and Effective Degrees of Freedom

                         Original Penalized
All                            10      8.20
Simple Terms                    4      3.40
Interaction or Nonlinear        6      4.81
Nonlinear                       4      3.27
Interaction                     4      3.22
Nonlinear Interaction           2      1.69
> 
> 
> 
> cleanEx()
> nameEx("plot.Predict")
> ### * plot.Predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.Predict
> ### Title: Plot Effects of Variables Estimated by a Regression Model Fit
> ### Aliases: plot.Predict pantext
> ### Keywords: models hplot htest
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
+                x=TRUE, y=TRUE)
> an <- anova(fit)
> # Plot effects of all 4 predictors with test statistics from anova, and P
> plot(Predict(fit), anova=an, pval=TRUE)
> plot(Predict(fit), data=llist(blood.pressure,age))
>                          # rug plot for two of the predictors
> 
> p <- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
> plot(p)
> 
> p <- Predict(fit, age=seq(20,80,length=100), sex, conf.int=FALSE)
>                          # Plot relationship between age and log
>                          # odds, separate curve for each sex,
> plot(p, subset=sex=='female' | age > 30)
> # No confidence interval, suppress estimates for males <= 30
> 
> p <- Predict(fit, age, sex)
> plot(p, label.curves=FALSE, data=llist(age,sex))
>                          # use label.curves=list(keys=c('a','b'))'
>                          # to use 1-letter abbreviations
>                          # data= allows rug plots (1-dimensional scatterplots)
>                          # on each sex's curve, with sex-
>                          # specific density of age
>                          # If data were in data frame could have used that
> p <- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
>                          # works if datadist not used
> plot(p, ylab=expression(hat(P)))
>                          # plot predicted probability in place of log odds
> 
> per <- function(x, y) x >= 30
> plot(p, perim=per)       # suppress output for age < 30 but leave scale alone
> 
> # Take charge of the plot setup by specifying a lattice formula
> p <- Predict(fit, age, blood.pressure=c(120,140,160),
+              cholesterol=c(180,200,215), sex)
> plot(p, ~ age | blood.pressure*cholesterol, subset=sex=='male')
> # plot(p, ~ age | cholesterol*blood.pressure, subset=sex=='female')
> # plot(p, ~ blood.pressure|cholesterol*round(age,-1), subset=sex=='male')
> plot(p)
> 
> # Plot the age effect as an odds ratio
> # comparing the age shown on the x-axis to age=30 years
> 
> ddist$limits$age[2] <- 30    # make 30 the reference value for age
> # Could also do: ddist$limits["Adjust to","age"] <- 30
> fit <- update(fit)   # make new reference value take effect
> p <- Predict(fit, age, ref.zero=TRUE, fun=exp)
> plot(p, ylab='Age=x:Age=30 Odds Ratio',
+      abline=list(list(h=1, lty=2, col=2), list(v=30, lty=2, col=2)))
> 
> # Compute predictions for three predictors, with superpositioning or
> # conditioning on sex, combined into one graph
> 
> p1 <- Predict(fit, age, sex)
> p2 <- Predict(fit, cholesterol, sex)
> p3 <- Predict(fit, blood.pressure, sex)
> p <- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
> plot(p, groups='sex', varypred=TRUE, adj.subtitle=FALSE)
> plot(p, cond='sex', varypred=TRUE, adj.subtitle=FALSE)
> 
> ## Not run: 
> ##D # For males at the median blood pressure and cholesterol, plot 3 types
> ##D # of confidence intervals for the probability on one plot, for varying age
> ##D ages <- seq(20, 80, length=100)
> ##D p1 <- Predict(fit, age=ages, sex='male', fun=plogis)  # standard pointwise
> ##D p2 <- Predict(fit, age=ages, sex='male', fun=plogis,
> ##D               conf.type='simultaneous')               # simultaneous
> ##D p3 <- Predict(fit, age=c(60,65,70), sex='male', fun=plogis,
> ##D               conf.type='simultaneous')               # simultaneous 3 pts
> ##D # The previous only adjusts for a multiplicity of 3 points instead of 100
> ##D f <- update(fit, x=TRUE, y=TRUE)
> ##D g <- bootcov(f, B=500, coef.reps=TRUE)
> ##D p4 <- Predict(g, age=ages, sex='male', fun=plogis)    # bootstrap percentile
> ##D p <- rbind(Pointwise=p1, 'Simultaneous 100 ages'=p2,
> ##D            'Simultaneous     3 ages'=p3, 'Bootstrap nonparametric'=p4)
> ##D xYplot(Cbind(yhat, lower, upper) ~ age, groups=.set.,
> ##D        data=p, type='l', method='bands', label.curve=list(keys='lines'))
> ## End(Not run)
> 
> # Plots for a parametric survival model
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('Male','Female'), n, 
+               rep=TRUE, prob=c(.6, .4)))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> t <- -log(runif(n))/h
> label(t) <- 'Follow-up Time'
> e <- ifelse(t<=cens,1,0)
> t <- pmin(t, cens)
> units(t) <- "Year"
> ddist <- datadist(age, sex)
> Srv <- Surv(t,e)
> 
> 
> # Fit log-normal survival model and plot median survival time vs. age
> f <- psm(Srv ~ rcs(age), dist='lognormal')
number of knots in rcs defaulting to 5
> med <- Quantile(f)       # Creates function to compute quantiles
>                          # (median by default)
> p <- Predict(f, age, fun=function(x) med(lp=x))
> plot(p, ylab="Median Survival Time")
> # Note: confidence intervals from this method are approximate since
> # they don't take into account estimation of scale parameter
> 
> 
> # Fit an ols model to log(y) and plot the relationship between x1
> # and the predicted mean(y) on the original scale without assuming
> # normality of residuals; use the smearing estimator
> # See help file for rbind.Predict for a method of showing two
> # types of confidence intervals simultaneously.
> set.seed(1)
> x1 <- runif(300)
> x2 <- runif(300)
> ddist <- datadist(x1,x2)
> y  <- exp(x1+x2-1+rnorm(300))
> f <- ols(log(y) ~ pol(x1,2)+x2)
> r <- resid(f)
> smean <- function(yhat)smearingEst(yhat, exp, res, statistic='mean')
> formals(smean) <- list(yhat=numeric(0), res=r[!is.na(r)])
> #smean$res <- r[!is.na(r)]   # define default res argument to function
> plot(Predict(f, x1, fun=smean), ylab='Predicted Mean on y-scale')
> 
> # Make an 'interaction plot', forcing the x-axis variable to be
> # plotted at integer values but labeled with category levels
> n <- 100
> set.seed(1)
> gender <- c(rep('male', n), rep('female',n))
> m <- sample(c('a','b'), 2*n, TRUE)
> d <-  datadist(gender, m); options(datadist='d')
> anxiety <- runif(2*n) + .2*(gender=='female') + .4*(gender=='female' & m=='b')
> tapply(anxiety, llist(gender,m), mean)
        m
gender         a       b
  female 0.74423 1.05654
  male   0.47459 0.39699
> f <- ols(anxiety ~ gender*m)
> p <- Predict(f, gender, m)
> plot(p)     # horizontal dot chart; usually preferred for categorical predictors
> Key(.5, .5)
> plot(p, ~gender, groups='m', nlines=TRUE)
> plot(p, ~m, groups='gender', nlines=TRUE)
> plot(p, ~gender|m, nlines=TRUE)
> 
> options(datadist=NULL)
> 
> ## Not run: 
> ##D # Example in which separate curves are shown for 4 income values
> ##D # For each curve the estimated percentage of voters voting for
> ##D # the democratic party is plotted against the percent of voters
> ##D # who graduated from college.  Data are county-level percents.
> ##D 
> ##D incomes <- seq(22900, 32800, length=4)  
> ##D # equally spaced to outer quintiles
> ##D p <- Predict(f, college, income=incomes, conf.int=FALSE)
> ##D plot(p, xlim=c(0,35), ylim=c(30,55))
> ##D 
> ##D # Erase end portions of each curve where there are fewer than 10 counties having
> ##D # percent of college graduates to the left of the x-coordinate being plotted,
> ##D # for the subset of counties having median family income with 1650
> ##D # of the target income for the curve
> ##D 
> ##D show.pts <- function(college.pts, income.pt) {
> ##D   s <- abs(income - income.pt) < 1650  #assumes income known to top frame
> ##D   x <- college[s]
> ##D   x <- sort(x[!is.na(x)])
> ##D   n <- length(x)
> ##D   low <- x[10]; high <- x[n-9]
> ##D   college.pts >= low & college.pts <= high
> ##D }
> ##D 
> ##D plot(p, xlim=c(0,35), ylim=c(30,55), perim=show.pts)
> ##D 
> ##D # Rename variables for better plotting of a long list of predictors
> ##D f <- ...
> ##D p <- Predict(f)
> ##D re <- c(trt='treatment', diabet='diabetes', sbp='systolic blood pressure')
> ##D 
> ##D for(n in names(re)) {
> ##D   names(p)[names(p)==n] <- re[n]
> ##D   p$.predictor.[p$.predictor.==n] <- re[n]
> ##D   }
> ##D plot(p)
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("plot.xmean.ordinaly")
> ### * plot.xmean.ordinaly
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.xmean.ordinaly
> ### Title: Plot Mean X vs. Ordinal Y
> ### Aliases: plot.xmean.ordinaly
> ### Keywords: category models regression hplot
> 
> ### ** Examples
> 
> # Simulate data from a population proportional odds model
> set.seed(1)
> n <- 400
> age <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> region <- factor(sample(c('north','south','east','west'), n, replace=TRUE))
> L <- .2*(age-50) + .1*(blood.pressure-120)
> p12 <- plogis(L)    # Pr(Y>=1)
> p2  <- plogis(L-1)  # Pr(Y=2)
> p   <- cbind(1-p12, p12-p2, p2)   # individual class probabilites
> # Cumulative probabilities:
> cp  <- matrix(cumsum(t(p)) - rep(0:(n-1), rep(3,n)), byrow=TRUE, ncol=3)
> y   <- (cp < runif(n)) %*% rep(1,3)
> # Thanks to Dave Krantz <dhk@paradox.psych.columbia.edu> for this trick
> 
> par(mfrow=c(2,2))
> plot.xmean.ordinaly(y ~ age + blood.pressure + region, cr=TRUE, topcats=2)
> par(mfrow=c(1,1))
> # Note that for unimportant predictors we don't care very much about the
> # shapes of these plots.  Use the Hmisc chiSquare function to compute
> # Pearson chi-square statistics to rank the variables by unadjusted
> # importance without assuming any ordering of the response:
> chiSquare(y ~ age + blood.pressure + region, g=3)

Pearson Chi-square Tests    Response variable:y

               chisquare df chisquare-df      P   n
age               104.47  4       100.47 0.0000 400
blood.pressure     72.28  4        68.28 0.0000 400
region              4.90  6        -1.10 0.5572 400
> chiSquare(y ~ age + blood.pressure + region, g=5)

Pearson Chi-square Tests    Response variable:y

               chisquare df chisquare-df      P   n
age               117.46  8       109.46 0.0000 400
blood.pressure     90.87  8        82.87 0.0000 400
region              4.90  6        -1.10 0.5572 400
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("plotIntercepts")
> ### * plotIntercepts
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotIntercepts
> ### Title: Plot Intercepts
> ### Aliases: plotIntercepts
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- orm(y ~ x1 + x2 + x3)
> ##D plotIntercepts(f)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotp.Predict")
> ### * plotp.Predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotp.Predict
> ### Title: Plot Effects of Variables Estimated by a Regression Model Fit
> ###   Using plotly
> ### Aliases: plotp.Predict
> ### Keywords: models hplot htest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D n <- 350     # define sample size
> ##D set.seed(17) # so can reproduce the results
> ##D age            <- rnorm(n, 50, 10)
> ##D blood.pressure <- rnorm(n, 120, 15)
> ##D cholesterol    <- rnorm(n, 200, 25)
> ##D sex            <- factor(sample(c('female','male'), n,TRUE))
> ##D label(age)            <- 'Age'      # label is in Hmisc
> ##D label(cholesterol)    <- 'Total Cholesterol'
> ##D label(blood.pressure) <- 'Systolic Blood Pressure'
> ##D label(sex)            <- 'Sex'
> ##D units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> ##D units(blood.pressure) <- 'mmHg'
> ##D 
> ##D # Specify population model for log odds that Y=1
> ##D L <- .4*(sex=='male') + .045*(age-50) +
> ##D     (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
> ##D     .01 * (blood.pressure - 120)
> ##D # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> ##D y <- ifelse(runif(n) < plogis(L), 1, 0)
> ##D 
> ##D ddist <- datadist(age, blood.pressure, cholesterol, sex)
> ##D options(datadist='ddist')
> ##D 
> ##D fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)),
> ##D            x=TRUE, y=TRUE)
> ##D 
> ##D p <- plotp(Predict(fit))
> ##D p$Continuous
> ##D p$Categorical
> ##D # When using Rmarkdown html notebook, best to use
> ##D # prList(p) to render the two objects
> ##D plotp(Predict(fit), rdata=llist(blood.pressure, age))$Continuous
> ##D # spike histogram plot for two of the predictors
> ##D 
> ##D p <- Predict(fit, name=c('age','cholesterol'))   # Make 2 plots
> ##D plotp(p)
> ##D 
> ##D p <- Predict(fit, age, sex)
> ##D plotp(p, rdata=llist(age,sex))
> ##D # rdata= allows rug plots (1-dimensional scatterplots)
> ##D # on each sex's curve, with sex-
> ##D # specific density of age
> ##D # If data were in data frame could have used that
> ##D p <- Predict(fit, age=seq(20,80,length=100), sex='male', fun=plogis)
> ##D # works if datadist not used
> ##D plotp(p, ylab='P')
> ##D # plot predicted probability in place of log odds
> ##D 
> ##D # Compute predictions for three predictors, with superpositioning or
> ##D # conditioning on sex, combined into one graph
> ##D 
> ##D p1 <- Predict(fit, age, sex)
> ##D p2 <- Predict(fit, cholesterol, sex)
> ##D p3 <- Predict(fit, blood.pressure, sex)
> ##D p <- rbind(age=p1, cholesterol=p2, blood.pressure=p3)
> ##D plotp(p, ncols=2, rdata=llist(age, cholesterol, sex))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("poma")
> ### * poma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: poma
> ### Title: Examine proportional odds and parallelism assumptions of 'orm'
> ###   and 'lrm' model fits.
> ### Aliases: poma
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D ## orm model (response variable has fewer than 10 unique levels)
> ##D mod.orm <- orm(carb ~ cyl + hp , x = TRUE, y = TRUE, data = mtcars)
> ##D poma(mod.orm)
> ##D 
> ##D 
> ##D ## runs rms::impactPO when its args are supplied
> ##D ## More examples: (https://yhpua.github.io/poma/)
> ##D d <- expand.grid(hp = c(90, 180), vs = c(0, 1))
> ##D mod.orm <- orm(cyl ~ vs + hp , x = TRUE, y = TRUE, data = mtcars)
> ##D poma(mod.orm, newdata = d)
> ##D 
> ##D 
> ##D ## orm model (response variable has >=10 unique levels)
> ##D mod.orm <- orm(mpg ~ cyl + hp , x=TRUE, y=TRUE, data = mtcars)
> ##D poma(mod.orm)
> ##D 
> ##D 
> ##D ## orm model using imputation
> ##D dat <- mtcars
> ##D ## introduce NAs
> ##D dat[sample(rownames(dat), 10), "cyl"] <- NA
> ##D im <- aregImpute(~ cyl + wt + mpg + am, data = dat)
> ##D aa <- fit.mult.impute(mpg ~ cyl + wt , xtrans = im, data = dat, fitter = orm)
> ##D poma(aa)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("pphsm")
> ### * pphsm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pphsm
> ### Title: Parametric Proportional Hazards form of AFT Models
> ### Aliases: pphsm print.pphsm vcov.pphsm
> ### Keywords: models survival regression
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> set.seed(1)
> S <- Surv(runif(100))
> x <- runif(100)
> dd <- datadist(x); options(datadist='dd')
> f <- psm(S ~ x, dist="exponential")
> summary(f)        # effects on log(T) scale
             Effects              Response : S 

 Factor               Low    High    Diff.   Effect   S.E.    Lower 0.95
 x                    0.2844 0.72993 0.44553 0.012754 0.15491 -0.29465  
  Survival Time Ratio 0.2844 0.72993 0.44553 1.012800      NA  0.74479  
 Upper 0.95
 0.32016   
 1.37730   

> f.ph <- pphsm(f)
Warning in pphsm(f) :
  at present, pphsm does not return the correct covariance matrix
> ## Not run: summary(f.ph)     # effects on hazard ratio scale
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("predab.resample")
> ### * predab.resample
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predab.resample
> ### Title: Predictive Ability using Resampling
> ### Aliases: predab.resample
> ### Keywords: models
> 
> ### ** Examples
> 
> # See the code for validate.ols for an example of the use of
> # predab.resample
> 
> 
> 
> cleanEx()
> nameEx("predict.lrm")
> ### * predict.lrm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.lrm
> ### Title: Predicted Values for Binary and Ordinal Logistic Models
> ### Aliases: predict.lrm predict.orm Mean.lrm Mean.orm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # See help for predict.rms for several binary logistic
> # regression examples
> 
> 
> # Examples of predictions from ordinal models
> set.seed(1)
> y <- factor(sample(1:3, 400, TRUE), 1:3, c('good','better','best'))
> x1 <- runif(400)
> x2 <- runif(400)
> f <- lrm(y ~ rcs(x1,4)*x2, x=TRUE)     #x=TRUE needed for se.fit
> # Get 0.95 confidence limits for Prob[better or best]
> L <- predict(f, se.fit=TRUE)           #omitted kint= so use 1st intercept
> plogis(with(L, linear.predictors + 1.96*cbind(-se.fit,se.fit)))
             se.fit
1   0.45740 0.70272
2   0.44115 0.73687
3   0.57401 0.74491
4   0.38785 0.79439
5   0.59693 0.78175
6   0.59257 0.76554
7   0.51158 0.71247
8   0.44367 0.73389
9   0.50123 0.79445
10  0.60184 0.75647
11  0.50947 0.73537
12  0.53119 0.71302
13  0.59426 0.77970
14  0.29831 0.70877
15  0.43497 0.71919
16  0.60366 0.75880
17  0.57481 0.71489
18  0.52095 0.69214
19  0.36104 0.70104
20  0.55497 0.77316
21  0.56806 0.80139
22  0.52503 0.76761
23  0.48618 0.74089
24  0.52268 0.72572
25  0.42537 0.71369
26  0.39531 0.69938
27  0.53721 0.70310
28  0.60114 0.77495
29  0.46559 0.74737
30  0.37950 0.71730
31  0.53284 0.67841
32  0.46732 0.72806
33  0.53054 0.70856
34  0.53819 0.69431
35  0.58221 0.80232
36  0.54092 0.72416
37  0.49968 0.71972
38  0.49153 0.70031
39  0.58900 0.74066
40  0.54328 0.80766
41  0.46416 0.78607
42  0.38016 0.69210
43  0.47610 0.75378
44  0.60476 0.75264
45  0.59096 0.73780
46  0.44732 0.76576
47  0.49534 0.70526
48  0.52510 0.74733
49  0.42046 0.71934
50  0.59386 0.75786
51  0.50528 0.69068
52  0.52608 0.76103
53  0.58428 0.79992
54  0.46541 0.79338
55  0.53154 0.82946
56  0.43016 0.68699
57  0.51011 0.72030
58  0.60898 0.75200
59  0.49539 0.68815
60  0.58036 0.72984
61  0.25193 0.72003
62  0.54341 0.71373
63  0.52323 0.73301
64  0.58881 0.73412
65  0.53471 0.70475
66  0.48764 0.70986
67  0.49300 0.70842
68  0.55235 0.77991
69  0.60072 0.75212
70  0.58628 0.74031
71  0.59245 0.78651
72  0.59604 0.75603
73  0.53320 0.68186
74  0.43331 0.68467
75  0.54439 0.70232
76  0.50291 0.78946
77  0.44452 0.83100
78  0.43362 0.85551
79  0.59956 0.75635
80  0.59637 0.75697
81  0.34240 0.69899
82  0.51528 0.70034
83  0.32310 0.70871
84  0.59809 0.75610
85  0.52450 0.74297
86  0.56074 0.81184
87  0.52944 0.69790
88  0.52960 0.78983
89  0.50134 0.73525
90  0.60218 0.77300
91  0.47511 0.71790
92  0.41859 0.70346
93  0.47087 0.74357
94  0.52080 0.74136
95  0.54293 0.73404
96  0.50224 0.72979
97  0.58717 0.79397
98  0.56303 0.77743
99  0.58471 0.73628
100 0.51517 0.71090
101 0.59750 0.73984
102 0.41533 0.68701
103 0.51341 0.67842
104 0.60542 0.75687
105 0.60166 0.76478
106 0.46102 0.73919
107 0.59702 0.75600
108 0.59022 0.78747
109 0.49394 0.69438
110 0.48216 0.68618
111 0.52916 0.79907
112 0.46645 0.70964
113 0.49497 0.73545
114 0.58443 0.79106
115 0.45124 0.68489
116 0.57452 0.71549
117 0.49588 0.69262
118 0.58471 0.72910
119 0.44929 0.69644
120 0.51655 0.69465
121 0.54343 0.71285
122 0.31288 0.70890
123 0.42715 0.71624
124 0.52529 0.67742
125 0.60605 0.75820
126 0.51949 0.79155
127 0.57006 0.79647
128 0.44137 0.72269
129 0.53559 0.72714
130 0.56513 0.72086
131 0.53989 0.70464
132 0.48435 0.74089
133 0.58121 0.80093
134 0.48871 0.75101
135 0.43224 0.76304
136 0.48695 0.71669
137 0.59700 0.75662
138 0.55945 0.81014
139 0.49767 0.71824
140 0.55284 0.75225
141 0.49469 0.79688
142 0.54408 0.71345
143 0.49701 0.68993
144 0.51048 0.75470
145 0.52286 0.68625
146 0.59085 0.72995
147 0.57029 0.78991
148 0.60769 0.75147
149 0.55767 0.76962
150 0.44654 0.75000
151 0.60026 0.75651
152 0.59684 0.74999
153 0.52156 0.68421
154 0.52641 0.68271
155 0.53991 0.70200
156 0.52320 0.75435
157 0.57536 0.76186
158 0.58238 0.77660
159 0.60401 0.75550
160 0.57615 0.78475
161 0.37227 0.79929
162 0.49470 0.70226
163 0.49319 0.68566
164 0.59379 0.76090
165 0.51128 0.69060
166 0.53933 0.71685
167 0.58156 0.76497
168 0.58212 0.77387
169 0.45147 0.71304
170 0.55880 0.70312
171 0.39832 0.69379
172 0.55645 0.79324
173 0.43035 0.68503
174 0.49873 0.72711
175 0.53100 0.68942
176 0.45465 0.71014
177 0.46520 0.71433
178 0.48871 0.70048
179 0.60742 0.75631
180 0.59163 0.72984
181 0.31181 0.70898
182 0.52648 0.69751
183 0.48996 0.69544
184 0.59403 0.75926
185 0.43694 0.75265
186 0.52836 0.71389
187 0.56912 0.77445
188 0.60833 0.74971
189 0.39187 0.69695
190 0.52683 0.71394
191 0.60408 0.76047
192 0.52225 0.73970
193 0.27749 0.73726
194 0.56675 0.78400
195 0.49741 0.68567
196 0.52991 0.78154
197 0.53562 0.75276
198 0.45546 0.71633
199 0.60027 0.77539
200 0.57527 0.72483
201 0.59958 0.76104
202 0.43109 0.69404
203 0.52061 0.77926
204 0.52049 0.73624
205 0.52987 0.68892
206 0.45044 0.68665
207 0.53462 0.70051
208 0.58577 0.78756
209 0.59309 0.76827
210 0.51716 0.69027
211 0.47334 0.73455
212 0.44436 0.68574
213 0.58760 0.79582
214 0.56654 0.72763
215 0.56433 0.78545
216 0.57990 0.78227
217 0.52796 0.73528
218 0.60163 0.75598
219 0.59816 0.74469
220 0.51656 0.69031
221 0.50237 0.72376
222 0.44809 0.71162
223 0.57825 0.77972
224 0.47629 0.77242
225 0.58793 0.76319
226 0.49377 0.73427
227 0.53020 0.80001
228 0.56046 0.73488
229 0.55759 0.79404
230 0.57359 0.75327
231 0.45376 0.81666
232 0.60790 0.74769
233 0.46484 0.70296
234 0.54059 0.71727
235 0.51170 0.67706
236 0.57524 0.72967
237 0.49039 0.74792
238 0.55618 0.70839
239 0.56272 0.81015
240 0.50236 0.69325
241 0.48427 0.68285
242 0.45948 0.70381
243 0.57839 0.77521
244 0.50583 0.70894
245 0.41878 0.68566
246 0.43479 0.73006
247 0.56264 0.75829
248 0.56867 0.78951
249 0.44340 0.71041
250 0.52372 0.75040
251 0.56629 0.74594
252 0.41804 0.87637
253 0.50221 0.70203
254 0.51852 0.69605
255 0.53514 0.69716
256 0.60459 0.74170
257 0.43923 0.71406
258 0.52942 0.73620
259 0.48420 0.71795
260 0.57334 0.74997
261 0.54993 0.78457
262 0.32618 0.74211
263 0.52788 0.72543
264 0.41802 0.77907
265 0.47305 0.77430
266 0.57379 0.76092
267 0.58508 0.73208
268 0.54360 0.82109
269 0.52036 0.75007
270 0.59098 0.76509
271 0.50613 0.69044
272 0.56907 0.81700
273 0.52161 0.68584
274 0.43625 0.73434
275 0.26744 0.71342
276 0.53631 0.72878
277 0.55098 0.75083
278 0.53667 0.79338
279 0.55939 0.76674
280 0.33980 0.70034
281 0.60325 0.75741
282 0.44354 0.71715
283 0.58291 0.80139
284 0.56284 0.77073
285 0.57293 0.81219
286 0.46947 0.76278
287 0.60890 0.75854
288 0.56675 0.80420
289 0.32604 0.71221
290 0.46455 0.79782
291 0.57277 0.73270
292 0.42462 0.71180
293 0.59060 0.75712
294 0.51288 0.73541
295 0.53908 0.72089
296 0.49653 0.71855
297 0.51745 0.69153
298 0.33657 0.72882
299 0.55179 0.77590
300 0.60697 0.76547
301 0.46191 0.79373
302 0.54153 0.70883
303 0.51707 0.69870
304 0.49901 0.71867
305 0.52562 0.68134
306 0.44736 0.71092
307 0.56541 0.80672
308 0.56799 0.79758
309 0.47898 0.68814
310 0.51709 0.67969
311 0.47123 0.72376
312 0.59491 0.75922
313 0.49218 0.70756
314 0.54194 0.69321
315 0.55243 0.79282
316 0.59039 0.74089
317 0.55094 0.79793
318 0.53613 0.79411
319 0.39021 0.79847
320 0.55257 0.78391
321 0.53545 0.75938
322 0.57267 0.81297
323 0.46375 0.79890
324 0.46306 0.70010
325 0.44948 0.73406
326 0.26680 0.71207
327 0.58255 0.72674
328 0.35652 0.71890
329 0.59380 0.74602
330 0.54093 0.70710
331 0.49208 0.76540
332 0.55738 0.78369
333 0.30236 0.73560
334 0.53676 0.75305
335 0.50993 0.70007
336 0.43960 0.72917
337 0.41518 0.70915
338 0.60956 0.75790
339 0.60617 0.73997
340 0.54312 0.81809
341 0.43588 0.70836
342 0.45647 0.69856
343 0.59695 0.76827
344 0.46024 0.73850
345 0.50875 0.71220
346 0.50857 0.75073
347 0.59698 0.75659
348 0.42180 0.72332
349 0.57719 0.77744
350 0.57287 0.77495
351 0.53407 0.73897
352 0.23871 0.71772
353 0.56183 0.82448
354 0.44490 0.68854
355 0.46921 0.73692
356 0.44782 0.68591
357 0.58782 0.76804
358 0.50955 0.68616
359 0.51155 0.68694
360 0.54939 0.75237
361 0.57811 0.72809
362 0.60433 0.76199
363 0.24773 0.75111
364 0.43533 0.76451
365 0.47581 0.73140
366 0.42238 0.68663
367 0.56637 0.81992
368 0.53109 0.69025
369 0.55745 0.71875
370 0.45170 0.68616
371 0.39916 0.68743
372 0.49598 0.76088
373 0.52940 0.67829
374 0.48138 0.69501
375 0.58516 0.77532
376 0.48254 0.75437
377 0.48017 0.76417
378 0.46126 0.72932
379 0.35495 0.73877
380 0.55053 0.69666
381 0.49111 0.69431
382 0.44799 0.68518
383 0.51023 0.75465
384 0.53415 0.68186
385 0.54365 0.71333
386 0.47948 0.70708
387 0.42637 0.72653
388 0.57238 0.77298
389 0.51550 0.73931
390 0.46948 0.70025
391 0.53766 0.76464
392 0.51235 0.68474
393 0.57396 0.75782
394 0.44558 0.83129
395 0.29351 0.70600
396 0.41617 0.69335
397 0.49574 0.73477
398 0.51539 0.69761
399 0.47125 0.69921
400 0.56972 0.80489
> predict(f, type="fitted.ind")[1:10,]   #gets Prob(better) and all others
    y=good y=better  y=best
1  0.41466  0.30945 0.27589
2  0.40212  0.31150 0.28638
3  0.33516  0.31613 0.34871
4  0.38993  0.31316 0.29691
5  0.30274  0.31391 0.38335
6  0.31455  0.31510 0.37035
7  0.38299  0.31395 0.30306
8  0.40273  0.31141 0.28586
9  0.33661  0.31616 0.34723
10 0.31577  0.31520 0.36903
> d <- data.frame(x1=c(.1,.5),x2=c(.5,.15))
> predict(f, d, type="fitted")        # Prob(Y>=j) for new observation
  y>=better y>=best
1   0.60437 0.29194
2   0.66074 0.34456
> predict(f, d, type="fitted.ind")    # Prob(Y=j)
   y=good y=better  y=best
1 0.39563  0.31242 0.29194
2 0.33926  0.31619 0.34456
> predict(f, d, type='mean', codes=TRUE) # predicts mean(y) using codes 1,2,3
     1      2 
1.8963 2.0053 
> m <- Mean(f, codes=TRUE)
> lp <- predict(f, d)
> m(lp)
     1      2 
1.8963 2.0053 
> # Can use function m as an argument to Predict or nomogram to
> # get predicted means instead of log odds or probabilities
> dd <- datadist(x1,x2); options(datadist='dd')
> m
function (lp = numeric(0), X = numeric(0), tmax = NULL, intercepts = c(`y>=better` = -0.0361587303189836, 
`y>=best` = -1.34581750143921), slopes = c(x1 = 3.93543209849838, 
`x1'` = -12.2075223167962, `x1''` = 34.6632429324729, x2 = 0.506562912671513, 
`x1 * x2` = -3.71091253363919, `x1' * x2` = 14.707821911861, 
`x1'' * x2` = -48.2796679033178), info = list(a = c(126.109101062346, 
118.175727956145, -63.2968626569114, 0), b = c(39.0371547417036, 
23.0246632098721, 4.66682224716965, 27.1414937329136, 18.2581529403106, 
10.8973427735895, 2.22957488594324, 23.0246632098721, 16.0092063195926, 
3.49420632081079, 13.4620802384037, 10.8973427735895, 7.66582811601502, 
1.68784562599841, 4.66682224716965, 3.49420632081079, 0.792679935198276, 
2.5776895383842, 2.22957488594324, 1.68784562599841, 0.385879707150393, 
27.1414937329136, 13.4620802384037, 2.5776895383842, 36.0923291934911, 
17.5649806328709, 8.75443363530595, 1.68179846970465, 18.2581529403106, 
10.8973427735895, 2.22957488594324, 17.5649806328709, 11.8497842700197, 
7.11186835355629, 1.46135442462476, 10.8973427735895, 7.66582811601502, 
1.68784562599841, 8.75443363530595, 7.11186835355629, 5.03663149273104, 
1.11498487537782, 2.22957488594324, 1.68784562599841, 0.385879707150393, 
1.68179846970465, 1.46135442462476, 1.11498487537782, 0.256365756028178
), ab = c(31.4555228419625, 26.7940585854611, 16.1827809674011, 
12.4549167731387, 3.14210485576544, 2.29013390062792, 29.7295352566445, 
25.9424335996839, 14.7197783780037, 12.4217153549099, 7.73187367183115, 
5.73020656657255, 1.5339542518158, 1.04373528656839), iname = c("y>=better", 
"y>=best"), xname = c("x1", "x1'", "x1''", "x2", "x1 * x2", "x1' * x2", 
"x1'' * x2")), values = 1:3, interceptRef = 1, Ncens = NULL, 
    famfunctions = expression(function(x) plogis(x), function(x) qlogis(x), 
        function(x, f) f * (1 - f), function(x, f, deriv) f * 
            (1 - 3 * f + 2 * f * f), function(x) {
            f <- plogis(x)
            f * (1 - f)
        }), conf.int = 0) 
{
    ns <- length(intercepts)
    lp <- if (length(lp)) 
        lp - intercepts[interceptRef]
    else matxv(X, slopes)
    xb <- sapply(intercepts, "+", lp)
    cumprob <- eval(famfunctions[1])
    deriv <- eval(famfunctions[5])
    P <- matrix(cumprob(xb), ncol = ns)
    if (!length(tmax)) {
        if (length(Ncens) && sum(Ncens) > 0 && min(1 - P) > 0.001) 
            warning("Computing the mean when the lowest P(Y < y) is ", 
                format(min(1 - P)), "\nand tmax omitted will result in only a lower limit to the mean")
    }
    else {
        if (tmax > max(values)) 
            stop("tmax=", tmax, " > maximum observed Y=", format(max(values)))
        values[values > tmax] <- tmax
    }
    P <- cbind(1, P) - cbind(P, 0)
    m <- drop(P %*% values)
    names(m) <- names(lp)
    if (conf.int) {
        if (!length(X)) 
            stop("must specify X if conf.int > 0")
        lb <- matrix(sapply(intercepts, "+", lp), ncol = ns)
        dmean.dalpha <- t(apply(deriv(lb), 1, FUN = function(x) x * 
            (values[2:length(values)] - values[1:ns])))
        dmean.dbeta <- apply(dmean.dalpha, 1, sum) * X
        dmean.dtheta <- cbind(dmean.dalpha, dmean.dbeta)
        if (getOption("rmsdebug", FALSE)) {
            prn(infoMxop(info, np = TRUE))
            prn(dim(dmean.dtheta))
        }
        mean.var <- diag(dmean.dtheta %*% infoMxop(info, B = t(dmean.dtheta)))
        w <- qnorm((1 + conf.int)/2) * sqrt(mean.var)
        attr(m, "limits") <- list(lower = m - w, upper = m + 
            w)
    }
    m
}
<environment: 0x564d782a6dd0>
> plot(Predict(f, x1, fun=m), ylab='Predicted Mean')
> # Note: Run f through bootcov with coef.reps=TRUE to get proper confidence
> # limits for predicted means from the prop. odds model
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("predictrms")
> ### * predictrms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predictrms
> ### Title: Predicted Values from Model Fit
> ### Aliases: predictrms predict.rms predict.bj predict.cph predict.Glm
> ###   predict.Gls predict.ols predict.psm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> treat          <- factor(sample(c('a','b','c'), n,TRUE))
> 
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male')) +
+   .3*sqrt(blood.pressure-60)-2.3 + 1*(treat=='b')
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex, treat)
> options(datadist='ddist')
> 
> 
> fit <- lrm(y ~ rcs(blood.pressure,4) + 
+            sex * (age + rcs(cholesterol,4)) + sex*treat*age)
> 
> 
> # Use xYplot to display predictions in 9 panels, with error bars,
> # with superposition of two treatments
> 
> 
> dat <- expand.grid(treat=levels(treat),sex=levels(sex),
+                    age=c(20,40,60),blood.pressure=120,
+                    cholesterol=seq(100,300,length=10))
> # Add variables linear.predictors and se.fit to dat
> dat <- cbind(dat, predict(fit, dat, se.fit=TRUE))
> # This is much easier with Predict
> # xYplot in Hmisc extends xyplot to allow error bars
> 
> xYplot(Cbind(linear.predictors,linear.predictors-1.96*se.fit,
+              linear.predictors+1.96*se.fit) ~ cholesterol | sex*age,
+        groups=treat, data=dat, type='b')
> 
> 
> 
> 
> # Since blood.pressure doesn't interact with anything, we can quickly and
> # interactively try various transformations of blood.pressure, taking
> # the fitted spline function as the gold standard. We are seeking a
> # linearizing transformation even though this may lead to falsely
> # narrow confidence intervals if we use this data-dredging-based transformation
> 
> 
> bp <- 70:160
> logit <- predict(fit, expand.grid(treat="a", sex='male', age=median(age),
+                  cholesterol=median(cholesterol),
+                  blood.pressure=bp), type="terms")[,"blood.pressure"]
> #Note: if age interacted with anything, this would be the age
> #      "main effect" ignoring interaction terms
> #Could also use Predict(f, age=ag)$yhat
> #which allows evaluation of the shape for any level of interacting
> #factors.  When age does not interact with anything, the result from
> #predict(f, \dots, type="terms") would equal the result from
> #plot if all other terms were ignored
> 
> 
> plot(bp^.5, logit)               # try square root vs. spline transform.
> plot(bp^1.5, logit)              # try 1.5 power
> plot(sqrt(bp-60), logit)
> 
> 
> #Some approaches to making a plot showing how predicted values
> #vary with a continuous predictor on the x-axis, with two other
> #predictors varying
> 
> 
> combos <- gendata(fit, age=seq(10,100,by=10), cholesterol=c(170,200,230),
+                   blood.pressure=c(80,120,160))
> #treat, sex not specified -> set to mode
> #can also used expand.grid
> 
> require(lattice)
Loading required package: lattice
> combos$pred <- predict(fit, combos)
> xyplot(pred ~ age | cholesterol*blood.pressure, data=combos, type='l')
> xYplot(pred ~ age | cholesterol, groups=blood.pressure, data=combos, type='l')
> Key()   # Key created by xYplot
> xYplot(pred ~ age, groups=interaction(cholesterol,blood.pressure),
+        data=combos, type='l', lty=1:9)
> Key()
> 
> 
> # Add upper and lower 0.95 confidence limits for individuals
> combos <- cbind(combos, predict(fit, combos, conf.int=.95))
> xYplot(Cbind(linear.predictors, lower, upper) ~ age | cholesterol,
+        groups=blood.pressure, data=combos, type='b')
> Key()
> 
> 
> # Plot effects of treatments (all pairwise comparisons) vs.
> # levels of interacting factors (age, sex)
> 
> 
> d <- gendata(fit, treat=levels(treat), sex=levels(sex), age=seq(30,80,by=10))
> x <- predict(fit, d, type="x")
> betas <- fit$coef
> cov   <- vcov(fit, intercepts='none')
> 
> 
> i <- d$treat=="a"; xa <- x[i,]; Sex <- d$sex[i]; Age <- d$age[i]
> i <- d$treat=="b"; xb <- x[i,]
> i <- d$treat=="c"; xc <- x[i,]
> 
> 
> doit <- function(xd, lab) {
+   xb <- matxv(xd, betas)
+   se <- apply((xd %*% cov) * xd, 1, sum)^.5
+   q <- qnorm(1-.01/2)   # 0.99 confidence limits
+   lower <- xb - q * se; upper <- xb + q * se
+   #Get odds ratios instead of linear effects
+   xb <- exp(xb); lower <- exp(lower); upper <- exp(upper)
+   #First elements of these agree with 
+   #summary(fit, age=30, sex='female',conf.int=.99))
+   for(sx in levels(Sex)) {
+     j <- Sex==sx
+     errbar(Age[j], xb[j], upper[j], lower[j], xlab="Age", 
+            ylab=paste(lab, "Odds Ratio"), ylim=c(.1, 20), log='y')
+     title(paste("Sex:", sx))
+     abline(h=1, lty=2)
+   }
+ }
> 
> 
> par(mfrow=c(3,2), oma=c(3,0,3,0))
> doit(xb - xa, "b:a")
> doit(xc - xa, "c:a")
> doit(xb - xa, "c:b")
> 
> # NOTE: This is much easier to do using contrast.rms
> 
> # Demonstrate type="terms", "cterms", "ccterms"
> set.seed(1)
> n <- 40
> x <- 1:n
> w <- factor(sample(c('a', 'b'), n, TRUE))
> u <- factor(sample(c('A', 'B'), n, TRUE))
> y <- .01*x + .2*(w=='b') + .3*(u=='B') + .2*(w=='b' & u=='B') + rnorm(n)/5
> ddist <- datadist(x, w, u)
> f <- ols(y ~ x*w*u, x=TRUE, y=TRUE)
> f
Linear Regression Model

ols(formula = y ~ x * w * u, x = TRUE, y = TRUE)

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs      40    LR chi2     42.69    R2       0.656    
sigma0.1885    d.f.            7    R2 adj   0.581    
d.f.     32    Pr(> chi2) 0.0000    g        0.263    

Residuals

      Min        1Q    Median        3Q       Max 
-0.318329 -0.116336  0.001699  0.079795  0.472002 


              Coef    S.E.   t     Pr(>|t|)
Intercept      0.2011 0.1443  1.39 0.1732  
x              0.0057 0.0057  0.99 0.3278  
w=b            0.2414 0.2217  1.09 0.2843  
u=B            0.1124 0.1719  0.65 0.5177  
x * w=b       -0.0034 0.0088 -0.38 0.7031  
x * u=B        0.0043 0.0075  0.58 0.5662  
w=b * u=B      0.0558 0.2717  0.21 0.8386  
x * w=b * u=B  0.0051 0.0111  0.46 0.6480  

> anova(f)
Warning in anova.rms(f) :
  tests of nonlinear interaction with respect to single component 
variables ignore 3-way interactions
                Analysis of Variance          Response: y 

 Factor                                   d.f. Partial SS MS        F    P     
 x  (Factor+Higher Order Factors)          4   0.4044281  0.1011070 2.85 0.0399
  All Interactions                         3   0.0583654  0.0194551 0.55 0.6533
 w  (Factor+Higher Order Factors)          4   0.7343790  0.1835948 5.17 0.0025
  All Interactions                         3   0.0642051  0.0214017 0.60 0.6182
 u  (Factor+Higher Order Factors)          4   0.7553646  0.1888412 5.32 0.0021
  All Interactions                         3   0.1240289  0.0413430 1.16 0.3388
 x * w  (Factor+Higher Order Factors)      2   0.0075815  0.0037907 0.11 0.8991
 x * u  (Factor+Higher Order Factors)      2   0.0582252  0.0291126 0.82 0.4497
 w * u  (Factor+Higher Order Factors)      2   0.0635351  0.0317676 0.89 0.4189
 x * w * u  (Factor+Higher Order Factors)  1   0.0075462  0.0075462 0.21 0.6480
 TOTAL INTERACTION                         4   0.1240951  0.0310238 0.87 0.4907
 REGRESSION                                7   2.1681606  0.3097372 8.72 <.0001
 ERROR                                    32   1.1368328  0.0355260            
> z <- predict(f, type='terms', center.terms=FALSE)
> z[1:5,]
          x       w       u      x * w     x * u    w * u x * w * u
1 0.0056691 0.00000 0.11241  0.0000000 0.0043201 0.000000  0.000000
2 0.0113382 0.24144 0.11241 -0.0067938 0.0086403 0.055783  0.010271
3 0.0170073 0.00000 0.00000  0.0000000 0.0000000 0.000000  0.000000
4 0.0226763 0.00000 0.11241  0.0000000 0.0172806 0.000000  0.000000
5 0.0283454 0.24144 0.11241 -0.0169845 0.0216007 0.055783  0.025677
> k <- coef(f)
> ## Manually compute combined terms
> wb <- w=='b'
> uB <- u=='B'
> h  <- k['x * w=b * u=B']*x*wb*uB
> tx <- k['x']  *x  + k['x * w=b']*x*wb + k['x * u=B']  *x*uB  + h
> tw <- k['w=b']*wb + k['x * w=b']*x*wb + k['w=b * u=B']*wb*uB + h
> tu <- k['u=B']*uB + k['x * u=B']*x*uB + k['w=b * u=B']*wb*uB + h
> h   <- z[,'x * w * u'] # highest order term is present in all cterms
> tx2 <- z[,'x']+z[,'x * w']+z[,'x * u']+h
> tw2 <- z[,'w']+z[,'x * w']+z[,'w * u']+h
> tu2 <- z[,'u']+z[,'x * u']+z[,'w * u']+h
> ae <- function(a, b) all.equal(a, b, check.attributes=FALSE)
> ae(tx, tx2)
[1] TRUE
> ae(tw, tw2)
[1] TRUE
> ae(tu, tu2)
[1] TRUE
> 
> zc <- predict(f, type='cterms')
> zc[1:5,]
          x       w       u
1 0.0099892 0.00000 0.11673
2 0.0234555 0.30070 0.18710
3 0.0170073 0.00000 0.00000
4 0.0399569 0.00000 0.12969
5 0.0586387 0.30592 0.21547
> ae(tx, zc[,'x'])
[1] TRUE
> ae(tw, zc[,'w'])
[1] TRUE
> ae(tu, zc[,'u'])
[1] TRUE
> 
> zc <- predict(f, type='ccterms')
> # As all factors are indirectly related, ccterms gives overall linear
> # predictor except for the intercept
> zc[1:5,]
[1] 0.122398 0.433089 0.017007 0.152365 0.468272
> ae(as.vector(zc + coef(f)[1]), f$linear.predictors)
[1] TRUE
> 
> ## Not run: 
> ##D #A variable state.code has levels "1", "5","13"
> ##D #Get predictions with or without converting variable in newdata to factor
> ##D predict(fit, data.frame(state.code=c(5,13)))
> ##D predict(fit, data.frame(state.code=factor(c(5,13))))
> ##D 
> ##D 
> ##D #Use gendata function (gendata.rms) for interactive specification of
> ##D #predictor variable settings (for 10 observations)
> ##D df <- gendata(fit, nobs=10, viewvals=TRUE)
> ##D df$predicted <- predict(fit, df)  # add variable to data frame
> ##D df
> ##D 
> ##D 
> ##D df <- gendata(fit, age=c(10,20,30))  # leave other variables at ref. vals.
> ##D predict(fit, df, type="fitted")
> ##D 
> ##D 
> ##D # See reShape (in Hmisc) for an example where predictions corresponding to 
> ##D # values of one of the varying predictors are reformatted into multiple
> ##D # columns of a matrix
> ## End(Not run)
> options(datadist=NULL)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:lattice’

> nameEx("print.Ocens")
> ### * print.Ocens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.Ocens
> ### Title: print Method for Ocens Objects
> ### Aliases: print.Ocens
> 
> ### ** Examples
> 
> Y <- Ocens(1:3, c(1, Inf, 3))
> Y
[1] 1  2+ 3 
> print(Y, ivalues=TRUE)  # doesn't change anything since were numeric
[1] 1  2+ 3 
> 
> 
> 
> cleanEx()
> nameEx("prmiInfo")
> ### * prmiInfo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prmiInfo
> ### Title: prmiInfo
> ### Aliases: prmiInfo
> 
> ### ** Examples
> 
> ## Not run: 
> ##D a <- aregImpute(...)
> ##D f <- fit.mult.impute(...)
> ##D v <- processMI(f, 'anova')
> ##D prmiInfo(v)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("psm")
> ### * psm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: psm
> ### Title: Parametric Survival Model
> ### Aliases: psm print.psm Hazard Survival Hazard.psm Mean.psm Quantile.psm
> ###   Survival.psm residuals.psm lines.residuals.psm.censored.normalized
> ###   survplot.residuals.psm.censored.normalized
> ### Keywords: models survival
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> n <- 400
> set.seed(1)
> age <- rnorm(n, 50, 12)
> sex <- factor(sample(c('Female','Male'),n,TRUE))
> dd <- datadist(age,sex)
> options(datadist='dd')
> # Population hazard function:
> h <- .02*exp(.06*(age-50)+.8*(sex=='Female'))
> d.time <- -log(runif(n))/h
> cens <- 15*runif(n)
> death <- ifelse(d.time <= cens,1,0)
> d.time <- pmin(d.time, cens)
> 
> f <- psm(Surv(d.time,death) ~ sex*pol(age,2), 
+          dist='lognormal')
> # Log-normal model is a bad fit for proportional hazards data
> print(f, r2=0:4, pg=TRUE)
Parametric Survival Model: Log Normal Distribution

psm(formula = Surv(d.time, death) ~ sex * pol(age, 2), dist = "lognormal")

                  Model Likelihood    Discrimination    
                        Ratio Test           Indexes    
Obs      400    LR chi2      37.75    R2       0.107    
Events    83    d.f.             5    R2(400)  0.090    
sigma 1.6360    Pr(> chi2) <0.0001    R2(5,400)0.079    
                                      R2(83)   0.365    
                                      R2(5,83) 0.326    
                                      Dxy      0.076    
                                      g        0.894    
                                      gr       2.446    

                 Coef    S.E.   Wald Z Pr(>|Z|)
(Intercept)       6.0037 2.4924  2.41  0.0160  
sex=Male          5.6166 4.9552  1.13  0.2570  
age              -0.0659 0.0933 -0.71  0.4800  
age^2             0.0001 0.0009  0.12  0.9056  
sex=Male * age   -0.1970 0.1842 -1.07  0.2850  
sex=Male * age^2  0.0019 0.0017  1.15  0.2499  
Log(scale)        0.4923 0.0839  5.87  <0.0001 

> 
> anova(f)
                Wald Statistics          Response: Surv(d.time, death) 

 Factor                                   Chi-Square d.f. P     
 sex  (Factor+Higher Order Factors)       12.84      3    0.0050
  All Interactions                         1.63      2    0.4421
 age  (Factor+Higher Order Factors)       20.48      4    0.0004
  All Interactions                         1.63      2    0.4421
  Nonlinear (Factor+Higher Order Factors)  2.00      2    0.3686
 sex * age  (Factor+Higher Order Factors)  1.63      2    0.4421
  Nonlinear                                1.32      1    0.2499
  Nonlinear Interaction : f(A,B) vs. AB    1.32      1    0.2499
 TOTAL NONLINEAR                           2.00      2    0.3686
 TOTAL NONLINEAR + INTERACTION             2.36      3    0.5015
 TOTAL                                    29.58      5    <.0001
> fastbw(f)  # if deletes sex while keeping age*sex ignore the result

 Deleted   Chi-Sq d.f. P      Residual d.f. P      AIC  
 sex * age 1.63   2    0.4421 1.63     2    0.4421 -2.37

Approximate Estimates after Deleting Factors

                  Coef    S.E.  Wald Z         P
(Intercept)  7.1717148 2.17068  3.3039 0.0009535
sex=Male     0.8257897 0.24666  3.3479 0.0008142
age         -0.1175160 0.08066 -1.4569 0.1451414
age^2        0.0006304 0.00074  0.8518 0.3943053

Factors in Final Model

[1] sex age
> f <- update(f, x=TRUE,y=TRUE)       # so can validate, compute certain resids
> validate(f, B=10)      # ordinarily use B=300 or more
          index.orig training    test optimism index.corrected  n
Dxy           0.3617   0.3998  0.3650   0.0348          0.3269 10
R2            0.1072   0.1201  0.0971   0.0230          0.0843 10
Intercept     0.0000   0.0000  0.2329  -0.2329          0.2329 10
Slope         1.0000   1.0000  0.9184   0.0816          0.9184 10
D             0.0502   0.0585  0.0451   0.0134          0.0368 10
U            -0.0027  -0.0029 -0.0074   0.0045         -0.0072 10
Q             0.0529   0.0613  0.0525   0.0089          0.0440 10
g             0.8945   1.0363  0.8829   0.1534          0.7411 10
> plot(Predict(f, age, sex))   # needs datadist since no explicit age, hosp.
> # Could have used ggplot(Predict(...))
> survplot(f, age=c(20,60))     # needs datadist since hospital not set here
> # latex(f)
> 
> 
> S <- Survival(f)
> plot(f$linear.predictors, S(6, f$linear.predictors),
+      xlab=expression(X*hat(beta)),
+      ylab=expression(S(6,X*hat(beta))))
> # plots 6-month survival as a function of linear predictor (X*Beta hat)
> 
> 
> times <- seq(0,24,by=.25)
> plot(times, S(times,0), type='l')   # plots survival curve at X*Beta hat=0
> lam <- Hazard(f)
> plot(times, lam(times,0), type='l') # similarly for hazard function
> 
> 
> med <- Quantile(f)        # new function defaults to computing median only
> lp <- seq(-3, 5, by=.1)
> plot(lp, med(lp=lp), ylab="Median Survival Time")
> med(c(.25,.5), f$linear.predictors)
            0.25      0.50
  [1,]    9.8247   29.6176
  [2,]    5.6878   17.1465
  [3,]   26.3991   79.5824
  [4,]    8.2282   24.8045
  [5,]    5.1653   15.5713
  [6,]   11.2310   33.8567
  [7,]    9.0945   27.4162
  [8,]    8.3457   25.1590
  [9,]    4.3960   13.2521
 [10,]    7.8929   23.7940
 [11,]    2.4206    7.2972
 [12,]    4.9644   14.9656
 [13,]    9.7896   29.5117
 [14,]   30.3430   91.4718
 [15,]    3.0879    9.3089
 [16,]    6.6233   19.9665
 [17,]   12.0844   36.4295
 [18,]    7.9962   24.1053
 [19,]    8.1785   24.6547
 [20,]    8.7293   26.3153
 [21,]    3.5216   10.6163
 [22,]    8.2528   24.8787
 [23,]   11.3551   34.2311
 [24,]  154.9616  467.1457
 [25,]    8.6514   26.0803
 [26,]    6.6731   20.1168
 [27,]   13.4251   40.4711
 [28,]   63.4809  191.3689
 [29,]    8.8764   26.7588
 [30,]    9.3747   28.2609
 [31,]    7.9114   23.8496
 [32,]   12.8819   38.8337
 [33,]    9.5079   28.6624
 [34,]    6.6628   20.0855
 [35,]   54.9477  165.6448
 [36,]   16.8250   50.7205
 [37,]    8.3837   25.2734
 [38,]   12.4683   37.5868
 [39,]    7.8701   23.7250
 [40,]    3.8930   11.7357
 [41,]   13.5188   40.7537
 [42,]   14.5481   43.8564
 [43,]    8.4432   25.4529
 [44,]    4.4511   13.4182
 [45,]   10.2547   30.9138
 [46,]   22.7618   68.6176
 [47,]    5.0475   15.2161
 [48,]    3.8795   11.6951
 [49,]   12.9766   39.1192
 [50,]    3.6082   10.8774
 [51,]    9.4612   28.5216
 [52,]    9.7279   29.3257
 [53,]    5.1260   15.4527
 [54,]   38.4600  115.9412
 [55,]    7.9812   24.0601
 [56,]    1.8130    5.4655
 [57,]   16.0915   48.5092
 [58,]   34.3022  103.4070
 [59,]    8.8058   26.5458
 [60,]   13.2073   39.8145
 [61,]   12.0460   36.3138
 [62,]    6.5981   19.8906
 [63,]    8.4613   25.5072
 [64,]    6.3081   19.0164
 [65,]   10.6474   32.0976
 [66,]    5.6685   17.0881
 [67,]   22.5231   67.8979
 [68,]    2.4915    7.5109
 [69,]   10.8008   32.5601
 [70,]    1.6133    4.8636
 [71,]    9.1401   27.5537
 [72,]   10.4055   31.3683
 [73,]    8.6783   26.1614
 [74,]   12.1525   36.6348
 [75,]   45.7914  138.0423
 [76,]    9.9797   30.0846
 [77,]   17.2861   52.1106
 [78,]    6.4225   19.3611
 [79,]    6.1162   18.4379
 [80,]   20.0281   60.3764
 [81,]   19.5969   59.0766
 [82,]   13.2085   39.8184
 [83,]    2.9856    9.0002
 [84,]   69.0195  208.0655
 [85,]    4.3443   13.0963
 [86,]    9.7668   29.4430
 [87,]    7.8895   23.7836
 [88,]    7.8865   23.7746
 [89,]    5.0295   15.1618
 [90,]    5.3823   16.2255
 [91,]    9.2755   27.9618
 [92,]    2.9298    8.8321
 [93,]    3.0192    9.1017
 [94,]    8.4352   25.4288
 [95,]    8.2120   24.7557
 [96,]    4.4458   13.4022
 [97,]   47.3386  142.7064
 [98,]   19.6907   59.3594
 [99,]   43.9274  132.4230
[100,]    8.8477   26.6722
[101,]   20.6931   62.3811
[102,]   11.6042   34.9820
[103,]   11.9582   36.0491
[104,]   10.7693   32.4652
[105,]   21.4707   64.7252
[106,]    8.6432   26.0558
[107,]    8.3955   25.3090
[108,]    8.0387   24.2335
[109,]    9.5237   28.7100
[110,]    2.1775    6.5643
[111,]    9.8876   29.8070
[112,]   17.5964   53.0459
[113,]    7.9804   24.0576
[114,]   21.3801   64.4523
[115,]    7.3871   22.2692
[116,]   16.4775   49.6730
[117,]    7.9714   24.0305
[118,]   14.8737   44.8381
[119,]    9.0691   27.3396
[120,]    7.2391   21.8228
[121,]    9.0465   27.2716
[122,]    7.9000   23.8154
[123,]   14.0814   42.4496
[124,]    7.2499   21.8556
[125,]   12.8564   38.7569
[126,]    4.0222   12.1252
[127,]    6.7516   20.3532
[128,]    6.5910   19.8692
[129,]   22.1174   66.6749
[130,]    7.9946   24.1003
[131,]    6.1743   18.6130
[132,]   20.0149   60.3367
[133,]    4.5247   13.6402
[134,]   18.3393   55.2854
[135,]    9.9005   29.8459
[136,]   70.4598  212.4075
[137,]   15.1605   45.7028
[138,]   18.8016   56.6792
[139,]    9.9993   30.1439
[140,]   12.4461   37.5199
[141,]   24.3768   73.4859
[142,]    7.8500   23.6646
[143,]   20.3657   61.3944
[144,]   17.6288   53.1435
[145,]   13.7978   41.5948
[146,]   23.9060   72.0666
[147,]    1.6990    5.1218
[148,]    6.3530   19.1516
[149,]   48.0129  144.7392
[150,]   20.0131   60.3313
[151,]    9.2404   27.8559
[152,]   12.1048   36.4911
[153,]    7.9610   23.9992
[154,]   29.6035   89.2424
[155,]   65.1716  196.4658
[156,]   13.4098   40.4250
[157,]    7.9375   23.9282
[158,]    9.7898   29.5123
[159,]   16.6732   50.2630
[160,]    8.9727   27.0489
[161,]    9.3442   28.1690
[162,]   14.3677   43.3127
[163,]    7.8924   23.7922
[164,]    3.5960   10.8403
[165,]    9.7762   29.4713
[166,]    1.5811    4.7662
[167,]   14.5687   43.9187
[168,]   59.0758  178.0895
[169,]   13.3045   40.1077
[170,]   10.4565   31.5220
[171,]   11.3004   34.0661
[172,]   11.1269   33.5431
[173,]    9.2130   27.7733
[174,]    6.7678   20.4022
[175,]    8.0475   24.2598
[176,]   12.2463   36.9175
[177,]    8.2418   24.8457
[178,]    9.8589   29.7204
[179,]    3.2857    9.9051
[180,]    7.8496   23.6634
[181,]   14.9634   45.1086
[182,]    7.9528   23.9743
[183,]    5.5528   16.7394
[184,]   63.1334  190.3213
[185,]    4.5558   13.7338
[186,]    7.1491   21.5516
[187,]    2.4930    7.5154
[188,]   10.8164   32.6070
[189,]   17.0704   51.4604
[190,]   12.0851   36.4318
[191,]    7.2380   21.8195
[192,]    4.9249   14.8465
[193,]   10.5631   31.8434
[194,]    3.7279   11.2381
[195,]   14.7206   44.3765
[196,]   13.1570   39.6630
[197,]    7.9905   24.0880
[198,]   12.8649   38.7825
[199,]    4.8928   14.7497
[200,]    8.3087   25.0474
[201,]    9.4116   28.3721
[202,]    8.4332   25.4228
[203,]    2.3105    6.9651
[204,]    8.0306   24.2090
[205,]   31.9563   96.3352
[206,]    1.3277    4.0025
[207,]    8.5198   25.6836
[208,]    8.9003   26.8309
[209,]    6.4850   19.5497
[210,]    9.0104   27.1627
[211,]    7.1762   21.6333
[212,]    9.3629   28.2254
[213,]   16.5927   50.0202
[214,]   54.3817  163.9386
[215,]    7.9489   23.9627
[216,]    8.0966   24.4080
[217,]   15.2648   46.0170
[218,]   15.1969   45.8124
[219,]    8.5873   25.8871
[220,]    6.6223   19.9635
[221,]   97.7714  294.7405
[222,]   11.9288   35.9605
[223,]    9.8507   29.6959
[224,]    8.0856   24.3747
[225,]   39.9267  120.3625
[226,]    2.0209    6.0923
[227,]   15.5725   46.9446
[228,]   19.5165   58.8344
[229,]    5.6370   16.9932
[230,]   10.1331   30.5473
[231,]   12.5983   37.9786
[232,] 1060.4936 3196.9535
[233,]    9.9199   29.9043
[234,]    4.4111   13.2977
[235,]   12.4721   37.5982
[236,]    6.8639   20.6920
[237,]    4.4390   13.3819
[238,]   41.6230  125.4764
[239,]    3.1437    9.4769
[240,]   11.9918   36.1503
[241,]    8.4180   25.3767
[242,]    7.9092   23.8430
[243,]   10.3609   31.2337
[244,]   27.8084   83.8310
[245,]    3.0143    9.0869
[246,]   25.9432   78.2080
[247,]   19.1205   57.6406
[248,]   14.5767   43.9429
[249,]    7.1846   21.6587
[250,]    3.3003    9.9490
[251,]   10.9151   32.9045
[252,]    4.9082   14.7963
[253,]    6.7339   20.2999
[254,]   14.4777   43.6445
[255,]    8.4467   25.4635
[256,]    3.0465    9.1839
[257,]  353.7709 1066.4743
[258,]    4.4047   13.2784
[259,]    5.0139   15.1150
[260,]   16.9901   51.2181
[261,]    7.9878   24.0801
[262,]   16.4227   49.5078
[263,]   14.9413   45.0418
[264,]    8.1168   24.4687
[265,]    2.1277    6.4141
[266,]    5.3718   16.1938
[267,]    8.5443   25.7575
[268,]   41.7782  125.9442
[269,]    8.0313   24.2110
[270,]   29.9943   90.4204
[271,]    7.6487   23.0578
[272,]    4.9496   14.9211
[273,]   26.9180   81.1466
[274,]    1.2137    3.6589
[275,]    5.7930   17.4636
[276,]    7.8589   23.6914
[277,]  278.9996  841.0694
[278,]    8.3398   25.1410
[279,]   50.1715  151.2464
[280,]    8.0260   24.1951
[281,]    9.4611   28.5213
[282,]   16.7067   50.3640
[283,]    7.8879   23.7789
[284,]   22.6031   68.1390
[285,]   19.8421   59.8159
[286,]   12.7329   38.3846
[287,]   10.1105   30.4791
[288,]    3.4630   10.4395
[289,]    4.8236   14.5411
[290,]    3.3327   10.0467
[291,]    8.3599   25.2018
[292,]    9.5594   28.8178
[293,]   10.2404   30.8705
[294,]   59.2366  178.5741
[295,]    2.0519    6.1857
[296,]    5.8765   17.7154
[297,]    8.2866   24.9808
[298,]    3.4410   10.3732
[299,]   12.3883   37.3458
[300,]   15.2253   45.8982
[301,]    8.0616   24.3025
[302,]   13.1507   39.6439
[303,]    9.3721   28.2529
[304,]   16.3373   49.2503
[305,]    2.2156    6.6792
[306,]    2.4200    7.2953
[307,]   11.2927   34.0428
[308,]    8.8139   26.5703
[309,]   33.4322  100.7845
[310,]    5.1875   15.6381
[311,]    3.2519    9.8032
[312,]    6.0164   18.1368
[313,]    8.7323   26.3242
[314,]   10.0247   30.2204
[315,]   12.2569   36.9495
[316,]    7.8859   23.7726
[317,]    8.9118   26.8653
[318,]   13.0637   39.3816
[319,]   48.5664  146.4078
[320,]    9.0686   27.3382
[321,]    7.8788   23.7512
[322,]    2.4430    7.3646
[323,]    3.7657   11.3521
[324,]   23.6028   71.1528
[325,]    4.6733   14.0882
[326,]    9.2164   27.7837
[327,]   15.8892   47.8996
[328,]    5.7376   17.2966
[329,]   11.5752   34.8945
[330,]    8.4880   25.5879
[331,]    8.0099   24.1466
[332,]   74.2765  223.9133
[333,]    8.2322   24.8167
[334,]    2.6545    8.0021
[335,]   15.6166   47.0776
[336,]    8.3583   25.1969
[337,]    3.4599   10.4301
[338,]    6.4083   19.3185
[339,]   15.8737   47.8526
[340,]    9.1945   27.7176
[341,]    8.3429   25.1505
[342,]   13.3001   40.0944
[343,]    5.4571   16.4509
[344,]    7.8085   23.5394
[345,]  265.5246  800.4478
[346,]   16.9646   51.1413
[347,]    8.0310   24.2100
[348,]    7.3074   22.0288
[349,]    8.2116   24.7545
[350,]    9.0385   27.2474
[351,]    8.0310   24.2100
[352,]    4.1155   12.4064
[353,]    4.9967   15.0630
[354,]   13.8313   41.6956
[355,]    2.3230    7.0029
[356,]    4.3378   13.0768
[357,]   40.8806  123.2381
[358,]   13.4234   40.4661
[359,]  136.0021  409.9905
[360,]   13.8591   41.7795
[361,]   40.1095  120.9136
[362,]    7.8820   23.7611
[363,]   21.0326   63.4047
[364,]   17.0666   51.4489
[365,]   13.5708   40.9105
[366,]    4.2930   12.9416
[367,]    4.1126   12.3978
[368,]    8.8115   26.5631
[369,]    9.4681   28.5425
[370,]   53.8178  162.2385
[371,]   16.4148   49.4840
[372,]    5.3440   16.1100
[373,]   26.0075   78.4020
[374,]    6.7302   20.2889
[375,]   14.3081   43.1330
[376,]   12.0169   36.2261
[377,]   10.9655   33.0564
[378,]   13.3200   40.1544
[379,]   13.5122   40.7337
[380,]    8.6324   26.0232
[381,]    3.8944   11.7401
[382,]    3.1145    9.3890
[383,]   29.3770   88.5595
[384,]   10.7281   32.3407
[385,]    3.0299    9.1339
[386,]    6.6749   20.1221
[387,]  202.6011  610.7592
[388,]    5.1134   15.4149
[389,]   24.2113   72.9870
[390,]   11.1589   33.6395
[391,]    7.8878   23.7785
[392,]    8.6637   26.1174
[393,]    7.8739   23.7366
[394,]    5.2438   15.8080
[395,]   12.9548   39.0535
[396,]   12.0701   36.3863
[397,]    8.2236   24.7907
[398,]    6.2370   18.8019
[399,]   22.9575   69.2075
[400,]    3.6453   10.9890
>                           # prints matrix with 2 columns
> 
> 
> # fit a model with no predictors
> f <- psm(Surv(d.time,death) ~ 1, dist="weibull")
> f
Parametric Survival Model: Weibull Distribution

psm(formula = Surv(d.time, death) ~ 1, dist = "weibull")

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs      400    LR chi2     0.00    R2       0.000    
Events    83    d.f.           0    R2(400)  0.000    
sigma 0.9871                        R2(83)   0.000    
                                    Dxy      0.000    

            Coef    S.E.   Wald Z Pr(>|Z|)
(Intercept)  3.3990 0.1628 20.87  <0.0001 
Log(scale)  -0.0130 0.0945 -0.14  0.8910  

> pphsm(f)          # print proportional hazards form
Warning in pphsm(f) :
  at present, pphsm does not return the correct covariance matrix
Parametric Survival Model Converted to PH Form

       Obs     Events Model L.R.       d.f.          P         R2    R2(400) 
       400         83          0          0         NA          0          0 
   R2(400)     R2(83)     R2(83)        Dxy          g         gr 
         0          0          0          0          0          1 

(Intercept)  Log(scale) 
     -3.443      -0.013 

> g <- survest(f)
> plot(g$time, g$surv, xlab='Time', type='l',
+      ylab=expression(S(t)))
> 
> 
> f <- psm(Surv(d.time,death) ~ age, 
+          dist="loglogistic", y=TRUE)
> r <- resid(f, 'cens') # note abbreviation
> survplot(npsurv(r ~ 1), conf='none') 
>                       # plot Kaplan-Meier estimate of 
>                       # survival function of standardized residuals
> survplot(npsurv(r ~ cut2(age, g=2)), conf='none')  
>                       # both strata should be n(0,1)
> lines(r)              # add theoretical survival function
> #More simply:
> survplot(r, age, g=2)
> 
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("recode2integer")
> ### * recode2integer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: recode2integer
> ### Title: recode2integer
> ### Aliases: recode2integer
> 
> ### ** Examples
> 
> w <- function(y, precision=7) {
+   v <- recode2integer(y, precision);
+   print(v)
+   print(table(y, ynew=v$y))
+ }
> set.seed(1)
> w(sample(1:3, 20, TRUE))
$y
 [1] 1 3 1 2 1 3 3 2 2 3 3 1 1 1 2 2 2 2 3 1

$ylevels
[1] 1 2 3

$freq
1 2 3 
7 7 6 

$median
[1] 2

$whichmedian
[1] 2

   ynew
y   1 2 3
  1 7 0 0
  2 0 7 0
  3 0 0 6
> w(sample(letters[1:3], 20, TRUE))
$y
 [1] 3 1 1 1 1 2 1 1 2 2 2 1 3 1 3 2 2 2 2 3

$ylevels
[1] "a" "b" "c"

$freq
a b c 
8 8 4 

$median
[1] 2

$whichmedian
[1] 2

   ynew
y   1 2 3
  a 8 0 0
  b 0 8 0
  c 0 0 4
> y <- runif(20)
> w(y)
$y
 [1] 14 13 18  1 10 17 16 11 19  8  4  2  3  6 12 15  7 20  5  9

$ylevels
 [1] 0.023331 0.070679 0.099466 0.244797 0.293603 0.316272 0.406830 0.438097
 [9] 0.459066 0.477230 0.477620 0.518634 0.529720 0.553036 0.662005 0.692732
[17] 0.732314 0.789356 0.861209 0.912876

$freq
0.0233312  0.070679 0.0994662 0.2447973 0.2936034 0.3162717 0.4068302 0.4380971 
        1         1         1         1         1         1         1         1 
0.4590657 0.4772301 0.4776196 0.5186343 0.5297196 0.5530363 0.6620051 0.6927316 
        1         1         1         1         1         1         1         1 
0.7323137 0.7893562 0.8612095 0.9128759 
        1         1         1         1 

$median
[1] 0.47742

$whichmedian
[1] 10

                    ynew
y                    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
  0.023331202333793  1 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.0706790471449494 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.0994661601725966 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.244797277031466  0 0 0 1 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.293603372760117  0 0 0 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.31627170718275   0 0 0 0 0 1 0 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.406830187188461  0 0 0 0 0 0 1 0 0  0  0  0  0  0  0  0  0  0  0  0
  0.438097107224166  0 0 0 0 0 0 0 1 0  0  0  0  0  0  0  0  0  0  0  0
  0.459065726259723  0 0 0 0 0 0 0 0 1  0  0  0  0  0  0  0  0  0  0  0
  0.477230065036565  0 0 0 0 0 0 0 0 0  1  0  0  0  0  0  0  0  0  0  0
  0.477619622135535  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0
  0.518634263193235  0 0 0 0 0 0 0 0 0  0  0  1  0  0  0  0  0  0  0  0
  0.529719580197707  0 0 0 0 0 0 0 0 0  0  0  0  1  0  0  0  0  0  0  0
  0.553036311641335  0 0 0 0 0 0 0 0 0  0  0  0  0  1  0  0  0  0  0  0
  0.662005076417699  0 0 0 0 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0  0
  0.692731556482613  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0
  0.7323137386702    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  1  0  0  0
  0.789356231689453  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  1  0  0
  0.8612094768323    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0
  0.912875924259424  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  1
> w(y, precision=2)
$y
 [1] 13 12 17  1 10 16 15 10 18  8  4  2  3  6 11 14  7 19  5  9

$ylevels
 [1] 0.02 0.07 0.10 0.24 0.29 0.32 0.41 0.44 0.46 0.48 0.52 0.53 0.55 0.66 0.69
[16] 0.73 0.79 0.86 0.91

$freq
0.02 0.07  0.1 0.24 0.29 0.32 0.41 0.44 0.46 0.48 0.52 0.53 0.55 0.66 0.69 0.73 
   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 
0.79 0.86 0.91 
   1    1    1 

$median
[1] 0.47742

$whichmedian
[1] 10

                    ynew
y                    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
  0.023331202333793  1 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.0706790471449494 0 1 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.0994661601725966 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.244797277031466  0 0 0 1 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.293603372760117  0 0 0 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.31627170718275   0 0 0 0 0 1 0 0 0  0  0  0  0  0  0  0  0  0  0
  0.406830187188461  0 0 0 0 0 0 1 0 0  0  0  0  0  0  0  0  0  0  0
  0.438097107224166  0 0 0 0 0 0 0 1 0  0  0  0  0  0  0  0  0  0  0
  0.459065726259723  0 0 0 0 0 0 0 0 1  0  0  0  0  0  0  0  0  0  0
  0.477230065036565  0 0 0 0 0 0 0 0 0  1  0  0  0  0  0  0  0  0  0
  0.477619622135535  0 0 0 0 0 0 0 0 0  1  0  0  0  0  0  0  0  0  0
  0.518634263193235  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0
  0.529719580197707  0 0 0 0 0 0 0 0 0  0  0  1  0  0  0  0  0  0  0
  0.553036311641335  0 0 0 0 0 0 0 0 0  0  0  0  1  0  0  0  0  0  0
  0.662005076417699  0 0 0 0 0 0 0 0 0  0  0  0  0  1  0  0  0  0  0
  0.692731556482613  0 0 0 0 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0
  0.7323137386702    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0
  0.789356231689453  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  1  0  0
  0.8612094768323    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  1  0
  0.912875924259424  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  1
> 
> 
> 
> cleanEx()
> nameEx("residuals.cph")
> ### * residuals.cph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residuals.cph
> ### Title: Residuals for a cph Fit
> ### Aliases: residuals.cph
> ### Keywords: survival
> 
> ### ** Examples
> 
> # fit <- cph(Surv(start, stop, event) ~ (age + surgery)* transplant, 
> #            data=jasa1)
> # mresid <- resid(fit, collapse=jasa1$id)
> 
> 
> # Get unadjusted relationships for several variables
> # Pick one variable that's not missing too much, for fit
> 
> require(survival)
Loading required package: survival
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> cens   <- 15*runif(n)
> h      <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> d.time <- -log(runif(n))/h
> death  <- ifelse(d.time <= cens,1,0)
> d.time <- pmin(d.time, cens)
> 
> 
> f <- cph(Surv(d.time, death) ~ age + blood.pressure + cholesterol, iter.max=0)
> res <- resid(f) # This re-inserts rows for NAs, unlike f$resid
> yl <- quantile(res, c(10/length(res),1-10/length(res)), na.rm=TRUE)
> # Scale all plots from 10th smallest to 10th largest residual
> par(mfrow=c(2,2), oma=c(3,0,3,0))
> p <- function(x) {
+   s <- !is.na(x+res)
+   plot(lowess(x[s], res[s], iter=0), xlab=label(x), ylab="Residual",
+        ylim=yl, type="l")
+ }
> p(age); p(blood.pressure); p(cholesterol)
> mtext("Smoothed Martingale Residuals", outer=TRUE)
> 
> 
> # Assess PH by estimating log relative hazard over time
> f <- cph(Surv(d.time,death) ~ age + sex + blood.pressure, x=TRUE, y=TRUE)
> r <- resid(f, "scaledsch")
> tt <- as.numeric(dimnames(r)[[1]])
> par(mfrow=c(3,2))
> for(i in 1:3) {
+   g <- areg.boot(I(r[,i]) ~ tt, B=20)
+   plot(g, boot=FALSE)  # shows bootstrap CIs
+ }                  # Focus on 3 graphs on right
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
> # Easier approach:
> plot(cox.zph(f))    # invokes plot.cox.zph
> par(mfrow=c(1,1))
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:survival’

> nameEx("residuals.lrm")
> ### * residuals.lrm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residuals.lrm
> ### Title: Residuals from an 'lrm' or 'orm' Fit
> ### Aliases: residuals.lrm residuals.orm plot.lrm.partial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- runif(200, -1, 1)
> x2 <- runif(200, -1, 1)
> L  <- x1^2 - .5 + x2
> y  <- ifelse(runif(200) <= plogis(L), 1, 0)
> f <- lrm(y ~ x1 + x2, x=TRUE, y=TRUE)
> resid(f)            #add rows for NAs back to data
  [1] -0.27089  0.74578 -0.39686 -0.29167 -0.23480 -0.40992  0.56567 -0.22803
  [9]  0.72253 -0.48120 -0.61272 -0.20496 -0.52921 -0.61258 -0.56126 -0.29560
 [17]  0.52923  0.36266 -0.61530 -0.31901  0.71029 -0.22920 -0.30676  0.62293
 [25]  0.40382 -0.38703  0.74078 -0.19210 -0.35885 -0.56314 -0.31239  0.77279
 [33] -0.32522  0.55915  0.65595 -0.48987 -0.49463  0.60125 -0.35932 -0.35933
 [41]  0.69467  0.56898  0.39149 -0.23007  0.65454 -0.26354  0.66572 -0.22435
 [49]  0.62550  0.37859 -0.52045  0.37740 -0.36903  0.57129  0.63723 -0.20598
 [57] -0.26429 -0.38571 -0.33023  0.39322  0.58678 -0.29288  0.71871  0.47177
 [65]  0.50408 -0.23030 -0.20042  0.47218  0.56873 -0.24951 -0.19623 -0.22548
 [73] -0.32355 -0.23400  0.70929  0.74097 -0.28519 -0.24011 -0.38467  0.45630
 [81] -0.18731  0.59261 -0.57895 -0.31886 -0.20179 -0.21914  0.69157  0.77716
 [89] -0.21798  0.75141  0.74660 -0.21280  0.36151  0.68357  0.60043  0.50943
 [97] -0.21176 -0.21756 -0.20399  0.38862 -0.48098 -0.20763 -0.37370 -0.38474
[105] -0.33034 -0.62767 -0.23098 -0.54737 -0.21313 -0.34096 -0.24135 -0.25472
[113]  0.44278 -0.49658 -0.26719  0.63443 -0.21287 -0.30284  0.37462 -0.45479
[121] -0.48928 -0.29737 -0.33945  0.37132  0.42055 -0.61792 -0.54805 -0.52058
[129] -0.26992 -0.52512  0.36162  0.72402  0.67891  0.44698  0.78366  0.67620
[137] -0.35978 -0.23461  0.55407 -0.62808  0.35622 -0.24430 -0.39719  0.67827
[145]  0.51484  0.72258 -0.35888 -0.24666 -0.30974 -0.51626 -0.38309 -0.20216
[153]  0.47348 -0.34433 -0.63313 -0.27470  0.43280  0.80360 -0.57747 -0.36153
[161] -0.21082  0.68008  0.44140 -0.29099 -0.44443 -0.55698  0.80752 -0.48385
[169]  0.50316 -0.36214  0.64048  0.56995  0.37978  0.74047 -0.25573 -0.36091
[177] -0.31162  0.41609 -0.24469 -0.39733  0.65585 -0.40660 -0.48075  0.36797
[185] -0.27499 -0.25768  0.45050  0.43374 -0.22998 -0.62616 -0.23625 -0.22596
[193] -0.59059 -0.40515 -0.22722 -0.31651  0.54731 -0.30836 -0.30889 -0.23844
> resid(f, "score")   #also adds back rows
                      x1         x2
1   -0.27089  0.12704416  0.1259608
2    0.74578 -0.19073432 -0.4196562
3   -0.39686 -0.05782553 -0.0133321
4   -0.29167 -0.23812395  0.1347804
5   -0.23480  0.14008732  0.1497203
6   -0.40992 -0.32661939 -0.0152296
7    0.56567  0.50307771  0.0710287
8   -0.22803 -0.07333487  0.1691300
9    0.72253  0.18657807 -0.3520644
10  -0.48120  0.42173382 -0.2097394
11  -0.61272  0.36030834 -0.5654268
12  -0.20496  0.13258434  0.1639084
13  -0.52921 -0.19794931 -0.2786010
14  -0.61258  0.14199039 -0.5488262
15  -0.56126 -0.30290185 -0.3576732
16  -0.29560  0.00136020  0.1133367
17   0.52923  0.23034196  0.1583249
18   0.36266  0.35678993  0.3288283
19  -0.61530  0.14762808 -0.5583610
20  -0.31901 -0.17701423  0.1020957
21   0.71029  0.61753163 -0.3374235
22  -0.22920  0.13195467  0.1533568
23  -0.30676 -0.09305630  0.1091051
24   0.62293 -0.46650835  0.0126147
25   0.40382 -0.18800140  0.3424130
26  -0.38703  0.08815462 -0.0084835
27   0.74078 -0.72093880 -0.3590973
28  -0.19210  0.04518613  0.1742481
29  -0.35885 -0.26532459  0.0589540
30  -0.56314  0.17981135 -0.3987040
31  -0.31239  0.01119610  0.0954482
32   0.77279  0.15388745 -0.5696372
33  -0.32522  0.00420094  0.0816378
34   0.55915 -0.35090458  0.1469680
35   0.65595  0.42948313 -0.1442061
36  -0.48987 -0.16505198 -0.1857842
37  -0.49463 -0.29108142 -0.1873802
38   0.60125 -0.47144577  0.0660177
39  -0.35932 -0.16076845  0.0505750
40  -0.35933  0.06376309  0.0339780
41   0.69467  0.44590458 -0.2689168
42   0.56898  0.16734828  0.0891635
43   0.39149  0.22152872  0.3213089
44  -0.23007 -0.02440412  0.1644521
45   0.65454  0.03890531 -0.1112095
46  -0.26354 -0.15251216  0.1523635
47   0.66572 -0.63465347 -0.0948642
48  -0.22435  0.01021668  0.1648090
49   0.62550  0.29062268 -0.0499190
50   0.37859  0.14593238  0.3353980
51  -0.52045  0.02329574 -0.2726886
52   0.37740  0.27264368  0.3267637
53  -0.36903  0.04568795  0.0216410
54   0.57129 -0.29158912  0.1183575
55   0.63723 -0.54715074 -0.0191300
56  -0.20598  0.16500104  0.1611533
57  -0.26429  0.09711564  0.1333473
58  -0.38571 -0.01437477  0.0011459
59  -0.33023 -0.10699733  0.0839660
60   0.39322 -0.07327305  0.3418614
61   0.58678  0.48453284  0.0281490
62  -0.29288  0.12089915  0.1071096
63   0.71871 -0.05883974 -0.3191561
64   0.47177 -0.15814382  0.2713085
65   0.50408  0.15210059  0.2041133
66  -0.23030  0.11145903  0.1542904
67  -0.20042  0.00859980  0.1745804
68   0.47218  0.25149354  0.2405342
69   0.56873 -0.47290640  0.1369627
70  -0.24951 -0.18729599  0.1648906
71  -0.19623  0.06315588  0.1718092
72  -0.22548 -0.15307511  0.1763134
73  -0.32355  0.09921075  0.0765411
74  -0.23400  0.07779470  0.1547652
75   0.70929 -0.03354747 -0.2856260
76   0.74097  0.58121602 -0.4561283
77  -0.28519 -0.20781334  0.1385063
78  -0.24011  0.05282982  0.1530806
79  -0.38467 -0.21335333  0.0174534
80   0.45630  0.42036056  0.2470750
81  -0.18731  0.02447841  0.1769043
82   0.59261  0.25187818  0.0323695
83  -0.57895  0.11579684 -0.4403727
84  -0.31886  0.11137729  0.0809507
85  -0.20179 -0.10375776  0.1824391
86  -0.21914  0.13030643  0.1583849
87   0.69157  0.29201114 -0.2469022
88   0.77716 -0.58801138 -0.5365017
89  -0.21798  0.11095780  0.1603352
90   0.75141 -0.53605009 -0.4188276
91   0.74660 -0.38878279 -0.4085655
92  -0.21280  0.18771693  0.1568686
93   0.36151  0.10287635  0.3481770
94   0.68357  0.51441453 -0.2364973
95   0.60043  0.33494024  0.0083334
96   0.50943  0.30291867  0.1848661
97  -0.21176  0.01894195  0.1697580
98  -0.21756  0.03912511  0.1658269
99  -0.20399 -0.12682775  0.1834100
100  0.38862  0.08155850  0.3336339
101 -0.48098 -0.14883785 -0.1671038
102 -0.20763  0.06096135  0.1682395
103 -0.37370  0.17170658  0.0055336
104 -0.38474 -0.37911395  0.0295854
105 -0.33034 -0.08819670  0.0824423
106 -0.62767  0.36002386 -0.6165009
107 -0.23098  0.17121607  0.1495138
108 -0.54737  0.02395515 -0.3431313
109 -0.21313 -0.18076907  0.1839571
110 -0.34096 -0.06734742  0.0678857
111 -0.24135 -0.22984752  0.1732196
112 -0.25472 -0.11808238  0.1562376
113  0.44278 -0.12687795  0.3022899
114 -0.49658  0.06805698 -0.2184078
115 -0.26719  0.18798840  0.1243970
116  0.63443 -0.61783385 -0.0063422
117 -0.21287 -0.09177669  0.1774882
118 -0.30284  0.24034569  0.0885002
119  0.37462 -0.04024610  0.3515516
120 -0.45479 -0.12743205 -0.1134366
121 -0.48928 -0.48129179 -0.1610882
122 -0.29737  0.00262067  0.1115196
123 -0.33945  0.01062499  0.0640266
124  0.37132 -0.24251755  0.3684111
125  0.42055  0.21432839  0.2986577
126 -0.61792  0.05697749 -0.5605102
127 -0.54805 -0.01224331 -0.3423206
128 -0.52058  0.30449432 -0.2937986
129 -0.26992  0.14647997  0.1253076
130 -0.52512 -0.10052000 -0.2753205
131  0.36162  0.05415053  0.3517197
132  0.72402 -0.61243166 -0.2988694
133  0.67891 -0.63065072 -0.1366627
134  0.44698  0.12765316  0.2790325
135  0.78366  0.67177984 -0.6627413
136  0.67620  0.13265936 -0.1843353
137 -0.35978 -0.04382129  0.0413077
138 -0.23461 -0.01221273  0.1610767
139  0.55407  0.53754976  0.0910943
140 -0.62808 -0.00959928 -0.5905946
141  0.35622  0.13022674  0.3487425
142 -0.24430 -0.04961249  0.1580838
143 -0.39719  0.20743926 -0.0334678
144  0.67827 -0.32805577 -0.1569455
145  0.51484  0.23611701  0.1813937
146  0.72258 -0.06854288 -0.3334081
147 -0.35888  0.23318387  0.0220707
148 -0.24666 -0.12170156  0.1619081
149 -0.30974  0.24469874  0.0810333
150 -0.51626 -0.37640206 -0.2327368
151 -0.38309 -0.08783794  0.0106122
152 -0.20216 -0.02311052  0.1763617
153  0.47348 -0.16214194  0.2694554
154 -0.34433  0.03227630  0.0562483
155 -0.63313 -0.00055838 -0.6090927
156 -0.27470  0.17532919  0.1192816
157  0.43280  0.02564823  0.3011265
158  0.80360 -0.68261465 -0.6714230
159 -0.57747  0.25667844 -0.4463365
160 -0.36153  0.20773340  0.0202956
161 -0.21082  0.09074300  0.1648224
162  0.68008  0.53739349 -0.2267696
163  0.44140 -0.04746299  0.2978684
164 -0.29099 -0.16294397  0.1298676
165 -0.44443 -0.33832030 -0.0773628
166 -0.55698  0.09677645 -0.3751074
167  0.80752 -0.70447081 -0.6926074
168 -0.48385  0.15920012 -0.1962307
169  0.50316  0.22514000  0.2000812
170 -0.36214  0.11761136  0.0261012
171  0.64048  0.16705477 -0.0807885
172  0.56995  0.38826362  0.0708748
173  0.37978  0.27050163  0.3254566
174  0.74047 -0.16088957 -0.3991610
175 -0.25573  0.06112350  0.1422832
176 -0.36091 -0.28544032  0.0575897
177 -0.31162 -0.08994217  0.1037551
178  0.41609  0.20062261  0.3035882
179 -0.24469 -0.05153411  0.1579767
180 -0.39733 -0.32030909  0.0053093
181  0.65585 -0.27056616 -0.0921957
182 -0.40660  0.25106474 -0.0522588
183 -0.48075 -0.37157538 -0.1501512
184  0.36797  0.00245766  0.3521872
185 -0.27499 -0.20737753  0.1473084
186 -0.25768  0.16017964  0.1335774
187  0.45050  0.23255147  0.2674500
188  0.43374  0.19474607  0.2877153
189 -0.22998 -0.20409910  0.1777740
190 -0.62616 -0.05966894 -0.5802165
191 -0.23625 -0.10004807  0.1666384
192 -0.22596  0.05020667  0.1610542
193 -0.59059  0.47144035 -0.5022727
194 -0.40515 -0.34624559 -0.0057010
195 -0.22722  0.09850832  0.1568503
196 -0.31651 -0.05733453  0.0960277
197  0.54731 -0.42650998  0.1749445
198 -0.30836 -0.20999472  0.1160822
199 -0.30889  0.11245885  0.0916954
200 -0.23844 -0.13488895  0.1679388
> r <- resid(f, "partial")  #for checking transformations of X's
> par(mfrow=c(1,2))
> for(i in 1:2) {
+   xx <- if(i==1)x1 else x2
+   plot(xx, r[,i], xlab=c('x1','x2')[i])
+   lines(lowess(xx,r[,i]))
+ }
> resid(f, "partial", pl="loess")  #same as last 3 lines
> resid(f, "partial", pl=TRUE) #plots for all columns of X using supsmu
> resid(f, "gof")           #global test of goodness of fit
Sum of squared errors     Expected value|H0                    SD 
             43.41604              43.32085               0.09529 
                    Z                     P 
              0.99898               0.31780 
> lp1 <- resid(f, "lp1")    #approx. leave-out-1 linear predictors
> -2*sum(y*lp1 + log(1-plogis(lp1)))  #approx leave-out-1 deviance
[1] 255.76
>                                     #formula assumes y is binary
> 
> 
> # Simulate data from a population proportional odds model
> set.seed(1)
> n   <- 400
> age <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> L <- .05*(age-50) + .03*(blood.pressure-120)
> p12 <- plogis(L)    # Pr(Y>=1)
> p2  <- plogis(L-1)  # Pr(Y=2)
> p   <- cbind(1-p12, p12-p2, p2)   # individual class probabilites
> # Cumulative probabilities:
> cp  <- matrix(cumsum(t(p)) - rep(0:(n-1), rep(3,n)), byrow=TRUE, ncol=3)
> # simulate multinomial with varying probs:
> y <- (cp < runif(n)) %*% rep(1,3)
> y <- as.vector(y)
> # Thanks to Dave Krantz for this trick
> f <- lrm(y ~ age + blood.pressure, x=TRUE, y=TRUE)
> par(mfrow=c(2,2))
> resid(f, 'score.binary',   pl=TRUE)              #plot score residuals
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  pseudoinverse used at 0.995
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  neighborhood radius 1.005
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  reciprocal condition number  0
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  There are other near singularities as well. 1.01
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  pseudoinverse used at 0.995
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  neighborhood radius 1.005
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  reciprocal condition number  0
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  There are other near singularities as well. 1.01
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  pseudoinverse used at 0.995
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  neighborhood radius 1.005
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  reciprocal condition number  0
Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,  :
  There are other near singularities as well. 1.01
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  pseudoinverse used at 0.995
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  neighborhood radius 1.005
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  reciprocal condition number  0
Warning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x else if (is.data.frame(newdata)) as.matrix(model.frame(delete.response(terms(object)),  :
  There are other near singularities as well. 1.01
> resid(f, 'partial', pl=TRUE)                     #plot partial residuals
> resid(f, 'gof')           #test GOF for each level separately
     Sum of squared errors Expected value|H0      SD       Z         P
y>=1                91.739            91.939 0.13870 -1.4398 0.1499349
y>=2                73.581            74.435 0.30202 -2.8301 0.0046529
> 
> 
> # Show use of Li-Shepherd residuals
> f.wrong <- lrm(y ~ blood.pressure, x=TRUE, y=TRUE)
> par(mfrow=c(2,1))
> # li.shepherd residuals from model without age
> plot(age, resid(f.wrong, type="li.shepherd"),
+      ylab="li.shepherd residual")
> lines(lowess(age, resid(f.wrong, type="li.shepherd")))
> # li.shepherd residuals from model including age
> plot(age, resid(f, type="li.shepherd"),
+      ylab="li.shepherd residual")
> lines(lowess(age, resid(f, type="li.shepherd")))
> 
> 
> # Make a series of binary fits and draw 2 partial residual plots
> #
> f1 <- lrm(y>=1 ~ age + blood.pressure, x=TRUE, y=TRUE)
> f2  <- update(f1, y==2 ~.)
> par(mfrow=c(2,1))
> plot.lrm.partial(f1, f2)
> 
> 
> # Simulate data from both a proportional odds and a non-proportional
> # odds population model.  Check how 3 kinds of residuals detect
> # non-prop. odds
> set.seed(71)
> n <- 400
> x <- rnorm(n)
> 
> par(mfrow=c(2,3))
> for(j in 1:2) {     # 1: prop.odds   2: non-prop. odds
+   if(j==1) 
+     L <- matrix(c(1.4,.4,-.1,-.5,-.9),
+                 nrow=n, ncol=5, byrow=TRUE) + x / 2
+     else {
+ 	  # Slopes and intercepts for cutoffs of 1:5 :
+ 	  slopes <- c(.7,.5,.3,.3,0)
+ 	  ints   <- c(2.5,1.2,0,-1.2,-2.5)
+       L <- matrix(ints,   nrow=n, ncol=5, byrow=TRUE) +
+            matrix(slopes, nrow=n, ncol=5, byrow=TRUE) * x
+     }
+   p <- plogis(L)
+   # Cell probabilities
+   p <- cbind(1-p[,1],p[,1]-p[,2],p[,2]-p[,3],p[,3]-p[,4],p[,4]-p[,5],p[,5])
+   # Cumulative probabilities from left to right
+   cp  <- matrix(cumsum(t(p)) - rep(0:(n-1), rep(6,n)), byrow=TRUE, ncol=6)
+   y   <- (cp < runif(n)) %*% rep(1,6)
+ 
+ 
+   f <- lrm(y ~ x, x=TRUE, y=TRUE)
+   for(cutoff in 1:5) print(lrm(y >= cutoff ~ x)$coef)
+ 
+ 
+   print(resid(f,'gof'))
+   resid(f, 'score', pl=TRUE)
+   # Note that full ordinal model score residuals exhibit a
+   # U-shaped pattern even under prop. odds
+   ti <- if(j==2) 'Non-Proportional Odds\nSlopes=.7 .5 .3 .3 0' else
+     'True Proportional Odds\nOrdinal Model Score Residuals'
+   title(ti)
+   resid(f, 'score.binary', pl=TRUE)
+   if(j==1) ti <- 'True Proportional Odds\nBinary Score Residuals'
+   title(ti)
+   resid(f, 'partial', pl=TRUE)
+   if(j==1) ti <- 'True Proportional Odds\nPartial Residuals'
+   title(ti)
+ }
Intercept         x 
  1.35503   0.50743 
Intercept         x 
  0.36942   0.49149 
Intercept         x 
-0.078609  0.497561 
Intercept         x 
 -0.55546   0.51133 
Intercept         x 
 -0.99113   0.54094 
     Sum of squared errors Expected value|H0       SD        Z          P
y>=1                63.843            63.676 0.231052  0.72198 4.7030e-01
y>=2                91.488            90.862 0.142683  4.38723 1.1480e-05
y>=3                94.598            94.020 0.082224  7.02790 2.0966e-12
y>=4                89.053            88.933 0.153922  0.78060 4.3504e-01
y>=5                77.708            78.238 0.215632 -2.45833 1.3958e-02
Intercept         x 
  2.27221   0.84681 
Intercept         x 
  1.11546   0.56997 
Intercept         x 
  0.05387   0.39202 
Intercept         x 
 -1.16278   0.44113 
Intercept         x 
 -2.30605   0.16273 
     Sum of squared errors Expected value|H0       SD        Z           P
y>=1                37.251            38.922 0.129915 -12.8687  6.7507e-38
y>=2                71.611            73.673 0.194905 -10.5772  3.8014e-26
y>=3                96.582            95.048 0.065741  23.3420 1.6613e-120
y>=4                72.637            71.900 0.192496   3.8262  1.3014e-04
y>=5                33.689            32.535 0.111215  10.3767  3.1663e-25
> par(mfrow=c(1,1))
> 
> # Shepherd-Li residuals from orm.  Thanks: Qi Liu
> 
> set.seed(3)
> n  <- 100
> x1 <- rnorm(n)
> y  <- x1 + rnorm(n)
> g <- orm(y ~ x1, family='probit', x=TRUE, y=TRUE)
> g.resid <- resid(g)
> plot(x1, g.resid, cex=0.4); lines(lowess(x1, g.resid)); abline(h=0, col=2,lty=2)
> 
> set.seed(3)
> n <- 100
> x1 <- rnorm(n)
> y <- x1 + x1^2 +rnorm(n)
> # model misspecification, the square term is left out in the model
> g <- orm(y ~ x1, family='probit', x=TRUE, y=TRUE)
> g.resid <- resid(g)
> plot(x1, g.resid, cex=0.4); lines(lowess(x1, g.resid)); abline(h=0, col=2,lty=2)
> 
> 
> ## Not run: 
> ##D # Get data used in Hosmer et al. paper and reproduce their calculations
> ##D v <- Cs(id, low, age, lwt, race, smoke, ptl, ht, ui, ftv, bwt)
> ##D d <- read.table("http://www.umass.edu/statdata/statdata/data/lowbwt.dat",
> ##D                 skip=6, col.names=v)
> ##D d <- upData(d, race=factor(race,1:3,c('white','black','other')))
> ##D f <- lrm(low ~ age + lwt + race + smoke, data=d, x=TRUE,y=TRUE)
> ##D f
> ##D resid(f, 'gof')
> ##D # Their Table 7 Line 2 found sum of squared errors=36.91, expected
> ##D # value under H0=36.45, variance=.065, P=.071
> ##D # We got 36.90, 36.45, SD=.26055 (var=.068), P=.085
> ##D # Note that two logistic regression coefficients differed a bit
> ##D # from their Table 1
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("residuals.ols")
> ### * residuals.ols
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residuals.ols
> ### Title: Residuals for ols
> ### Aliases: residuals.ols
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> x1[1] <- 100
> y <- x1 + x2 + rnorm(100)
> f <- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
> resid(f, "dfbetas")
     Intercept          x1          x2
1    0.0214792 -1.9890e+00 -1.0515e-04
2    0.1624541 -1.4264e-02  1.2633e-02
3    0.1462485 -3.7509e-02 -1.3812e-01
4   -0.0339635 -2.0799e-03 -7.1306e-03
5   -0.2235785  2.7109e-02  1.4892e-01
6    0.2777151 -1.9924e-02  4.8653e-01
7    0.0678307 -9.2997e-04  5.2181e-02
8    0.0565733  1.3008e-03  5.4613e-02
9   -0.0019196  5.0703e-05 -8.3376e-04
10   0.0613358 -1.7820e-03  1.0336e-01
11  -0.0212894 -2.2512e-05  1.3764e-02
12   0.0356295 -3.6231e-03 -1.6252e-02
13  -0.0367406  2.7298e-03 -5.3060e-02
14  -0.1393879  5.1702e-02  9.1889e-02
15   0.0908260 -9.2193e-04 -1.6392e-02
16   0.1423171 -1.9981e-02 -5.4473e-02
17  -0.0336663  4.4589e-03  1.0264e-02
18  -0.1238286  4.1232e-03  3.1962e-02
19   0.0631806  3.1190e-04  3.4708e-02
20  -0.0076647  4.7331e-04  1.1557e-03
21  -0.1698389  8.6843e-03  8.5555e-02
22   0.0045717  2.4591e-04  6.3459e-03
23  -0.0643622  7.4738e-03  1.2393e-02
24  -0.0363234  1.1402e-02  5.9780e-03
25  -0.1146350  6.1869e-03  7.8968e-03
26   0.1826480 -1.2198e-02  1.3837e-01
27  -0.0348783  4.5052e-03  1.5764e-03
28  -0.1609446  4.0907e-02  2.4965e-03
29   0.0135238 -2.7601e-03 -9.3940e-03
30   0.0212780 -1.9021e-03 -6.5581e-03
31  -0.0973792 -3.0184e-03 -1.0237e-02
32  -0.2824841  4.5205e-02  1.6785e-01
33  -0.0633405  2.2403e-03 -3.6857e-02
34   0.0445482 -9.9541e-03 -7.2920e-02
35  -0.0062671  1.3835e-03 -2.1039e-03
36  -0.0189379  4.9565e-03  3.1311e-02
37   0.0507747 -8.5563e-03 -1.4511e-02
38  -0.1188548  1.7992e-02  6.2819e-02
39   0.0979612 -4.1690e-03 -6.5065e-02
40  -0.0033479  1.2290e-04  7.4749e-05
41   0.0557827 -1.4806e-02 -1.1717e-01
42   0.1107792 -6.1841e-03  1.3381e-01
43   0.0105525 -1.6531e-03 -1.9190e-02
44  -0.0887037  7.5396e-03  4.0623e-02
45   0.1039668 -2.6747e-02 -1.2192e-01
46  -0.1984536  4.6020e-02  1.5271e-01
47  -0.0507382 -2.9532e-03 -1.0528e-01
48  -0.0272119  8.5049e-04 -1.5223e-03
49  -0.0242391  5.1369e-03  3.3196e-02
50   0.0853583 -1.1568e-02 -1.5299e-01
51   0.0132589 -5.2537e-04  6.6247e-03
52   0.0377658 -6.4202e-03  3.5531e-04
53  -0.0106551  1.0300e-03  3.2156e-03
54  -0.0303713  8.7133e-03  2.9261e-02
55   0.0550681 -3.6408e-03 -8.9150e-02
56   0.0985653  2.0332e-03 -1.1280e-01
57  -0.2432451  1.8969e-02 -2.5180e-01
58   0.0505521 -1.2915e-02 -3.1782e-02
59   0.0262210 -3.9109e-03 -3.9039e-02
60  -0.0381933  6.3239e-05 -7.1130e-02
61   0.0908574  1.4537e-02  4.5159e-02
62  -0.0411369  5.3097e-03  8.9885e-03
63  -0.0260654 -7.1553e-04 -2.8943e-02
64   0.0891181 -4.2275e-03  8.2930e-02
65   0.1611301 -3.6378e-02 -1.0098e-01
66   0.0397327  1.9345e-03  8.6544e-02
67  -0.0446331  1.3477e-02  1.0808e-02
68  -0.1203459  7.0020e-03  1.8597e-01
69  -0.0351308  3.6356e-03  4.1454e-03
70  -0.0922236 -1.1373e-02 -2.4633e-02
71  -0.0185521 -1.5107e-03 -4.2265e-02
72   0.0373744 -6.3851e-03  5.1265e-03
73  -0.0842834  1.5384e-03 -4.2901e-02
74   0.2580059 -5.2989e-02 -1.3752e-02
75   0.0117449 -2.9923e-03 -3.7953e-03
76   0.1071639 -8.8209e-03 -1.9946e-04
77  -0.2301992  2.2811e-02 -1.9019e-01
78   0.0904545  2.1199e-03  1.8583e-01
79  -0.1315078  4.4692e-03 -1.4062e-01
80   0.0997969 -8.5477e-03  1.2292e-01
81   0.0299898 -7.6159e-03 -3.9083e-02
82  -0.0388383  2.2083e-03 -3.9745e-02
83   0.1269136  2.9205e-03  3.4386e-02
84  -0.0766894  2.8191e-02  1.1971e-01
85  -0.0573739  9.0558e-04 -3.2923e-02
86  -0.0999405  8.6539e-03  1.3222e-02
87  -0.0644883 -5.6921e-03 -9.7412e-02
88   0.0847220 -1.6343e-02 -6.6687e-02
89   0.0370739 -3.7632e-03 -1.5653e-02
90   0.0886617 -1.3024e-02 -8.5650e-02
91  -0.0410437  7.1553e-03  6.3896e-03
92   0.0360625  1.3550e-03  1.6573e-02
93   0.0168777 -7.0522e-04 -1.2701e-02
94  -0.1415199 -2.0211e-03 -1.2534e-01
95   0.1573141 -4.7429e-03 -2.0366e-01
96   0.0052202 -6.5717e-04 -5.7649e-03
97   0.0871631 -1.1744e-02  1.2543e-01
98   0.0844836 -2.0124e-02 -8.9702e-02
99  -0.0048801  9.7082e-04 -2.1626e-03
100 -0.0336388  6.1219e-03  1.2484e-02
> which.influence(f)
$Intercept
[1] "5"  "6"  "32" "57" "74" "77"

$x1
[1] "1"

$x2
[1] "6"  "57" "95"

> i <- resid(f, 'influence.measures') # dfbeta, dffit, etc.
> 
> 
> 
> cleanEx()
> nameEx("rexVar")
> ### * rexVar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rexVar
> ### Title: rexVar
> ### Aliases: rexVar
> 
> ### ** Examples
> 
> set.seed(1)
> n <- 100
> x1 <- rnorm(n)
> x2 <- rnorm(n)
> x3 <- rnorm(n)
> yo  <- x1 + x2 + rnorm(n) / 2.
> # Minimally group y so that bootstrap samples are very unlikely to miss a
> # value of y
> y <- ordGroupBoot(yo)
Minimum m: 12 
> d <- data.frame(x1, x2, x3, y)
> dd <- datadist(d); options(datadist='dd')
> f  <- ols(y ~ pol(x1, 2) * pol(x2, 2) + x3,
+           data=d, x=TRUE, y=TRUE)
> plot(anova(f), what='proportion R2', pl=FALSE)
> rexVar(f)

Relative Explained Variation

     x1      x2      x3 x1 * x2 
  0.501   0.521   0.000   0.034 
> g <- bootcov(f, B=20, coef.reps=TRUE)
> rexVar(g, data=d)

Relative Explained Variation

          REV Lower Upper
x1      0.501 0.459 0.566
x2      0.521 0.448 0.561
x3      0.000 0.000 0.004
x1 * x2 0.034 0.014 0.061
> f <- orm(y ~ pol(x1,2) * pol(x2, 2) + x3,
+          data=d, x=TRUE, y=TRUE)
> rexVar(f, data=d)

Relative Explained Variation

     x1      x2      x3 x1 * x2 
  0.481   0.535   0.000   0.025 
> g <- bootcov(f, B=20, coef.reps=TRUE)
> rexVar(g, data=d)

Relative Explained Variation

          REV Lower Upper
x1      0.481 0.420 0.575
x2      0.535 0.434 0.612
x3      0.000 0.000 0.007
x1 * x2 0.025 0.007 0.082
> ## Not run: 
> ##D require(rmsb)
> ##D h <- blrm(y ~ pol(x1,2) * pol(x2, 2) + x3, data=d)
> ##D rexVar(h, data=d)
> ## End(Not run)
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("rms")
> ### * rms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rms
> ### Title: rms Methods and Generic Functions
> ### Aliases: rms Design modelData
> ### Keywords: models regression survival math manip methods
> 
> ### ** Examples
> 
> ## Not run: 
> ##D require(rms)
> ##D require(ggplot2)
> ##D require(survival)
> ##D dist <- datadist(data=2)     # can omit if not using summary, (gg)plot, survplot,
> ##D                              # or if specify all variable values to them. Can
> ##D                              # also  defer.  data=2: get distribution summaries
> ##D                              # for all variables in search position 2
> ##D                              # run datadist once, for all candidate variables
> ##D dist <- datadist(age,race,bp,sex,height)   # alternative
> ##D options(datadist="dist")
> ##D f <- cph(Surv(d.time, death) ~ rcs(age,4)*strat(race) +
> ##D          bp*strat(sex)+lsp(height,60),x=TRUE,y=TRUE)
> ##D anova(f)
> ##D anova(f,age,height)          # Joint test of 2 vars
> ##D fastbw(f)
> ##D summary(f, sex="female")     # Adjust sex to "female" when testing
> ##D                              # interacting factor bp
> ##D bplot(Predict(f, age, height))   # 3-D plot
> ##D ggplot(Predict(f, age=10:70, height=60))
> ##D latex(f)                     # LaTeX representation of fit
> ##D 
> ##D 
> ##D f <- lm(y ~ x)               # Can use with any fitting function that
> ##D                              # calls model.frame.default, e.g. lm, glm
> ##D specs.rms(f)                 # Use .rms since class(f)="lm"
> ##D anova(f)                     # Works since Varcov(f) (=Varcov.lm(f)) works
> ##D fastbw(f)
> ##D options(datadist=NULL)
> ##D f <- ols(y ~ x1*x2)          # Saves enough information to do fastbw, anova
> ##D anova(f)                     # Will not do Predict since distributions
> ##D fastbw(f)                    # of predictors not saved
> ##D plot(f, x1=seq(100,300,by=.5), x2=.5) 
> ##D                              # all values defined - don't need datadist
> ##D dist <- datadist(x1,x2)      # Equivalent to datadist(f)
> ##D options(datadist="dist")
> ##D plot(f, x1, x2=.5)        # Now you can do plot, summary
> ##D plot(nomogram(f, interact=list(x2=c(.2,.7))))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rms.trans")
> ### * rms.trans
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rms.trans
> ### Title: rms Special Transformation Functions
> ### Aliases: rms.trans asis pol lsp rcs catg scored strat matrx gTrans %ia%
> ###   makepredictcall.rms
> ### Keywords: models regression math manip methods survival smooth
> 
> ### ** Examples
> 
> ## Not run: 
> ##D options(knots=4, poly.degree=2)
> ##D # To get the old behavior of rcspline.eval knot placement (which didnt' handle
> ##D # clumping at the lowest or highest value of the predictor very well):
> ##D # options(fractied = 1.0)   # see rcspline.eval for details
> ##D country <- factor(country.codes)
> ##D blood.pressure <- cbind(sbp=systolic.bp, dbp=diastolic.bp)
> ##D fit <- lrm(Y ~ sqrt(x1)*rcs(x2) + rcs(x3,c(5,10,15)) + 
> ##D        lsp(x4,c(10,20)) + country + blood.pressure + poly(age,2))
> ##D # sqrt(x1) is an implicit asis variable, but limits of x1, not sqrt(x1)
> ##D #       are used for later plotting and effect estimation
> ##D # x2 fitted with restricted cubic spline with 4 default knots
> ##D # x3 fitted with r.c.s. with 3 specified knots
> ##D # x4 fitted with linear spline with 2 specified knots
> ##D # country is an implied catg variable
> ##D # blood.pressure is an implied matrx variable
> ##D # since poly is not an rms function (pol is), it creates a
> ##D #       matrx type variable with no automatic linearity testing
> ##D #       or plotting
> ##D f1 <- lrm(y ~ rcs(x1) + rcs(x2) + rcs(x1) %ia% rcs(x2))
> ##D # %ia% restricts interactions. Here it removes terms nonlinear in
> ##D # both x1 and x2
> ##D f2 <- lrm(y ~ rcs(x1) + rcs(x2) + x1 %ia% rcs(x2))
> ##D # interaction linear in x1
> ##D f3 <- lrm(y ~ rcs(x1) + rcs(x2) + x1 %ia% x2)
> ##D # simple product interaction (doubly linear)
> ##D # Use x1 %ia% x2 instead of x1:x2 because x1 %ia% x2 triggers
> ##D # anova to pool x1*x2 term into x1 terms to test total effect
> ##D # of x1
> ##D #
> ##D # Examples of gTrans
> ##D #
> ##D # Linear relationship with a discontinuity at zero:
> ##D ldisc <- function(x) {z <- cbind(x == 0, x); attr(z, 'nonlinear') <- 1; z}
> ##D gTrans(x, ldisc)
> ##D # Duplicate pol(x, 2):
> ##D pol2 <- function(x) {z <- cbind(x, x^2); attr(z, 'nonlinear') <- 2; z}
> ##D gTrans(x, pol2)
> ##D # Linear spline with a knot at x=10 with the new slope taking effect
> ##D # until x=20 and the spline turning flat at that point but with a
> ##D # discontinuous vertical shift
> ##D # tex is only needed if you will be using latex(fit)
> ##D dspl <- function(x) {
> ##D   z <- cbind(x, pmax(pmin(x, 20) - 10, 0), x > 20)
> ##D   attr(z, 'nonlinear') <- 2:3
> ##D   attr(z, 'tex') <- function(x) sprintf(c('%s', '(\min(%s, 20) - 10)_{+}',
> ##D                                           '[%s > 20]'), x)
> ##D   z }
> ##D gTrans(x, dspl)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rmsMisc")
> ### * rmsMisc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rmsMisc
> ### Title: Miscellaneous Design Attributes and Utility Functions
> ### Aliases: rmsMisc calibrate.rms DesignAssign vcov.rms vcov.cph vcov.Glm
> ###   vcov.Gls vcov.lrm vcov.ols vcov.orm vcov.psm oos.loglik
> ###   oos.loglik.ols oos.loglik.lrm oos.loglik.cph oos.loglik.psm
> ###   oos.loglik.Glm Getlim Getlimi related.predictors
> ###   interactions.containing combineRelatedPredictors param.order
> ###   Penalty.matrix Penalty.setup logLik.Gls logLik.ols logLik.rms AIC.rms
> ###   nobs.rms lrtest univarLR Newlabels Newlevels Newlabels.rms
> ###   Newlevels.rms rmsArgs print.rms print.lrtest survest.rms prModFit
> ###   prStats reListclean formatNP latex.naprint.delete html.naprint.delete
> ###   removeFormulaTerms
> ### Keywords: models methods
> 
> ### ** Examples
> 
> ## Not run: 
> ##D f <- psm(S ~ x1 + x2 + sex + race, dist='gau')
> ##D g <- psm(S ~ x1 + sex + race, dist='gau', 
> ##D          fixed=list(scale=exp(f$parms)))
> ##D lrtest(f, g)
> ##D 
> ##D 
> ##D g <- Newlabels(f, c(x2='Label for x2'))
> ##D g <- Newlevels(g, list(sex=c('Male','Female'),race=c('B','W')))
> ##D nomogram(g)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("robcov")
> ### * robcov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: robcov
> ### Title: Robust Covariance Matrix Estimates
> ### Aliases: robcov
> ### Keywords: models regression robust
> 
> ### ** Examples
> 
> # In OLS test against more manual approach
> set.seed(1)
> n <- 15
> x1 <- 1:n
> x2 <- sample(1:n)
> y <- round(x1 + x2 + 8*rnorm(n))
> f <- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
> vcov(f)
          Intercept       x1       x2
Intercept   31.3364 -1.61044 -1.61044
x1          -1.6104  0.38873 -0.18742
x2          -1.6104 -0.18742  0.38873
> vcov(robcov(f))
          Intercept        x1        x2
Intercept   39.0364 -1.838303 -2.528271
x1          -1.8383  0.259921  0.006282
x2          -2.5283  0.006282  0.278833
> X <- f$x
> G <- diag(resid(f)^2)
> solve(t(X) %*% X) %*% (t(X) %*% G %*% X) %*% solve(t(X) %*% X)
         x1       x2
x1  0.17407 -0.11502
x2 -0.11502  0.12207
> 
> # Duplicate data and adjust for intra-cluster correlation to see that
> # the cluster sandwich estimator completely ignored the duplicates
> x1 <- c(x1,x1)
> x2 <- c(x2,x2)
> y  <- c(y, y)
> g <- ols(y ~ x1 + x2, x=TRUE, y=TRUE)
> vcov(robcov(g, c(1:n, 1:n)))
          Intercept        x1        x2
Intercept   39.0364 -1.838303 -2.528271
x1          -1.8383  0.259921  0.006282
x2          -2.5283  0.006282  0.278833
> 
> # A dataset contains a variable number of observations per subject,
> # and all observations are laid out in separate rows. The responses
> # represent whether or not a given segment of the coronary arteries
> # is occluded. Segments of arteries may not operate independently
> # in the same patient.  We assume a "working independence model" to
> # get estimates of the coefficients, i.e., that estimates assuming
> # independence are reasonably efficient.  The job is then to get
> # unbiased estimates of variances and covariances of these estimates.
> 
> n.subjects <- 30
> ages <- rnorm(n.subjects, 50, 15)
> sexes  <- factor(sample(c('female','male'), n.subjects, TRUE))
> logit <- (ages-50)/5
> prob <- plogis(logit)  # true prob not related to sex
> id <- sample(1:n.subjects, 300, TRUE) # subjects sampled multiple times
> table(table(id))  # frequencies of number of obs/subject

 5  6  7  8  9 10 11 12 13 14 15 
 1  1  4  2  4  5  3  7  1  1  1 
> age <- ages[id]
> sex <- sexes[id]
> # In truth, observations within subject are independent:
> y   <- ifelse(runif(300) <= prob[id], 1, 0)
> f <- lrm(y ~ lsp(age,50)*sex, x=TRUE, y=TRUE)
> g <- robcov(f, id)
> diag(g$var)/diag(f$var)
<0 x 0 matrix>
> # add ,group=w to re-sample from within each level of w
> anova(g)            # cluster-adjusted Wald statistics
                Wald Statistics          Response: y 

 Factor                                   Chi-Square d.f. P     
 age  (Factor+Higher Order Factors)       77.46      4    <.0001
  All Interactions                         0.68      2    0.7110
  Nonlinear (Factor+Higher Order Factors)  0.50      2    0.7773
 sex  (Factor+Higher Order Factors)        0.79      3    0.8525
  All Interactions                         0.68      2    0.7110
 age * sex  (Factor+Higher Order Factors)  0.68      2    0.7110
  Nonlinear                                0.43      1    0.5135
  Nonlinear Interaction : f(A,B) vs. AB    0.43      1    0.5135
 TOTAL NONLINEAR                           0.50      2    0.7773
 TOTAL NONLINEAR + INTERACTION             0.68      3    0.8773
 TOTAL                                    99.43      5    <.0001
> # fastbw(g)         # cluster-adjusted backward elimination
> plot(Predict(g, age=30:70, sex='female'))  # cluster-adjusted confidence bands
> # or use ggplot(...)
> 
> # Get design effects based on inflation of the variances when compared
> # with bootstrap estimates which ignore clustering
> g2 <- robcov(f)
> diag(g$var)/diag(g2$var)
      Intercept             age            age'        sex=male  age * sex=male 
         1.4778          1.4727          1.3919          1.3227          1.3188 
age' * sex=male 
         1.1594 
> 
> 
> # Get design effects based on pooled tests of factors in model
> anova(g2)[,1] / anova(g)[,1]
      age  (Factor+Higher Order Factors) 
                                 0.99183 
                        All Interactions 
                                 1.33194 
 Nonlinear (Factor+Higher Order Factors) 
                                 1.34489 
      sex  (Factor+Higher Order Factors) 
                                 1.31775 
                        All Interactions 
                                 1.33194 
age * sex  (Factor+Higher Order Factors) 
                                 1.33194 
                               Nonlinear 
                                 1.15937 
   Nonlinear Interaction : f(A,B) vs. AB 
                                 1.15937 
                         TOTAL NONLINEAR 
                                 1.34489 
           TOTAL NONLINEAR + INTERACTION 
                                 1.35555 
                                   TOTAL 
                                 0.77811 
> 
> 
> 
> 
> # A dataset contains one observation per subject, but there may be
> # heteroscedasticity or other model misspecification. Obtain
> # the robust sandwich estimator of the covariance matrix.
> 
> 
> # f <- ols(y ~ pol(age,3), x=TRUE, y=TRUE)
> # f.adj <- robcov(f)
> 
> 
> 
> cleanEx()
> nameEx("sensuc")
> ### * sensuc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sensuc
> ### Title: Sensitivity to Unmeasured Covariables
> ### Aliases: sensuc plot.sensuc
> ### Keywords: regression htest models survival
> 
> ### ** Examples
> 
> set.seed(17)
> x <- sample(0:1, 500,TRUE)
> y <- sample(0:1, 500,TRUE)
> y[1:100] <- x[1:100]  # induce an association between x and y
> x2 <- rnorm(500)
> 
> 
> f <- lrm(y ~ x + x2, x=TRUE, y=TRUE)
> 
> 
> #Note: in absence of U odds ratio for x is exp(2nd coefficient)
> 
> 
> g <- sensuc(f, c(1,3))
Current odds ratio for x:u=1 3 

> 
> 
> # Note: If the generated sample of U was typical, the odds ratio for
> # x dropped had U been known, where U had an odds ratio
> # with x of 3 and an odds ratio with y of 3
> 
> 
> plot(g)
> 
> 
> # Fit a Cox model and check sensitivity to an unmeasured confounder
> 
> # require(survival)
> # f <- cph(Surv(d.time,death) ~ treatment + pol(age,2)*sex, x=TRUE, y=TRUE)
> # sensuc(f, event=function(y) y[,2] & y[,1] < 365.25 )
> # Event = failed, with event time before 1 year
> # Note: Analysis uses f$y which is a 2-column Surv object
> 
> 
> 
> cleanEx()
> nameEx("setPb")
> ### * setPb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: setPb
> ### Title: Progress Bar for Simulations
> ### Aliases: setPb
> ### Keywords: utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D options(showprogress=TRUE)   # same as ='tk'
> ##D pb <- setPb(1000)
> ##D for(i in 1:1000) {
> ##D    pb(i)   # pb(i, every=10) to only show for multiples of 10
> ##D    # your calculations
> ##D   }
> ##D # Force rms functions to do simulations to not report progress
> ##D options(showprogress='none')
> ##D # For functions that do simulations to use the console instead of pop-up
> ##D # Even with tcltk is installed
> ##D options(showprogress='console')
> ##D pb <- setPb(1000, label='Random Sampling')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("specs.rms")
> ### * specs.rms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: specs.rms
> ### Title: rms Specifications for Models
> ### Aliases: specs.rms specs print.specs.rms
> ### Keywords: models regression methods
> 
> ### ** Examples
> 
> set.seed(1)
> blood.pressure <- rnorm(200, 120, 15)
> dd <- datadist(blood.pressure)
> options(datadist='dd')
> L <- .03*(blood.pressure-120)
> sick <- ifelse(runif(200) <= plogis(L), 1, 0)
> f <- lrm(sick ~ rcs(blood.pressure,5))
> specs(f)    # find out where 5 knots are placed
lrm(formula = sick ~ rcs(blood.pressure, 5))

               Assumption Parameters                          d.f.
blood.pressure rcspline    97.926 111.75 119.26 128.42 143.99 4   
> g <- Glm(sick ~ rcs(blood.pressure,5), family=binomial)
> specs(g,long=TRUE)
Glm(formula = sick ~ rcs(blood.pressure, 5), family = binomial)

               Assumption Parameters                          d.f.
blood.pressure rcspline    97.926 111.75 119.26 128.42 143.99 4   

                blood.pressure
Low:effect             110.793
Adjust to              119.259
High:effect            129.195
Low:prediction          97.926
High:prediction        143.994
Low                     86.780
High                   156.024
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("summary.rms")
> ### * summary.rms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.rms
> ### Title: Summary of Effects in Model
> ### Aliases: summary.rms print.summary.rms latex.summary.rms
> ###   html.summary.rms plot.summary.rms
> ### Keywords: models regression htest survival hplot interface
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> set.seed(17) # so can reproduce the results
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> label(age)            <- 'Age'      # label is in Hmisc
> label(cholesterol)    <- 'Total Cholesterol'
> label(blood.pressure) <- 'Systolic Blood Pressure'
> label(sex)            <- 'Sex'
> units(cholesterol)    <- 'mg/dl'   # uses units.default in Hmisc
> units(blood.pressure) <- 'mmHg'
> 
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> 
> ddist <- datadist(age, blood.pressure, cholesterol, sex)
> options(datadist='ddist')
> 
> 
> fit <- lrm(y ~ blood.pressure + sex * (age + rcs(cholesterol,4)))
> 
> 
> s <- summary(fit)                # Estimate effects using default ranges
>                                  # Gets odds ratio for age=3rd quartile
>                                  # compared to 1st quartile
> ## Not run: 
> ##D latex(s)                         # Use LaTeX to print nice version
> ##D latex(s, file="")                # Just write LaTeX code to console
> ##D html(s)                          # html/LaTeX to console for knitr
> ##D # Or:
> ##D options(prType='latex')
> ##D summary(fit)                     # prints with LaTeX, table.env=FALSE
> ##D options(prType='html')
> ##D summary(fit)                     # prints with html
> ## End(Not run)
> summary(fit, sex='male', age=60) # Specify ref. cell and adjustment val
             Effects              Response : y 

 Factor            Low     High    Diff.  Effect    S.E.     Lower 0.95
 blood.pressure    109.580 130.580 20.991 -0.015029 0.091678 -0.19471  
  Odds Ratio       109.580 130.580 20.991  0.985080       NA  0.82307  
 age                43.531  56.677 13.146  0.637940 0.134050  0.37521  
  Odds Ratio        43.531  56.677 13.146  1.892600       NA  1.45530  
 cholesterol       183.730 216.530 32.796  0.107070 0.265280 -0.41287  
  Odds Ratio       183.730 216.530 32.796  1.113000       NA  0.66175  
 sex - female:male   2.000   1.000     NA -0.597890 0.223010 -1.03500  
  Odds Ratio         2.000   1.000     NA  0.549970       NA  0.35523  
 Upper 0.95
  0.16466  
  1.17900  
  0.90067  
  2.46130  
  0.62702  
  1.87200  
 -0.16079  
  0.85147  

Adjusted to: sex=male age=60 cholesterol=200.48  

> summary(fit, age=c(50,70))       # Estimate effect of increasing age from
             Effects              Response : y 

 Factor            Low    High   Diff.  Effect    S.E.     Lower 0.95
 blood.pressure    109.58 130.58 20.991 -0.015029 0.091678 -0.194710 
  Odds Ratio       109.58 130.58 20.991  0.985080       NA  0.823070 
 age                50.00  70.00 20.000  0.627240 0.180170  0.274110 
  Odds Ratio        50.00  70.00 20.000  1.872400       NA  1.315400 
 cholesterol       183.73 216.53 32.796  0.113610 0.236050 -0.349030 
  Odds Ratio       183.73 216.53 32.796  1.120300       NA  0.705370 
 sex - male:female   1.00   2.00     NA  0.430540 0.174000  0.089504 
  Odds Ratio         1.00   2.00     NA  1.538100       NA  1.093600 
 Upper 0.95
 0.16466   
 1.17900   
 0.98037   
 2.66540   
 0.57625   
 1.77940   
 0.77158   
 2.16320   

Adjusted to: sex=female age=50.25 cholesterol=200.48  

>                                  # 50 to 70
> s <- summary(fit, age=c(50,60,70)) 
>                                  # Increase age from 50 to 70, adjust to
>                                  # 60 when estimating effects of other factors
> #Could have omitted datadist if specified 3 values for all non-categorical
> #variables (1 value for categorical ones - adjustment level)
> plot(s, log=TRUE, at=c(.1,.5,1,1.5,2,4,8))
> 
> 
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("survest.cph")
> ### * survest.cph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survest.cph
> ### Title: Cox Survival Estimates
> ### Aliases: survest survest.cph
> ### Keywords: models survival regression
> 
> ### ** Examples
> 
> # Simulate data from a population model in which the log hazard
> # function is linear in age and there is no age x sex interaction
> # Proportional hazards holds for both variables but we
> # unnecessarily stratify on sex to see what happens
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('Male','Female'), n, TRUE))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> dt <- -log(runif(n))/h
> label(dt) <- 'Follow-up Time'
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> dd <- datadist(age, sex)
> options(datadist='dd')
> Srv <- Surv(dt,e)
> 
> 
> f <- cph(Srv ~ age*strat(sex), x=TRUE, y=TRUE) #or surv=T
> survest(f, expand.grid(age=c(20,40,60),sex=c("Male","Female")),
+ 	    times=c(2,4,6), conf.int=.9)
$time
[1] 2 4 6

$surv
        [,1]    [,2]    [,3]
[1,] 0.98764 0.96796 0.95059
[2,] 0.97663 0.93996 0.90815
[3,] 0.95603 0.88895 0.83261
[4,] 0.98511 0.96693 0.95447
[5,] 0.95760 0.90748 0.87411
[6,] 0.88242 0.75553 0.67806

$std.err
          [,1]      [,2]     [,3]
[1,] 0.0051177 0.0117628 0.017732
[2,] 0.0068841 0.0133340 0.018894
[3,] 0.0118832 0.0210360 0.028458
[4,] 0.0042464 0.0086976 0.011824
[5,] 0.0083641 0.0151807 0.019994
[6,] 0.0202133 0.0327569 0.041564

$lower
        [,1]    [,2]    [,3]
[1,] 0.97936 0.94941 0.92326
[2,] 0.96563 0.91957 0.88036
[3,] 0.93752 0.85871 0.79453
[4,] 0.97825 0.95320 0.93609
[5,] 0.94452 0.88510 0.84583
[6,] 0.85357 0.71590 0.63325

$upper
        [,1]    [,2]    [,3]
[1,] 0.99599 0.98687 0.97872
[2,] 0.98775 0.96081 0.93681
[3,] 0.97490 0.92024 0.87251
[4,] 0.99201 0.98087 0.97321
[5,] 0.97087 0.93042 0.90333
[6,] 0.91225 0.79735 0.72604

$strata
[1] 2 2 2 1 1 1
attr(,"levels")
[1] "sex=Female" "sex=Male"  

> f <- update(f, surv=TRUE)
> lp <- c(0, .5, 1)
> f$strata   # check strata names
   [1] sex=Female sex=Male   sex=Male   sex=Male   sex=Male   sex=Male  
   [7] sex=Male   sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
  [13] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
  [19] sex=Male   sex=Female sex=Female sex=Female sex=Female sex=Female
  [25] sex=Female sex=Male   sex=Male   sex=Female sex=Male   sex=Female
  [31] sex=Female sex=Female sex=Male   sex=Female sex=Female sex=Female
  [37] sex=Male   sex=Female sex=Female sex=Male   sex=Male   sex=Female
  [43] sex=Female sex=Female sex=Female sex=Female sex=Male   sex=Male  
  [49] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
  [55] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Female
  [61] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Female
  [67] sex=Female sex=Female sex=Male   sex=Male   sex=Male   sex=Male  
  [73] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Female
  [79] sex=Male   sex=Male   sex=Male   sex=Female sex=Female sex=Male  
  [85] sex=Female sex=Male   sex=Male   sex=Female sex=Male   sex=Male  
  [91] sex=Male   sex=Female sex=Female sex=Male   sex=Male   sex=Female
  [97] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [103] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Female
 [109] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [115] sex=Male   sex=Female sex=Female sex=Male   sex=Male   sex=Female
 [121] sex=Male   sex=Male   sex=Female sex=Female sex=Male   sex=Female
 [127] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Male  
 [133] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [139] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [145] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Male  
 [151] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [157] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [163] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Male  
 [169] sex=Male   sex=Female sex=Female sex=Female sex=Male   sex=Female
 [175] sex=Male   sex=Male   sex=Female sex=Female sex=Female sex=Male  
 [181] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [187] sex=Female sex=Male   sex=Male   sex=Female sex=Male   sex=Female
 [193] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Female
 [199] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Male  
 [205] sex=Male   sex=Female sex=Male   sex=Female sex=Male   sex=Male  
 [211] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Female
 [217] sex=Female sex=Female sex=Male   sex=Female sex=Female sex=Female
 [223] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
 [229] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Male  
 [235] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Male  
 [241] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Male  
 [247] sex=Male   sex=Female sex=Female sex=Male   sex=Female sex=Female
 [253] sex=Female sex=Female sex=Male   sex=Female sex=Female sex=Male  
 [259] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [265] sex=Male   sex=Female sex=Male   sex=Female sex=Female sex=Female
 [271] sex=Male   sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [277] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [283] sex=Male   sex=Female sex=Male   sex=Male   sex=Female sex=Female
 [289] sex=Male   sex=Female sex=Male   sex=Male   sex=Male   sex=Female
 [295] sex=Female sex=Female sex=Male   sex=Female sex=Male   sex=Male  
 [301] sex=Female sex=Female sex=Male   sex=Male   sex=Male   sex=Female
 [307] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Male  
 [313] sex=Male   sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [319] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Male  
 [325] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [331] sex=Male   sex=Male   sex=Female sex=Female sex=Male   sex=Female
 [337] sex=Male   sex=Female sex=Female sex=Male   sex=Male   sex=Male  
 [343] sex=Male   sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [349] sex=Male   sex=Female sex=Female sex=Female sex=Female sex=Female
 [355] sex=Male   sex=Male   sex=Male   sex=Female sex=Female sex=Male  
 [361] sex=Male   sex=Female sex=Female sex=Male   sex=Female sex=Male  
 [367] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [373] sex=Male   sex=Male   sex=Female sex=Female sex=Male   sex=Male  
 [379] sex=Female sex=Female sex=Male   sex=Female sex=Male   sex=Female
 [385] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [391] sex=Female sex=Male   sex=Female sex=Female sex=Male   sex=Male  
 [397] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [403] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [409] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Male  
 [415] sex=Male   sex=Female sex=Female sex=Male   sex=Female sex=Female
 [421] sex=Female sex=Male   sex=Male   sex=Female sex=Male   sex=Male  
 [427] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [433] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [439] sex=Female sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [445] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [451] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [457] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Female
 [463] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Male  
 [469] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [475] sex=Male   sex=Female sex=Female sex=Female sex=Male   sex=Male  
 [481] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Female
 [487] sex=Female sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [493] sex=Female sex=Female sex=Male   sex=Female sex=Male   sex=Female
 [499] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [505] sex=Male   sex=Male   sex=Female sex=Female sex=Male   sex=Male  
 [511] sex=Male   sex=Female sex=Male   sex=Female sex=Male   sex=Male  
 [517] sex=Male   sex=Female sex=Female sex=Male   sex=Female sex=Female
 [523] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [529] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [535] sex=Male   sex=Male   sex=Female sex=Male   sex=Male   sex=Female
 [541] sex=Female sex=Female sex=Female sex=Female sex=Male   sex=Male  
 [547] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
 [553] sex=Male   sex=Female sex=Male   sex=Female sex=Female sex=Male  
 [559] sex=Male   sex=Female sex=Male   sex=Female sex=Female sex=Female
 [565] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Female
 [571] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [577] sex=Female sex=Male   sex=Male   sex=Male   sex=Male   sex=Male  
 [583] sex=Male   sex=Female sex=Female sex=Female sex=Male   sex=Female
 [589] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Male  
 [595] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Female
 [601] sex=Male   sex=Female sex=Female sex=Female sex=Female sex=Female
 [607] sex=Male   sex=Male   sex=Male   sex=Male   sex=Male   sex=Female
 [613] sex=Male   sex=Male   sex=Male   sex=Male   sex=Male   sex=Female
 [619] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Female
 [625] sex=Male   sex=Female sex=Male   sex=Female sex=Female sex=Female
 [631] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Female
 [637] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Female
 [643] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [649] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [655] sex=Female sex=Male   sex=Male   sex=Male   sex=Male   sex=Male  
 [661] sex=Male   sex=Female sex=Male   sex=Female sex=Male   sex=Female
 [667] sex=Female sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [673] sex=Male   sex=Male   sex=Male   sex=Male   sex=Male   sex=Female
 [679] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Male  
 [685] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Female
 [691] sex=Male   sex=Male   sex=Female sex=Female sex=Female sex=Female
 [697] sex=Male   sex=Male   sex=Female sex=Female sex=Female sex=Male  
 [703] sex=Female sex=Female sex=Female sex=Female sex=Male   sex=Female
 [709] sex=Male   sex=Female sex=Male   sex=Female sex=Male   sex=Male  
 [715] sex=Male   sex=Male   sex=Female sex=Male   sex=Male   sex=Female
 [721] sex=Male   sex=Male   sex=Female sex=Male   sex=Male   sex=Female
 [727] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [733] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [739] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
 [745] sex=Male   sex=Female sex=Male   sex=Female sex=Female sex=Female
 [751] sex=Male   sex=Male   sex=Female sex=Female sex=Female sex=Male  
 [757] sex=Female sex=Female sex=Male   sex=Male   sex=Female sex=Female
 [763] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Female
 [769] sex=Female sex=Female sex=Female sex=Male   sex=Male   sex=Male  
 [775] sex=Male   sex=Female sex=Female sex=Female sex=Female sex=Male  
 [781] sex=Female sex=Male   sex=Female sex=Female sex=Male   sex=Male  
 [787] sex=Male   sex=Male   sex=Male   sex=Female sex=Female sex=Female
 [793] sex=Female sex=Female sex=Male   sex=Female sex=Female sex=Male  
 [799] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Female
 [805] sex=Female sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [811] sex=Male   sex=Female sex=Male   sex=Male   sex=Male   sex=Male  
 [817] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [823] sex=Male   sex=Female sex=Male   sex=Male   sex=Female sex=Female
 [829] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Male  
 [835] sex=Male   sex=Male   sex=Male   sex=Female sex=Male   sex=Male  
 [841] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Female
 [847] sex=Female sex=Female sex=Male   sex=Male   sex=Female sex=Female
 [853] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
 [859] sex=Male   sex=Female sex=Female sex=Female sex=Male   sex=Male  
 [865] sex=Male   sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [871] sex=Male   sex=Male   sex=Male   sex=Female sex=Female sex=Male  
 [877] sex=Female sex=Male   sex=Male   sex=Female sex=Female sex=Male  
 [883] sex=Female sex=Female sex=Male   sex=Male   sex=Female sex=Male  
 [889] sex=Male   sex=Male   sex=Female sex=Female sex=Female sex=Female
 [895] sex=Female sex=Male   sex=Male   sex=Female sex=Male   sex=Female
 [901] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Male  
 [907] sex=Male   sex=Female sex=Female sex=Male   sex=Female sex=Female
 [913] sex=Female sex=Female sex=Female sex=Female sex=Female sex=Male  
 [919] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [925] sex=Male   sex=Male   sex=Female sex=Male   sex=Female sex=Female
 [931] sex=Female sex=Female sex=Male   sex=Female sex=Female sex=Male  
 [937] sex=Male   sex=Male   sex=Male   sex=Male   sex=Female sex=Male  
 [943] sex=Female sex=Female sex=Male   sex=Female sex=Male   sex=Female
 [949] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [955] sex=Male   sex=Male   sex=Male   sex=Male   sex=Male   sex=Male  
 [961] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Female
 [967] sex=Female sex=Male   sex=Male   sex=Male   sex=Male   sex=Female
 [973] sex=Female sex=Female sex=Female sex=Male   sex=Female sex=Female
 [979] sex=Female sex=Male   sex=Female sex=Female sex=Female sex=Female
 [985] sex=Male   sex=Female sex=Male   sex=Male   sex=Male   sex=Female
 [991] sex=Female sex=Male   sex=Female sex=Male   sex=Male   sex=Male  
 [997] sex=Male   sex=Female sex=Female sex=Male  
Levels: sex=Female sex=Male
> attr(lp,'strata') <- rep(1,3)  # or rep('sex=Female',3)
> survest(f, linear.predictors=lp, times=c(2,4,6))
Warning in survest.cph(f, linear.predictors = lp, times = c(2, 4, 6)) :
  S.E. and confidence intervals are approximate except at predictor means.
Use cph(...,x=TRUE,y=TRUE) (and don't use linear.predictors=) for better estimates.
$time
[1] 2 4 6

$surv
           2       4       6
[1,] 0.95809 0.90850 0.87548
[2,] 0.93184 0.85367 0.80311
[3,] 0.89013 0.77040 0.69663

$lower
           2       4       6
[1,] 0.93110 0.85533 0.80609
[2,] 0.90559 0.80371 0.73947
[3,] 0.86506 0.72531 0.64142

$upper
           2       4       6
[1,] 0.98586 0.96498 0.95083
[2,] 0.95885 0.90674 0.87224
[3,] 0.91593 0.81829 0.75659

$std.err
            2       4        6
[1,] 0.014578 0.03077 0.042126
[2,] 0.014578 0.03077 0.042126
[3,] 0.014578 0.03077 0.042126

$requested.strata
[1] 1 1 1

> 
> # Test survest by comparing to survfit.coxph for a more complex model
> f <- cph(Srv ~ pol(age,2)*strat(sex), x=TRUE, y=TRUE)
> survest(f, data.frame(age=median(age), sex=levels(sex)), times=6)
$time
[1] 6

$surv
[1] 0.80230 0.87377

$std.err
[1] 0.026882 0.022648

$lower
[1] 0.76112 0.83583

$upper
[1] 0.84570 0.91343

$strata
[1] 1 2
attr(,"levels")
[1] "sex=Female" "sex=Male"  

> 
> age2 <- age^2
> f2 <- coxph(Srv ~ (age + age2)*strata(sex))
> new <- data.frame(age=median(age), age2=median(age)^2, sex='Male')
> summary(survfit(f2, new), times=6)
Call: survfit(formula = f2, newdata = new)

                1 
        time       n.risk      n.event     survival      std.err lower 95% CI 
      6.0000     250.0000      49.0000       0.8738       0.0198       0.8358 
upper 95% CI 
      0.9134 

> new$sex <- 'Female'
> summary(survfit(f2, new), times=6)
Call: survfit(formula = f2, newdata = new)

                1 
        time       n.risk      n.event     survival      std.err lower 95% CI 
      6.0000     241.0000      96.0000       0.8021       0.0216       0.7609 
upper 95% CI 
      0.8456 

> 
> options(datadist=NULL)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("survest.orm")
> ### * survest.orm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survest.orm
> ### Title: Title survest.orm
> ### Aliases: survest.orm
> 
> ### ** Examples
> 
> # See survest.psm
> 
> 
> 
> cleanEx()
> nameEx("survest.psm")
> ### * survest.psm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survest.psm
> ### Title: Parametric Survival Estimates
> ### Aliases: survest.psm print.survest.psm
> ### Keywords: survival regression models
> 
> ### ** Examples
> 
> # Simulate data from a proportional hazards population model
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50))
> dt <- -log(runif(n))/h
> label(dt) <- 'Follow-up Time'
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> S <- Surv(dt,e)
> 
> f <- psm(S ~ lsp(age,c(40,70)))
> survest(f, data.frame(age=seq(20,80,by=5)), times=2)

N: 1000 	Events: 139	Time:2 Years

   LinearPredictor survival   Lower   Upper      SE
1           6.2065  0.99663 0.97663 0.99952 0.99277
2           5.6785  0.99419 0.97554 0.99863 0.73817
3           5.1506  0.99000 0.97406 0.99617 0.49051
4           4.6226  0.98282 0.97103 0.98984 0.26965
5           4.0947  0.97056 0.95649 0.98013 0.20306
6           3.9544  0.96606 0.95288 0.97559 0.17083
7           3.8141  0.96087 0.94798 0.97062 0.14875
8           3.6739  0.95492 0.94093 0.96566 0.14164
9           3.5336  0.94808 0.93075 0.96117 0.15163
10          3.3934  0.94024 0.91670 0.95728 0.17583
11          3.2531  0.93126 0.89821 0.95385 0.20936
12          2.9313  0.90550 0.86250 0.93555 0.20351
13          2.6095  0.87078 0.78557 0.92373 0.28383
> 
> #Get predicted survival curve for 40 year old
> survest(f, data.frame(age=40))

N: 1000 	Events: 139

         Time survival   Lower   Upper      SE
1    0.000000  1.00000 1.00000 1.00000     Inf
2    0.075369  0.99899 0.99778 0.99954 0.40040
3    0.150739  0.99793 0.99587 0.99896 0.35318
4    0.226108  0.99685 0.99405 0.99834 0.32635
5    0.301477  0.99577 0.99228 0.99768 0.30778
6    0.376847  0.99468 0.99055 0.99700 0.29370
7    0.452216  0.99358 0.98886 0.99630 0.28243
8    0.527585  0.99248 0.98718 0.99559 0.27309
9    0.602955  0.99137 0.98553 0.99486 0.26515
10   0.678324  0.99026 0.98389 0.99412 0.25828
11   0.753693  0.98915 0.98227 0.99337 0.25225
12   0.829063  0.98803 0.98065 0.99261 0.24689
13   0.904432  0.98691 0.97905 0.99184 0.24209
14   0.979802  0.98579 0.97746 0.99106 0.23774
15   1.055171  0.98467 0.97587 0.99028 0.23379
16   1.130540  0.98355 0.97430 0.98949 0.23018
17   1.205910  0.98243 0.97273 0.98870 0.22686
18   1.281279  0.98131 0.97116 0.98790 0.22379
19   1.356648  0.98018 0.96961 0.98710 0.22094
20   1.432018  0.97906 0.96805 0.98630 0.21830
21   1.507387  0.97793 0.96650 0.98549 0.21584
22   1.582756  0.97680 0.96496 0.98467 0.21353
23   1.658126  0.97568 0.96342 0.98386 0.21137
24   1.733495  0.97455 0.96189 0.98304 0.20934
25   1.808864  0.97342 0.96036 0.98222 0.20743
26   1.884234  0.97229 0.95883 0.98140 0.20563
27   1.959603  0.97117 0.95730 0.98057 0.20393
28   2.034972  0.97004 0.95578 0.97975 0.20233
29   2.110342  0.96891 0.95426 0.97892 0.20081
30   2.185711  0.96778 0.95275 0.97809 0.19936
31   2.261080  0.96665 0.95124 0.97726 0.19799
32   2.336450  0.96553 0.94973 0.97642 0.19669
33   2.411819  0.96440 0.94822 0.97559 0.19546
34   2.487189  0.96327 0.94671 0.97475 0.19428
35   2.562558  0.96214 0.94521 0.97392 0.19315
36   2.637927  0.96102 0.94371 0.97308 0.19208
37   2.713297  0.95989 0.94221 0.97224 0.19106
38   2.788666  0.95877 0.94071 0.97140 0.19008
39   2.864035  0.95764 0.93922 0.97057 0.18915
40   2.939405  0.95651 0.93772 0.96973 0.18826
41   3.014774  0.95539 0.93623 0.96889 0.18741
42   3.090143  0.95426 0.93474 0.96805 0.18659
43   3.165513  0.95314 0.93325 0.96720 0.18581
44   3.240882  0.95202 0.93177 0.96636 0.18506
45   3.316251  0.95089 0.93028 0.96552 0.18434
46   3.391621  0.94977 0.92880 0.96468 0.18365
47   3.466990  0.94865 0.92731 0.96384 0.18299
48   3.542359  0.94753 0.92583 0.96300 0.18236
49   3.617729  0.94640 0.92436 0.96216 0.18175
50   3.693098  0.94528 0.92288 0.96132 0.18117
51   3.768467  0.94416 0.92140 0.96047 0.18061
52   3.843837  0.94304 0.91993 0.95963 0.18007
53   3.919206  0.94192 0.91845 0.95879 0.17955
54   3.994576  0.94081 0.91698 0.95795 0.17905
55   4.069945  0.93969 0.91551 0.95711 0.17858
56   4.145314  0.93857 0.91404 0.95627 0.17812
57   4.220684  0.93746 0.91257 0.95543 0.17768
58   4.296053  0.93634 0.91110 0.95459 0.17725
59   4.371422  0.93523 0.90964 0.95375 0.17685
60   4.446792  0.93411 0.90817 0.95291 0.17645
61   4.522161  0.93300 0.90671 0.95208 0.17608
62   4.597530  0.93189 0.90525 0.95124 0.17571
63   4.672900  0.93077 0.90378 0.95040 0.17537
64   4.748269  0.92966 0.90232 0.94956 0.17503
65   4.823638  0.92855 0.90087 0.94873 0.17471
66   4.899008  0.92744 0.89941 0.94789 0.17440
67   4.974377  0.92633 0.89795 0.94706 0.17410
68   5.049746  0.92523 0.89649 0.94622 0.17382
69   5.125116  0.92412 0.89504 0.94539 0.17354
70   5.200485  0.92301 0.89359 0.94455 0.17328
71   5.275854  0.92191 0.89213 0.94372 0.17303
72   5.351224  0.92080 0.89068 0.94289 0.17278
73   5.426593  0.91970 0.88923 0.94206 0.17255
74   5.501963  0.91859 0.88778 0.94123 0.17233
75   5.577332  0.91749 0.88634 0.94040 0.17211
76   5.652701  0.91639 0.88489 0.93957 0.17191
77   5.728071  0.91529 0.88344 0.93874 0.17171
78   5.803440  0.91419 0.88200 0.93791 0.17152
79   5.878809  0.91309 0.88056 0.93708 0.17134
80   5.954179  0.91199 0.87911 0.93625 0.17116
81   6.029548  0.91090 0.87767 0.93543 0.17100
82   6.104917  0.90980 0.87623 0.93460 0.17084
83   6.180287  0.90870 0.87479 0.93378 0.17068
84   6.255656  0.90761 0.87335 0.93296 0.17054
85   6.331025  0.90652 0.87192 0.93213 0.17040
86   6.406395  0.90542 0.87048 0.93131 0.17027
87   6.481764  0.90433 0.86905 0.93049 0.17014
88   6.557133  0.90324 0.86761 0.92967 0.17002
89   6.632503  0.90215 0.86618 0.92885 0.16990
90   6.707872  0.90106 0.86475 0.92803 0.16979
91   6.783241  0.89997 0.86332 0.92721 0.16969
92   6.858611  0.89889 0.86189 0.92640 0.16959
93   6.933980  0.89780 0.86046 0.92558 0.16950
94   7.009350  0.89672 0.85903 0.92477 0.16941
95   7.084719  0.89563 0.85761 0.92395 0.16933
96   7.160088  0.89455 0.85618 0.92314 0.16925
97   7.235458  0.89347 0.85476 0.92232 0.16917
98   7.310827  0.89238 0.85334 0.92151 0.16910
99   7.386196  0.89130 0.85191 0.92070 0.16904
100  7.461566  0.89022 0.85049 0.91989 0.16898
101  7.536935  0.88914 0.84908 0.91908 0.16892
102  7.612304  0.88807 0.84766 0.91827 0.16886
103  7.687674  0.88699 0.84624 0.91747 0.16882
104  7.763043  0.88591 0.84482 0.91666 0.16877
105  7.838412  0.88484 0.84341 0.91585 0.16873
106  7.913782  0.88377 0.84200 0.91505 0.16869
107  7.989151  0.88269 0.84058 0.91425 0.16865
108  8.064520  0.88162 0.83917 0.91344 0.16862
109  8.139890  0.88055 0.83776 0.91264 0.16859
110  8.215259  0.87948 0.83635 0.91184 0.16857
111  8.290628  0.87841 0.83495 0.91104 0.16855
112  8.365998  0.87734 0.83354 0.91024 0.16853
113  8.441367  0.87628 0.83213 0.90944 0.16851
114  8.516737  0.87521 0.83073 0.90864 0.16850
115  8.592106  0.87415 0.82933 0.90785 0.16849
116  8.667475  0.87308 0.82793 0.90705 0.16848
117  8.742845  0.87202 0.82653 0.90626 0.16848
118  8.818214  0.87096 0.82513 0.90546 0.16847
119  8.893583  0.86990 0.82373 0.90467 0.16847
120  8.968953  0.86884 0.82233 0.90388 0.16848
121  9.044322  0.86778 0.82094 0.90309 0.16848
122  9.119691  0.86672 0.81954 0.90230 0.16849
123  9.195061  0.86566 0.81815 0.90151 0.16850
124  9.270430  0.86461 0.81676 0.90072 0.16851
125  9.345799  0.86355 0.81536 0.89993 0.16852
126  9.421169  0.86250 0.81398 0.89915 0.16854
127  9.496538  0.86144 0.81259 0.89836 0.16856
128  9.571907  0.86039 0.81120 0.89758 0.16858
129  9.647277  0.85934 0.80981 0.89679 0.16860
130  9.722646  0.85829 0.80843 0.89601 0.16863
131  9.798015  0.85724 0.80705 0.89523 0.16865
132  9.873385  0.85620 0.80566 0.89445 0.16868
133  9.948754  0.85515 0.80428 0.89367 0.16871
134 10.024124  0.85410 0.80290 0.89289 0.16874
135 10.099493  0.85306 0.80153 0.89211 0.16877
136 10.174862  0.85201 0.80015 0.89133 0.16881
137 10.250232  0.85097 0.79877 0.89056 0.16884
138 10.325601  0.84993 0.79740 0.88978 0.16888
139 10.400970  0.84889 0.79603 0.88901 0.16892
140 10.476340  0.84785 0.79465 0.88823 0.16896
141 10.551709  0.84681 0.79328 0.88746 0.16900
142 10.627078  0.84577 0.79191 0.88669 0.16905
143 10.702448  0.84474 0.79055 0.88592 0.16909
144 10.777817  0.84370 0.78918 0.88515 0.16914
145 10.853186  0.84267 0.78782 0.88438 0.16919
146 10.928556  0.84163 0.78645 0.88361 0.16924
147 11.003925  0.84060 0.78509 0.88284 0.16929
148 11.079294  0.83957 0.78373 0.88208 0.16934
149 11.154664  0.83854 0.78237 0.88131 0.16939
150 11.230033  0.83751 0.78101 0.88055 0.16945
151 11.305402  0.83648 0.77965 0.87978 0.16950
152 11.380772  0.83546 0.77830 0.87902 0.16956
153 11.456141  0.83443 0.77694 0.87826 0.16962
154 11.531511  0.83341 0.77559 0.87750 0.16968
155 11.606880  0.83238 0.77424 0.87674 0.16973
156 11.682249  0.83136 0.77289 0.87598 0.16980
157 11.757619  0.83034 0.77154 0.87522 0.16986
158 11.832988  0.82932 0.77019 0.87447 0.16992
159 11.908357  0.82830 0.76884 0.87371 0.16998
160 11.983727  0.82728 0.76750 0.87295 0.17005
161 12.059096  0.82626 0.76616 0.87220 0.17011
162 12.134465  0.82524 0.76481 0.87145 0.17018
163 12.209835  0.82423 0.76347 0.87069 0.17025
164 12.285204  0.82321 0.76213 0.86994 0.17032
165 12.360573  0.82220 0.76080 0.86919 0.17038
166 12.435943  0.82119 0.75946 0.86844 0.17045
167 12.511312  0.82018 0.75812 0.86769 0.17053
168 12.586681  0.81917 0.75679 0.86695 0.17060
169 12.662051  0.81816 0.75546 0.86620 0.17067
170 12.737420  0.81715 0.75413 0.86545 0.17074
171 12.812789  0.81614 0.75280 0.86471 0.17082
172 12.888159  0.81513 0.75147 0.86396 0.17089
173 12.963528  0.81413 0.75014 0.86322 0.17096
174 13.038898  0.81313 0.74882 0.86248 0.17104
175 13.114267  0.81212 0.74749 0.86173 0.17112
176 13.189636  0.81112 0.74617 0.86099 0.17119
177 13.265006  0.81012 0.74485 0.86025 0.17127
178 13.340375  0.80912 0.74353 0.85951 0.17135
179 13.415744  0.80812 0.74221 0.85878 0.17143
180 13.491114  0.80712 0.74090 0.85804 0.17151
181 13.566483  0.80613 0.73958 0.85730 0.17159
182 13.641852  0.80513 0.73827 0.85657 0.17167
183 13.717222  0.80414 0.73696 0.85583 0.17175
184 13.792591  0.80314 0.73565 0.85510 0.17183
185 13.867960  0.80215 0.73434 0.85437 0.17191
186 13.943330  0.80116 0.73303 0.85363 0.17200
187 14.018699  0.80017 0.73172 0.85290 0.17208
188 14.094068  0.79918 0.73042 0.85217 0.17216
189 14.169438  0.79819 0.72911 0.85144 0.17225
190 14.244807  0.79720 0.72781 0.85071 0.17233
191 14.320176  0.79622 0.72651 0.84999 0.17242
192 14.395546  0.79523 0.72521 0.84926 0.17250
193 14.470915  0.79425 0.72392 0.84853 0.17259
194 14.546285  0.79327 0.72262 0.84781 0.17267
195 14.621654  0.79228 0.72132 0.84708 0.17276
196 14.697023  0.79130 0.72003 0.84636 0.17285
197 14.772393  0.79032 0.71874 0.84564 0.17294
198 14.847762  0.78934 0.71745 0.84492 0.17302
199 14.923131  0.78837 0.71616 0.84419 0.17311
200 14.998501  0.78739 0.71487 0.84347 0.17320
> 
> #Get hazard function for 40 year old
> survest(f, data.frame(age=40), what="hazard")$surv #still called surv
Warning in survest.psm(f, data.frame(age = 40), what = "hazard") :
  conf.int ignored for what="hazard"
  [1] 0.015080 0.013881 0.014192 0.014378 0.014511 0.014616 0.014701 0.014774
  [9] 0.014838 0.014894 0.014944 0.014990 0.015032 0.015070 0.015106 0.015140
 [17] 0.015171 0.015200 0.015228 0.015255 0.015280 0.015304 0.015327 0.015349
 [25] 0.015369 0.015390 0.015409 0.015428 0.015446 0.015463 0.015480 0.015496
 [33] 0.015512 0.015527 0.015542 0.015557 0.015571 0.015584 0.015598 0.015611
 [41] 0.015623 0.015636 0.015648 0.015660 0.015671 0.015682 0.015693 0.015704
 [49] 0.015715 0.015725 0.015735 0.015745 0.015755 0.015765 0.015774 0.015784
 [57] 0.015793 0.015802 0.015810 0.015819 0.015828 0.015836 0.015844 0.015852
 [65] 0.015860 0.015868 0.015876 0.015884 0.015891 0.015899 0.015906 0.015913
 [73] 0.015920 0.015928 0.015934 0.015941 0.015948 0.015955 0.015961 0.015968
 [81] 0.015974 0.015981 0.015987 0.015993 0.015999 0.016005 0.016011 0.016017
 [89] 0.016023 0.016029 0.016035 0.016040 0.016046 0.016052 0.016057 0.016063
 [97] 0.016068 0.016073 0.016079 0.016084 0.016089 0.016094 0.016099 0.016104
[105] 0.016109 0.016114 0.016119 0.016124 0.016129 0.016134 0.016138 0.016143
[113] 0.016148 0.016152 0.016157 0.016161 0.016166 0.016170 0.016175 0.016179
[121] 0.016183 0.016188 0.016192 0.016196 0.016200 0.016205 0.016209 0.016213
[129] 0.016217 0.016221 0.016225 0.016229 0.016233 0.016237 0.016241 0.016245
[137] 0.016248 0.016252 0.016256 0.016260 0.016264 0.016267 0.016271 0.016275
[145] 0.016278 0.016282 0.016285 0.016289 0.016293 0.016296 0.016300 0.016303
[153] 0.016306 0.016310 0.016313 0.016317 0.016320 0.016323 0.016327 0.016330
[161] 0.016333 0.016337 0.016340 0.016343 0.016346 0.016349 0.016353 0.016356
[169] 0.016359 0.016362 0.016365 0.016368 0.016371 0.016374 0.016377 0.016380
[177] 0.016383 0.016386 0.016389 0.016392 0.016395 0.016398 0.016401 0.016404
[185] 0.016407 0.016410 0.016412 0.016415 0.016418 0.016421 0.016424 0.016426
[193] 0.016429 0.016432 0.016435 0.016437 0.016440 0.016443 0.016445 0.016448
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("survplot")
> ### * survplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survplot
> ### Title: Plot Survival Curves and Hazard Functions
> ### Aliases: survplot survplotp survplot.rms survplot.npsurv
> ###   survplotp.npsurv survdiffplot
> ### Keywords: survival hplot nonparametric models
> 
> ### ** Examples
> 
> # Simulate data from a population model in which the log hazard
> # function is linear in age and there is no age x sex interaction
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('male','female'), n, TRUE))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='female'))
> dt <- -log(runif(n))/h
> label(dt) <- 'Follow-up Time'
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> dd <- datadist(age, sex)
> options(datadist='dd')
> S <- Surv(dt,e)
> 
> # When age is in the model by itself and we predict at the mean age,
> # approximate confidence intervals are ok
> 
> f <- cph(S ~ age, surv=TRUE)
> survplot(f, age=mean(age), conf.int=.95)
Warning in survest.cph(fit, newdata = adj, fun = fun, what = what, conf.int = ci,  :
  S.E. and confidence intervals are approximate except at predictor means.
Use cph(...,x=TRUE,y=TRUE) (and don't use linear.predictors=) for better estimates.
> g <- cph(S ~ age, x=TRUE, y=TRUE)
> survplot(g, age=mean(age), conf.int=.95, add=TRUE, col='red', conf='bars')
> 
> # Repeat for an age far from the mean; not ok
> survplot(f, age=75, conf.int=.95)
Warning in survest.cph(fit, newdata = adj, fun = fun, what = what, conf.int = ci,  :
  S.E. and confidence intervals are approximate except at predictor means.
Use cph(...,x=TRUE,y=TRUE) (and don't use linear.predictors=) for better estimates.
> survplot(g, age=75, conf.int=.95, add=TRUE, col='red', conf='bars')
> 
> 
> #Plot stratified survival curves by sex, adj for quadratic age effect
> # with age x sex interaction (2 d.f. interaction)
> 
> f <- cph(S ~ pol(age,2)*strat(sex), x=TRUE, y=TRUE)
> #or f <- psm(S ~ pol(age,2)*sex)
> Predict(f, sex, age=c(30,50,70))
     sex age     yhat    lower    upper
1 female  30 -0.39197 -2.25968  1.47575
2   male  30 -2.01910 -3.80096 -0.23723
3 female  50  1.30732 -0.86654  3.48119
4   male  50 -1.10004 -3.49172  1.29165
5 female  70  2.38662  0.15407  4.61917
6   male  70 -0.55735 -2.86845  1.75374

Response variable (y): log Relative Hazard 

Limits are 0.95 confidence limits
> survplot(f, sex, n.risk=TRUE, levels.only=TRUE)   #Adjust age to median
> survplot(f, sex, logt=TRUE, loglog=TRUE)   #Check for Weibull-ness (linearity)
> survplot(f, sex=c("male","female"), age=50)
>                                         #Would have worked without datadist
>                                         #or with an incomplete datadist
> survplot(f, sex, label.curves=list(keys=c(2,0), point.inc=2))
>                                         #Identify curves with symbols
> 
> 
> survplot(f, sex, label.curves=list(keys=c('m','f')))
>                                         #Identify curves with single letters
> 
> 
> #Plots by quintiles of age, adjusting sex to male
> options(digits=3)
> survplot(f, age=quantile(age,(1:4)/5), sex="male")
> 
> 
> #Plot survival Kaplan-Meier survival estimates for males
> f <- npsurv(S ~ 1, subset=sex=="male")
> survplot(f)
> 
> 
> #Plot survival for both sexes and show exponential hazard estimates
> f <- npsurv(S ~ sex)
> survplot(f, aehaz=TRUE)
> #Check for log-normal and log-logistic fits
> survplot(f, fun=qnorm, ylab="Inverse Normal Transform")
> survplot(f, fun=function(y)log(y/(1-y)), ylab="Logit S(t)")
> 
> #Plot the difference between sexes
> survdiffplot(f)
> 
> #Similar but show half-width of confidence intervals centered
> #at average of two survival estimates
> #See Boers (2004)
> survplot(f, conf='diffbands')
> 
> options(datadist=NULL)
> ## Not run: 
> ##D #
> ##D # Time to progression/death for patients with monoclonal gammopathy
> ##D # Competing risk curves (cumulative incidence)
> ##D # status variable must be a factor with first level denoting right censoring
> ##D m <- upData(mgus1, stop = stop / 365.25, units=c(stop='years'),
> ##D             labels=c(stop='Follow-up Time'), subset=start == 0)
> ##D f <- npsurv(Surv(stop, event) ~ 1, data=m)
> ##D 
> ##D # Use survplot for enhanced displays of cumulative incidence curves for
> ##D # competing risks
> ##D 
> ##D survplot(f, state='pcm', n.risk=TRUE, xlim=c(0, 20), ylim=c(0, .5), col=2)
> ##D survplot(f, state='death', aehaz=TRUE, col=3,
> ##D          label.curves=list(keys='lines'))
> ##D f <- npsurv(Surv(stop, event) ~ sex, data=m)
> ##D survplot(f, state='death', aehaz=TRUE, n.risk=TRUE, conf='diffbands',
> ##D          label.curves=list(keys='lines'))
> ##D 
> ##D # Plot survival curves estimated from an ordinal semiparametric model
> ##D f <- orm(Ocens(y, ifelse(y <= cens, y, Inf)) ~ age)
> ##D survplot(f, age=c(30, 50))
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("survplot.orm")
> ### * survplot.orm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survplot.orm
> ### Title: Title Survival Curve Plotting
> ### Aliases: survplot.orm
> 
> ### ** Examples
> 
> set.seed(1)
> d <- expand.grid(x1=c('a', 'b', 'c'), x2=c('A','B'), x3=1:2, irep=1:20)
> y <- sample(1:10, nrow(d), TRUE)
> dd <- datadist(d); options(datadist='dd')
> f <- orm(y ~ x1 + x2 + x3, data=d)
> 
> survplot(f, x1='a')
> survplot(f, x1='a', conf.int=.95)
> survplot(f, x1=c('a','b'), x2='A')
> survplot(f, x1=c('a', 'b'), x2='A', conf.int=.95)
> survplot(f, x1=c('a','b'), x2='A', facet=TRUE)
> survplot(f, x1=c('a','b'), x2='A', facet=TRUE, conf.int=.95)
> 
> survplot(f, x1=c('a', 'b'), x2=c('A', 'B'))
> survplot(f, x1=c('a', 'b'), x2=c('A', 'B'), conf.int=.95)
> survplot(f, x1=c('a', 'b'), x2=c('A', 'B'), facet=TRUE)
> 
> survplot(f, x1=c('a', 'b'), x2=c('A', 'B'), x3=1:2)
> 
> g <- psm(Surv(y) ~ x1 + x2 + x3, data=d)
> survplot(g, x1=c('a','b'), x2=c('A', 'B'), ggplot=TRUE)  # calls survplot.orm
> # See https://hbiostat.org/rmsc/parsurv#sec-parsurv-assess
> # where nonparametric and parametric estimates are combined into one ggplot
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("val.prob")
> ### * val.prob
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: val.prob
> ### Title: Validate Predicted Probabilities
> ### Aliases: val.prob print.val.prob plot.val.prob
> ### Keywords: models regression htest smooth
> 
> ### ** Examples
> 
> # Fit logistic model on 100 observations simulated from the actual
> # model given by Prob(Y=1 given X1, X2, X3) = 1/(1+exp[-(-1 + 2X1)]),
> # where X1 is a random uniform [0,1] variable.  Hence X2 and X3 are
> # irrelevant.  After fitting a linear additive model in X1, X2,
> # and X3, the coefficients are used to predict Prob(Y=1) on a
> # separate sample of 100 observations.  Note that data splitting is
> # an inefficient validation method unless n > 20,000.
> 
> 
> set.seed(1)
> n <- 200
> x1 <- runif(n)
> x2 <- runif(n)
> x3 <- runif(n)
> logit <- 2*(x1-.5)
> P <- 1/(1+exp(-logit))
> y <- ifelse(runif(n)<=P, 1, 0)
> d <- data.frame(x1,x2,x3,y)
> dd <- datadist(d); options(datadist='dd')
> f <- lrm(y ~ x1 + x2 + x3, subset=1:100)
> pred.logit <- predict(f, d[101:200,])
> phat <- 1/(1+exp(-pred.logit))
> val.prob(phat, y[101:200], m=20, cex=.5)  # subgroups of 20 obs.
      Dxy   C (ROC)        R2         D  D:Chi-sq       D:p         U  U:Chi-sq 
  0.32053   0.66026   0.09391   0.06303   7.30296   0.00688  -0.01913   0.08715 
      U:p         Q     Brier Intercept     Slope      Emax       E90      Eavg 
  0.95736   0.08216   0.23190   0.05229   0.95652   0.09992   0.05324   0.02584 
      S:z       S:p 
  0.14401   0.88549 
> 
> 
> # Validate predictions more stringently by stratifying on whether
> # x1 is above or below the median
> 
> 
> v <- val.prob(phat, y[101:200], group=x1[101:200], g.group=2)
> v
                 n  Pavg  Obs ChiSq ChiSq2  Eavg Eavg/P90 Med OR     C     B
[0.0131,0.526)  50 0.364 0.36 0.004  0.149 0.046    0.170   1.23 0.637 0.220
[0.5260,0.993]  50 0.590 0.62 0.195  1.902 0.058    0.213   1.08 0.514 0.244
Overall        100 0.477 0.49 0.073  0.088 0.026    0.065   1.08 0.660 0.232
               B ChiSq B cal
[0.0131,0.526)   0.073 0.212
[0.5260,0.993]   0.327 0.230
Overall          0.021 0.230

Quantiles of Predicted Probabilities

                0.01 0.025  0.05   0.1  0.25   0.5  0.75   0.9  0.95 0.975
[0.0131,0.526) 0.213 0.222 0.242 0.270 0.301 0.364 0.407 0.471 0.516 0.531
[0.5260,0.993] 0.397 0.411 0.444 0.488 0.535 0.601 0.650 0.668 0.715 0.748
Overall        0.216 0.243 0.271 0.292 0.365 0.479 0.598 0.654 0.668 0.714
                0.99
[0.0131,0.526) 0.545
[0.5260,0.993] 0.764
Overall        0.750
> plot(v)
Group  [0.0131,0.526)  [0.5260,0.993]  Overall 
n  50  50 100 
Pavg 0.364 0.590 0.477 
Obs 0.36 0.62 0.49 
ChiSq 0.0 0.2 0.1 
ChiSq2 0.1 1.9 0.1 
Eavg 0.046 0.058 0.026 
Eavg/P90 0.170 0.213 0.065 
Med OR 1.23 1.08 1.08 
C 0.637 0.514 0.660 
B 0.220 0.244 0.232 
B ChiSq 0.1 0.3 0.0 
B cal 0.212 0.230 0.230 
> plot(v, flag=function(stats) ifelse(
+   stats[,'ChiSq2'] > qchisq(.95,2) |
+   stats[,'B ChiSq'] > qchisq(.95,1), '*', ' ') )
Group  [0.0131,0.526)  [0.5260,0.993]  Overall 
n  50  50 100 
Pavg 0.364 0.590 0.477 
Obs 0.36 0.62 0.49 
ChiSq 0.0 0.2 0.1 
ChiSq2 0.1 1.9 0.1 
Eavg 0.046 0.058 0.026 
Eavg/P90 0.170 0.213 0.065 
Med OR 1.23 1.08 1.08 
C 0.637 0.514 0.660 
B 0.220 0.244 0.232 
B ChiSq 0.1 0.3 0.0 
B cal 0.212 0.230 0.230 
> # Stars rows of statistics in plot corresponding to significant
> # mis-calibration at the 0.05 level instead of the default, 0.01
> 
> 
> plot(val.prob(phat, y[101:200], group=x1[101:200], g.group=2),
+               col=1:3) # 3 colors (1 for overall)
Group  [0.0131,0.526)  [0.5260,0.993]  Overall 
n  50  50 100 
Pavg 0.364 0.590 0.477 
Obs 0.36 0.62 0.49 
ChiSq 0.0 0.2 0.1 
ChiSq2 0.1 1.9 0.1 
Eavg 0.046 0.058 0.026 
Eavg/P90 0.170 0.213 0.065 
Med OR 1.23 1.08 1.08 
C 0.637 0.514 0.660 
B 0.220 0.244 0.232 
B ChiSq 0.1 0.3 0.0 
B cal 0.212 0.230 0.230 
> 
> 
> # Weighted calibration curves
> # plot(val.prob(pred, y, group=age, weights=freqs))
> options(datadist=NULL)
> 
> 
> 
> cleanEx()
> nameEx("val.surv")
> ### * val.surv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: val.surv
> ### Title: Validate Predicted Probabilities Against Observed Survival Times
> ### Aliases: val.surv plot.val.surv plot.val.survh print.val.survh
> ### Keywords: models regression smooth survival
> 
> ### ** Examples
> 
> # Generate failure times from an exponential distribution
> require(survival)
Loading required package: survival
> set.seed(123)              # so can reproduce results
> n <- 1000
> age <- 50 + 12*rnorm(n)
> sex <- factor(sample(c('Male','Female'), n, rep=TRUE, prob=c(.6, .4)))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> t <- -log(runif(n))/h
> units(t) <- 'Year'
> label(t) <- 'Time to Event'
> ev <- ifelse(t <= cens, 1, 0)
> t <- pmin(t, cens)
> S <- Surv(t, ev)
> 
> # First validate true model used to generate data
> 
> # If hare is available, make a smooth calibration plot for 1-year
> # survival probability where we predict 1-year survival using the
> # known true population survival probability
> # In addition, use groupkm to show that grouping predictions into
> # intervals and computing Kaplan-Meier estimates is not as accurate.
> 
> s1 <- exp(-h*1)
> w <- val.surv(est.surv=s1, S=S, u=1,
+               fun=function(p)log(-log(p)))
> plot(w, lim=c(.85,1), scat1d.opts=list(nhistSpike=200, side=1))
> groupkm(s1, S, m=100, u=1, pl=TRUE, add=TRUE)
          x   n events    KM std.err
 [1,] 0.913 100     41 0.949  0.0234
 [2,] 0.946 100     29 0.957  0.0218
 [3,] 0.957 100     24 0.949  0.0236
 [4,] 0.965 100     22 0.990  0.0101
 [5,] 0.971 100     19 0.990  0.0101
 [6,] 0.975 100     14 0.969  0.0180
 [7,] 0.979 100     10 1.000  0.0000
 [8,] 0.982 100     12 0.990  0.0102
 [9,] 0.985 100      9 0.989  0.0110
[10,] 0.990 100      7 0.990  0.0104
> 
> # Now validate the true model using residuals
> 
> w <- val.surv(est.surv=exp(-h*t), S=S)
> plot(w)
> plot(w, group=sex)  # stratify by sex
> 
> 
> # Now fit an exponential model and validate
> # Note this is not really a validation as we're using the
> # training data here
> f <- psm(S ~ age + sex, dist='exponential', y=TRUE)
> w <- val.surv(f)
> plot(w, group=sex)
> 
> 
> # We know the censoring time on every subject, so we can
> # compare the predicted Pr[T <= observed T | T>c, X] to
> # its expectation 0.5 Pr[T <= C | X] where C = censoring time
> # We plot a ratio that should equal one
> w <- val.surv(f, censor=cens)
> plot(w)
     Mean F(T|T<C,X) Expected
[1,]           0.155    0.145
> plot(w, group=age, g=3)   # stratify by tertile of age
            Mean F(T|T<C,X) Expected
[16.3,45.4)           0.106   0.0871
[45.4,55.0)           0.132   0.1179
[55.0,88.9]           0.188   0.1857
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("validate")
> ### * validate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate
> ### Title: Resampling Validation of a Fitted Model's Indexes of Fit
> ### Aliases: validate print.validate latex.validate html.validate
> ### Keywords: models regression methods survival
> 
> ### ** Examples
> 
> # See examples for validate.cph, validate.lrm, validate.ols
> # Example of validating a parametric survival model:
> 
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('Male','Female'), n, TRUE))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> dt <- -log(runif(n))/h
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> S <- Surv(dt,e)
> 
> 
> f <- psm(S ~ age*sex, x=TRUE, y=TRUE)  # Weibull model
> # Validate full model fit
> validate(f, B=10)                # usually B=150
          index.orig training    test optimism index.corrected  n
Dxy           0.3924   0.3875  0.3922  -0.0046          0.3970 10
R2            0.1108   0.1080  0.1086  -0.0006          0.1114 10
Intercept     0.0000   0.0000 -0.0227   0.0227         -0.0227 10
Slope         1.0000   1.0000  1.0029  -0.0029          1.0029 10
D             0.0536   0.0525  0.0525   0.0000          0.0536 10
U            -0.0011  -0.0011 -0.0020   0.0009         -0.0020 10
Q             0.0547   0.0536  0.0545  -0.0008          0.0556 10
g             0.6589   0.6488  0.6440   0.0048          0.6541 10
> 
> 
> # Validate stepwise model with typical (not so good) stopping rule
> # bw=TRUE does not preserve hierarchy of terms at present
> validate(f, B=10, bw=TRUE, rule="p", sls=.1, type="individual")

		Backwards Step-down - Original Model

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 sex     0.77   1    0.3801 0.77     1    0.3801 -1.23

Approximate Estimates after Deleting Factors

                   Coef     S.E. Wald Z         P
(Intercept)     5.63631 0.345476 16.315 0.000e+00
age            -0.04931 0.005581 -8.837 0.000e+00
age * sex=Male  0.01274 0.002580  4.937 7.917e-07

Factors in Final Model

[1] age       age * sex
          index.orig training    test optimism index.corrected  n
Dxy           0.3889   0.4047  0.3878   0.0170          0.3720 10
R2            0.1100   0.1163  0.1078   0.0085          0.1015 10
Intercept     0.0000   0.0000  0.0847  -0.0847          0.0847 10
Slope         1.0000   1.0000  0.9754   0.0246          0.9754 10
D             0.0532   0.0574  0.0520   0.0053          0.0478 10
U            -0.0011  -0.0011 -0.0028   0.0016         -0.0027 10
Q             0.0543   0.0585  0.0548   0.0037          0.0506 10
g             0.6967   0.7121  0.6879   0.0242          0.6725 10

Factors Retained in Backwards Elimination

 age sex age * sex
 *   *            
 *   *            
 *       *        
 *       *        
 *       *        
 *   *   *        
 *       *        
 *       *        
 *   *   *        
 *       *        

Frequencies of Numbers of Factors Retained

2 3 
8 2 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("validate.Rq")
> ### * validate.Rq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate.Rq
> ### Title: Validation of a Quantile Regression Model
> ### Aliases: validate.Rq
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- runif(200)
> x2 <- sample(0:3, 200, TRUE)
> x3 <- rnorm(200)
> distance <- (x1 + x2/3 + rnorm(200))^2
> 
> f <- Rq(sqrt(distance) ~ rcs(x1,4) + scored(x2) + x3, x=TRUE, y=TRUE)
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
> 
> #Validate full model fit (from all observations) but for x1 < .75
> validate(f, B=20, subset=x1 < .75)   # normally B=300
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
          index.orig training  test optimism index.corrected  n
MAD            0.610   0.5773 0.626  -0.0483          0.6588 20
rho            0.169   0.2200 0.144   0.0757          0.0933 20
g              0.226   0.2922 0.171   0.1214          0.1049 20
Intercept      0.188   0.0825 0.445  -0.3621          0.5503 20
Slope          0.819   0.9065 0.550   0.3567          0.4619 20
> 
> #Validate stepwise model with typical (not so good) stopping rule
> validate(f, B=20, bw=TRUE, rule="p", sls=.1, type="individual")

		Backwards Step-down - Original Model

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x2      4.87   3    0.1818 4.87     3    0.1818 -1.13

Approximate Estimates after Deleting Factors

             Coef     S.E.  Wald Z         P
Intercept  0.9019  0.16383  5.5049 3.693e-08
x1        -0.2449  0.99216 -0.2469 8.050e-01
x1'        2.5699  3.28206  0.7830 4.336e-01
x1''      -7.3551 10.99832 -0.6687 5.037e-01
x3        -0.1604  0.07537 -2.1276 3.337e-02

Factors in Final Model

[1] x1 x3
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  3 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  3 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  2 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  3 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  2 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  1 non-positive fis
Warning in quantreg::summary.rq(fit, covariance = TRUE, se = se, hs = hs) :
  1 non-positive fis
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
Warning in rq.fit.br(x, y, tau = tau, ...) : Solution may be nonunique
          index.orig training  test optimism index.corrected  n
MAD            0.640    0.600 0.655  -0.0555           0.696 20
rho            0.230    0.293 0.207   0.0861           0.143 20
g              0.279    0.379 0.247   0.1319           0.147 20
Intercept      0.000    0.000 0.341  -0.3414           0.341 20
Slope          1.000    1.000 0.679   0.3209           0.679 20

Factors Retained in Backwards Elimination

 x1 x2 x3
 *  *    
 *  *  * 
 *  *  * 
       * 
 *       
 *  *    
 *     * 
 *  *  * 
    *    
 *     * 
 *  *  * 
 *  *  * 
 *     * 
 *  *    
 *  *  * 
 *     * 
 *       
 *  *  * 
 *     * 
 *       

Frequencies of Numbers of Factors Retained

1 2 3 
5 8 7 
> 
> 
> 
> cleanEx()
> nameEx("validate.cph")
> ### * validate.cph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate.cph
> ### Title: Validation of a Fitted Cox or Parametric Survival Model's
> ###   Indexes of Fit
> ### Aliases: validate.cph validate.psm dxy.cens
> ### Keywords: models regression survival
> 
> ### ** Examples
> 
> require(survival)
Loading required package: survival
> n <- 1000
> set.seed(731)
> age <- 50 + 12*rnorm(n)
> label(age) <- "Age"
> sex <- factor(sample(c('Male','Female'), n, TRUE))
> cens <- 15*runif(n)
> h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
> dt <- -log(runif(n))/h
> e <- ifelse(dt <= cens,1,0)
> dt <- pmin(dt, cens)
> units(dt) <- "Year"
> S <- Surv(dt,e)
> 
> f <- cph(S ~ age*sex, x=TRUE, y=TRUE)
> # Validate full model fit
> validate(f, B=10)               # normally B=150
      index.orig training   test optimism index.corrected  n
Dxy       0.3924   0.3874 0.3919  -0.0045          0.3969 10
R2        0.0962   0.0938 0.0943  -0.0005          0.0966 10
Slope     1.0000   1.0000 0.9971   0.0029          0.9971 10
D         0.0359   0.0353 0.0351   0.0001          0.0357 10
U        -0.0008  -0.0008 0.0008  -0.0016          0.0008 10
Q         0.0367   0.0361 0.0344   0.0017          0.0349 10
g         0.6794   0.6779 0.6637   0.0142          0.6652 10
> 
> # Validate a model with stratification.  Dxy is the only
> # discrimination measure for such models, by Dxy requires
> # one to choose a single time at which to predict S(t|X)
> f <- cph(S ~ rcs(age)*strat(sex), 
+          x=TRUE, y=TRUE, surv=TRUE, time.inc=2)
number of knots in rcs defaulting to 5
> validate(f, u=2, B=10)   # normally B=150
      index.orig training   test optimism index.corrected  n
Dxy       0.4099   0.4259 0.3959   0.0300          0.3799 10
R2        0.0977   0.1039 0.0904   0.0135          0.0842 10
Slope     1.0000   1.0000 0.8943   0.1057          0.8943 10
D         0.0398   0.0434 0.0366   0.0068          0.0330 10
U        -0.0009  -0.0009 0.0009  -0.0018          0.0009 10
Q         0.0406   0.0443 0.0357   0.0086          0.0320 10
g         1.2531   1.6692 1.4858   0.1834          1.0697 10
> # Note u=time.inc
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("validate.lrm")
> ### * validate.lrm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate.lrm
> ### Title: Resampling Validation of a Logistic or Ordinal Regression Model
> ### Aliases: validate.lrm validate.orm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> n <- 1000    # define sample size
> age            <- rnorm(n, 50, 10)
> blood.pressure <- rnorm(n, 120, 15)
> cholesterol    <- rnorm(n, 200, 25)
> sex            <- factor(sample(c('female','male'), n,TRUE))
> 
> 
> # Specify population model for log odds that Y=1
> L <- .4*(sex=='male') + .045*(age-50) +
+   (log(cholesterol - 10)-5.2)*(-2*(sex=='female') + 2*(sex=='male'))
> # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
> y <- ifelse(runif(n) < plogis(L), 1, 0)
> 
> 
> f <- lrm(y ~ sex*rcs(cholesterol)+pol(age,2)+blood.pressure, x=TRUE, y=TRUE)
number of knots in rcs defaulting to 5
> #Validate full model fit
> validate(f, B=10)              # normally B=300
          index.orig training   test optimism index.corrected  n
Dxy           0.2915    0.306 0.2716   0.0344          0.2571 10
R2            0.0907    0.105 0.0788   0.0262          0.0645 10
Intercept     0.0000    0.000 0.0669  -0.0669          0.0669 10
Slope         1.0000    1.000 0.8603   0.1397          0.8603 10
Emax          0.0000    0.000 0.0450   0.0450          0.0450 10
D             0.0692    0.081 0.0597   0.0212          0.0479 10
U            -0.0020   -0.002 0.0018  -0.0038          0.0018 10
Q             0.0712    0.083 0.0579   0.0251          0.0461 10
B             0.2307    0.229 0.2337  -0.0047          0.2354 10
g             0.6374    0.688 0.5857   0.1027          0.5347 10
gp            0.1473    0.157 0.1359   0.0211          0.1262 10
> validate(f, B=10, group=y)  
          index.orig training    test optimism index.corrected  n
Dxy           0.2915   0.2941  0.2717   0.0224          0.2690 10
R2            0.0907   0.0954  0.0796   0.0158          0.0749 10
Intercept     0.0000   0.0000  0.0080  -0.0080          0.0080 10
Slope         1.0000   1.0000  0.9083   0.0917          0.9083 10
Emax          0.0000   0.0000  0.0228   0.0228          0.0228 10
D             0.0692   0.0731  0.0603   0.0127          0.0564 10
U            -0.0020  -0.0020 -0.0001  -0.0019         -0.0001 10
Q             0.0712   0.0751  0.0605   0.0146          0.0566 10
B             0.2307   0.2299  0.2331  -0.0032          0.2339 10
g             0.6374   0.6546  0.5904   0.0642          0.5732 10
gp            0.1473   0.1491  0.1367   0.0125          0.1349 10
> # two-sample validation: make resamples have same numbers of
> # successes and failures as original sample
> 
> 
> #Validate stepwise model with typical (not so good) stopping rule
> validate(f, B=10, bw=TRUE, rule="p", sls=.1, type="individual")

		Backwards Step-down - Original Model

 Deleted        Chi-Sq d.f. P      Residual d.f. P      AIC  
 blood.pressure 0.22   1    0.6424 0.22     1    0.6424 -1.78

Approximate Estimates after Deleting Factors

                                Coef      S.E.  Wald Z        P
Intercept                  6.960e+00 3.4502264  2.0172 0.043674
sex=male                  -1.182e+01 4.0556874 -2.9140 0.003568
cholesterol               -5.123e-02 0.0191481 -2.6757 0.007457
cholesterol'               1.421e-01 0.0848588  1.6748 0.093973
cholesterol''             -5.694e-01 0.5307843 -1.0728 0.283370
cholesterol'''             4.965e-01 0.8197201  0.6056 0.544748
age                        4.372e-02 0.0466080  0.9381 0.348201
age^2                     -1.011e-04 0.0004664 -0.2168 0.828335
sex=male * cholesterol     7.040e-02 0.0242525  2.9027 0.003699
sex=male * cholesterol'   -1.990e-01 0.1145697 -1.7367 0.082438
sex=male * cholesterol''   8.881e-01 0.7406796  1.1990 0.230538
sex=male * cholesterol''' -8.674e-01 1.1701137 -0.7413 0.458489

Factors in Final Model

[1] sex               cholesterol       age               sex * cholesterol
          index.orig training   test optimism index.corrected  n
Dxy           0.2906   0.3202 0.2699   0.0504          0.2402 10
R2            0.0904   0.1116 0.0773   0.0343          0.0562 10
Intercept     0.0000   0.0000 0.0542  -0.0542          0.0542 10
Slope         1.0000   1.0000 0.8110   0.1890          0.8110 10
Emax          0.0000   0.0000 0.0559   0.0559          0.0559 10
D             0.0690   0.0862 0.0585   0.0277          0.0413 10
U            -0.0020  -0.0020 0.0037  -0.0057          0.0037 10
Q             0.0710   0.0882 0.0548   0.0334          0.0376 10
B             0.2308   0.2270 0.2341  -0.0071          0.2379 10
g             0.6364   0.7181 0.5773   0.1407          0.4956 10
gp            0.1471   0.1610 0.1336   0.0274          0.1198 10

Factors Retained in Backwards Elimination

 sex cholesterol age blood.pressure sex * cholesterol
     *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                
 *   *           *                  *                

Frequencies of Numbers of Factors Retained

3 4 
1 9 
> 
> 
> ## Not run: 
> ##D #Fit a continuation ratio model and validate it for the predicted
> ##D #probability that y=0
> ##D u <- cr.setup(y)
> ##D Y <- u$y
> ##D cohort <- u$cohort
> ##D attach(mydataframe[u$subs,])
> ##D f <- lrm(Y ~ cohort+rcs(age,4)*sex, penalty=list(interaction=2))
> ##D validate(f, cluster=u$subs, subset=cohort=='all') 
> ##D #see predab.resample for cluster and subset
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("validate.ols")
> ### * validate.ols
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate.ols
> ### Title: Validation of an Ordinary Linear Model
> ### Aliases: validate.ols
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- runif(200)
> x2 <- sample(0:3, 200, TRUE)
> x3 <- rnorm(200)
> distance <- (x1 + x2/3 + rnorm(200))^2
> 
> f <- ols(sqrt(distance) ~ rcs(x1,4) + scored(x2) + x3, x=TRUE, y=TRUE)
> 
> #Validate full model fit (from all observations) but for x1 < .75
> validate(f, B=20, subset=x1 < .75)   # normally B=300
          index.orig training  test optimism index.corrected  n
R-square      0.0404   0.0592 0.004   0.0552         -0.0149 20
MSE           0.5647   0.5276 0.586  -0.0586          0.6233 20
g             0.1914   0.2224 0.164   0.0582          0.1332 20
Intercept     0.2749   0.2407 0.468  -0.2272          0.5022 20
Slope         0.7374   0.7612 0.576   0.1857          0.5516 20
> 
> #Validate stepwise model with typical (not so good) stopping rule
> validate(f, B=20, bw=TRUE, rule="p", sls=.1, type="individual")

		Backwards Step-down - Original Model

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC   R2   
 x3      0.43   1    0.5104 0.43     1    0.5104 -1.57 0.102

Approximate Estimates after Deleting Factors

             Coef   S.E.  Wald Z        P
Intercept  0.8158 0.2570  3.1737 0.001505
x1        -0.3109 1.0161 -0.3059 0.759659
x1'        2.7718 2.8858  0.9605 0.336814
x1''      -7.9049 9.4398 -0.8374 0.402366
x2         0.2197 0.1621  1.3549 0.175439
x2=2      -0.1404 0.2926 -0.4799 0.631296
x2=3      -0.2549 0.4349 -0.5860 0.557844

Factors in Final Model

[1] x1 x2
          index.orig training   test optimism index.corrected  n
R-square       0.102    0.125 0.0609   0.0641          0.0381 20
MSE            0.656    0.593 0.6859  -0.0934          0.7491 20
g              0.316    0.320 0.2534   0.0669          0.2489 20
Intercept      0.000    0.000 0.2224  -0.2224          0.2224 20
Slope          1.000    1.000 0.8155   0.1845          0.8155 20

Factors Retained in Backwards Elimination

 x1 x2 x3
 *  *    
 *  *    
 *     * 
 *  *    
         
 *  *    
 *       
 *  *    
 *       
 *  *  * 
 *  *    
 *  *    
 *  *  * 
 *  *    
 *  *    
 *     * 
 *       
 *  *  * 
 *  *    
 *       

Frequencies of Numbers of Factors Retained

 0  1  2  3 
 1  4 12  3 
> 
> 
> 
> cleanEx()
> nameEx("validate.rpart")
> ### * validate.rpart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: validate.rpart
> ### Title: Dxy and Mean Squared Error by Cross-validating a Tree Sequence
> ### Aliases: validate.rpart print.validate.rpart plot.validate.rpart
> ### Keywords: models tree category
> 
> ### ** Examples
> 
> ## Not run: 
> ##D n <- 100
> ##D set.seed(1)
> ##D x1 <- runif(n)
> ##D x2 <- runif(n)
> ##D x3 <- runif(n)
> ##D y  <- 1*(x1+x2+rnorm(n) > 1)
> ##D table(y)
> ##D require(rpart)
> ##D f <- rpart(y ~ x1 + x2 + x3, model=TRUE)
> ##D v <- validate(f)
> ##D v    # note the poor validation
> ##D par(mfrow=c(1,2))
> ##D plot(v, legendloc=c(.2,.5))
> ##D par(mfrow=c(1,1))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vif")
> ### * vif
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vif
> ### Title: Variance Inflation Factors
> ### Aliases: vif
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(100)
> x2 <- x1+.1*rnorm(100)
> y  <- sample(0:1, 100, TRUE)
> f  <- lrm(y ~ x1 + x2)
> vif(f)
  x1   x2 
81.7 81.7 
> 
> 
> 
> cleanEx()
> nameEx("which.influence")
> ### * which.influence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: which.influence
> ### Title: Which Observations are Influential
> ### Aliases: which.influence show.influence
> ### Keywords: models regression survival
> 
> ### ** Examples
> 
> #print observations in data frame that are influential,
> #separately for each factor in the model
> x1 <- 1:20
> x2 <- abs(x1-10)
> x3 <- factor(rep(0:2,length.out=20))
> y  <- c(rep(0:1,8),1,1,1,1)
> f  <- lrm(y ~ rcs(x1,3) + x2 + x3, x=TRUE,y=TRUE)
> w <- which.influence(f, .55)
> nam <- names(w)
> d   <- data.frame(x1,x2,x3,y)
> for(i in 1:length(nam)) {
+  print(paste("Influential observations for effect of ",nam[i]),quote=FALSE)
+  print(d[w[[i]],])
+ }
[1] Influential observations for effect of  Intercept
   x1 x2 x3 y
10 10  0  0 1
[1] Influential observations for effect of  x1
   x1 x2 x3 y
10 10  0  0 1
13 13  3  0 0
[1] Influential observations for effect of  x2
   x1 x2 x3 y
10 10  0  0 1
13 13  3  0 0
[1] Influential observations for effect of  x3
   x1 x2 x3 y
6   6  4  2 1
11 11  1  1 0
> 
> show.influence(w, d)  # better way to show results
   Count  x1 x2 x3
6      1   6  4 *2
10     3 *10 *0  0
11     1  11  1 *1
13     2 *13 *3  0
> 
> 
> 
> cleanEx()
> nameEx("zzzrmsOverview")
> ### * zzzrmsOverview
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rmsOverview
> ### Title: Overview of rms Package
> ### Aliases: rmsOverview rms.Overview
> ### Keywords: models
> 
> ### ** Examples
> 
> ## To run several comprehensive examples, run the following command
> ## Not run: 
> ##D demo(all, 'rms')
> ## End(Not run)
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  25.359 17.886 23.284 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
