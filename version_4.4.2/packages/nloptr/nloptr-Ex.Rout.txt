
R version 4.4.2 (2024-10-31) -- "Pile of Leaves"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "nloptr"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('nloptr')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("auglag")
> ### * auglag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: auglag
> ### Title: Augmented Lagrangian Algorithm
> ### Aliases: auglag
> 
> ### ** Examples
> 
> 
> x0 <- c(1, 1)
> fn <- function(x) (x[1] - 2) ^ 2 + (x[2] - 1) ^ 2
> hin <- function(x) 0.25 * x[1]^2 + x[2] ^ 2 - 1  # hin <= 0
> heq <- function(x) x[1] - 2 * x[2] + 1           # heq = 0
> gr <- function(x) nl.grad(x, fn)
> hinjac <- function(x) nl.jacobian(x, hin)
> heqjac <- function(x) nl.jacobian(x, heq)
> 
> # with COBYLA
> auglag(x0, fn, gr = NULL, hin = hin, heq = heq, deprecatedBehavior = FALSE)
$par
[1] 0.8228755 0.9114379

$value
[1] 1.393465

$iter
[1] 1001

$global_solver
[1] "NLOPT_LN_AUGLAG"

$local_solver
[1] "NLOPT_LN_COBYLA"

$convergence
[1] 5

$message
[1] "NLOPT_MAXEVAL_REACHED: Optimization stopped because maxeval (above) was reached."

> 
> # $par:   0.8228761 0.9114382
> # $value:   1.393464
> # $iter:  1001
> 
> auglag(x0, fn, gr = NULL, hin = hin, heq = heq, localsolver = "SLSQP",
+        deprecatedBehavior = FALSE)
$par
[1] 0.8228757 0.9114378

$value
[1] 1.393465

$iter
[1] 184

$global_solver
[1] "NLOPT_LD_AUGLAG"

$local_solver
[1] "NLOPT_LD_SLSQP"

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> # $par:   0.8228757 0.9114378
> # $value:   1.393465
> # $iter   184
> 
> ##  Example from the alabama::auglag help page
> ##  Parameters should be roughly (0, 0, 1) with an objective value of 1.
> 
> fn <- function(x) (x[1] + 3 * x[2] + x[3]) ^ 2 + 4 * (x[1] - x[2]) ^ 2
> heq <- function(x) x[1] + x[2] + x[3] - 1
> # hin restated from alabama example to be <= 0.
> hin <- function(x) c(-6 * x[2] - 4 * x[3] + x[1] ^ 3 + 3, -x[1], -x[2], -x[3])
> 
> set.seed(12)
> auglag(runif(3), fn, hin = hin, heq = heq, localsolver= "lbfgs",
+        deprecatedBehavior = FALSE)
$par
[1] 4.861756e-08 4.732373e-08 9.999999e-01

$value
[1] 1

$iter
[1] 145

$global_solver
[1] "NLOPT_LD_AUGLAG"

$local_solver
[1] "NLOPT_LD_LBFGS"

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> # $par:   4.861756e-08 4.732373e-08 9.999999e-01
> # $value:   1
> # $iter:  145
> 
> ##  Powell problem from the Rsolnp::solnp help page
> ##  Parameters should be roughly (-1.7171, 1.5957, 1.8272, -0.7636, -0.7636)
> ##  with an objective value of 0.0539498478.
> 
> x0 <- c(-2, 2, 2, -1, -1)
> fn1  <- function(x) exp(x[1] * x[2] * x[3] * x[4] * x[5])
> eqn1 <-function(x)
+ 	c(x[1] * x[1] + x[2] * x[2] + x[3] * x[3] + x[4] * x[4] + x[5] * x[5] - 10,
+ 	  x[2] * x[3] - 5 * x[4] * x[5],
+ 	  x[1] * x[1] * x[1] + x[2] * x[2] * x[2] + 1)
> 
> auglag(x0, fn1, heq = eqn1, localsolver = "mma", deprecatedBehavior = FALSE)
$par
[1] -1.7172821  1.5958701  1.8269884 -0.7636277 -0.7636277

$value
[1] 0.05394986

$iter
[1] 932

$global_solver
[1] "NLOPT_LD_AUGLAG"

$local_solver
[1] "NLOPT_LD_MMA"

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> # $par: -1.7173645  1.5959655  1.8268352 -0.7636185 -0.7636185
> # $value:   0.05394987
> # $iter:  916
> 
> 
> 
> 
> cleanEx()
> nameEx("bobyqa")
> ### * bobyqa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bobyqa
> ### Title: Bound Optimization by Quadratic Approximation
> ### Aliases: bobyqa
> 
> ### ** Examples
> 
> 
> ## Rosenbrock Banana function
> 
> rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}
> 
> ## The function as written above has a minimum of 0 at (1, 1)
> 
> S <- bobyqa(c(0, 0), rbf)
> 
> 
> S
$par
[1] 0.9999999 0.9999998

$value
[1] 9.022277e-15

$iter
[1] 118

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> ## Rosenbrock Banana function with both parameters constrained to [0, 0.5]
> 
> S <- bobyqa(c(0, 0), rbf, lower = c(0, 0), upper = c(0.5, 0.5))
> 
> S
$par
[1] 0.50 0.25

$value
[1] 0.25

$iter
[1] 44

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> 
> 
> 
> cleanEx()
> nameEx("ccsaq")
> ### * ccsaq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ccsaq
> ### Title: Conservative Convex Separable Approximation with Affine
> ###   Approximation plus Quadratic Penalty
> ### Aliases: ccsaq
> 
> ### ** Examples
> 
> 
> ##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
> ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm
> 
> x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1)
> fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +
+                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +
+                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}
> 
> hin.hs100 <- function(x) {c(
+ 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127,
+ 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282,
+ 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196,
+ 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -
+  11 * x[7])
+ }
> 
> gr.hs100 <- function(x) {
+  c( 2 * x[1] - 20,
+    10 * x[2] - 120,
+     4 * x[3] ^ 3,
+     6 * x[4] - 66,
+    60 * x[5] ^ 5,
+    14 * x[6] - 4 * x[7] - 10,
+     4 * x[7] ^ 3 - 4 * x[6] - 8)
+ }
> 
> hinjac.hs100 <- function(x) {
+   matrix(c(4 * x[1], 12 * x[2] ^ 3, 1, 8 * x[4], 5, 0, 0,
+            7, 3, 20 * x[3], 1, -1, 0, 0,
+            23, 2 * x[2], 0, 0, 0, 12 * x[6], -8,
+            8 * x[1] - 3 * x[2], 2 * x[2] - 3 * x[1], 4 * x[3], 0, 0, 5, -11),
+            nrow = 4, byrow = TRUE)
+ }
> 
> ##  The optimum value of the objective function should be 680.6300573
> ##  A suitable parameter vector is roughly
> ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)
> 
> # Results with exact Jacobian
> S <- ccsaq(x0.hs100, fn.hs100, gr = gr.hs100,
+       hin = hin.hs100, hinjac = hinjac.hs100,
+       nl.info = TRUE, control = list(xtol_rel = 1e-8),
+       deprecatedBehavior = FALSE)

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 173 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630049959532 
Optimal value of controls: 2.330501 1.951372 -0.4775423 4.365727 -0.6244867 1.038133 1.59423


> 
> # Results without Jacobian
> S <- ccsaq(x0.hs100, fn.hs100, hin = hin.hs100,
+       nl.info = TRUE, control = list(xtol_rel = 1e-8),
+       deprecatedBehavior = FALSE)

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 52 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630056814947 
Optimal value of controls: 2.330499 1.951374 -0.477733 4.365728 -0.6244887 1.038021 1.594209


> 
> 
> 
> 
> cleanEx()
> nameEx("check.derivatives")
> ### * check.derivatives
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: check.derivatives
> ### Title: Check analytic gradients of a function using finite difference
> ###   approximations
> ### Aliases: check.derivatives
> ### Keywords: interface optimize
> 
> ### ** Examples
> 
> 
> library('nloptr')
> 
> # example with correct gradient
> f <- function(x, a) sum((x - a) ^ 2)
> 
> f_grad <- function(x, a)  2 * (x - a)
> 
> check.derivatives(.x = 1:10, func = f, func_grad = f_grad,
+           check_derivatives_print = 'none', a = runif(10))
Derivative checker results: 0 error(s) detected.
$analytic
 [1]  1.468983  3.255752  4.854293  6.183584  9.596636 10.203221 12.110649
 [8] 14.678404 16.741772 19.876427

$finite_difference
 [1]  1.468983  3.255753  4.854294  6.183585  9.596636 10.203221 12.110650
 [8] 14.678405 16.741772 19.876428

$relative_error
 [1] 1.553279e-08 1.114182e-07 1.132906e-07 1.207909e-07 6.463315e-09
 [6] 3.657178e-08 1.577572e-08 2.677526e-08 1.252263e-08 1.922487e-08

$flag_derivative_warning
 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

> 
> # example with incorrect gradient
> f_grad <- function(x, a)  2 * (x - a) + c(0, 0.1, rep(0, 8))
> 
> check.derivatives(.x = 1:10, func = f, func_grad = f_grad,
+           check_derivatives_print = 'errors', a = runif(10))
Derivative checker results: 1 error(s) detected.

* grad_f[ 2] = 3.746886e+00 ~ 3.646887e+00   [2.742056e-02]


$analytic
 [1]  1.588051  3.746886  4.625954  7.231793  8.460317 11.004602 12.564763
 [8] 14.016188 17.239930 18.445110

$finite_difference
 [1]  1.588051  3.646887  4.625955  7.231793  8.460317 11.004602 12.564763
 [8] 14.016188 17.239930 18.445110

$relative_error
 [1] 4.984879e-09 2.742056e-02 1.381764e-07 1.161611e-07 8.300129e-09
 [6] 2.550193e-08 1.766216e-08 2.385419e-08 2.353826e-08 2.072174e-08

$flag_derivative_warning
 [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

> 
> # example with incorrect gradient of vector-valued function
> g <- function(x, a) c(sum(x - a), sum((x - a) ^ 2))
> 
> g_grad <- function(x, a) {
+    rbind(rep(1, length(x)) + c(0, 0.01, rep(0, 8)),
+    2 * (x - a) + c(0, 0.1, rep(0, 8)))
+ }
> 
> check.derivatives(.x = 1:10, func = g, func_grad = g_grad,
+           check_derivatives_print = 'all', a = runif(10))
Derivative checker results: 2 error(s) detected.

  grad_f[1,  1] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  1] = 1.305895e-01 ~ 1.305885e-01   [7.705838e-06]
* grad_f[1,  2] = 1.010000e+00 ~ 1.000000e+00   [1.000000e-02]
* grad_f[2,  2] = 3.675715e+00 ~ 3.575714e+00   [2.796668e-02]
  grad_f[1,  3] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  3] = 4.696652e+00 ~ 4.696653e+00   [5.588613e-08]
  grad_f[1,  4] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  4] = 7.748890e+00 ~ 7.748890e+00   [1.484320e-08]
  grad_f[1,  5] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  5] = 9.465559e+00 ~ 9.465559e+00   [4.043856e-09]
  grad_f[1,  6] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  6] = 1.122777e+01 ~ 1.122777e+01   [4.976887e-09]
  grad_f[1,  7] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  7] = 1.397322e+01 ~ 1.397322e+01   [1.025942e-08]
  grad_f[1,  8] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  8] = 1.523522e+01 ~ 1.523522e+01   [1.057541e-08]
  grad_f[1,  9] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2,  9] = 1.626062e+01 ~ 1.626062e+01   [6.962062e-09]
  grad_f[1, 10] = 1.000000e+00 ~ 1.000000e+00   [0.000000e+00]
  grad_f[2, 10] = 1.931930e+01 ~ 1.931930e+01   [1.031627e-09]


$analytic
          [,1]     [,2]     [,3]    [,4]     [,5]     [,6]     [,7]     [,8]
[1,] 1.0000000 1.010000 1.000000 1.00000 1.000000  1.00000  1.00000  1.00000
[2,] 0.1305895 3.675715 4.696652 7.74889 9.465559 11.22777 13.97322 15.23522
         [,9]   [,10]
[1,]  1.00000  1.0000
[2,] 16.26062 19.3193

$finite_difference
          [,1]     [,2]     [,3]    [,4]     [,5]     [,6]     [,7]     [,8]
[1,] 1.0000000 1.000000 1.000000 1.00000 1.000000  1.00000  1.00000  1.00000
[2,] 0.1305885 3.575714 4.696653 7.74889 9.465559 11.22777 13.97322 15.23522
         [,9]   [,10]
[1,]  1.00000  1.0000
[2,] 16.26062 19.3193

$relative_error
             [,1]       [,2]         [,3]        [,4]         [,5]         [,6]
[1,] 0.000000e+00 0.01000000 0.000000e+00 0.00000e+00 0.000000e+00 0.000000e+00
[2,] 7.705838e-06 0.02796668 5.588613e-08 1.48432e-08 4.043856e-09 4.976887e-09
             [,7]         [,8]         [,9]        [,10]
[1,] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
[2,] 1.025942e-08 1.057541e-08 6.962062e-09 1.031627e-09

$flag_derivative_warning
      [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
[1,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[2,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

> 
> 
> 
> 
> cleanEx()
> nameEx("cobyla")
> ### * cobyla
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cobyla
> ### Title: Constrained Optimization by Linear Approximations
> ### Aliases: cobyla
> 
> ### ** Examples
> 
> 
> ##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
> ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm
> 
> x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1)
> fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +
+                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +
+                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}
> 
> hin.hs100 <- function(x) {c(
+ 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127,
+ 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282,
+ 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196,
+ 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -
+  11 * x[7])
+ }
> 
> S <- cobyla(x0.hs100, fn.hs100, hin = hin.hs100,
+       nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 2000),
+       deprecatedBehavior = FALSE)

Call:
nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, eval_g_ineq = hin, 
    opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 1623 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 2000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630057374426 
Optimal value of controls: 2.330499 1.951372 -0.4775447 4.365726 -0.624487 1.038131 1.594227


> 
> ##  The optimum value of the objective function should be 680.6300573
> ##  A suitable parameter vector is roughly
> ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)
> 
> S
$par
[1]  2.3304990  1.9513724 -0.4775447  4.3657263 -0.6244870  1.0381308  1.5942267

$value
[1] 680.6301

$iter
[1] 1623

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> 
> 
> 
> cleanEx()
> nameEx("crs2lm")
> ### * crs2lm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: crs2lm
> ### Title: Controlled Random Search
> ### Aliases: crs2lm
> 
> ### ** Examples
> 
> 
> ## Minimize the Hartmann 6-Dimensional function
> ## See https://www.sfu.ca/~ssurjano/hart6.html
> 
> a <- c(1.0, 1.2, 3.0, 3.2)
> A <- matrix(c(10,  0.05, 3, 17,
+               3, 10, 3.5, 8,
+               17, 17, 1.7, 0.05,
+               3.5, 0.1, 10, 10,
+               1.7, 8, 17, 0.1,
+               8, 14, 8, 14), nrow = 4)
> 
> B  <- matrix(c(.1312, .2329, .2348, .4047,
+                .1696, .4135, .1451, .8828,
+                .5569, .8307, .3522, .8732,
+                .0124, .3736, .2883, .5743,
+                .8283, .1004, .3047, .1091,
+                .5886, .9991, .6650, .0381), nrow = 4)
> 
> hartmann6 <- function(x, a, A, B) {
+   fun <- 0
+   for (i in 1:4) {
+     fun <- fun - a[i] * exp(-sum(A[i, ] * (x - B[i, ]) ^ 2))
+   }
+ 
+   fun
+ }
> 
> ## The function has a global minimum of -3.32237 at
> ## (0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573)
> 
> S <- crs2lm(x0 = rep(0, 6), hartmann6, lower = rep(0, 6), upper = rep(1, 6),
+             ranseed = 10L, nl.info = TRUE, xtol_rel=1e-8, maxeval = 10000,
+             a = a, A = A, B = B)

Call:
nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 5690 
Termination conditions:  maxeval: 10000	xtol_rel: 1e-08 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  -3.32236801141551 
Optimal value of controls: 0.2016895 0.1500107 0.476874 0.2753324 0.3116516 0.6573005


> 
> S
$par
[1] 0.2016895 0.1500107 0.4768740 0.2753324 0.3116516 0.6573005

$value
[1] -3.322368

$iter
[1] 5690

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> 
> 
> 
> cleanEx()
> nameEx("direct")
> ### * direct
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: direct
> ### Title: DIviding RECTangles Algorithm for Global Optimization
> ### Aliases: direct directL
> 
> ### ** Examples
> 
> 
> ### Minimize the Hartmann6 function
> hartmann6 <- function(x) {
+   a <- c(1.0, 1.2, 3.0, 3.2)
+   A <- matrix(c(10.0,  0.05, 3.0, 17.0,
+          3.0, 10.0,  3.5,  8.0,
+           17.0, 17.0,  1.7,  0.05,
+          3.5,  0.1, 10.0, 10.0,
+          1.7,  8.0, 17.0,  0.1,
+          8.0, 14.0,  8.0, 14.0), nrow=4, ncol=6)
+   B  <- matrix(c(.1312,.2329,.2348,.4047,
+          .1696,.4135,.1451,.8828,
+          .5569,.8307,.3522,.8732,
+          .0124,.3736,.2883,.5743,
+          .8283,.1004,.3047,.1091,
+          .5886,.9991,.6650,.0381), nrow=4, ncol=6)
+   fun <- 0
+   for (i in 1:4) {
+     fun <- fun - a[i] * exp(-sum(A[i,] * (x - B[i,]) ^ 2))
+   }
+   fun
+ }
> S <- directL(hartmann6, rep(0, 6), rep(1, 6),
+        nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 1000))

Call:
nloptr(x0 = x0, eval_f = fn, lb = lower, ub = upper, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 5 ( NLOPT_MAXEVAL_REACHED: Optimization stopped because 
maxeval (above) was reached. )

Number of Iterations....: 1000 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Current value of objective function:  -3.32236800687327 
Current value of controls: 0.2016884 0.1500025 0.4768667 0.2753391 0.311648 0.6572931


> ## Number of Iterations....: 1000
> ## Termination conditions:  stopval: -Inf
> ##   xtol_rel: 1e-08,  maxeval: 1000,  ftol_rel: 0,  ftol_abs: 0
> ## Number of inequality constraints:  0
> ## Number of equality constraints:  0
> ## Current value of objective function:  -3.32236800687327
> ## Current value of controls:
> ##   0.2016884 0.1500025 0.4768667 0.2753391 0.311648 0.6572931
> 
> 
> 
> 
> cleanEx()
> nameEx("isres")
> ### * isres
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: isres
> ### Title: Improved Stochastic Ranking Evolution Strategy
> ### Aliases: isres
> 
> ### ** Examples
> 
> 
> ## Rosenbrock Banana objective function
> 
> rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}
> 
> x0 <- c(-1.2, 1)
> lb <- c(-3, -3)
> ub <- c(3,  3)
> 
> ## The function as written above has a minimum of 0 at (1, 1)
> 
> isres(x0 = x0, fn = rbf, lower = lb, upper = ub)
$par
[1] 1.000015 1.000029

$value
[1] 2.785245e-10

$iter
[1] 10000

$convergence
[1] 5

$message
[1] "NLOPT_MAXEVAL_REACHED: Optimization stopped because maxeval (above) was reached."

> 
> ## Now subject to the inequality that x[1] + x[2] <= 1.5
> 
> hin <- function(x) {x[1] + x[2] - 1.5}
> 
> S <- isres(x0 = x0, fn = rbf, hin = hin, lower = lb, upper = ub,
+            maxeval = 2e5L, deprecatedBehavior = FALSE)
> 
> S
$par
[1] 0.8231316 0.6768683

$value
[1] 0.03132831

$iter
[1] 13126

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> sum(S$par)
[1] 1.5
> 
> 
> 
> 
> cleanEx()
> nameEx("lbfgs")
> ### * lbfgs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lbfgs
> ### Title: Low-storage BFGS
> ### Aliases: lbfgs
> 
> ### ** Examples
> 
> 
> flb <- function(x) {
+   p <- length(x)
+   sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2)
+ }
> # 25-dimensional box constrained: par[24] is *not* at the boundary
> S <- lbfgs(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),
+      nl.info = TRUE, control = list(xtol_rel=1e-8))

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. )

Number of Iterations....: 19 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  368.105912874334 
Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4


> ## Optimal value of objective function:  368.105912874334
> ## Optimal value of controls: 2  ...  2  2.109093  4
> 
> 
> 
> 
> cleanEx()
> nameEx("mlsl")
> ### * mlsl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mlsl
> ### Title: Multi-level Single-linkage
> ### Aliases: mlsl
> 
> ### ** Examples
> 
> 
> ## Minimize the Hartmann 6-Dimensional function
> ## See https://www.sfu.ca/~ssurjano/hart6.html
> 
> a <- c(1.0, 1.2, 3.0, 3.2)
> A <- matrix(c(10,  0.05, 3, 17,
+               3, 10, 3.5, 8,
+               17, 17, 1.7, 0.05,
+               3.5, 0.1, 10, 10,
+               1.7, 8, 17, 0.1,
+               8, 14, 8, 14), nrow = 4)
> 
> B  <- matrix(c(.1312, .2329, .2348, .4047,
+                .1696, .4135, .1451, .8828,
+                .5569, .8307, .3522, .8732,
+                .0124, .3736, .2883, .5743,
+                .8283, .1004, .3047, .1091,
+                .5886, .9991, .6650, .0381), nrow = 4)
> 
> hartmann6 <- function(x, a, A, B) {
+   fun <- 0
+   for (i in 1:4) {
+     fun <- fun - a[i] * exp(-sum(A[i, ] * (x - B[i, ]) ^ 2))
+   }
+ 
+   fun
+ }
> 
> ## The function has a global minimum of -3.32237 at
> ## (0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573)
> 
> S <- mlsl(x0 = rep(0, 6), hartmann6, lower = rep(0, 6), upper = rep(1, 6),
+       nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 1000),
+       a = a, A = A, B = B)

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 5 ( NLOPT_MAXEVAL_REACHED: Optimization stopped because 
maxeval (above) was reached. )

Number of Iterations....: 1000 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Current value of objective function:  -3.32236801141544 
Current value of controls: 0.2016895 0.1500107 0.476874 0.2753324 0.3116516 0.6573005


> 
> 
> 
> 
> cleanEx()
> nameEx("mma")
> ### * mma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mma
> ### Title: Method of Moving Asymptotes
> ### Aliases: mma
> 
> ### ** Examples
> 
> 
> #  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
> #  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm
> 
> x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1)
> fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +
+                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +
+                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}
> 
> hin.hs100 <- function(x) {c(
+ 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127,
+ 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282,
+ 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196,
+ 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -
+  11 * x[7])
+ }
> 
> gr.hs100 <- function(x) {
+  c( 2 * x[1] - 20,
+    10 * x[2] - 120,
+     4 * x[3] ^ 3,
+     6 * x[4] - 66,
+    60 * x[5] ^ 5,
+    14 * x[6] - 4 * x[7] - 10,
+     4 * x[7] ^ 3 - 4 * x[6] - 8)
+ }
> 
> hinjac.hs100 <- function(x) {
+   matrix(c(4 * x[1], 12 * x[2] ^ 3, 1, 8 * x[4], 5, 0, 0,
+            7, 3, 20 * x[3], 1, -1, 0, 0,
+            23, 2 * x[2], 0, 0, 0, 12 * x[6], -8,
+            8 * x[1] - 3 * x[2], 2 * x[2] - 3 * x[1], 4 * x[3], 0, 0, 5, -11),
+            nrow = 4, byrow = TRUE)
+ }
> 
> #  The optimum value of the objective function should be 680.6300573
> #  A suitable parameter vector is roughly
> #  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)
> 
> # Using analytic Jacobian
> S <- mma(x0.hs100, fn.hs100, gr = gr.hs100,
+       hin = hin.hs100, hinjac = hinjac.hs100,
+       nl.info = TRUE, control = list(xtol_rel = 1e-8),
+       deprecatedBehavior = FALSE)

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 229 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630044400453 
Optimal value of controls: 2.3305 1.951372 -0.477538 4.365726 -0.6244892 1.038134 1.594228


> 
> # Using computed Jacobian
> S <- mma(x0.hs100, fn.hs100, hin = hin.hs100,
+       nl.info = TRUE, control = list(xtol_rel = 1e-8),
+       deprecatedBehavior = FALSE)

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    eval_g_ineq = hin, eval_jac_g_ineq = hinjac, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 277 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630053840435 
Optimal value of controls: 2.330504 1.951372 -0.477535 4.365726 -0.6244867 1.038129 1.59423


> 
> 
> 
> 
> cleanEx()
> nameEx("neldermead")
> ### * neldermead
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: neldermead
> ### Title: Nelder-Mead Simplex
> ### Aliases: neldermead
> 
> ### ** Examples
> 
> 
> # Fletcher and Powell's helic valley
> fphv <- function(x)
+   100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 +
+     (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2
> x0 <- c(-1, 0, 0)
> neldermead(x0, fphv)  #  1 0 0
$par
[1] 1.000000e+00 4.940772e-08 6.181353e-08

$value
[1] 5.887553e-14

$iter
[1] 282

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> # Powell's Singular Function (PSF)
> psf <- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 +
+           (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4
> x0 <- c(3, -1, 0, 1)
> neldermead(x0, psf)   #  0 0 0 0, needs maximum number of function calls
$par
[1] -4.085274e-11  4.085274e-12 -3.522447e-11 -3.522447e-11

$value
[1] 3.721119e-41

$iter
[1] 1000

$convergence
[1] 5

$message
[1] "NLOPT_MAXEVAL_REACHED: Optimization stopped because maxeval (above) was reached."

> 
> ## Not run: 
> ##D # Bounded version of Nelder-Mead
> ##D rosenbrock <- function(x) { ## Rosenbrock Banana function
> ##D   100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2 +
> ##D   100 * (x[3] - x[2]^2)^2 + (1 - x[2])^2
> ##D }
> ##D lower <- c(-Inf, 0,   0)
> ##D upper <- c( Inf, 0.5, 1)
> ##D x0 <- c(0, 0.1, 0.1)
> ##D S <- neldermead(c(0, 0.1, 0.1), rosenbrock, lower, upper, nl.info = TRUE)
> ##D # $xmin = c(0.7085595, 0.5000000, 0.2500000)
> ##D # $fmin = 0.3353605
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("newuoa")
> ### * newuoa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: newuoa
> ### Title: New Unconstrained Optimization with quadratic Approximation
> ### Aliases: newuoa
> 
> ### ** Examples
> 
> 
> ## Rosenbrock Banana function
> 
> rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}
> 
> S <- newuoa(c(1, 2), rbf)
> 
> ## The function as written above has a minimum of 0 at (1, 1)
> 
> S
$par
[1] 1 1

$value
[1] 0

$iter
[1] 33

$convergence
[1] 1

$message
[1] "NLOPT_SUCCESS: Generic success return value."

> 
> 
> 
> 
> cleanEx()
> nameEx("nl.grad")
> ### * nl.grad
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nl.grad
> ### Title: Numerical Gradients and Jacobians
> ### Aliases: nl.grad nl.jacobian
> 
> ### ** Examples
> 
> 
>   fn1 <- function(x) sum(x ^ 2)
>   nl.grad(seq(0, 1, by = 0.2), fn1)
[1] 0.0 0.4 0.8 1.2 1.6 2.0
>   ## [1] 0.0  0.4  0.8  1.2  1.6  2.0
>   nl.grad(rep(1, 5), fn1)
[1] 2 2 2 2 2
>   ## [1] 2  2  2  2  2
> 
>   fn2 <- function(x) c(sin(x), cos(x))
>   x <- (0:1) * 2 * pi
>   nl.jacobian(x, fn2)
     [,1] [,2]
[1,]    1    0
[2,]    0    1
[3,]    0    0
[4,]    0    0
>   ##    [,1] [,2]
>   ## [1,]  1  0
>   ## [2,]  0  1
>   ## [3,]  0  0
>   ## [4,]  0  0
> 
> 
> 
> 
> cleanEx()
> nameEx("nl.opts")
> ### * nl.opts
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nl.opts
> ### Title: Setting NL Options
> ### Aliases: nl.opts
> 
> ### ** Examples
> 
> 
> nl.opts(list(xtol_rel = 1e-8, maxeval = 2000))
$stopval
[1] -Inf

$xtol_rel
[1] 1e-08

$maxeval
[1] 2000

$ftol_rel
[1] 0

$ftol_abs
[1] 0

$check_derivatives
[1] FALSE

$algorithm
NULL

> 
> 
> 
> 
> cleanEx()
> nameEx("nloptr-package")
> ### * nloptr-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nloptr-package
> ### Title: R interface to NLopt
> ### Aliases: nloptr-package
> ### Keywords: interface internal optimize
> 
> ### ** Examples
> 
> 
> # Example problem, number 71 from the Hock-Schittkowsky test suite.
> #
> # \min_{x} x1 * x4 * (x1 + x2 + x3) + x3
> # s.t.
> #    x1 * x2 * x3 * x4 >= 25
> #    x1 ^ 2 + x2 ^ 2 + x3 ^ 2 + x4 ^ 2 = 40
> #    1 <= x1, x2, x3, x4 <= 5
> #
> # we re-write the inequality as
> #   25 - x1 * x2 * x3 * x4 <= 0
> #
> # and the equality as
> #   x1 ^ 2 + x2 ^ 2 + x3 ^ 2 + x4 ^ 2 - 40 = 0
> #
> # x0 = (1, 5, 5, 1)
> #
> # optimal solution = (1.000000, 4.742999, 3.821151, 1.379408)
> 
> library('nloptr')
> 
> #
> # f(x) = x1 * x4 * (x1 + x2 + x3) + x3
> #
> eval_f <- function(x) {
+     list("objective" = x[1] * x[4] * (x[1] + x[2] + x[3]) + x[3],
+          "gradient" = c(x[1] * x[4] + x[4] * (x[1] + x[2] + x[3]),
+                         x[1] * x[4],
+                         x[1] * x[4] + 1.0,
+                         x[1] * (x[1] + x[2] + x[3])))
+ }
> 
> # constraint functions
> # inequalities
> eval_g_ineq <- function(x) {
+     constr <- c(25 - x[1] * x[2] * x[3] * x[4])
+ 
+     grad   <- c(-x[2] * x[3] * x[4],
+                 -x[1] * x[3] * x[4],
+                 -x[1] * x[2] * x[4],
+                 -x[1] * x[2] * x[3] )
+     list("constraints" = constr, "jacobian" = grad)
+ }
> 
> # equalities
> eval_g_eq <- function(x) {
+     constr <- c(x[1] ^ 2 + x[2] ^ 2 + x[3] ^ 2 + x[4] ^ 2 - 40)
+ 
+     grad <- c(2.0 * x[1],
+               2.0 * x[2],
+               2.0 * x[3],
+               2.0 * x[4])
+     list("constraints" = constr, "jacobian" = grad)
+ }
> 
> # initial values
> x0 <- c(1, 5, 5, 1)
> 
> # lower and upper bounds of control
> lb <- c(1, 1, 1, 1)
> ub <- c(5, 5, 5, 5)
> 
> 
> local_opts <- list("algorithm" = "NLOPT_LD_MMA", "xtol_rel"  = 1.0e-7)
> opts <- list("algorithm"  = "NLOPT_LD_AUGLAG",
+              "xtol_rel"   = 1.0e-7,
+              "maxeval"    = 1000,
+              "local_opts" = local_opts)
> 
> res <- nloptr(x0 = x0,
+               eval_f = eval_f,
+               lb = lb,
+               ub = ub,
+               eval_g_ineq = eval_g_ineq,
+               eval_g_eq = eval_g_eq,
+               opts = opts)
> print(res)

Call:

nloptr(x0 = x0, eval_f = eval_f, lb = lb, ub = ub, eval_g_ineq = eval_g_ineq, 
    eval_g_eq = eval_g_eq, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 476 
Termination conditions:  xtol_rel: 1e-07	maxeval: 1000 
Number of inequality constraints:  1 
Number of equality constraints:    1 
Optimal value of objective function:  17.0140172892472 
Optimal value of controls: 1 4.742999 3.821151 1.379408


> 
> 
> 
> cleanEx()
> nameEx("nloptr")
> ### * nloptr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nloptr
> ### Title: R interface to NLopt
> ### Aliases: nloptr
> ### Keywords: interface optimize
> 
> ### ** Examples
> 
> 
> library('nloptr')
> 
> ## Rosenbrock Banana function and gradient in separate functions
> eval_f <- function(x) {
+   return(100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2)
+ }
> 
> eval_grad_f <- function(x) {
+   return(c(-400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
+         200 * (x[2] - x[1] * x[1])))
+ }
> 
> 
> # initial values
> x0 <- c(-1.2, 1)
> 
> opts <- list("algorithm"="NLOPT_LD_LBFGS",
+        "xtol_rel"=1.0e-8)
> 
> # solve Rosenbrock Banana function
> res <- nloptr(x0=x0,
+        eval_f=eval_f,
+        eval_grad_f=eval_grad_f,
+        opts=opts)
> print(res)

Call:
nloptr(x0 = x0, eval_f = eval_f, eval_grad_f = eval_grad_f, opts = opts)



Minimization using NLopt version 2.7.1 

NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. )

Number of Iterations....: 56 
Termination conditions:  xtol_rel: 1e-08 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  7.35727226897802e-23 
Optimal value of controls: 1 1


> 
> 
> ## Rosenbrock Banana function and gradient in one function
> # this can be used to economize on calculations
> eval_f_list <- function(x) {
+   return(
+   list(
+     "objective" = 100 * (x[2] - x[1] * x[1]) ^ 2 + (1 - x[1]) ^ 2,
+     "gradient"  = c(-400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
+             200 * (x[2] - x[1] * x[1]))))
+ }
> 
> # solve Rosenbrock Banana function using an objective function that
> # returns a list with the objective value and its gradient
> res <- nloptr(x0=x0,
+        eval_f=eval_f_list,
+        opts=opts)
> print(res)

Call:
nloptr(x0 = x0, eval_f = eval_f_list, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. )

Number of Iterations....: 56 
Termination conditions:  xtol_rel: 1e-08 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  7.35727226897802e-23 
Optimal value of controls: 1 1


> 
> 
> 
> # Example showing how to solve the problem from the NLopt tutorial.
> #
> # min sqrt(x2)
> # s.t. x2 >= 0
> #    x2 >= (a1*x1 + b1)^3
> #    x2 >= (a2*x1 + b2)^3
> # where
> # a1 = 2, b1 = 0, a2 = -1, b2 = 1
> #
> # re-formulate constraints to be of form g(x) <= 0
> #    (a1*x1 + b1)^3 - x2 <= 0
> #    (a2*x1 + b2)^3 - x2 <= 0
> 
> library('nloptr')
> 
> 
> # objective function
> eval_f0 <- function(x, a, b) {
+   return(sqrt(x[2]))
+ }
> 
> # constraint function
> eval_g0 <- function(x, a, b) {
+   return((a*x[1] + b)^3 - x[2])
+ }
> 
> # gradient of objective function
> eval_grad_f0 <- function(x, a, b) {
+   return(c(0, .5/sqrt(x[2])))
+ }
> 
> # Jacobian of constraint
> eval_jac_g0 <- function(x, a, b) {
+   return(rbind(c(3*a[1]*(a[1]*x[1] + b[1])^2, -1.0),
+          c(3*a[2]*(a[2]*x[1] + b[2])^2, -1.0)))
+ }
> 
> 
> # functions with gradients in objective and constraint function
> # this can be useful if the same calculations are needed for
> # the function value and the gradient
> eval_f1 <- function(x, a, b) {
+   return(list("objective"=sqrt(x[2]),
+          "gradient"=c(0,.5/sqrt(x[2]))))
+ }
> 
> eval_g1 <- function(x, a, b) {
+   return(list("constraints"=(a*x[1] + b)^3 - x[2],
+           "jacobian"=rbind(c(3*a[1]*(a[1]*x[1] + b[1])^2, -1.0),
+                   c(3*a[2]*(a[2]*x[1] + b[2])^2, -1.0))))
+ }
> 
> 
> # define parameters
> a <- c(2,-1)
> b <- c(0, 1)
> 
> # Solve using NLOPT_LD_MMA with gradient information supplied in separate
> # function.
> res0 <- nloptr(x0=c(1.234,5.678),
+         eval_f=eval_f0,
+         eval_grad_f=eval_grad_f0,
+         lb = c(-Inf,0),
+         ub = c(Inf,Inf),
+         eval_g_ineq = eval_g0,
+         eval_jac_g_ineq = eval_jac_g0,
+         opts = list("algorithm"="NLOPT_LD_MMA"),
+         a = a,
+         b = b)
Warning in nloptr.add.default.options(opts.user = opts, x0 = x0, num_constraints_ineq = num_constraints_ineq,  :
  No termination criterion specified, using default(relative x-tolerance = 1e-04)
> print(res0)

Call:

nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, eval_grad_f = eval_grad_f0, 
    lb = c(-Inf, 0), ub = c(Inf, Inf), eval_g_ineq = eval_g0, 
    eval_jac_g_ineq = eval_jac_g0, opts = list(algorithm = "NLOPT_LD_MMA"), 
    a = a, b = b)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 11 
Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT) 
Number of inequality constraints:  2 
Number of equality constraints:    0 
Optimal value of objective function:  0.54433104762009 
Optimal value of controls: 0.3333333 0.2962963


> 
> # Solve using NLOPT_LN_COBYLA without gradient information
> res1 <- nloptr(x0=c(1.234,5.678),
+         eval_f=eval_f0,
+         lb = c(-Inf, 0),
+         ub = c(Inf, Inf),
+         eval_g_ineq = eval_g0,
+         opts = list("algorithm" = "NLOPT_LN_COBYLA"),
+         a = a,
+         b = b)
Warning in nloptr.add.default.options(opts.user = opts, x0 = x0, num_constraints_ineq = num_constraints_ineq,  :
  No termination criterion specified, using default(relative x-tolerance = 1e-04)
> print(res1)

Call:
nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, lb = c(-Inf, 0), 
    ub = c(Inf, Inf), eval_g_ineq = eval_g0, opts = list(algorithm = "NLOPT_LN_COBYLA"), 
    a = a, b = b)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 31 
Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT) 
Number of inequality constraints:  2 
Number of equality constraints:    0 
Optimal value of objective function:  0.544242301658176 
Optimal value of controls: 0.3333292 0.2961997


> 
> 
> # Solve using NLOPT_LD_MMA with gradient information in objective function
> res2 <- nloptr(x0=c(1.234, 5.678),
+         eval_f=eval_f1,
+         lb = c(-Inf, 0),
+         ub = c(Inf, Inf),
+         eval_g_ineq = eval_g1,
+         opts = list("algorithm"="NLOPT_LD_MMA",
+               "check_derivatives" = TRUE),
+         a = a,
+         b = b)
Warning in nloptr.add.default.options(opts.user = opts, x0 = x0, num_constraints_ineq = num_constraints_ineq,  :
  No termination criterion specified, using default(relative x-tolerance = 1e-04)
Checking gradients of objective function.
Derivative checker results: 0 error(s) detected.

  eval_grad_f[1] = 0.000000e+00 ~ 0.000000e+00   [0.000000e+00]
  eval_grad_f[2] = 2.098323e-01 ~ 2.098323e-01   [1.422937e-09]


Checking gradients of inequality constraints.

Derivative checker results: 0 error(s) detected.

  eval_jac_g_ineq[1, 1] =  3.654614e+01 ~  3.654614e+01   [1.667794e-08]
  eval_jac_g_ineq[2, 1] = -1.642680e-01 ~ -1.642680e-01   [2.103453e-07]
  eval_jac_g_ineq[1, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]
  eval_jac_g_ineq[2, 2] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]


> print(res2)

Call:
nloptr(x0 = c(1.234, 5.678), eval_f = eval_f1, lb = c(-Inf, 0), 
    ub = c(Inf, Inf), eval_g_ineq = eval_g1, opts = list(algorithm = "NLOPT_LD_MMA", 
        check_derivatives = TRUE), a = a, b = b)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 11 
Termination conditions:  relative x-tolerance = 1e-04 (DEFAULT) 
Number of inequality constraints:  2 
Number of equality constraints:    0 
Optimal value of objective function:  0.54433104762009 
Optimal value of controls: 0.3333333 0.2962963


> 
> 
> 
> 
> cleanEx()
> nameEx("nloptr.print.options")
> ### * nloptr.print.options
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nloptr.print.options
> ### Title: Print description of nloptr options
> ### Aliases: nloptr.print.options
> ### Keywords: interface optimize
> 
> ### ** Examples
> 
> 
> library('nloptr')
> nloptr.print.options()
algorithm
	possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L,
	         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL,
	         NLOPT_GN_DIRECT_L_NOSCAL,
	         NLOPT_GN_DIRECT_L_RAND_NOSCAL,
	         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L,
	         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND,
	         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS_NOCEDAL,
	         NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, NLOPT_LD_VAR1,
	         NLOPT_LD_VAR2, NLOPT_LD_TNEWTON,
	         NLOPT_LD_TNEWTON_RESTART,
	         NLOPT_LD_TNEWTON_PRECOND,
	         NLOPT_LD_TNEWTON_PRECOND_RESTART,
	         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL,
	         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS,
	         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA,
	         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND,
	         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX,
	         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG,
	         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ,
	         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES
	default value:   none

	This option is required. Check the NLopt website for a description of
	the algorithms.

stopval
	possible values: -Inf <= stopval <= Inf
	default value:   -Inf

	Stop minimization when an objective value <= stopval is found.
	Setting stopval to -Inf disables this stopping criterion (default).

ftol_rel
	possible values: ftol_rel > 0
	default value:   0.0

	Stop when an optimization step (or an estimate of the optimum)
	changes the objective function value by less than ftol_rel multiplied
	by the absolute value of the function value. If there is any chance
	that your optimum function value is close to zero, you might want to
	set an absolute tolerance with ftol_abs as well. Criterion is
	disabled if ftol_rel is non-positive (default).

ftol_abs
	possible values: ftol_abs > 0
	default value:   0.0

	Stop when an optimization step (or an estimate of the optimum)
	changes the function value by less than ftol_abs. Criterion is
	disabled if ftol_abs is non-positive (default).

xtol_rel
	possible values: xtol_rel > 0
	default value:   1.0e-04

	Stop when an optimization step (or an estimate of the optimum)
	changes every parameter by less than xtol_rel multiplied by the
	absolute value of the parameter. If there is any chance that an
	optimal parameter is close to zero, you might want to set an absolute
	tolerance with xtol_abs as well. Criterion is disabled if xtol_rel is
	non-positive.

xtol_abs
	possible values: xtol_abs > 0
	default value:   rep(0.0, length(x0))

	xtol_abs is a vector of length n (the number of elements in x) giving
	the tolerances: stop when an optimization step (or an estimate of the
	optimum) changes every parameter x[i] by less than xtol_abs[i].
	Criterion is disabled if all elements of xtol_abs are non-positive
	(default).

maxeval
	possible values: maxeval is a positive integer
	default value:   100

	Stop when the number of function evaluations exceeds maxeval. This is
	not a strict maximum: the number of function evaluations may exceed
	maxeval slightly, depending upon the algorithm. Criterion is disabled
	if maxeval is non-positive.

maxtime
	possible values: maxtime > 0
	default value:   -1.0

	Stop when the optimization time (in seconds) exceeds maxtime. This is
	not a strict maximum: the time may exceed maxtime slightly, depending
	upon the algorithm and on how slow your function evaluation is.
	Criterion is disabled if maxtime is non-positive (default).

tol_constraints_ineq
	possible values: tol_constraints_ineq > 0.0
	default value:   rep(1e-8, num_constraints_ineq)

	The parameter tol_constraints_ineq is a vector of tolerances. Each
	tolerance corresponds to one of the inequality constraints. The
	tolerance is used for the purpose of stopping criteria only: a point
	x is considered feasible for judging whether to stop the optimization
	if eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will
	try not to consider any x to be converged unless eval_g_ineq(x) is
	strictly non-positive; generally, at least a small positive tolerance
	is advisable to reduce sensitivity to rounding errors. By default the
	tolerances for all inequality constraints are set to 1e-8.

tol_constraints_eq
	possible values: tol_constraints_eq > 0.0
	default value:   rep(1e-8, num_constraints_eq)

	The parameter tol_constraints_eq is a vector of tolerances. Each
	tolerance corresponds to one of the equality constraints. The
	tolerance is used for the purpose of stopping criteria only: a point
	x is considered feasible for judging whether to stop the optimization
	if abs(eval_g_ineq(x)) <= tol. For equality constraints, a small
	positive tolerance is strongly advised in order to allow NLopt to
	converge even if the equality constraint is slightly nonzero. By
	default the tolerances for all quality constraints are set to 1e-8.

print_level
	possible values: 0, 1, 2, or 3
	default value:   0

	The option print_level controls how much output is shown during the
	optimization process. Possible values: 0 (default): no output; 1:
	show iteration number and value of objective function; 2: 1 + show
	value of (in)equalities; 3: 2 + show value of controls.

check_derivatives
	possible values: TRUE or FALSE
	default value:   FALSE

	The option check_derivatives can be activated to compare the
	user-supplied analytic gradients with finite difference
	approximations.

check_derivatives_tol
	possible values: check_derivatives_tol > 0.0
	default value:   1e-04

	The option check_derivatives_tol determines when a difference between
	an analytic gradient and its finite difference approximation is
	flagged as an error.

check_derivatives_print
	possible values: 'none', 'all', 'errors',
	default value:   all

	The option check_derivatives_print controls the output of the
	derivative checker (if check_derivatives == TRUE). All comparisons
	are shown ('all'), only those comparisions that resulted in an error
	('error'), or only the number of errors is shown ('none').

print_options_doc
	possible values: TRUE or FALSE
	default value:   FALSE

	If TRUE, a description of all options and their current and default
	values is printed to the screen.

population
	possible values: population is a positive integer
	default value:   0

	Several of the stochastic search algorithms (e.g., CRS, MLSL, and
	ISRES) start by generating some initial population of random points
	x. By default, this initial population size is chosen heuristically
	in some algorithm-specific way, but the initial population can by
	changed by setting a positive integer value for population. A
	population of zero implies that the heuristic default will be used.

vector_storage
	possible values: vector_storage is a positive integer
	default value:   20

	Number of gradients to remember from previous optimization steps.

ranseed
	possible values: ranseed is a positive integer
	default value:   0

	For stochastic optimization algorithms, pseudorandom numbers are
	generated. Set the random seed using ranseed if you want to use a
	'deterministic' sequence of pseudorandom numbers, i.e. the same
	sequence from run to run. If ranseed is 0 (default), the seed for the
	random numbers is generated from the system time, so that you will
	get a different sequence of pseudorandom numbers each time you run
	your program.

> 
> nloptr.print.options(opts.show = c("algorithm", "check_derivatives"))
algorithm
	possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L,
	         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL,
	         NLOPT_GN_DIRECT_L_NOSCAL,
	         NLOPT_GN_DIRECT_L_RAND_NOSCAL,
	         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L,
	         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND,
	         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS_NOCEDAL,
	         NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, NLOPT_LD_VAR1,
	         NLOPT_LD_VAR2, NLOPT_LD_TNEWTON,
	         NLOPT_LD_TNEWTON_RESTART,
	         NLOPT_LD_TNEWTON_PRECOND,
	         NLOPT_LD_TNEWTON_PRECOND_RESTART,
	         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL,
	         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS,
	         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA,
	         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND,
	         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX,
	         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG,
	         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ,
	         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES
	default value:   none

	This option is required. Check the NLopt website for a description of
	the algorithms.

check_derivatives
	possible values: TRUE or FALSE
	default value:   FALSE

	The option check_derivatives can be activated to compare the
	user-supplied analytic gradients with finite difference
	approximations.

> 
> opts <- list("algorithm"="NLOPT_LD_LBFGS",
+        "xtol_rel"=1.0e-8)
> nloptr.print.options(opts.user = opts)
algorithm
	possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L,
	         NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL,
	         NLOPT_GN_DIRECT_L_NOSCAL,
	         NLOPT_GN_DIRECT_L_RAND_NOSCAL,
	         NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L,
	         NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND,
	         NLOPT_LD_SLSQP, NLOPT_LD_LBFGS_NOCEDAL,
	         NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, NLOPT_LD_VAR1,
	         NLOPT_LD_VAR2, NLOPT_LD_TNEWTON,
	         NLOPT_LD_TNEWTON_RESTART,
	         NLOPT_LD_TNEWTON_PRECOND,
	         NLOPT_LD_TNEWTON_PRECOND_RESTART,
	         NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL,
	         NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS,
	         NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA,
	         NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND,
	         NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX,
	         NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG,
	         NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ,
	         NLOPT_LN_BOBYQA, NLOPT_GN_ISRES
	default value:   none
	current value:   NLOPT_LD_LBFGS

	This option is required. Check the NLopt website for a description of
	the algorithms.

stopval
	possible values: -Inf <= stopval <= Inf
	default value:   -Inf
	current value:   (default)

	Stop minimization when an objective value <= stopval is found.
	Setting stopval to -Inf disables this stopping criterion (default).

ftol_rel
	possible values: ftol_rel > 0
	default value:   0.0
	current value:   (default)

	Stop when an optimization step (or an estimate of the optimum)
	changes the objective function value by less than ftol_rel multiplied
	by the absolute value of the function value. If there is any chance
	that your optimum function value is close to zero, you might want to
	set an absolute tolerance with ftol_abs as well. Criterion is
	disabled if ftol_rel is non-positive (default).

ftol_abs
	possible values: ftol_abs > 0
	default value:   0.0
	current value:   (default)

	Stop when an optimization step (or an estimate of the optimum)
	changes the function value by less than ftol_abs. Criterion is
	disabled if ftol_abs is non-positive (default).

xtol_rel
	possible values: xtol_rel > 0
	default value:   1.0e-04
	current value:   1e-08

	Stop when an optimization step (or an estimate of the optimum)
	changes every parameter by less than xtol_rel multiplied by the
	absolute value of the parameter. If there is any chance that an
	optimal parameter is close to zero, you might want to set an absolute
	tolerance with xtol_abs as well. Criterion is disabled if xtol_rel is
	non-positive.

xtol_abs
	possible values: xtol_abs > 0
	default value:   rep(0.0, length(x0))
	current value:   (default)

	xtol_abs is a vector of length n (the number of elements in x) giving
	the tolerances: stop when an optimization step (or an estimate of the
	optimum) changes every parameter x[i] by less than xtol_abs[i].
	Criterion is disabled if all elements of xtol_abs are non-positive
	(default).

maxeval
	possible values: maxeval is a positive integer
	default value:   100
	current value:   (default)

	Stop when the number of function evaluations exceeds maxeval. This is
	not a strict maximum: the number of function evaluations may exceed
	maxeval slightly, depending upon the algorithm. Criterion is disabled
	if maxeval is non-positive.

maxtime
	possible values: maxtime > 0
	default value:   -1.0
	current value:   (default)

	Stop when the optimization time (in seconds) exceeds maxtime. This is
	not a strict maximum: the time may exceed maxtime slightly, depending
	upon the algorithm and on how slow your function evaluation is.
	Criterion is disabled if maxtime is non-positive (default).

tol_constraints_ineq
	possible values: tol_constraints_ineq > 0.0
	default value:   rep(1e-8, num_constraints_ineq)
	current value:   (default)

	The parameter tol_constraints_ineq is a vector of tolerances. Each
	tolerance corresponds to one of the inequality constraints. The
	tolerance is used for the purpose of stopping criteria only: a point
	x is considered feasible for judging whether to stop the optimization
	if eval_g_ineq(x) <= tol. A tolerance of zero means that NLopt will
	try not to consider any x to be converged unless eval_g_ineq(x) is
	strictly non-positive; generally, at least a small positive tolerance
	is advisable to reduce sensitivity to rounding errors. By default the
	tolerances for all inequality constraints are set to 1e-8.

tol_constraints_eq
	possible values: tol_constraints_eq > 0.0
	default value:   rep(1e-8, num_constraints_eq)
	current value:   (default)

	The parameter tol_constraints_eq is a vector of tolerances. Each
	tolerance corresponds to one of the equality constraints. The
	tolerance is used for the purpose of stopping criteria only: a point
	x is considered feasible for judging whether to stop the optimization
	if abs(eval_g_ineq(x)) <= tol. For equality constraints, a small
	positive tolerance is strongly advised in order to allow NLopt to
	converge even if the equality constraint is slightly nonzero. By
	default the tolerances for all quality constraints are set to 1e-8.

print_level
	possible values: 0, 1, 2, or 3
	default value:   0
	current value:   (default)

	The option print_level controls how much output is shown during the
	optimization process. Possible values: 0 (default): no output; 1:
	show iteration number and value of objective function; 2: 1 + show
	value of (in)equalities; 3: 2 + show value of controls.

check_derivatives
	possible values: TRUE or FALSE
	default value:   FALSE
	current value:   (default)

	The option check_derivatives can be activated to compare the
	user-supplied analytic gradients with finite difference
	approximations.

check_derivatives_tol
	possible values: check_derivatives_tol > 0.0
	default value:   1e-04
	current value:   (default)

	The option check_derivatives_tol determines when a difference between
	an analytic gradient and its finite difference approximation is
	flagged as an error.

check_derivatives_print
	possible values: 'none', 'all', 'errors',
	default value:   all
	current value:   (default)

	The option check_derivatives_print controls the output of the
	derivative checker (if check_derivatives == TRUE). All comparisons
	are shown ('all'), only those comparisions that resulted in an error
	('error'), or only the number of errors is shown ('none').

print_options_doc
	possible values: TRUE or FALSE
	default value:   FALSE
	current value:   (default)

	If TRUE, a description of all options and their current and default
	values is printed to the screen.

population
	possible values: population is a positive integer
	default value:   0
	current value:   (default)

	Several of the stochastic search algorithms (e.g., CRS, MLSL, and
	ISRES) start by generating some initial population of random points
	x. By default, this initial population size is chosen heuristically
	in some algorithm-specific way, but the initial population can by
	changed by setting a positive integer value for population. A
	population of zero implies that the heuristic default will be used.

vector_storage
	possible values: vector_storage is a positive integer
	default value:   20
	current value:   (default)

	Number of gradients to remember from previous optimization steps.

ranseed
	possible values: ranseed is a positive integer
	default value:   0
	current value:   (default)

	For stochastic optimization algorithms, pseudorandom numbers are
	generated. Set the random seed using ranseed if you want to use a
	'deterministic' sequence of pseudorandom numbers, i.e. the same
	sequence from run to run. If ranseed is 0 (default), the seed for the
	random numbers is generated from the system time, so that you will
	get a different sequence of pseudorandom numbers each time you run
	your program.

> 
> 
> 
> 
> cleanEx()
> nameEx("sbplx")
> ### * sbplx
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sbplx
> ### Title: Subplex Algorithm
> ### Aliases: sbplx
> 
> ### ** Examples
> 
> 
> # Fletcher and Powell's helic valley
> fphv <- function(x)
+   100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 +
+     (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2
> x0 <- c(-1, 0, 0)
> sbplx(x0, fphv)  #  1 0 0
$par
[1] 1.000000e+00 3.706887e-12 5.858708e-12

$value
[1] 3.449246e-23

$iter
[1] 994

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> # Powell's Singular Function (PSF)
> psf <- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 +
+           (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4
> x0 <- c(3, -1, 0, 1)
> sbplx(x0, psf, control = list(maxeval = Inf, ftol_rel = 1e-6)) #  0 0 0 0 (?)
Warning in nloptr(x0, fn, lb = lower, ub = upper, opts = opts) :
  NAs introduced by coercion to integer range
$par
[1]  0.012385093 -0.001238441  0.007823193  0.007827134

$value
[1] 8.567466e-08

$iter
[1] 78796

$convergence
[1] 3

$message
[1] "NLOPT_FTOL_REACHED: Optimization stopped because ftol_rel or ftol_abs (above) was reached."

> 
> 
> 
> 
> cleanEx()
> nameEx("slsqp")
> ### * slsqp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: slsqp
> ### Title: Sequential Quadratic Programming (SQP)
> ### Aliases: slsqp
> 
> ### ** Examples
> 
> 
> ##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
> ##  See https://apmonitor.com/wiki/uploads/Apps/hs100.apm
> 
> x0.hs100 <- c(1, 2, 0, 4, 0, 1, 1)
> fn.hs100 <- function(x) {(x[1] - 10) ^ 2 + 5 * (x[2] - 12) ^ 2 + x[3] ^ 4 +
+                          3 * (x[4] - 11) ^ 2 + 10 * x[5] ^ 6 + 7 * x[6] ^ 2 +
+                          x[7] ^ 4 - 4 * x[6] * x[7] - 10 * x[6] - 8 * x[7]}
> 
> hin.hs100 <- function(x) {c(
+ 2 * x[1] ^ 2 + 3 * x[2] ^ 4 + x[3] + 4 * x[4] ^ 2 + 5 * x[5] - 127,
+ 7 * x[1] + 3 * x[2] + 10 * x[3] ^ 2 + x[4] - x[5] - 282,
+ 23 * x[1] + x[2] ^ 2 + 6 * x[6] ^ 2 - 8 * x[7] - 196,
+ 4 * x[1] ^ 2 + x[2] ^ 2 - 3 * x[1] * x[2] + 2 * x[3] ^ 2 + 5 * x[6] -
+  11 * x[7])
+ }
> 
> S <- slsqp(x0.hs100, fn = fn.hs100,   # no gradients and jacobians provided
+      hin = hin.hs100,
+      nl.info = TRUE,
+      control = list(xtol_rel = 1e-8, check_derivatives = TRUE),
+      deprecatedBehavior = FALSE)
Checking gradients of objective function.
Derivative checker results: 0 error(s) detected.

  eval_grad_f[1] = -1.800000e+01 ~ -1.8e+01   [ 3.023892e-10]
  eval_grad_f[2] = -1.000000e+02 ~ -1.0e+02   [ 8.540724e-14]
  eval_grad_f[3] =  0.000000e+00 ~  0.0e+00   [ 0.000000e+00]
  eval_grad_f[4] = -4.200000e+01 ~ -4.2e+01   [ 4.384556e-12]
  eval_grad_f[5] =  0.000000e+00 ~  0.0e+00   [ 0.000000e+00]
  eval_grad_f[6] = -1.877429e-08 ~  0.0e+00   [-1.877429e-08]
  eval_grad_f[7] = -8.000000e+00 ~ -8.0e+00   [ 6.102499e-10]


Checking gradients of inequality constraints.

Derivative checker results: 0 error(s) detected.

  eval_jac_g_ineq[1, 1] =  4.0e+00 ~  4.0e+00   [2.355338e-11]
  eval_jac_g_ineq[2, 1] =  7.0e+00 ~  7.0e+00   [2.278881e-10]
  eval_jac_g_ineq[3, 1] =  2.3e+01 ~  2.3e+01   [5.297235e-11]
  eval_jac_g_ineq[4, 1] =  2.0e+00 ~  2.0e+00   [1.311518e-11]
  eval_jac_g_ineq[1, 2] =  9.6e+01 ~  9.6e+01   [1.985688e-08]
  eval_jac_g_ineq[2, 2] =  3.0e+00 ~  3.0e+00   [2.191188e-10]
  eval_jac_g_ineq[3, 2] =  4.0e+00 ~  4.0e+00   [5.631432e-10]
  eval_jac_g_ineq[4, 2] =  1.0e+00 ~  1.0e+00   [4.978373e-11]
  eval_jac_g_ineq[1, 3] =  1.0e+00 ~  1.0e+00   [5.631432e-10]
  eval_jac_g_ineq[2, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[3, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[4, 3] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[1, 4] =  3.2e+01 ~  3.2e+01   [7.463696e-09]
  eval_jac_g_ineq[2, 4] =  1.0e+00 ~  1.0e+00   [2.909929e-09]
  eval_jac_g_ineq[3, 4] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[4, 4] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[1, 5] =  5.0e+00 ~  5.0e+00   [9.378596e-11]
  eval_jac_g_ineq[2, 5] = -1.0e+00 ~ -1.0e+00   [2.909929e-09]
  eval_jac_g_ineq[3, 5] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[4, 5] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[1, 6] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[2, 6] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[3, 6] =  1.2e+01 ~  1.2e+01   [1.720122e-10]
  eval_jac_g_ineq[4, 6] =  5.0e+00 ~  5.0e+00   [5.781509e-12]
  eval_jac_g_ineq[1, 7] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[2, 7] =  0.0e+00 ~  0.0e+00   [0.000000e+00]
  eval_jac_g_ineq[3, 7] = -8.0e+00 ~ -8.0e+00   [2.355338e-11]
  eval_jac_g_ineq[4, 7] = -1.1e+01 ~ -1.1e+01   [3.114761e-12]



Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    eval_g_ineq = hin, eval_jac_g_ineq = hinjac, eval_g_eq = heq, 
    eval_jac_g_eq = heqjac, opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped because 
xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 60 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  4 
Number of equality constraints:    0 
Optimal value of objective function:  680.630057364075 
Optimal value of controls: 2.330497 1.951371 -0.4775421 4.36573 -0.6244872 1.038139 1.594228


> 
> ##  The optimum value of the objective function should be 680.6300573
> ##  A suitable parameter vector is roughly
> ##  (2.330, 1.9514, -0.4775, 4.3657, -0.6245, 1.0381, 1.5942)
> 
> S
$par
[1]  2.3304967  1.9513713 -0.4775421  4.3657298 -0.6244872  1.0381386  1.5942275

$value
[1] 680.6301

$iter
[1] 60

$convergence
[1] 4

$message
[1] "NLOPT_XTOL_REACHED: Optimization stopped because xtol_rel or xtol_abs (above) was reached."

> 
> 
> 
> 
> cleanEx()
> nameEx("stogo")
> ### * stogo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: stogo
> ### Title: Stochastic Global Optimization
> ### Aliases: stogo
> 
> ### ** Examples
> 
> 
> ## Rosenbrock Banana objective function
> 
> rbf <- function(x) {(1 - x[1]) ^ 2 + 100 * (x[2] - x[1] ^ 2) ^ 2}
> 
> x0 <- c(-1.2, 1)
> lb <- c(-3, -3)
> ub <- c(3,  3)
> 
> ## The function as written above has a minimum of 0 at (1, 1)
> 
> stogo(x0 = x0, fn = rbf, lower = lb, upper = ub)
$par
[1] 0.9999934 0.9999865

$value
[1] 5.618383e-11

$iter
[1] 10000

$convergence
[1] 1

$message
[1] "NLOPT_SUCCESS: Generic success return value."

> 
> 
> 
> 
> cleanEx()
> nameEx("tnewton")
> ### * tnewton
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tnewton
> ### Title: Preconditioned Truncated Newton
> ### Aliases: tnewton
> 
> ### ** Examples
> 
> 
> flb <- function(x) {
+   p <- length(x)
+   sum(c(1, rep(4, p - 1)) * (x - c(1, x[-p]) ^ 2) ^ 2)
+ }
> # 25-dimensional box constrained: par[24] is *not* at boundary
> S <- tnewton(rep(3, 25L), flb, lower = rep(2, 25L), upper = rep(4, 25L),
+        nl.info = TRUE, control = list(xtol_rel = 1e-8))

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. )

Number of Iterations....: 17 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  368.105912874334 
Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4


> ## Optimal value of objective function:  368.105912874334
> ## Optimal value of controls: 2  ...  2  2.109093  4
> 
> 
> 
> 
> cleanEx()
> nameEx("varmetric")
> ### * varmetric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varmetric
> ### Title: Shifted Limited-memory Variable-metric
> ### Aliases: varmetric
> 
> ### ** Examples
> 
> 
> flb <- function(x) {
+   p <- length(x)
+   sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2)
+ }
> # 25-dimensional box constrained: par[24] is *not* at the boundary
> S <- varmetric(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),
+      nl.info = TRUE, control = list(xtol_rel=1e-8))

Call:
nloptr(x0 = x0, eval_f = fn, eval_grad_f = gr, lb = lower, ub = upper, 
    opts = opts)


Minimization using NLopt version 2.7.1 

NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success return value. )

Number of Iterations....: 19 
Termination conditions:  stopval: -Inf	xtol_rel: 1e-08	maxeval: 1000	ftol_rel: 0	ftol_abs: 0 
Number of inequality constraints:  0 
Number of equality constraints:    0 
Optimal value of objective function:  368.105912874334 
Optimal value of controls: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2.109093 4


> ## Optimal value of objective function:  368.105912874334
> ## Optimal value of controls: 2  ...  2  2.109093  4
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  0.899 0.016 0.915 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
