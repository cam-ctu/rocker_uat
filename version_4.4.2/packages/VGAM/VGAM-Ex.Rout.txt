
R version 4.4.2 (2024-10-31) -- "Pile of Leaves"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "VGAM"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('VGAM')
Loading required package: stats4
Loading required package: splines
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("A1A2A3")
> ### * A1A2A3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: A1A2A3
> ### Title: The A1A2A3 Blood Group System
> ### Aliases: A1A2A3
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- cbind(108, 196, 429, 143, 513, 559)
> fit <- vglm(ymat ~ 1, A1A2A3(link = probitlink), trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -1.04223062, -0.19394454
VGLM    linear loop  2 :  coefficients = -1.06897654, -0.24733346
VGLM    linear loop  3 :  coefficients = -1.06935967, -0.24763763
VGLM    linear loop  4 :  coefficients = -1.06935975, -0.24763764
> fit <- vglm(ymat ~ 1, A1A2A3(link = logitlink, ip1 = 0.3, ip2 = 0.3, iF = 0.02),
+             trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -1.59751787, -0.36059599
VGLM    linear loop  2 :  coefficients = -1.78234702, -0.39615909
VGLM    linear loop  3 :  coefficients = -1.79499991, -0.39627603
VGLM    linear loop  4 :  coefficients = -1.79505733, -0.39627603
VGLM    linear loop  5 :  coefficients = -1.79505733, -0.39627603
> Coef(fit)  # Estimated p1 and p2
       p1        p2 
0.1424538 0.4022074 
> rbind(ymat, sum(ymat) * fitted(fit))
       A1A1     A1A2    A1A3     A2A2     A2A3     A3A3
  108.00000 196.0000 429.000 143.0000 513.0000 559.0000
1  39.53093 223.2251 252.713 315.1295 713.5159 403.8855
> sqrt(diag(vcov(fit)))
(Intercept):1 (Intercept):2 
   0.04583790    0.03267309 
> 
> 
> 
> cleanEx()
> nameEx("AA.Aa.aa")
> ### * AA.Aa.aa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AA.Aa.aa
> ### Title: The AA-Aa-aa Blood Group System
> ### Aliases: AA.Aa.aa
> ### Keywords: models regression
> 
> ### ** Examples
> 
> y <- cbind(53, 95, 38)
> fit1 <- vglm(y ~ 1, AA.Aa.aa, trace = TRUE)
VGLM    linear loop  1 :  deviance = 0.1478917
VGLM    linear loop  2 :  deviance = 0.1478917
VGLM    linear loop  3 :  deviance = 0.1478917
Taking a modified step...
VGLM    linear loop  3 :  deviance = 0.1478917
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
> fit2 <- vglm(y ~ 1, AA.Aa.aa(inbreeding = TRUE), trace = TRUE)
VGLM    linear loop  1 :  deviance = 2.915934
VGLM    linear loop  2 :  deviance = 0.5269085
VGLM    linear loop  3 :  deviance = 0.179768
VGLM    linear loop  4 :  deviance = 0.1478923
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  1 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  5 :  deviance = 0.1478917
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  1 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  6 :  deviance = 0.1478917
Taking a modified step.
VGLM    linear loop  6 :  deviance = 0.1478917
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
> rbind(y, sum(y) * fitted(fit1))
        AA       Aa       aa
  53.00000 95.00000 38.00000
1 54.30242 92.39516 39.30242
> Coef(fit1)  # Estimated pA
       pA 
0.5403226 
> Coef(fit2)  # Estimated pA and f
       pA         f 
0.5403226 0.0000000 
> summary(fit1)
Call:
vglm(formula = y ~ 1, family = AA.Aa.aa, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.1616     0.1040   1.554     0.12

Name of linear predictor: logitlink(pA) 

Residual deviance: 0.1479 on 0 degrees of freedom

Log-likelihood: -5.384 on 0 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("AB.Ab.aB.ab")
> ### * AB.Ab.aB.ab
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AB.Ab.aB.ab
> ### Title: The AB-Ab-aB-ab Blood Group System
> ### Aliases: AB.Ab.aB.ab
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- cbind(AB=1997, Ab=906, aB=904, ab=32)  # Data from Fisher (1925)
> fit <- vglm(ymat ~ 1, AB.Ab.aB.ab(link = "identitylink"), trace = TRUE)
VGLM    linear loop  1 :  deviance = 2.019143
VGLM    linear loop  2 :  deviance = 2.018723
VGLM    linear loop  3 :  deviance = 2.018721
VGLM    linear loop  4 :  deviance = 2.018721
> fit <- vglm(ymat ~ 1, AB.Ab.aB.ab, trace = TRUE)
VGLM    linear loop  1 :  deviance = 2.0189
VGLM    linear loop  2 :  deviance = 2.018722
VGLM    linear loop  3 :  deviance = 2.018721
VGLM    linear loop  4 :  deviance = 2.018721
> rbind(ymat, sum(ymat)*fitted(fit))
        AB       Ab       aB       ab
  1997.000 906.0000 904.0000 32.00000
1 1953.775 925.4751 925.4751 34.27486
> Coef(fit)  # Estimated p
        p 
0.1889769 
> p <- sqrt(4*(fitted(fit)[, 4]))
> p*p
[1] 0.03571228
> summary(fit)
Call:
vglm(formula = ymat ~ 1, family = AB.Ab.aB.ab, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.45667    0.05039  -28.91   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logitlink(p) 

Residual deviance: 2.0187 on 0 degrees of freedom

Log-likelihood: -11.983 on 0 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept)'

> 
> 
> 
> cleanEx()
> nameEx("ABO")
> ### * ABO
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ABO
> ### Title: The ABO Blood Group System
> ### Aliases: ABO
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- cbind(A = 725, B = 258, AB = 72, O = 1073)  # Order matters, not the name
> fit <- vglm(ymat ~ 1, ABO(link.pA = "identitylink",
+                           link.pB = "identitylink"), trace = TRUE,
+             crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.209130654, 0.080801008
VGLM    linear loop  2 :  coefficients = 0.209130655, 0.080801008
> coef(fit, matrix = TRUE)
                   pA         pB
(Intercept) 0.2091307 0.08080101
> Coef(fit)  # Estimated pA and pB
        pA         pB 
0.20913065 0.08080101 
> rbind(ymat, sum(ymat) * fitted(fit))
         A       B       AB        O
  725.0000 258.000 72.00000 1073.000
1 725.0729 258.078 71.91775 1072.931
> sqrt(diag(vcov(fit)))
(Intercept):1 (Intercept):2 
  0.006628726   0.004267233 
> 
> 
> 
> cleanEx()
> nameEx("AICvlm")
> ### * AICvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AICvlm
> ### Title: Akaike's Information Criterion
> ### Aliases: AICvlm AICvgam AICrrvglm AICdrrvglm AICqrrvglm AICrrvgam
> ###   AICc,vglm-method
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = TRUE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> coef(fit1, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> AIC(fit1)
[1] 56.18052
> AICc(fit1)  # Quick way
[1] 62.18052
> AIC(fit1, corrected = TRUE)  # Slow way
[1] 62.18052
> (fit2 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = FALSE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = FALSE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
    -9.593308    -11.104791      2.571300      2.743550 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.884404 
Log-likelihood: -25.01905 
> coef(fit2, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.593308          -11.10479
let                   2.571300            2.74355
> AIC(fit2)
[1] 58.0381
> AICc(fit2)
[1] 71.37144
> AIC(fit2, corrected = TRUE)
[1] 71.37144
> 
> 
> 
> cleanEx()
> nameEx("AR1")
> ### * AR1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AR1
> ### Title: Autoregressive Process with Order-1 Family Function
> ### Aliases: AR1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ### Example 1: using  arima.sim() to generate a 0-mean stationary time series.
> ##D nn <- 500
> ##D tsdata <- data.frame(x2 =  runif(nn))
> ##D ar.coef.1 <- rhobitlink(-1.55, inverse = TRUE)  # Approx -0.65
> ##D ar.coef.2 <- rhobitlink( 1.0, inverse = TRUE)   # Approx  0.50
> ##D set.seed(1)
> ##D tsdata  <- transform(tsdata,
> ##D               index = 1:nn,
> ##D               TS1 = arima.sim(nn, model = list(ar = ar.coef.1),
> ##D                               sd = exp(1.5)),
> ##D               TS2 = arima.sim(nn, model = list(ar = ar.coef.2),
> ##D                               sd = exp(1.0 + 1.5 * x2)))
> ##D 
> ##D ### An autoregressive intercept--only model.   ###
> ##D ### Using the exact EIM, and "nodrift = TRUE"  ###
> ##D fit1a <- vglm(TS1 ~ 1, data = tsdata, trace = TRUE,
> ##D               AR1(var.arg = FALSE, nodrift = TRUE,
> ##D                   type.EIM = "exact",
> ##D                   print.EIM = FALSE),
> ##D               crit = "coefficients")
> ##D Coef(fit1a)
> ##D summary(fit1a)
> ##D 
> ##D ### Two responses. Here, the white noise standard deviation of TS2   ###
> ##D ### is modelled in terms of 'x2'. Also, 'type.EIM = exact'.  ###
> ##D fit1b <- vglm(cbind(TS1, TS2) ~ x2,
> ##D               AR1(zero = NULL, nodrift = TRUE,
> ##D                   var.arg = FALSE,
> ##D                   type.EIM = "exact"),
> ##D               constraints = list("(Intercept)" = diag(4),
> ##D                                  "x2" = rbind(0, 0, 1, 0)),
> ##D               data = tsdata, trace = TRUE, crit = "coefficients")
> ##D coef(fit1b, matrix = TRUE)
> ##D summary(fit1b)
> ##D 
> ##D ### Example 2: another stationary time series
> ##D nn     <- 500
> ##D my.rho <- rhobitlink(1.0, inverse = TRUE)
> ##D my.mu  <- 1.0
> ##D my.sd  <- exp(1)
> ##D tsdata  <- data.frame(index = 1:nn, TS3 = runif(nn))
> ##D 
> ##D set.seed(2)
> ##D for (ii in 2:nn)
> ##D   tsdata$TS3[ii] <- my.mu/(1 - my.rho) +
> ##D                     my.rho * tsdata$TS3[ii-1] + rnorm(1, sd = my.sd)
> ##D tsdata <- tsdata[-(1:ceiling(nn/5)), ]  # Remove the burn-in data:
> ##D 
> ##D ### Fitting an AR(1). The exact EIMs are used.
> ##D fit2a <- vglm(TS3 ~ 1, AR1(type.likelihood = "exact",  # "conditional",
> ##D                                 type.EIM = "exact"),
> ##D               data = tsdata, trace = TRUE, crit = "coefficients")
> ##D 
> ##D Coef(fit2a)
> ##D summary(fit2a)      # SEs are useful to know
> ##D 
> ##D Coef(fit2a)["rho"]    # Estimate of rho, for intercept-only models
> ##D my.rho                # The 'truth' (rho)
> ##D Coef(fit2a)["drift"]  # Estimate of drift, for intercept-only models
> ##D my.mu /(1 - my.rho)   # The 'truth' (drift)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("AR1EIM")
> ### * AR1EIM
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AR1EIM
> ### Title: Computation of the Exact EIM of an Order-1 Autoregressive
> ###   Process
> ### Aliases: AR1EIM
> 
> ### ** Examples
> 
>   set.seed(1)
>   nn <- 500
>   ARcoeff1 <- c(0.3, 0.25)        # Will be recycled.
>   WNsd     <- c(exp(1), exp(1.5)) # Will be recycled.
>   p.drift  <- c(0, 0)             # Zero-mean gaussian time series.
> 
>   ### Generate two (zero-mean) AR(1) processes ###
>   ts1 <- p.drift[1]/(1 - ARcoeff1[1]) +
+                    arima.sim(model = list(ar = ARcoeff1[1]), n = nn,
+                    sd = WNsd[1])
>   ts2 <- p.drift[2]/(1 - ARcoeff1[2]) +
+                    arima.sim(model = list(ar = ARcoeff1[2]), n = nn,
+                    sd = WNsd[2])
> 
>   ARdata <- matrix(cbind(ts1, ts2), ncol = 2)
> 
> 
>   ### Compute the exact EIMs: TWO responses. ###
>   ExactEIM <- AR1EIM(x = ARdata, var.arg = FALSE, p.drift = p.drift,
+                            WNsd = WNsd, ARcoeff1 = ARcoeff1)
> 
>   ### For response 1:
>   head(ExactEIM[, 1 ,])      # NOTICE THAT THIS IS A (nn x 6) MATRIX!
          [,1]      [,2]      [,3] [,4]       [,5] [,6]
[1,] 0.1353353 0.2661579 0.2137411    0 0.00000000    0
[2,] 0.1416722 0.2586982 1.1004563    0 0.03907790    0
[3,] 0.1448187 0.2589088 1.1278822    0 0.04040631    0
[4,] 0.1580258 0.2619689 1.1706908    0 0.04447967    0
[5,] 0.1580258 0.2619689 1.1706908    0 0.04447967    0
[6,] 0.1580258 0.2619689 1.1706908    0 0.04447967    0
> 
>   ### For response 2:
>   head(ExactEIM[, 2 ,])      # NOTICE THAT THIS IS A (nn x 6) MATRIX!
           [,1]       [,2]      [,3] [,4]       [,5] [,6]
[1,] 0.04978707 0.08745090 0.1249065    0 0.00000000    0
[2,] 0.06335618 0.08377299 0.9958541    0 0.06589681    0
[3,] 0.07087537 0.08573583 1.1436757    0 0.07824984    0
[4,] 0.07562382 0.08664118 1.2082894    0 0.08589809    0
[5,] 0.08745281 0.09021526 1.3012102    0 0.10003970    0
[6,] 0.08745281 0.09021526 1.3012102    0 0.10003970    0
> 
> 
> 
> cleanEx()
> nameEx("AR1UC")
> ### * AR1UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dAR1
> ### Title: The AR-1 Autoregressive Process
> ### Aliases: dAR1 dAR1
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D nn <- 100; set.seed(1)
> ##D tdata <- data.frame(index = 1:nn,
> ##D                     TS1 = arima.sim(nn, model = list(ar = -0.50),
> ##D                                     sd = exp(1)))
> ##D fit1 <- vglm(TS1 ~ 1, AR1, data = tdata, trace = TRUE)
> ##D rhobitlink(-0.5)
> ##D coef(fit1, matrix = TRUE)
> ##D (Cfit1 <- Coef(fit1))
> ##D summary(fit1)  # SEs are useful to know
> ##D logLik(fit1)
> ##D sum(dAR1(depvar(fit1), drift = Cfit1[1], var.error = (Cfit1[2])^2,
> ##D          ARcoef1 = Cfit1[3], log = TRUE))
> ##D 
> ##D fit2 <- vglm(TS1 ~ 1, AR1(type.likelihood = "cond"), data = tdata, trace = TRUE)
> ##D (Cfit2 <- Coef(fit2))  # Okay for intercept-only models
> ##D logLik(fit2)
> ##D head(keep <- dAR1(depvar(fit2), drift = Cfit2[1], var.error = (Cfit2[2])^2,
> ##D                   ARcoef1 = Cfit2[3], type.likelihood = "cond", log = TRUE))
> ##D sum(keep[-1])
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("BICvlm")
> ### * BICvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BICvlm
> ### Title: Bayesian Information Criterion
> ### Aliases: BICvlm BICvgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = TRUE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> coef(fit1, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> BIC(fit1)
[1] 56.41885
> (fit2 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = FALSE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = FALSE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
    -9.593308    -11.104791      2.571300      2.743550 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.884404 
Log-likelihood: -25.01905 
> coef(fit2, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.593308          -11.10479
let                   2.571300            2.74355
> BIC(fit2)
[1] 58.35587
> 
> 
> 
> cleanEx()
> nameEx("CM.equid")
> ### * CM.equid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: CM.equid
> ### Title: Constraint Matrices for Symmetry, Order, Parallelism, etc.
> ### Aliases: CM.equid CM.free CM.ones CM.symm0 CM.symm1 CM.qnorm CM.qlogis
> ### Keywords: models utilities
> 
> ### ** Examples
> 
> CM.equid(4)
     [,1] [,2]
[1,]    1    0
[2,]    1    1
[3,]    1    2
[4,]    1    3
> CM.equid(4, Trev = TRUE, Tref = 3)
     [,1] [,2]
[1,]    1    2
[2,]    1    1
[3,]    1    0
[4,]    1   -1
> CM.symm1(5)
     [,1] [,2] [,3]
[1,]    1    0   -1
[2,]    1   -1    0
[3,]    1    0    0
[4,]    1    1    0
[5,]    1    0    1
> CM.symm0(5)
     [,1] [,2]
[1,]    0   -1
[2,]   -1    0
[3,]    0    0
[4,]    1    0
[5,]    0    1
> CM.qnorm(5)
           [,1]
[1,] -0.9674216
[2,] -0.4307273
[3,]  0.0000000
[4,]  0.4307273
[5,]  0.9674216
> 
> 
> 
> cleanEx()
> nameEx("Coef")
> ### * Coef
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef
> ### Title: Computes Model Coefficients and Quantities
> ### Aliases: Coef
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> bdata <- data.frame(y = rbeta(nn, shape1 = 1, shape2 = 3))  # Original scale
> fit <- vglm(y ~ 1, betaR, data = bdata, trace = TRUE)  # Intercept-only model
VGLM    linear loop  1 :  loglikelihood = 435.53903
VGLM    linear loop  2 :  loglikelihood = 435.5517
VGLM    linear loop  3 :  loglikelihood = 435.5517
> coef(fit, matrix = TRUE)  # Both on a log scale
            loglink(shape1) loglink(shape2)
(Intercept)     -0.04893505        1.061287
> Coef(fit)  # On the original scale
  shape1   shape2 
0.952243 2.890089 
> 
> 
> 
> cleanEx()
> nameEx("Coef.qrrvglm-class")
> ### * Coef.qrrvglm-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef.qrrvglm-class
> ### Title: Class "Coef.qrrvglm"
> ### Aliases: Coef.qrrvglm-class
> ### Keywords: classes nonlinear
> 
> ### ** Examples
> 
> x2 <- rnorm(n <- 100)
> x3 <- rnorm(n)
> x4 <- rnorm(n)
> latvar1 <- 0 + x3 - 2*x4
> lambda1 <- exp(3 - 0.5 * ( latvar1-0)^2)
> lambda2 <- exp(2 - 0.5 * ( latvar1-1)^2)
> lambda3 <- exp(2 - 0.5 * ((latvar1+4)/2)^2)
> y1 <- rpois(n, lambda1)
> y2 <- rpois(n, lambda2)
> y3 <- rpois(n, lambda3)
> yy <- cbind(y1, y2, y3)
> # vvv p1 <- cqo(yy ~ x2 + x3 + x4, fam = poissonff, trace = FALSE)
> ## Not run: 
> ##D lvplot(p1, y = TRUE, lcol = 1:3, pch = 1:3, pcol = 1:3)
> ## End(Not run)
> # vvv print(Coef(p1), digits = 3)
> 
> 
> 
> cleanEx()
> nameEx("Coef.qrrvglm")
> ### * Coef.qrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef.qrrvglm
> ### Title: Returns Important Matrices etc. of a QO Object
> ### Aliases: Coef.qrrvglm
> ### Keywords: models nonlinear regression
> 
> ### ** Examples
> 
> set.seed(123)
> x2 <- rnorm(n <- 100)
> x3 <- rnorm(n)
> x4 <- rnorm(n)
> latvar1 <- 0 + x3 - 2*x4
> lambda1 <- exp(3 - 0.5 * ( latvar1-0)^2)
> lambda2 <- exp(2 - 0.5 * ( latvar1-1)^2)
> lambda3 <- exp(2 - 0.5 * ((latvar1+4)/2)^2)  # Unequal tolerances
> y1 <- rpois(n, lambda1)
> y2 <- rpois(n, lambda2)
> y3 <- rpois(n, lambda3)
> set.seed(111)
> # vvv p1 <- cqo(cbind(y1, y2, y3) ~ x2 + x3 + x4, poissonff, trace = FALSE)
> ## Not run:  lvplot(p1, y = TRUE, lcol = 1:3, pch = 1:3, pcol = 1:3)
> 
> # vvv Coef(p1)
> # vvv print(Coef(p1), digits=3)
> 
> 
> 
> cleanEx()
> nameEx("Coef.rrvglm-class")
> ### * Coef.rrvglm-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef.rrvglm-class
> ### Title: Class "Coef.rrvglm"
> ### Aliases: Coef.rrvglm-class
> ### Keywords: classes
> 
> ### ** Examples
> 
> # Rank-1 stereotype model of Anderson (1984)
> pneumo <- transform(pneumo, let = log(exposure.time), x3 = runif(nrow(pneumo)))
> fit <- rrvglm(cbind(normal, mild, severe) ~ let + x3, multinomial, data = pneumo)
> coef(fit, matrix = TRUE)
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)        11.98357706         3.03133603
let                -3.07754828        -0.90215166
x3                  0.04021543         0.01178874
> Coef(fit)
A matrix:
                      latvar
log(mu[,1]/mu[,3]) 1.0000000
log(mu[,2]/mu[,3]) 0.2931397

C matrix:
         latvar
let -3.07754828
x3   0.04021543

B1 matrix:
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)           11.98358           3.031336
> # print(Coef(fit), digits = 3)
> 
> 
> 
> cleanEx()
> nameEx("Coef.rrvglm")
> ### * Coef.rrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef.rrvglm
> ### Title: Returns Important Matrices etc. of a RR-VGLM Object
> ### Aliases: Coef.rrvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Rank-1 stereotype model of Anderson (1984)
> pneumo <- transform(pneumo, let = log(exposure.time), x3 = runif(nrow(pneumo)))
> fit <- rrvglm(cbind(normal, mild, severe) ~ let + x3, multinomial, data = pneumo)
> coef(fit, matrix = TRUE)
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)        11.98357706         3.03133603
let                -3.07754828        -0.90215166
x3                  0.04021543         0.01178874
> Coef(fit)
A matrix:
                      latvar
log(mu[,1]/mu[,3]) 1.0000000
log(mu[,2]/mu[,3]) 0.2931397

C matrix:
         latvar
let -3.07754828
x3   0.04021543

B1 matrix:
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)           11.98358           3.031336
> 
> 
> 
> cleanEx()
> nameEx("Coef.vlm")
> ### * Coef.vlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Coef.vlm
> ### Title: Extract Model Coefficients for VLM Objects
> ### Aliases: Coef.vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(123); nn <- 1000
> bdata <- data.frame(y = rbeta(nn, shape1 = 1, shape2 = 3))
> fit <- vglm(y ~ 1, betaff, data = bdata, trace = TRUE)  # intercept-only model
VGLM    linear loop  1 :  loglikelihood = 459.82736
VGLM    linear loop  2 :  loglikelihood = 460.06832
VGLM    linear loop  3 :  loglikelihood = 460.06838
VGLM    linear loop  4 :  loglikelihood = 460.06838
> coef(fit, matrix = TRUE)  # log scale
            logitlink(mu) loglink(phi)
(Intercept)     -1.144202     1.391377
> Coef(fit)  # On the original scale
       mu       phi 
0.2415498 4.0203818 
> 
> 
> 
> cleanEx()
> nameEx("CommonVGAMffArguments")
> ### * CommonVGAMffArguments
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: CommonVGAMffArguments
> ### Title: Common VGAM Family Function Arguments
> ### Aliases: CommonVGAMffArguments TypicalVGAMfamilyFunction
> ### Keywords: models
> 
> ### ** Examples
> 
> # Example 1
> cumulative()
Family:  cumulative 
Informal classes: cumulative, VGAMordinal, VGAMcategorical 

Cumulative logitlink model

Links:   logitlink(P[Y<=j])
> cumulative(link = "probitlink", reverse = TRUE, parallel = TRUE)
Family:  cumulative 
Informal classes: cumulative, VGAMordinal, VGAMcategorical 

Cumulative probitlink model

Links:   probitlink(P[Y>=j+1])
> 
> # Example 2
> wdata <- data.frame(x2 = runif(nn <- 1000))
> wdata <- transform(wdata,
+          y = rweibull(nn, shape = 2 + exp(1 + x2), scale = exp(-0.5)))
> fit <- vglm(y ~ x2, weibullR(lshape = logofflink(offset = -2), zero = 2),
+             data = wdata)
> coef(fit, mat = TRUE)
            loglink(scale) logofflink(shape, offset = -2)
(Intercept)    -0.48196683                       1.456728
x2             -0.02253987                       0.000000
> 
> # Example 3; multivariate (multiple) response
> ## Not run: 
> ##D ndata <- data.frame(x = runif(nn <- 500))
> ##D ndata <- transform(ndata,
> ##D            y1 = rnbinom(nn, exp(1), mu = exp(3+x)),  # k is size
> ##D            y2 = rnbinom(nn, exp(0), mu = exp(2-x)))
> ##D fit <- vglm(cbind(y1, y2) ~ x, negbinomial(zero = -2), ndata)
> ##D coef(fit, matrix = TRUE)
> ## End(Not run)
> # Example 4
> ## Not run: 
> ##D # fit1 and fit2 are equivalent
> ##D fit1 <- vglm(ymatrix ~ x2 + x3 + x4 + x5,
> ##D              cumulative(parallel = FALSE ~ 1 + x3 + x5), cdata)
> ##D fit2 <- vglm(ymatrix ~ x2 + x3 + x4 + x5,
> ##D              cumulative(parallel = TRUE ~ x2 + x4), cdata)
> ## End(Not run)
> 
> # Example 5
> udata <- data.frame(x2 = rnorm(nn <- 200))
> udata <- transform(udata,
+            x1copy = 1,  # Copy of the intercept
+            x3 = runif(nn),
+            y1 = rnorm(nn, 1 - 3*x2, sd = exp(1 + 0.2*x2)),
+            y2 = rnorm(nn, 1 - 3*x2, sd = exp(1)))
> args(uninormal)
function (lmean = "identitylink", lsd = "loglink", lvar = "loglink", 
    var.arg = FALSE, imethod = 1, isd = NULL, parallel = FALSE, 
    vfl = FALSE, Form2 = NULL, smallno = 1e-05, zero = if (var.arg) "var" else "sd") 
NULL
> fit1 <- vglm(y1 ~ x2, uninormal, udata)            # This is okay
> fit2 <- vglm(y2 ~ x2, uninormal(zero = 2), udata)  # This is okay
> fit4 <- vglm(y2 ~ x2 + x1copy + x3,
+              uninormal(zero = NULL, vfl = TRUE,
+                        Form2 = ~ x1copy + x3 - 1), udata)
> coef(fit4,  matrix = TRUE)  # VFL model
                  mean loglink(sd)
(Intercept)  0.8652016  0.00000000
x2          -2.7897932  0.00000000
x1copy       0.0000000  0.99107307
x3           0.0000000  0.05165398
> 
> # This creates potential conflict
> clist <- list("(Intercept)" = diag(2), "x2" = diag(2))
> fit3 <- vglm(y2 ~ x2, uninormal(zero = 2), data = udata,
+              constraints = clist)  # Conflict!
> coef(fit3, matrix = TRUE)  # Shows that clist[["x2"]] was overwritten,
                 mean loglink(sd)
(Intercept)  0.861629    1.018856
x2          -2.788635    0.000000
> constraints(fit3)  # i.e., 'zero' seems to override the 'constraints' arg
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$x2
     [,1]
[1,]    1
[2,]    0

> 
> # Example 6 ('whitespace' argument)
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let,
+              sratio(whitespace = FALSE, parallel = TRUE), pneumo)
> fit2 <- vglm(cbind(normal, mild, severe) ~ let,
+              sratio(whitespace = TRUE,  parallel = TRUE), pneumo)
> head(predict(fit1), 2)  # No white spaces
  logitlink(P[Y=1|Y>=1]) logitlink(P[Y=2|Y>=2])
1               4.653177               3.970682
2               2.447440               1.764945
> head(predict(fit2), 2)  # Uses white spaces
  logitlink(P[Y = 1|Y >= 1]) logitlink(P[Y = 2|Y >= 2])
1                   4.653177                   3.970682
2                   2.447440                   1.764945
> 
> # Example 7 ('zero' argument with character input)
> set.seed(123); n <- 1000
> ldata <- data.frame(x2 = runif(n))
> ldata <- transform(ldata, y1 = rlogis(n, loc = 5*x2, scale = exp(2)))
> ldata <- transform(ldata, y2 = rlogis(n, loc = 5*x2, scale = exp(1*x2)))
> ldata <- transform(ldata, w1 = runif(n))
> ldata <- transform(ldata, w2 = runif(n))
> fit7 <- vglm(cbind(y1, y2) ~ x2,
+ #        logistic(zero = "location1"),  # location1 is intercept-only
+ #        logistic(zero = "location2"),
+ #        logistic(zero = "location*"),  # Not okay... all is unmatched
+ #        logistic(zero = "scale1"),
+ #        logistic(zero = "scale2"),
+ #        logistic(zero = "scale"),  # Both scale parameters are matched
+          logistic(zero = c("location", "scale2")),  # All but scale1
+ #        logistic(zero = c("LOCAT", "scale2")),  # Only scale2 is matched
+ #        logistic(zero = c("LOCAT")),  # Nothing is matched
+ #        trace = TRUE,
+ #        weights = cbind(w1, w2),
+          weights = w1,
+          data = ldata)
> coef(fit7, matrix = TRUE)
            location1 loglink(scale1) location2 loglink(scale2)
(Intercept)  2.724712       1.9049773  2.206739       0.6750725
x2           0.000000       0.1538263  0.000000       0.0000000
> 
> 
> 
> cleanEx()
> nameEx("Huggins89.t1")
> ### * Huggins89.t1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Huggins89.t1
> ### Title: Table 1 of Huggins (1989)
> ### Aliases: Huggins89.t1 Huggins89table1
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D Huggins89table1 <-
> ##D   transform(Huggins89table1, x3.tij = t01,
> ##D             T02 = t02, T03 = t03, T04 = t04, T05 = t05, T06 = t06,
> ##D             T07 = t07, T08 = t08, T09 = t09, T10 = t10)
> ##D small.table1 <- subset(Huggins89table1,
> ##D   y01 + y02 + y03 + y04 + y05 + y06 + y07 + y08 + y09 + y10 > 0)
> ##D # fit.tbh is the bottom equation on p.133.
> ##D # It is a M_tbh model.
> ##D fit.tbh <-
> ##D   vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~
> ##D        x2 + x3.tij,
> ##D        xij = list(x3.tij ~ t01 + t02 + t03 + t04 + t05 +
> ##D                            t06 + t07 + t08 + t09 + t10 +
> ##D                            T02 + T03 + T04 + T05 + T06 +
> ##D                            T07 + T08 + T09 + T10 - 1),
> ##D        posbernoulli.tb(parallel.t = TRUE ~ x2 + x3.tij),
> ##D        data = small.table1, trace = TRUE,
> ##D        form2 = ~  x2 + x3.tij +
> ##D                   t01 + t02 + t03 + t04 + t05 + t06 +
> ##D                   t07 + t08 + t09 + t10 +
> ##D                   T02 + T03 + T04 + T05 + T06 +
> ##D                   T07 + T08 + T09 + T10)
> ##D 
> ##D # These results differ a bit from Huggins (1989), probably because
> ##D # two animals had to be removed here (they were never caught):
> ##D coef(fit.tbh)  # First element is the behavioural effect
> ##D sqrt(diag(vcov(fit.tbh)))  # SEs
> ##D constraints(fit.tbh, matrix = TRUE)
> ##D summary(fit.tbh, presid = FALSE)
> ##D fit.tbh@extra$N.hat     # Estimate of the population site N; cf. 20.86
> ##D fit.tbh@extra$SE.N.hat  # Its standard error; cf. 1.87 or 4.51
> ##D 
> ##D fit.th <-
> ##D   vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~ x2,
> ##D        posbernoulli.t, data = small.table1, trace = TRUE)
> ##D coef(fit.th)
> ##D constraints(fit.th)
> ##D coef(fit.th, matrix = TRUE)  # M_th model
> ##D summary(fit.th, presid = FALSE)
> ##D fit.th@extra$N.hat     # Estimate of the population size N
> ##D fit.th@extra$SE.N.hat  # Its standard error
> ##D 
> ##D fit.bh <-
> ##D   vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~ x2,
> ##D        posbernoulli.b(I2 = FALSE), data = small.table1, trace = TRUE)
> ##D coef(fit.bh)
> ##D constraints(fit.bh)
> ##D coef(fit.bh, matrix = TRUE)  # M_bh model
> ##D summary(fit.bh, presid = FALSE)
> ##D fit.bh@extra$N.hat
> ##D fit.bh@extra$SE.N.hat
> ##D 
> ##D fit.h <-
> ##D   vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~ x2,
> ##D        posbernoulli.b, data = small.table1, trace = TRUE)
> ##D coef(fit.h, matrix = TRUE)  # M_h model (version 1)
> ##D coef(fit.h)
> ##D summary(fit.h, presid = FALSE)
> ##D fit.h@extra$N.hat
> ##D fit.h@extra$SE.N.hat
> ##D 
> ##D Fit.h <-
> ##D   vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~ x2,
> ##D        posbernoulli.t(parallel.t = TRUE ~ x2),
> ##D        data = small.table1, trace = TRUE)
> ##D coef(Fit.h)
> ##D coef(Fit.h, matrix = TRUE)  # M_h model (version 2)
> ##D summary(Fit.h, presid = FALSE)
> ##D Fit.h@extra$N.hat
> ##D Fit.h@extra$SE.N.hat
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("Influence")
> ### * Influence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Influence
> ### Title: Influence Function (S4 generic) of a Fitted Model
> ### Aliases: Influence Influence.vglm
> ### Keywords: models
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, acat, data = pneumo)
> coef(fit)  # 8-vector
(Intercept):1 (Intercept):2         let:1         let:2 
   -8.9360297    -3.0390622     2.1653729     0.9020936 
> Influence(fit)  # 8 x 4
  (Intercept):1 (Intercept):2       let:1        let:2
1   -0.73827808    0.35415869  0.20987191 -0.100483419
2   -0.20653006   -0.08847618  0.05691954  0.024135324
3    0.68701036   -0.53528307 -0.17723146  0.138551572
4   -0.14332598    0.72736516  0.03367589 -0.182188428
5   -0.01184705   -0.07833113  0.01542707  0.006407492
6    0.19287499    0.13332817 -0.06828049 -0.045333144
7    0.05572438   -0.15655193 -0.01821525  0.049445161
8    0.16437144   -0.35620972 -0.05216720  0.109465443
> all(abs(colSums(Influence(fit))) < 1e-6)  # TRUE
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("Inv.gaussian")
> ### * Inv.gaussian
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Inv.gaussian
> ### Title: The Inverse Gaussian Distribution
> ### Aliases: Inv.gaussian dinv.gaussian pinv.gaussian rinv.gaussian
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(-0.05, 4, len = 300)
> ##D plot(x, dinv.gaussian(x, mu = 1, lambda = 1), type = "l",
> ##D      col = "blue",las = 1, main =
> ##D      "blue is density, orange is cumulative distribution function")
> ##D abline(h = 0, col = "gray", lty = 2)
> ##D lines(x, pinv.gaussian(x, mu = 1, lambda = 1), type = "l", col = "orange") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("KLDvglm")
> ### * KLDvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: KLD
> ### Title: Kullback-Leibler Divergence
> ### Aliases: KLD KLDvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> #  McKendrick (1925): Data from 223 Indian village households
> cholera <- data.frame(ncases = 0:4,  # Number of cholera cases,
+                       wfreq  = c(168, 32, 16, 6, 1))  # Frequencies
> fit7 <- vglm(ncases ~ 1, gaitdpoisson(i.mlm = 0, ilambda.p = 1),
+              weight = wfreq, data = cholera, trace = TRUE)
Minimum baseline (reserve) probability =  0.772 
VGLM    linear loop  1 :  loglikelihood = -182.4529
Minimum baseline (reserve) probability =  0.287 
VGLM    linear loop  2 :  loglikelihood = -179.3695
Minimum baseline (reserve) probability =  0.408 
VGLM    linear loop  3 :  loglikelihood = -179.3477
Minimum baseline (reserve) probability =  0.397 
VGLM    linear loop  4 :  loglikelihood = -179.3477
> coef(fit7, matrix = TRUE)
            loglink(lambda.p) multilogitlink(pstr.mlm0)
(Intercept)       -0.02821651                  0.419289
> KLD(fit7)
[1] 0.4285833
> 
> 
> 
> cleanEx()
> nameEx("Links")
> ### * Links
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Links
> ### Title: Link functions for VGLM/VGAM/etc. families
> ### Aliases: Links TypicalVGAMlink
> ### Keywords: models
> 
> ### ** Examples
> 
> logitlink("a")
[1] "logitlink(a)"
> logitlink("a", short = FALSE)
[1] "log(a/(1-a))"
> logitlink("a", short = FALSE, tag = TRUE)
[1] "Logit: log(a/(1-a))"
> 
> logofflink(1:5, offset = 1)  # Same as log(1:5 + 1)
[1] 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595
> powerlink(1:5, power = 2)  # Same as (1:5)^2
[1]  1  4  9 16 25
> 
> ## Not run: 
> ##D  # This is old and no longer works:
> ##D logofflink(1:5, earg = list(offset = 1))
> ##D powerlink(1:5, earg = list(power = 2))
> ## End(Not run)
> 
> fit1 <- vgam(agaaus ~ altitude,
+              binomialff(link = "clogloglink"), hunua)  # best
> fit2 <- vgam(agaaus ~ altitude,
+              binomialff(link =  clogloglink ), hunua)  # okay
> 
> ## Not run: 
> ##D # This no longer works since "clog" is not a valid VGAM link function:
> ##D fit3 <- vgam(agaaus ~ altitude,
> ##D              binomialff(link = "clog"), hunua)  # not okay
> ##D 
> ##D 
> ##D # No matter what the link, the estimated var-cov matrix is the same
> ##D y <- rbeta(n = 1000, shape1 = exp(0), shape2 = exp(1))
> ##D fit1 <- vglm(y ~ 1, betaR(lshape1 = "identitylink",
> ##D                           lshape2 = "identitylink"),
> ##D              trace = TRUE, crit = "coef")
> ##D fit2 <- vglm(y ~ 1, betaR(lshape1 = logofflink(offset = 1.1),
> ##D                           lshape2 = logofflink(offset = 1.1)), trace=TRUE)
> ##D vcov(fit1, untransform = TRUE)
> ##D vcov(fit1, untransform = TRUE) -
> ##D vcov(fit2, untransform = TRUE)  # Should be all 0s
> ##D \dontrun{ # This is old:
> ##D fit1@misc$earg  # Some 'special' parameters
> ##D fit2@misc$earg  # Some 'special' parameters are here
> ##D }
> ##D 
> ##D 
> ##D par(mfrow = c(2, 2))
> ##D p <- seq(0.05, 0.95, len = 200)  # A rather restricted range
> ##D x <- seq(-4, 4, len = 200)
> ##D plot(p, logitlink(p), type = "l", col = "blue")
> ##D plot(x, logitlink(x, inverse = TRUE), type = "l", col = "blue")
> ##D plot(p, logitlink(p, deriv=1), type="l", col="blue") # 1 / (p*(1-p))
> ##D plot(p, logitlink(p, deriv=2), type="l", col="blue") # (2*p-1)/(p*(1-p))^2
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("MNSs")
> ### * MNSs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MNSs
> ### Title: The MNSs Blood Group System
> ### Aliases: MNSs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Order matters only:
> y <- cbind(MS = 295, Ms = 107, MNS = 379, MNs = 322, NS = 102, Ns = 214)
> fit <- vglm(y ~ 1, MNSs("logitlink", .25, .28, .08), trace = TRUE)
VGLM    linear loop  1 :  deviance = 1.752635
VGLM    linear loop  2 :  deviance = 1.752591
VGLM    linear loop  3 :  deviance = 1.75259
> fit <- vglm(y ~ 1, MNSs(link = logitlink), trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
-1.11324317, -0.92882439, -2.43891461
VGLM    linear loop  2 :  coefficients = 
-1.11331541, -0.92938385, -2.43952450
VGLM    linear loop  3 :  coefficients = 
-1.1133252, -0.9293749, -2.4394867
VGLM    linear loop  4 :  coefficients = 
-1.11332484, -0.92937527, -2.43948828
VGLM    linear loop  5 :  coefficients = 
-1.11332486, -0.92937526, -2.43948821
> Coef(fit)
        mS         ms         nS 
0.24725155 0.28305148 0.08021066 
> rbind(y, sum(y)*fitted(fit))
        MS       Ms      MNS      MNs        NS       Ns
  295.0000 107.0000 379.0000 322.0000 102.00000 214.0000
1 285.3654 113.6876 394.0196 312.8744  97.79132 215.2617
> sqrt(diag(vcov(fit)))
(Intercept):1 (Intercept):2 (Intercept):3 
   0.05146917    0.04870453    0.09795120 
> 
> 
> 
> cleanEx()
> nameEx("Max")
> ### * Max
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Max
> ### Title: Maximums
> ### Aliases: Max
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(111)  # This leads to the global solution
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Standardized environmental vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, Bestof = 2, data = hspider, Crow1positive = FALSE)
> ##D Max(p1)
> ##D 
> ##D index <- 1:ncol(depvar(p1))
> ##D persp(p1, col = index, las = 1, llwd = 2)
> ##D abline(h = Max(p1), lty = 2, col = index)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("N1binomUC")
> ### * N1binomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: N1binom
> ### Title: Linear Model and Binomial Mixed Data Type Distribution
> ### Aliases: dN1binom rN1binom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  
> ##D nn <- 1000;  apar <- rhobitlink(1.5, inverse = TRUE)
> ##D prob <- logitlink(0.5, inverse = TRUE)
> ##D mymu <- 1; sdev <- exp(1)
> ##D mat <- rN1binom(nn, mymu, sdev, prob, apar)
> ##D bndata <- data.frame(y1 = mat[, 1], y2 = mat[, 2])
> ##D with(bndata, plot(jitter(y1), jitter(y2), col = "blue"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("N1binomial")
> ### * N1binomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: N1binomial
> ### Title: Linear Model and Binomial Mixed Data Type Family Function
> ### Aliases: N1binomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000; mymu <- 1; sdev <- exp(1)
> apar <- rhobitlink(0.5, inverse = TRUE)
> prob <-  logitlink(0.5, inverse = TRUE)
> mat <- rN1binom(nn, mymu, sdev, prob, apar)
> nbdata <- data.frame(y1 = mat[, 1], y2 = mat[, 2])
> fit1 <- vglm(cbind(y1, y2) ~ 1, N1binomial,
+              nbdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -6172.5841
VGLM    linear loop  2 :  loglikelihood = -6171.8144
VGLM    linear loop  3 :  loglikelihood = -6171.7681
VGLM    linear loop  4 :  loglikelihood = -6171.7655
VGLM    linear loop  5 :  loglikelihood = -6171.7654
VGLM    linear loop  6 :  loglikelihood = -6171.7653
> coef(fit1, matrix = TRUE)
                 mean loglink(sd) logitlink(prob) rhobitlink(apar)
(Intercept) 0.9683371     1.03382       0.5667847        0.5380395
> Coef(fit1)
     mean        sd      prob      apar 
0.9683371 2.8117860 0.6380209 0.2627125 
> head(fitted(fit1))
         y1        y2
1 0.9683371 0.6380209
2 0.9683371 0.6380209
3 0.9683371 0.6380209
4 0.9683371 0.6380209
5 0.9683371 0.6380209
6 0.9683371 0.6380209
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2) ~ 1, family = N1binomial, data = nbdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.96834    0.08892  10.890  < 2e-16 ***
(Intercept):2  1.03382    0.02236  46.234  < 2e-16 ***
(Intercept):3  0.56678    0.07292   7.773 7.67e-15 ***
(Intercept):4  0.53804    0.09145   5.883 4.02e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: mean, loglink(sd), logitlink(prob), 
rhobitlink(apar)

Log-likelihood: -6171.765 on 3996 degrees of freedom

Number of Fisher scoring iterations: 6 

> confint(fit1)
                  2.5 %    97.5 %
(Intercept):1 0.7940640 1.1426102
(Intercept):2 0.9899937 1.0776460
(Intercept):3 0.4238683 0.7097012
(Intercept):4 0.3588001 0.7172788
> 
> 
> 
> cleanEx()
> nameEx("N1poisUC")
> ### * N1poisUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: N1pois
> ### Title: Linear Model and Poisson Mixed Data Type Distribution
> ### Aliases: dN1pois rN1pois
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  
> ##D nn <- 1000; mymu <- 1; sdev <- exp(1)
> ##D apar <- rhobitlink(0.4, inverse = TRUE)
> ##D lambda <- loglink(1, inverse = TRUE)
> ##D mat <- rN1pois(nn, mymu, sdev, lambda, apar)
> ##D pndata <- data.frame(y1 = mat[, 1], y2 = mat[, 2])
> ##D with(pndata, plot(jitter(y1), jitter(y2), col = 4))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("N1poisson")
> ### * N1poisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: N1poisson
> ### Title: Linear Model and Poisson Mixed Data Type Family Function
> ### Aliases: N1poisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> apar <- rhobitlink(0.3, inverse = TRUE)
> nn <- 1000; mymu <- 1; sdev <- exp(1)
> lambda <- loglink(1, inverse = TRUE)
> mat <- rN1pois(nn, mymu, sdev, lambda, apar)
> npdata <- data.frame(y1 = mat[, 1], y2 = mat[, 2])
> with(npdata, var(y2) / mean(y2))  # Overdispersion
[1] 1.450752
> fit1 <- vglm(cbind(y1, y2) ~ 1, N1poisson,
+              npdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -8648.8904
VGLM    linear loop  2 :  loglikelihood = -8629.6337
VGLM    linear loop  3 :  loglikelihood = -8627.1667
VGLM    linear loop  4 :  loglikelihood = -8626.8092
VGLM    linear loop  5 :  loglikelihood = -8626.7496
VGLM    linear loop  6 :  loglikelihood = -8626.7386
VGLM    linear loop  7 :  loglikelihood = -8626.7365
VGLM    linear loop  8 :  loglikelihood = -8626.736
VGLM    linear loop  9 :  loglikelihood = -8626.736
VGLM    linear loop  10 :  loglikelihood = -8626.7359
> coef(fit1, matrix = TRUE)
                 mean loglink(sd) loglink(lambda) rhobitlink(apar)
(Intercept) 0.9683371     1.03382        0.988429        0.3003815
> Coef(fit1)
     mean        sd    lambda      apar 
0.9683371 2.8117860 2.6870097 0.1490716 
> head(fitted(fit1))
         y1      y2
1 0.9683371 2.68701
2 0.9683371 2.68701
3 0.9683371 2.68701
4 0.9683371 2.68701
5 0.9683371 2.68701
6 0.9683371 2.68701
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2) ~ 1, family = N1poisson, data = npdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.96834    0.08892   10.89   <2e-16 ***
(Intercept):2  1.03382    0.02236   46.23   <2e-16 ***
(Intercept):3  0.98843    0.01966   50.28   <2e-16 ***
(Intercept):4  0.30038    0.01350   22.25   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: mean, loglink(sd), loglink(lambda), 
rhobitlink(apar)

Log-likelihood: -8626.736 on 3996 degrees of freedom

Number of Fisher scoring iterations: 10 

> confint(fit1)
                  2.5 %   97.5 %
(Intercept):1 0.7940640 1.142610
(Intercept):2 0.9899937 1.077646
(Intercept):3 0.9499023 1.026956
(Intercept):4 0.2739271 0.326836
> 
> 
> 
> cleanEx()
> nameEx("Opt")
> ### * Opt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Opt
> ### Title: Optimums
> ### Aliases: Opt
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(111)  # This leads to the global solution
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Standardized environmental vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           family = poissonff, data = hspider, Crow1positive = FALSE)
> ##D Opt(p1)
> ##D 
> ##D clr <- (1:(ncol(depvar(p1))+1))[-7]  # Omits yellow
> ##D persp(p1, col = clr, las = 1, main = "Vertical lines at the optimums")
> ##D abline(v = Opt(p1), lty = 2, col = clr)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ParetoUC")
> ### * ParetoUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Pareto
> ### Title: The Pareto Distribution
> ### Aliases: Pareto dpareto ppareto qpareto rpareto
> ### Keywords: distribution
> 
> ### ** Examples
> 
> alpha <- 3; k <- exp(1); x <- seq(2.8, 8, len = 300)
> ## Not run: 
> ##D plot(x, dpareto(x, scale = alpha, shape = k), type = "l",
> ##D      main = "Pareto density split into 10 equal areas")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D qvec <- qpareto(seq(0.1, 0.9, by = 0.1), scale = alpha, shape = k)
> ##D lines(qvec, dpareto(qvec, scale = alpha, shape = k),
> ##D       col = "purple", lty = 3, type = "h")
> ## End(Not run)
> pvec <- seq(0.1, 0.9, by = 0.1)
> qvec <- qpareto(pvec, scale = alpha, shape = k)
> ppareto(qvec, scale = alpha, shape = k)
[1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
> qpareto(ppareto(qvec, scale = alpha, shape = k),
+         scale = alpha, shape = k) - qvec  # Should be 0
[1] 0 0 0 0 0 0 0 0 0
> 
> 
> 
> cleanEx()
> nameEx("QvarUC")
> ### * QvarUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Qvar
> ### Title: Quasi-variances Preprocessing Function
> ### Aliases: Qvar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1
> data("ships", package = "MASS")
> 
> Shipmodel <- vglm(incidents ~ type + year + period,
+                   poissonff, offset = log(service),
+ #                 trace = TRUE, model = TRUE,
+                   data = ships, subset = (service > 0))
> 
> # Easiest form of input
> fit1 <- rcim(Qvar(Shipmodel, "type"), uninormal("explink"), maxit = 99)
> qvar(fit1)              # Easy method to get the quasi-variances
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> qvar(fit1, se = TRUE)   # Easy method to get the quasi-standard errors
[1] 0.15574074 0.08631623 0.28812676 0.24622326 0.17557954
> 
> (quasiVar <- exp(diag(fitted(fit1))) / 2)                 # Version 1
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (quasiVar <- diag(predict(fit1)[, c(TRUE, FALSE)]) / 2)   # Version 2
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (quasiSE  <- sqrt(quasiVar))
[1] 0.15574074 0.08631623 0.28812676 0.24622326 0.17557954
> 
> # Another form of input
> fit2 <- rcim(Qvar(Shipmodel, coef.ind = c(0, 2:5), reference.name = "typeA"),
+              uninormal("explink"), maxit = 99)
> ## Not run:  qvplot(fit2, col = "green", lwd = 3, scol = "blue", slwd = 2, las = 1) 
> 
> # The variance-covariance matrix is another form of input (not recommended)
> fit3 <- rcim(Qvar(cbind(0, rbind(0, vcov(Shipmodel)[2:5, 2:5])),
+                   labels = c("typeA", "typeB", "typeC", "typeD", "typeE"),
+                   estimates = c(typeA = 0, coef(Shipmodel)[2:5])),
+              uninormal("explink"), maxit = 99)
> (QuasiVar <- exp(diag(fitted(fit3))) / 2)                 # Version 1
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (QuasiVar <- diag(predict(fit3)[, c(TRUE, FALSE)]) / 2)   # Version 2
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (QuasiSE  <- sqrt(quasiVar))
[1] 0.15574074 0.08631623 0.28812676 0.24622326 0.17557954
> ## Not run:  qvplot(fit3) 
> 
> 
> # Example 2: a model with M > 1 linear predictors
> ## Not run: 
> ##D  require("VGAMdata")
> ##D xs.nz.f <- subset(xs.nz, sex == "F")
> ##D xs.nz.f <- subset(xs.nz.f, !is.na(babies)  & !is.na(age) & !is.na(ethnicity))
> ##D xs.nz.f <- subset(xs.nz.f, ethnicity != "Other")
> ##D 
> ##D clist <- list("sm.bs(age, df = 4)" = rbind(1, 0),
> ##D               "sm.bs(age, df = 3)" = rbind(0, 1),
> ##D               "ethnicity"          = diag(2),
> ##D               "(Intercept)"        = diag(2))
> ##D fit1 <- vglm(babies ~ sm.bs(age, df = 4) + sm.bs(age, df = 3) + ethnicity,
> ##D             zipoissonff(zero = NULL), xs.nz.f,
> ##D             constraints = clist, trace = TRUE)
> ##D Fit1 <- rcim(Qvar(fit1, "ethnicity", which.linpred = 1),
> ##D              uninormal("explink", imethod = 1), maxit = 99, trace = TRUE)
> ##D Fit2 <- rcim(Qvar(fit1, "ethnicity", which.linpred = 2),
> ##D              uninormal("explink", imethod = 1), maxit = 99, trace = TRUE)
> ## End(Not run)
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D qvplot(Fit1, scol = "blue", pch = 16, main = expression(eta[1]),
> ##D        slwd = 1.5, las = 1, length.arrows = 0.07)
> ##D qvplot(Fit2, scol = "blue", pch = 16, main = expression(eta[2]),
> ##D        slwd = 1.5, las = 1, length.arrows = 0.07)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("R2latvar")
> ### * R2latvar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: R2latvar
> ### Title: R-squared for Latent Variable Models
> ### Aliases: R2latvar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> R2latvar(fit)
[1] 0.5142885
> 
> 
> 
> cleanEx()
> nameEx("Rank")
> ### * Rank
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rank
> ### Title: Rank
> ### Aliases: Rank Rank.rrvglm Rank.qrrvglm Rank.rrvgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time),
+                             x3  = runif(nrow(pneumo)))
> (fit1 <- rrvglm(cbind(normal, mild, severe) ~ let + x3,
+                 acat, data = pneumo))
Call:
rrvglm(formula = cbind(normal, mild, severe) ~ let + x3, family = acat, 
    data = pneumo)

Coefficients:
(Intercept):1 (Intercept):2           let            x3 
  -8.95224104   -3.03133603    2.17539662   -0.02842669 

Residual deviance: 5.3424 
Log-likelihood: -25.24805 
> coef(fit1, matrix = TRUE)
            loglink(P[Y=2]/P[Y=1]) loglink(P[Y=3]/P[Y=2])
(Intercept)            -8.95224104            -3.03133603
let                     2.17539662             0.90215166
x3                     -0.02842669            -0.01178874
> constraints(fit1)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
          [,1]
[1,] 1.0000000
[2,] 0.4147067

$x3
          [,1]
[1,] 1.0000000
[2,] 0.4147067

> Rank(fit1)
[1] 1
> 
> 
> 
> cleanEx()
> nameEx("Rcim")
> ### * Rcim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rcim
> ### Title: Mark the Baseline of Row and Column on a Matrix data
> ### Aliases: Rcim
> 
> ### ** Examples
> 
> (alcoff.e <- moffset(alcoff, roffset = "6", postfix = "*"))
   Mon* Tue* Wed* Thu* Fri* Sat* Sun*
6    13    9    9   55   70  155  213
7    20    6    9   42   40  160  200
8     5    8   12   29   40   79   96
9     7    8   14   28   38   44   58
10    7   13   20   36   38   52   69
11    8   10   20   32   27   44   39
12   10   13    8   39   37   73   59
13    9   26   32   37   27   41   45
14   22   41   31   46   42   58   53
15   39   48   62   69   59   75   70
16   28   48   71   85   55   96   95
17   46   59   98  141  136  154  130
18   53  100  117  185  223  236  121
19   74  119  155  289  335  337  146
20   74  135  283  508  591  490  166
21   84  154  326  610  866  754  131
22   90  143  345  765  976 1026  114
23  110  169  363  899 1265 1179  159
0*   98  165  324  827 1379 1332  121
1*   92  157  278  619 1327 1356   97
2*   69  107  229  410  979 1011   60
3*   60   75  238  401  693  718   55
4*   38   48  145  223  346  410   25
5*   10   20   56  139  188  287   19
> (aa <- Rcim(alcoff,    rbaseline = "11", cbaseline = "Sun"))
    Sun Mon Tue Wed Thu  Fri  Sat
11   39   8  10  20  32   27   44
12   59  10  13   8  39   37   73
13   45   9  26  32  37   27   41
14   53  22  41  31  46   42   58
15   70  39  48  62  69   59   75
16   95  28  48  71  85   55   96
17  130  46  59  98 141  136  154
18  121  53 100 117 185  223  236
19  146  74 119 155 289  335  337
20  166  74 135 283 508  591  490
21  131  84 154 326 610  866  754
22  114  90 143 345 765  976 1026
23  159 110 169 363 899 1265 1179
0  1332 121  98 165 324  827 1379
1  1356  97  92 157 278  619 1327
2  1011  60  69 107 229  410  979
3   718  55  60  75 238  401  693
4   410  25  38  48 145  223  346
5   287  19  10  20  56  139  188
6   213  13   9   9  55   70  155
7   200  20   6   9  42   40  160
8    96   5   8  12  29   40   79
9    58   7   8  14  28   38   44
10   69   7  13  20  36   38   52
> (bb <- moffset(alcoff,             "11",             "Sun", postfix = "*"))
    Sun* Mon* Tue* Wed* Thu* Fri* Sat*
11    39    8   10   20   32   27   44
12    59   10   13    8   39   37   73
13    45    9   26   32   37   27   41
14    53   22   41   31   46   42   58
15    70   39   48   62   69   59   75
16    95   28   48   71   85   55   96
17   130   46   59   98  141  136  154
18   121   53  100  117  185  223  236
19   146   74  119  155  289  335  337
20   166   74  135  283  508  591  490
21   131   84  154  326  610  866  754
22   114   90  143  345  765  976 1026
23   159  110  169  363  899 1265 1179
0*   121   98  165  324  827 1379 1332
1*    97   92  157  278  619 1327 1356
2*    60   69  107  229  410  979 1011
3*    55   60   75  238  401  693  718
4*    25   38   48  145  223  346  410
5*    19   10   20   56  139  188  287
6*    13    9    9   55   70  155  213
7*    20    6    9   42   40  160  200
8*     5    8   12   29   40   79   96
9*     7    8   14   28   38   44   58
10*    7   13   20   36   38   52   69
> aa - bb  # Note the difference!
    Sun Mon Tue  Wed  Thu  Fri Sat
11    0   0   0    0    0    0   0
12    0   0   0    0    0    0   0
13    0   0   0    0    0    0   0
14    0   0   0    0    0    0   0
15    0   0   0    0    0    0   0
16    0   0   0    0    0    0   0
17    0   0   0    0    0    0   0
18    0   0   0    0    0    0   0
19    0   0   0    0    0    0   0
20    0   0   0    0    0    0   0
21    0   0   0    0    0    0   0
22    0   0   0    0    0    0   0
23    0   0   0    0    0    0   0
0  1211  23 -67 -159 -503 -552  47
1  1259   5 -65 -121 -341 -708 -29
2   951  -9 -38 -122 -181 -569 -32
3   663  -5 -15 -163 -163 -292 -25
4   385 -13 -10  -97  -78 -123 -64
5   268   9 -10  -36  -83  -49 -99
6   200   4   0  -46  -15  -85 -58
7   180  14  -3  -33    2 -120 -40
8    91  -3  -4  -17  -11  -39 -17
9    51  -1  -6  -14  -10   -6 -14
10   62  -6  -7  -16   -2  -14 -17
> 
> 
> 
> cleanEx()
> nameEx("SURff")
> ### * SURff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SURff
> ### Title: Seemingly Unrelated Regressions Family Function
> ### Aliases: SURff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Obtain some of the results of p.1199 of Kmenta and Gilbert (1968)
> clist <- list("(Intercept)" = diag(2),
+               "capital.g"   = rbind(1, 0),
+               "value.g"     = rbind(1, 0),
+               "capital.w"   = rbind(0, 1),
+               "value.w"     = rbind(0, 1))
> zef1 <- vglm(cbind(invest.g, invest.w) ~
+              capital.g + value.g + capital.w + value.w,
+              SURff(divisor = "sqrt"), maxit = 1,
+              data = gew, trace = TRUE, constraints = clist)
VGLM    linear loop  1 :  coefficients = 
-27.719317124,  -1.251988228,   0.139036274,   0.038310207,   0.063978067, 
  0.057629796
> 
> round(coef(zef1, matrix = TRUE), digits = 4)  # ZEF
            invest.g invest.w
(Intercept) -27.7193  -1.2520
capital.g     0.1390   0.0000
value.g       0.0383   0.0000
capital.w     0.0000   0.0640
value.w       0.0000   0.0576
> zef1@extra$ncols.X.lm
[1] 3 3
> zef1@misc$divisor
[1] "sqrt((n-pj)*(n-pk))"
> zef1@misc$values.divisor
[1] 17 17 17
> round(sqrt(diag(vcov(zef1))),    digits = 4)  # SEs
(Intercept):1 (Intercept):2     capital.g       value.g     capital.w 
      29.3212        7.5452        0.0250        0.0144        0.0530 
      value.w 
       0.0145 
> nobs(zef1, type = "lm")
[1] 20
> df.residual(zef1, type = "lm")
invest.g invest.w 
      17       17 
> 
> 
> mle1 <- vglm(cbind(invest.g, invest.w) ~
+              capital.g + value.g + capital.w + value.w,
+              SURff(mle.normal = TRUE),
+              epsilon = 1e-11,
+              data = gew, trace = TRUE, constraints = clist)
VGLM    linear loop  1 :  loglikelihood = -158.398407229
VGLM    linear loop  2 :  loglikelihood = -158.306427301
VGLM    linear loop  3 :  loglikelihood = -158.303257158
VGLM    linear loop  4 :  loglikelihood = -158.303113124
VGLM    linear loop  5 :  loglikelihood = -158.303106338
VGLM    linear loop  6 :  loglikelihood = -158.303106016
VGLM    linear loop  7 :  loglikelihood = -158.303106
VGLM    linear loop  8 :  loglikelihood = -158.303106
VGLM    linear loop  9 :  loglikelihood = -158.303106
> round(coef(mle1, matrix = TRUE), digits = 4)  # MLE
            invest.g invest.w
(Intercept) -30.7484  -1.7016
capital.g     0.1359   0.0000
value.g       0.0405   0.0000
capital.w     0.0000   0.0557
value.w       0.0000   0.0594
> round(sqrt(diag(vcov(mle1))),    digits = 4)  # SEs
(Intercept):1 (Intercept):2     capital.g       value.g     capital.w 
      27.3459        6.9284        0.0235        0.0134        0.0488 
      value.w 
       0.0133 
> 
> 
> 
> cleanEx()
> nameEx("Select")
> ### * Select
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Select
> ### Title: Select Variables for a Formula Response or the RHS of a Formula
> ### Aliases: Select subsetcol
> ### Keywords: models regression
> 
> ### ** Examples
> 
> Pneumo <- pneumo
> colnames(Pneumo) <- c("y1", "y2", "y3", "x2")  # The "y" variables are response
> Pneumo$x1 <- 1; Pneumo$x3 <- 3; Pneumo$x <- 0; Pneumo$x4 <- 4  # Add these
> 
> Select(data = Pneumo)  # Same as with(Pneumo, cbind(y1, y2, y3))
    y1 y2 y3
1  5.8 98  0
2 15.0 51  2
3 21.5 34  6
4 27.5 35  5
5 33.5 32 10
6 39.5 23  7
7 46.0 12  6
8 51.5  4  2
> Select(Pneumo, "x")
  x x1 x2 x3 x4
1 0  1  0  3  4
2 0  1  1  3  4
3 0  1  3  3  4
4 0  1  8  3  4
5 0  1  9  3  4
6 0  1  8  3  4
7 0  1 10  3  4
8 0  1  5  3  4
> Select(Pneumo, "x", sort = FALSE, as.char = TRUE)
[1] "cbind(x2, x1, x3, x, x4)"
> Select(Pneumo, "x", exclude = "x1")
  x x2 x3 x4
1 0  0  3  4
2 0  1  3  4
3 0  3  3  4
4 0  8  3  4
5 0  9  3  4
6 0  8  3  4
7 0 10  3  4
8 0  5  3  4
> Select(Pneumo, "x", exclude = "x1", as.char = TRUE)
[1] "cbind(x, x2, x3, x4)"
> Select(Pneumo, c("x", "y"))
  x x1 x2 x3 x4   y1 y2 y3
1 0  1  0  3  4  5.8 98  0
2 0  1  1  3  4 15.0 51  2
3 0  1  3  3  4 21.5 34  6
4 0  1  8  3  4 27.5 35  5
5 0  1  9  3  4 33.5 32 10
6 0  1  8  3  4 39.5 23  7
7 0  1 10  3  4 46.0 12  6
8 0  1  5  3  4 51.5  4  2
> Select(Pneumo, "z")  # Now returns a NULL
NULL
> Select(Pneumo, " ")  # Now returns a NULL
NULL
> Select(Pneumo, prefix = TRUE, as.formula = TRUE)
~x + x1 + x2 + x3 + x4 + y1 + y2 + y3
<environment: 0x556f685ecde8>
> Select(Pneumo, "x", exclude = c("x3", "x1"), as.formula = TRUE,
+        lhs = "cbind(y1, y2, y3)", rhs = "0")
cbind(y1, y2, y3) ~ x + x2 + x4 + 0
<environment: 0x556f68631258>
> Select(Pneumo, "x", exclude = "x1", as.formula = TRUE, as.char = TRUE,
+        lhs = "cbind(y1, y2, y3)", rhs = "0")
[1] "cbind(y1, y2, y3) ~ x + x2 + x3 + x4 + 0"
> 
> # Now a 'real' example:
> Huggins89table1 <- transform(Huggins89table1, x3.tij = t01)
> tab1 <- subset(Huggins89table1,
+                rowSums(Select(Huggins89table1, "y")) > 0)
> # Same as
> # subset(Huggins89table1, y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 > 0)
> 
> # Long way to do it:
> fit.th <-
+    vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10) ~ x2 + x3.tij,
+         xij = list(x3.tij ~ t01 + t02 + t03 + t04 + t05 + t06 + t07 + t08 +
+                             t09 + t10 - 1),
+         posbernoulli.t(parallel.t = TRUE ~ x2 + x3.tij),
+         data = tab1, trace = TRUE,
+         form2 = ~ x2 + x3.tij + t01 + t02 + t03 + t04 + t05 + t06 + t07 + t08 +
+                                 t09 + t10)
VGLM    linear loop  1 :  loglikelihood = -97.120355
VGLM    linear loop  2 :  loglikelihood = -97.079804
VGLM    linear loop  3 :  loglikelihood = -97.079782
VGLM    linear loop  4 :  loglikelihood = -97.079782
> # Short way to do it:
> Fit.th <- vglm(Select(tab1, "y") ~ x2 + x3.tij,
+                xij = list(Select(tab1, "t", as.formula = TRUE,
+                                  sort = FALSE, lhs = "x3.tij", rhs = "0")),
+                posbernoulli.t(parallel.t = TRUE ~ x2 + x3.tij),
+                data = tab1, trace = TRUE,
+                form2 = Select(tab1, prefix = TRUE, as.formula = TRUE))
VGLM    linear loop  1 :  loglikelihood = -97.120355
VGLM    linear loop  2 :  loglikelihood = -97.079804
VGLM    linear loop  3 :  loglikelihood = -97.079782
VGLM    linear loop  4 :  loglikelihood = -97.079782
> 
> 
> 
> cleanEx()
> nameEx("SurvS4-class")
> ### * SurvS4-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SurvS4-class
> ### Title: Class "SurvS4"
> ### Aliases: SurvS4-class show,SurvS4-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("SurvS4")
Virtual Class "SurvS4" [package "VGAM"]

Slots:
                
Name:   .S3Class
Class: character

Extends: 
Class "Surv", directly
Class "matrix", directly
Class "oldClass", by class "Surv", distance 2
Class "array", by class "matrix", distance 2
Class "structure", by class "matrix", distance 3
Class "vector", by class "matrix", distance 4, with explicit coerce
> 
> 
> 
> cleanEx()
> nameEx("SurvS4")
> ### * SurvS4
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SurvS4
> ### Title: Create a Survival Object
> ### Aliases: SurvS4 is.SurvS4 show.SurvS4 Math.SurvS4 Summary.SurvS4
> ###   [.SurvS4 format.SurvS4 as.data.frame.SurvS4 as.character.SurvS4
> ###   is.na.SurvS4 Ops.SurvS4
> ### Keywords: survival
> 
> ### ** Examples
> 
> with(leukemia, SurvS4(time, status))
      time status
 [1,]    9      1
 [2,]   13      1
 [3,]   13      0
 [4,]   18      1
 [5,]   23      1
 [6,]   28      0
 [7,]   31      1
 [8,]   34      1
 [9,]   45      0
[10,]   48      1
[11,]  161      0
[12,]    5      1
[13,]    5      1
[14,]    8      1
[15,]    8      1
[16,]   12      1
[17,]   16      0
[18,]   23      1
[19,]   27      1
[20,]   30      1
[21,]   33      1
[22,]   43      1
[23,]   45      1
attr(,"type")
[1] "right"
attr(,"class")
[1] "SurvS4"
> class(with(leukemia, SurvS4(time, status)))
[1] "SurvS4"
> 
> 
> 
> cleanEx()
> nameEx("TICvlm")
> ### * TICvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: TIC
> ### Title: Takeuchi's Information Criterion
> ### Aliases: TIC TICvlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = TRUE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> coef(fit1, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> TIC(fit1)
[1] 50.25913
> (fit2 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = FALSE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = FALSE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
    -9.593308    -11.104791      2.571300      2.743550 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.884404 
Log-likelihood: -25.01905 
> coef(fit2, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.593308          -11.10479
let                   2.571300            2.74355
> TIC(fit2)
[1] 50.11018
> 
> 
> 
> cleanEx()
> nameEx("Tol")
> ### * Tol
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tol
> ### Title: Tolerances
> ### Aliases: Tol
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(111)  # This leads to the global solution
> ##D hspider[,1:6] <- scale(hspider[, 1:6])  # Standardized environmental vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, Crow1positive = FALSE)
> ##D 
> ##D Tol(p1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Trunc")
> ### * Trunc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Trunc
> ### Title: Truncated Values for the GT-Expansion Method
> ### Aliases: Trunc
> ### Keywords: models regression utilities
> 
> ### ** Examples
> 
> Trunc(c(1, 8), 2)
[1]  3  5  7  9 11 13 15
> 
> ## Not run: 
> ##D set.seed(1)  # The following example is based on the normal
> ##D mymean <- 20; m.truth <- 3  # approximation to the Poisson.
> ##D gdata <- data.frame(y1 = round(rnorm((nn <- 1000), mymean,
> ##D                                      sd = sqrt(mymean / m.truth))))
> ##D org1 <- with(gdata, range(y1))  # Original range of the raw data
> ##D m.max <- 5  # Try multipliers 1:m.max
> ##D logliks <- numeric(m.max)
> ##D names(logliks) <- as.character(1:m.max)
> ##D for (i in 1:m.max) {
> ##D   logliks[i] <- logLik(vglm(i * y1 ~ offset(rep(log(i), nn)),
> ##D     gaitdpoisson(truncate = Trunc(org1, i)), data = gdata))
> ##D }
> ##D sort(logliks, decreasing = TRUE)  # Best to worst
> ##D  par(mfrow = c(1, 2))
> ##D plot(with(gdata, table(y1)))  # Underdispersed wrt Poisson
> ##D plot(logliks, col = "blue", type = "b", xlab = "Multiplier") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("UtilitiesVGAM")
> ### * UtilitiesVGAM
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: UtilitiesVGAM
> ### Title: Utility Functions for the VGAM Package
> ### Aliases: UtilitiesVGAM param.names dimm interleave.VGAM
> ### Keywords: distribution regression programming models utilities
> 
> ### ** Examples
> 
> param.names("shape", 1)  # "shape"
[1] "shape1"
> param.names("shape", 3)  # c("shape1", "shape2", "shape3")
[1] "shape1" "shape2" "shape3"
> 
> dimm(3, hbw = 1)  # Diagonal matrix; the 3 elements need storage.
[1] 3
> dimm(3)  # A general 3 x 3 symmetrix matrix has 6 unique elements.
[1] 6
> dimm(3, hbw = 2)  # Tridiagonal matrix; the 3-3 element is 0 and unneeded.
[1] 5
> 
> M1 <- 2; ncoly <- 3; M <- ncoly * M1
> mynames1 <- param.names("location", ncoly)
> mynames2 <- param.names("scale",    ncoly)
> (parameters.names <- c(mynames1, mynames2)[interleave.VGAM(M, M1 = M1)])
[1] "location1" "scale1"    "location2" "scale2"    "location3" "scale3"   
> # The  following is/was in Yee (2015) and has a poor/deceptive style:
> (parameters.names <- c(mynames1, mynames2)[interleave.VGAM(M, M  = M1)])
[1] "location1" "scale1"    "location2" "scale2"    "location3" "scale3"   
> parameters.names[interleave.VGAM(M, M1 = M1, inverse = TRUE)]
[1] "location1" "location2" "location3" "scale1"    "scale2"    "scale3"   
> 
> 
> 
> cleanEx()
> nameEx("V1")
> ### * V1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: V1
> ### Title: V1 Flying-Bombs Hits in London
> ### Aliases: V1
> ### Keywords: datasets
> 
> ### ** Examples
> 
> V1
  hits ofreq
1    0   229
2    1   211
3    2    93
4    3    35
5    4     7
6    7     1
> mean(with(V1, rep(hits, times = ofreq)))
[1] 0.9322917
>  var(with(V1, rep(hits, times = ofreq)))
[1] 0.9710598
>  sum(with(V1, rep(hits, times = ofreq)))
[1] 537
> ## Not run: 
> ##D  barplot(with(V1, ofreq),
> ##D           names.arg = as.character(with(V1, hits)),
> ##D           main = "London V1 buzz bomb hits",
> ##D           col = "lightblue", las = 1,
> ##D           ylab = "Frequency", xlab = "Hits") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("V2")
> ### * V2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: V2
> ### Title: V2 Missile Hits in London
> ### Aliases: V2
> ### Keywords: datasets
> 
> ### ** Examples
> 
> V2
  hits ofreq
1    0   356
2    1    41
3    2    10
4    3     1
> mean(with(V2, rep(hits, times = ofreq)))
[1] 0.1568627
>  var(with(V2, rep(hits, times = ofreq)))
[1] 0.1964638
>  sum(with(V2, rep(hits, times = ofreq)))
[1] 64
> ## Not run: 
> ##D  barplot(with(V2, ofreq),
> ##D           names.arg = as.character(with(V2, hits)),
> ##D           main = "London V2 rocket hits",
> ##D           col = "lightgreen", las = 1,
> ##D           ylab = "Frequency", xlab = "Hits") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("VGAM-package")
> ### * VGAM-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: VGAM-package
> ### Title: Vector Generalized Linear and Additive Models and Other
> ###   Associated Models
> ### Aliases: VGAM-package VGAM
> ### Keywords: package models regression
> 
> ### ** Examples
> 
> # Example 1; proportional odds model
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> depvar(fit1)  # Better than using fit1@y; dependent variable (response)
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> weights(fit1, type = "prior")  # Number of observations
  [,1]
1   98
2   54
3   43
4   48
5   51
6   38
7   28
8   11
> coef(fit1, matrix = TRUE)      # p.179, in McCullagh and Nelder (1989)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> constraints(fit1)              # Constraint matrices
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> summary(fit1)  # HDE could affect these results
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -9.6761     1.3241  -7.308 2.72e-13 ***
(Intercept):2 -10.5817     1.3454  -7.865 3.69e-15 ***
let             2.5968     0.3811   6.814 9.50e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 5.0268 on 13 degrees of freedom

Log-likelihood: -25.0903 on 13 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'


Exponentiated coefficients:
     let 
13.42081 
> summary(fit1, lrt0 = TRUE, score0 = TRUE, wald0 = TRUE)  # No HDE
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)

Likelihood ratio test coefficients: 
    Estimate z value Pr(>|z|)    
let    2.597   9.829   <2e-16 ***

Rao score test coefficients: 
    Estimate Std. Error z value Pr(>|z|)    
let   2.5968     0.1622   8.361   <2e-16 ***

Wald (modified by IRLS iterations) coefficients: 
    Estimate Std. Error z value Pr(>|z|)    
let   2.5968     0.1622   16.01   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 5.0268 on 13 degrees of freedom

Log-likelihood: -25.0903 on 13 degrees of freedom

Number of Fisher scoring iterations: 4 


Exponentiated coefficients:
     let 
13.42081 
> hdeff(fit1)  # Check for any Hauck-Donner effect
(Intercept):1 (Intercept):2           let 
         TRUE         FALSE         FALSE 
> 
> # Example 2; zero-inflated Poisson model
> zdata <- data.frame(x2 = runif(nn <- 2000))
> zdata <- transform(zdata, pstr0  = logitlink(-0.5 + 1*x2, inverse = TRUE),
+                           lambda = loglink(  0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata, y = rzipois(nn, lambda, pstr0 = pstr0))
> with(zdata, table(y))
y
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
1083   98  135  129  113   95   74   64   40   41   34   28   22   12   15    6 
  16   18   19   21   24 
   6    1    2    1    1 
> fit2 <- vglm(y ~ x2, zipoisson, data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3256.23567
VGLM    linear loop  2 :  loglikelihood = -3241.07881
VGLM    linear loop  3 :  loglikelihood = -3241.05482
VGLM    linear loop  4 :  loglikelihood = -3241.05482
> coef(fit2, matrix = TRUE)  # These should agree with the above values
            logitlink(pstr0) loglink(lambda)
(Intercept)       -0.3786106       0.5603284
x2                 0.9405549       1.9649620
> 
> 
> # Example 3; fit a two species GAM simultaneously
> fit3 <- vgam(cbind(agaaus, kniexc) ~ s(altitude, df = c(2, 3)),
+              binomialff(multiple.responses = TRUE), data = hunua)
> coef(fit3, matrix = TRUE)   # Not really interpretable
                          logitlink(E[agaaus]) logitlink(E[kniexc])
(Intercept)                      -1.301849e+00         -0.072104658
s(altitude, df = c(2, 3))         3.919179e-05          0.002721115
> ## Not run: 
> ##D  plot(fit3, se = TRUE, overlay = TRUE, lcol = 3:4, scol = 3:4)
> ##D 
> ##D ooo <- with(hunua, order(altitude))
> ##D with(hunua,  matplot(altitude[ooo], fitted(fit3)[ooo, ], type = "l",
> ##D      lwd = 2, col = 3:4,
> ##D      xlab = "Altitude (m)", ylab = "Probability of presence", las = 1,
> ##D      main = "Two plant species' response curves", ylim = c(0, 0.8)))
> ##D with(hunua, rug(altitude)) 
> ## End(Not run)
> 
> 
> # Example 4; LMS quantile regression
> fit4 <- vgam(BMI ~ s(age, df = c(4, 2)), lms.bcn(zero = 1),
+              data = bmi.nz, trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = -6429.7568
VGAM  s.vam  loop  2 :  loglikelihood = -6327.3502
VGAM  s.vam  loop  3 :  loglikelihood = -6313.2224
VGAM  s.vam  loop  4 :  loglikelihood = -6312.8069
VGAM  s.vam  loop  5 :  loglikelihood = -6312.8166
VGAM  s.vam  loop  6 :  loglikelihood = -6312.8032
VGAM  s.vam  loop  7 :  loglikelihood = -6312.8088
VGAM  s.vam  loop  8 :  loglikelihood = -6312.8062
VGAM  s.vam  loop  9 :  loglikelihood = -6312.8074
VGAM  s.vam  loop  10 :  loglikelihood = -6312.8068
VGAM  s.vam  loop  11 :  loglikelihood = -6312.8071
VGAM  s.vam  loop  12 :  loglikelihood = -6312.807
> head(predict(fit4))
      lambda       mu loglink(sigma)
1 -0.6589584 25.48922      -1.851162
2 -0.6589584 26.19783      -1.852918
3 -0.6589584 26.66334      -1.853294
4 -0.6589584 25.75937      -1.852382
5 -0.6589584 27.17650      -1.852765
6 -0.6589584 26.17517      -1.852911
> head(fitted(fit4))
       25%      50%      75%
1 23.00836 25.48922 28.44767
2 23.65211 26.19783 29.23269
3 24.07328 26.66334 29.75085
4 23.25503 25.75937 28.74518
5 24.53531 27.17650 30.32525
6 23.63164 26.17517 29.20742
> head(bmi.nz)  # Person 1 is near the lower quartile among people his age
       age      BMI
1 31.52966 22.77107
2 39.38045 27.70033
3 43.38940 28.18127
4 34.84894 25.08380
5 53.81990 26.46388
6 39.17002 36.19648
> head(cdf(fit4))
        1         2         3         4         5         6 
0.2280309 0.6365499 0.6356761 0.4321450 0.4321311 0.9686738 
> 
> ## Not run: 
> ##D  par(mfrow = c(1,1), bty = "l", mar = c(5,4,4,3)+0.1, xpd=TRUE)
> ##D qtplot(fit4, percentiles = c(5,50,90,99), main = "Quantiles", las = 1,
> ##D        xlim = c(15, 90), ylab = "BMI", lwd=2, lcol=4)  # Quantile plot
> ##D 
> ##D ygrid <- seq(15, 43, len = 100)  # BMI ranges
> ##D par(mfrow = c(1, 1), lwd = 2)  # Density plot
> ##D aa <- deplot(fit4, x0 = 20, y = ygrid, xlab = "BMI", col = "black",
> ##D     main = "Density functions at Age=20 (black), 42 (red) and 55 (blue)")
> ##D aa
> ##D aa <- deplot(fit4, x0 = 42, y = ygrid, add = TRUE, llty = 2, col = "red")
> ##D aa <- deplot(fit4, x0 = 55, y = ygrid, add = TRUE, llty = 4, col = "blue",
> ##D             Attach = TRUE)
> ##D aa@post$deplot  # Contains density function values
> ## End(Not run)
> 
> 
> # Example 5; GEV distribution for extremes
> (fit5 <- vglm(maxtemp ~ 1, gevff, data = oxtemp, trace = TRUE))
VGLM    linear loop  1 :  loglikelihood = -228.90576
VGLM    linear loop  2 :  loglikelihood = -228.89653
VGLM    linear loop  3 :  loglikelihood = -228.89652
VGLM    linear loop  4 :  loglikelihood = -228.89652

Call:
vglm(formula = maxtemp ~ 1, family = gevff, data = oxtemp, trace = TRUE)


Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 
    83.838467      1.449270     -1.547556 

Degrees of Freedom: 240 Total; 237 Residual
Log-likelihood: -228.8965 
> head(fitted(fit5))
       95%      99%
1 92.35044 94.71293
2 92.35044 94.71293
3 92.35044 94.71293
4 92.35044 94.71293
5 92.35044 94.71293
6 92.35044 94.71293
> coef(fit5, matrix = TRUE)
            location loglink(scale) logofflink(shape, offset = 0.5)
(Intercept) 83.83847        1.44927                       -1.547556
> Coef(fit5)
  location      scale      shape 
83.8384672  4.2600036 -0.2872327 
> vcov(fit5)
              (Intercept):1 (Intercept):2 (Intercept):3
(Intercept):1  0.2669002973 -0.0009746616   -0.04928180
(Intercept):2 -0.0009746616  0.0072373297   -0.01358841
(Intercept):3 -0.0492818027 -0.0135884143    0.07543028
> vcov(fit5, untransform = TRUE)
             location        scale        shape
location  0.266900297 -0.004152062 -0.010485558
scale    -0.004152062  0.131340385 -0.012316398
shape    -0.010485558 -0.012316398  0.003414724
> sqrt(diag(vcov(fit5)))  # Approximate standard errors
(Intercept):1 (Intercept):2 (Intercept):3 
    0.5166239     0.0850725     0.2746457 
> ## Not run:  rlplot(fit5) 
> 
> 
> 
> cleanEx()
> nameEx("acat")
> ### * acat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: acat
> ### Title: Ordinal Regression with Adjacent Categories Probabilities
> ### Aliases: acat
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let, acat, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = acat, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
   -8.9360297    -3.0390622     2.1653729     0.9020936 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 5.347382 
Log-likelihood: -25.25054 

This is an adjacent categories model with 3 levels
> coef(fit, matrix = TRUE)
            loglink(P[Y=2]/P[Y=1]) loglink(P[Y=3]/P[Y=2])
(Intercept)              -8.936030             -3.0390622
let                       2.165373              0.9020936
> constraints(fit)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1] [,2]
[1,]    1    0
[2,]    0    1

> model.matrix(fit)
    (Intercept):1 (Intercept):2    let:1    let:2
1:1             1             0 1.757858 0.000000
1:2             0             1 0.000000 1.757858
2:1             1             0 2.708050 0.000000
2:2             0             1 0.000000 2.708050
3:1             1             0 3.068053 0.000000
3:2             0             1 0.000000 3.068053
4:1             1             0 3.314186 0.000000
4:2             0             1 0.000000 3.314186
5:1             1             0 3.511545 0.000000
5:2             0             1 0.000000 3.511545
6:1             1             0 3.676301 0.000000
6:2             0             1 0.000000 3.676301
7:1             1             0 3.828641 0.000000
7:2             0             1 0.000000 3.828641
8:1             1             0 3.941582 0.000000
8:2             0             1 0.000000 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1 2

attr(,"assign")$let
[1] 3 4

attr(,"vassign")
attr(,"vassign")$`(Intercept):1`
[1] 1

attr(,"vassign")$`(Intercept):2`
[1] 2

attr(,"vassign")$`let:1`
[1] 3

attr(,"vassign")$`let:2`
[1] 4

attr(,"constraints")
attr(,"constraints")$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"constraints")$let
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"orig.assign.lm")
[1] 0 1
> 
> 
> 
> cleanEx()
> nameEx("alogitlink")
> ### * alogitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: alogitlink
> ### Title: Arcsine-Logit Link Mixtures
> ### Aliases: alogitlink lcalogitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, length= 10)
> alogitlink(p)
 [1] -4.5768539 -1.9761899 -1.2097002 -0.6756555 -0.2185496  0.2185496
 [7]  0.6756555  1.2097002  1.9761899  4.5768539
> max(abs(alogitlink(alogitlink(p), inv = TRUE) - p))  # 0?
[1] 4.774826e-14
> 
> ## Not run: 
> ##D par(mfrow = c(2, 2), lwd = (mylwd <- 2))
> ##D y <- seq(-4, 4, length = 100)
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D 
> ##D for (d in 0:1) {
> ##D   matplot(p, cbind(logitlink(p, deriv = d), probitlink(p, deriv = d)),
> ##D           type = "n", col = "blue", ylab = "transformation",
> ##D           las = 1, main = if (d == 0) "Some probability link functions"
> ##D           else "First derivative")
> ##D   lines(p,   logitlink(p, deriv = d), col = "green")
> ##D   lines(p,  probitlink(p, deriv = d), col = "blue")
> ##D   lines(p, clogloglink(p, deriv = d), col = "tan")
> ##D   lines(p,  alogitlink(p, deriv = d), col = "red3")
> ##D   if (d == 0) {
> ##D     abline(v = 0.5, h = 0, lty = "dashed")
> ##D     legend(0, 4.5, c("logitlink", "probitlink", "clogloglink",
> ##D            "alogitlink"), lwd = mylwd,
> ##D            col = c("green", "blue", "tan", "red3"))
> ##D   } else
> ##D     abline(v = 0.5, lwd = 0.5, col = "gray")
> ##D }
> ##D 
> ##D for (d in 0) {
> ##D   matplot(y, cbind( logitlink(y, deriv = d, inverse = TRUE),
> ##D                    probitlink(y, deriv = d, inverse = TRUE)),
> ##D           type  = "n", col = "blue", xlab = "transformation", ylab = "p",
> ##D           main = if (d == 0) "Some inverse probability link functions"
> ##D           else "First derivative", las=1)
> ##D   lines(y,   logitlink(y, deriv = d, inverse = TRUE), col = "green")
> ##D   lines(y,  probitlink(y, deriv = d, inverse = TRUE), col = "blue")
> ##D   lines(y, clogloglink(y, deriv = d, inverse = TRUE), col = "tan")
> ##D   lines(y,  alogitlink(y, deriv = d, inverse = TRUE), col = "red3")
> ##D   if (d == 0) {
> ##D       abline(h = 0.5, v = 0, lwd = 0.5, col = "gray")
> ##D       legend(-4, 1, c("logitlink", "probitlink", "clogloglink",
> ##D              "alogitlink"), lwd = mylwd,
> ##D              col = c("green", "blue", "tan", "red3"))
> ##D   }
> ##D }
> ##D par(lwd = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("alteredvglm")
> ### * alteredvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: altered
> ### Title: Altered, Inflated, Truncated and Deflated Values in GAITD
> ###   Regression
> ### Aliases: altered deflated inflated truncated is.altered is.inflated
> ###   is.deflated is.truncated
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D abdata <- data.frame(y = 0:7, w = c(182, 41, 12, 2, 2, 0, 0, 1))
> ##D fit1 <- vglm(y ~ 1, gaitdpoisson(a.mix = 0),
> ##D              data = abdata, weight = w, subset = w > 0)
> ##D specials(fit1)  # All three sets
> ##D altered(fit1)  # Subject to change
> ##D inflated(fit1)  # Subject to change
> ##D truncated(fit1)  # Subject to change
> ##D is.altered(fit1)
> ##D is.inflated(fit1)
> ##D is.truncated(fit1)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("amlbinomial")
> ### * amlbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: amlbinomial
> ### Title: Binomial Logistic Regression by Asymmetric Maximum Likelihood
> ###   Estimation
> ### Aliases: amlbinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example: binomial data with lots of trials per observation
> set.seed(1234)
> sizevec <- rep(100, length = (nn <- 200))
> mydat <- data.frame(x = sort(runif(nn)))
> mydat <- transform(mydat,
+                    prob = logitlink(-0 + 2.5*x + x^2, inverse = TRUE))
> mydat <- transform(mydat, y = rbinom(nn, size = sizevec, prob = prob))
> (fit <- vgam(cbind(y, sizevec - y) ~ s(x, df = 3),
+              amlbinomial(w = c(0.01, 0.2, 1, 5, 60)),
+              mydat, trace = TRUE))
VGAM  s.vam  loop  1 :  deviance = 7483.0231
VGAM  s.vam  loop  2 :  deviance = 2782.8447
VGAM  s.vam  loop  3 :  deviance = 1793.1691
VGAM  s.vam  loop  4 :  deviance = 1622.5871
VGAM  s.vam  loop  5 :  deviance = 1608.0831
VGAM  s.vam  loop  6 :  deviance = 1605.259
VGAM  s.vam  loop  7 :  deviance = 1605.2346
VGAM  s.vam  loop  8 :  deviance = 1605.2345
VGAM  s.vam  loop  9 :  deviance = 1605.2345

Call:
vgam(formula = cbind(y, sizevec - y) ~ s(x, df = 3), family = amlbinomial(w = c(0.01, 
    0.2, 1, 5, 60)), data = mydat, trace = TRUE)


Degrees of Freedom: 1000 Total; 979.07 Residual
Residual deviance: 1605.235 
> fit@extra
$w.aml
[1]  0.01  0.20  1.00  5.00 60.00

$M
[1] 5

$n
[1] 200

$y.names
[1] "w.aml = 0.01" "w.aml = 0.2"  "w.aml = 1"    "w.aml = 5"    "w.aml = 60"  

$individual
[1] TRUE

$percentile
w.aml = 0.01  w.aml = 0.2    w.aml = 1    w.aml = 5   w.aml = 60 
         5.0         24.5         54.0         70.5         92.0 

$deviance
w.aml = 0.01  w.aml = 0.2    w.aml = 1    w.aml = 5   w.aml = 60 
    8.147741    73.670217   187.502222   400.585393   935.328975 

> 
> ## Not run: 
> ##D par(mfrow = c(1,2))
> ##D # Quantile plot
> ##D with(mydat, plot(x, jitter(y), col = "blue", las = 1, main =
> ##D      paste(paste(round(fit@extra$percentile, digits = 1), collapse = ", "),
> ##D            "percentile-expectile curves")))
> ##D with(mydat, matlines(x, 100 * fitted(fit), lwd = 2, col = "blue", lty=1))
> ##D 
> ##D # Compare the fitted expectiles with the quantiles
> ##D with(mydat, plot(x, jitter(y), col = "blue", las = 1, main =
> ##D      paste(paste(round(fit@extra$percentile, digits = 1), collapse = ", "),
> ##D            "percentile curves are red")))
> ##D with(mydat, matlines(x, 100 * fitted(fit), lwd = 2, col = "blue", lty = 1))
> ##D 
> ##D for (ii in fit@extra$percentile)
> ##D     with(mydat, matlines(x, 100 *
> ##D          qbinom(p = ii/100, size = sizevec, prob = prob) / sizevec,
> ##D                   col = "red", lwd = 2, lty = 1))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("amlexponential")
> ### * amlexponential
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: amlexponential
> ### Title: Exponential Regression by Asymmetric Maximum Likelihood
> ###   Estimation
> ### Aliases: amlexponential
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 2000
> mydat <- data.frame(x = seq(0, 1, length = nn))
> mydat <- transform(mydat,
+                    mu = loglink(-0 + 1.5*x + 0.2*x^2, inverse = TRUE))
> mydat <- transform(mydat, mu = loglink(0 - sin(8*x), inverse = TRUE))
> mydat <- transform(mydat,  y = rexp(nn, rate = 1/mu))
> (fit <- vgam(y ~ s(x, df=5), amlexponential(w=c(0.001, 0.1, 0.5, 5, 60)),
+              mydat, trace = TRUE))
VGAM  s.vam  loop  1 :  deviance = 16325.2663
VGAM  s.vam  loop  2 :  deviance = 12274.2298
VGAM  s.vam  loop  3 :  deviance = 11416.5472
VGAM  s.vam  loop  4 :  deviance = 11243.4453
VGAM  s.vam  loop  5 :  deviance = 11180.9451
VGAM  s.vam  loop  6 :  deviance = 11161.2323
VGAM  s.vam  loop  7 :  deviance = 11155.5064
VGAM  s.vam  loop  8 :  deviance = 11154.4816
VGAM  s.vam  loop  9 :  deviance = 11154.3043
VGAM  s.vam  loop  10 :  deviance = 11154.2938
VGAM  s.vam  loop  11 :  deviance = 11154.2921
VGAM  s.vam  loop  12 :  deviance = 11154.292

Call:
vgam(formula = y ~ s(x, df = 5), family = amlexponential(w = c(0.001, 
    0.1, 0.5, 5, 60)), data = mydat, trace = TRUE)


Degrees of Freedom: 10000 Total; 9969.76 Residual
Residual deviance: 11154.29 
> fit@extra
$w.aml
[1] 1e-03 1e-01 5e-01 5e+00 6e+01

$M
[1] 5

$n
[1] 2000

$y.names
[1] "w.aml = 0.001" "w.aml = 0.1"   "w.aml = 0.5"   "w.aml = 5"    
[5] "w.aml = 60"   

$individual
[1] TRUE

$percentile
w.aml = 0.001   w.aml = 0.1   w.aml = 0.5     w.aml = 5    w.aml = 60 
         4.30         32.10         53.75         82.10         96.25 

$deviance
w.aml = 0.001   w.aml = 0.1   w.aml = 0.5     w.aml = 5    w.aml = 60 
     146.9378     1125.6521     1884.9613     3249.6790     4747.0619 

> 
> ## Not run: 
> ##D  # These plots are against the sqrt scale (to increase clarity)
> ##D par(mfrow = c(1,2))
> ##D # Quantile plot
> ##D with(mydat, plot(x, sqrt(y), col = "blue", las = 1, main =
> ##D      paste(paste(round(fit@extra$percentile, digits = 1), collapse=", "),
> ##D            "percentile-expectile curves")))
> ##D with(mydat, matlines(x, sqrt(fitted(fit)), lwd = 2, col = "blue", lty=1))
> ##D 
> ##D # Compare the fitted expectiles with the quantiles
> ##D with(mydat, plot(x, sqrt(y), col = "blue", las = 1, main =
> ##D      paste(paste(round(fit@extra$percentile, digits = 1), collapse=", "),
> ##D            "percentile curves are orange")))
> ##D with(mydat, matlines(x, sqrt(fitted(fit)), lwd = 2, col = "blue", lty=1))
> ##D 
> ##D for (ii in fit@extra$percentile)
> ##D   with(mydat, matlines(x, sqrt(qexp(p = ii/100, rate = 1/mu)),
> ##D                        col = "orange")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("amlnormal")
> ### * amlnormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: amlnormal
> ### Title: Asymmetric Least Squares Quantile Regression
> ### Aliases: amlnormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1
> ##D ooo <- with(bmi.nz, order(age))
> ##D bmi.nz <- bmi.nz[ooo, ]  # Sort by age
> ##D (fit <- vglm(BMI ~ sm.bs(age), amlnormal(w.aml = 0.1), bmi.nz))
> ##D fit@extra  # Gives the w value and the percentile
> ##D coef(fit, matrix = TRUE)
> ##D 
> ##D # Quantile plot
> ##D with(bmi.nz, plot(age, BMI, col = "blue", main =
> ##D      paste(round(fit@extra$percentile, digits = 1),
> ##D            "expectile-percentile curve")))
> ##D with(bmi.nz, lines(age, c(fitted(fit)), col = "black"))
> ##D 
> ##D # Example 2
> ##D # Find the w values that give the 25, 50 and 75 percentiles
> ##D find.w <- function(w, percentile = 50) {
> ##D   fit2 <- vglm(BMI ~ sm.bs(age), amlnormal(w = w), data = bmi.nz)
> ##D   fit2@extra$percentile - percentile
> ##D }
> ##D # Quantile plot
> ##D with(bmi.nz, plot(age, BMI, col = "blue", las = 1, main =
> ##D      "25, 50 and 75 expectile-percentile curves"))
> ##D for (myp in c(25, 50, 75)) {
> ##D # Note: uniroot() can only find one root at a time
> ##D   bestw <- uniroot(f = find.w, interval = c(1/10^4, 10^4),
> ##D                    percentile = myp)
> ##D   fit2 <- vglm(BMI ~ sm.bs(age), amlnormal(w = bestw$root), bmi.nz)
> ##D   with(bmi.nz, lines(age, c(fitted(fit2)), col = "orange"))
> ##D }
> ##D 
> ##D # Example 3; this is Example 1 but with smoothing splines and
> ##D # a vector w and a parallelism assumption.
> ##D ooo <- with(bmi.nz, order(age))
> ##D bmi.nz <- bmi.nz[ooo, ]  # Sort by age
> ##D fit3 <- vgam(BMI ~ s(age, df = 4), data = bmi.nz, trace = TRUE,
> ##D              amlnormal(w = c(0.1, 1, 10), parallel = TRUE))
> ##D fit3@extra  # The w values, percentiles and weighted deviances
> ##D 
> ##D # The linear components of the fit; not for human consumption:
> ##D coef(fit3, matrix = TRUE)
> ##D 
> ##D # Quantile plot
> ##D with(bmi.nz, plot(age, BMI, col="blue", main =
> ##D   paste(paste(round(fit3@extra$percentile, digits = 1), collapse = ", "),
> ##D         "expectile-percentile curves")))
> ##D with(bmi.nz, matlines(age, fitted(fit3), col = 1:fit3@extra$M, lwd = 2))
> ##D with(bmi.nz, lines(age, c(fitted(fit )), col = "black"))  # For comparison
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("amlpoisson")
> ### * amlpoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: amlpoisson
> ### Title: Poisson Regression by Asymmetric Maximum Likelihood Estimation
> ### Aliases: amlpoisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1234)
> mydat <- data.frame(x = sort(runif(nn <- 200)))
> mydat <- transform(mydat, y = rpois(nn, exp(0 - sin(8*x))))
> (fit <- vgam(y ~ s(x), fam = amlpoisson(w.aml = c(0.02, 0.2, 1, 5, 50)),
+              mydat, trace = TRUE))
VGAM  s.vam  loop  1 :  deviance = 1585.1076
VGAM  s.vam  loop  2 :  deviance = 1373.0754
VGAM  s.vam  loop  3 :  deviance = 1347.0821
VGAM  s.vam  loop  4 :  deviance = 1334.2349
VGAM  s.vam  loop  5 :  deviance = 1332.5501
VGAM  s.vam  loop  6 :  deviance = 1332.1471
VGAM  s.vam  loop  7 :  deviance = 1332.0551
VGAM  s.vam  loop  8 :  deviance = 1332.0424
VGAM  s.vam  loop  9 :  deviance = 1332.0419
VGAM  s.vam  loop  10 :  deviance = 1332.0419

Call:
vgam(formula = y ~ s(x), family = amlpoisson(w.aml = c(0.02, 
    0.2, 1, 5, 50)), data = mydat, trace = TRUE)


Degrees of Freedom: 1000 Total; 975.76 Residual
Residual deviance: 1332.042 
> fit@extra
$w.aml
[1]  0.02  0.20  1.00  5.00 50.00

$M
[1] 5

$n
[1] 200

$y.names
[1] "w.aml = 0.02" "w.aml = 0.2"  "w.aml = 1"    "w.aml = 5"    "w.aml = 50"  

$individual
[1] TRUE

$percentile
w.aml = 0.02  w.aml = 0.2    w.aml = 1    w.aml = 5   w.aml = 50 
        41.5         48.0         62.5         75.0         92.5 

$deviance
w.aml = 0.02  w.aml = 0.2    w.aml = 1    w.aml = 5   w.aml = 50 
    22.16971     96.51532    211.59585    374.49388    627.26714 

> 
> ## Not run: 
> ##D # Quantile plot
> ##D with(mydat, plot(x, jitter(y), col = "blue", las = 1, main =
> ##D      paste(paste(round(fit@extra$percentile, digits = 1), collapse = ", "),
> ##D            "percentile-expectile curves")))
> ##D with(mydat, matlines(x, fitted(fit), lwd = 2)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("anovavglm")
> ### * anovavglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: anova.vglm
> ### Title: Analysis of Deviance for Vector Generalized Linear Model Fits
> ### Aliases: anova.vglm
> ### Keywords: htest
> 
> ### ** Examples
> 
> # Example 1: a proportional odds model fitted to pneumo.
> set.seed(1)
> pneumo <- transform(pneumo, let = log(exposure.time), x3 = runif(8))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let     , propodds, pneumo)
> fit2 <- vglm(cbind(normal, mild, severe) ~ let + x3, propodds, pneumo)
> fit3 <- vglm(cbind(normal, mild, severe) ~ let + x3, cumulative, pneumo)
> anova(fit1, fit2, fit3, type = 1)  # Remember to specify 'type'!!
Analysis of Deviance Table

Model 1: cbind(normal, mild, severe) ~ let
Model 2: cbind(normal, mild, severe) ~ let + x3
Model 3: cbind(normal, mild, severe) ~ let + x3
  Resid. Df Resid. Dev Df Deviance Pr(>Chi)
1        13     5.0268                     
2        12     5.0192  1  0.00762   0.9304
3        10     4.4299  2  0.58934   0.7448
> anova(fit2)
Analysis of Deviance Table (Type II tests)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: cbind(normal, mild, severe)

    Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
let  1   65.908        13     70.927 4.724e-16 ***
x3   1    0.008        13      5.027    0.9304    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> anova(fit2, type = "I")
Analysis of Deviance Table (Type I tests: terms added sequentially from
first to last)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: cbind(normal, mild, severe)

     Df Deviance Resid. Df Resid. Dev Pr(>Chi)    
NULL                    14    101.641             
let   1   96.614        13      5.027   <2e-16 ***
x3    1    0.008        12      5.019   0.9304    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> anova(fit2, type = "III")
Analysis of Deviance Table (Type III tests: each term added last)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: cbind(normal, mild, severe)

    Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
let  1   65.908        13     70.927 4.724e-16 ***
x3   1    0.008        13      5.027    0.9304    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> # Example 2: a proportional odds model fitted to backPain2.
> data("backPain2", package = "VGAM")
> summary(backPain2)
 x2     x3     x4                       pain   
 1:39   1:21   1:64   worse               : 5  
 2:62   2:52   2:37   same                :14  
        3:28          slight.improvement  :18  
                      moderate.improvement:20  
                      marked.improvement  :28  
                      complete.relief     :16  
> fitlogit <- vglm(pain ~ x2 * x3 * x4, propodds, data = backPain2)
> coef(fitlogit)
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5 
   5.62742642    4.03372036    3.02435251    2.00848426    0.17216407 
          x22           x32           x33           x42       x22:x32 
  -1.84984157   -1.47577010    0.05432835   -1.46663732    0.95375608 
      x22:x33       x22:x42       x32:x42       x33:x42   x22:x32:x42 
  -1.46519553    1.49258460    0.53828907   -0.46855766   -1.86609030 
  x22:x33:x42 
  -0.02565193 
> anova(fitlogit)
Analysis of Deviance Table (Type II tests)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: pain

         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
x2        1  13.2238       495     329.41 0.0002764 ***
x3        2   5.1993       497     321.48 0.0743001 .  
x4        1   7.5525       495     320.79 0.0059929 ** 
x2:x3     2   3.5567       493     315.99 0.1689175    
x2:x4     1   0.6212       492     313.06 0.4306016    
x3:x4     2   0.3554       493     312.79 0.8372021    
x2:x3:x4  2   1.2912       491     312.44 0.5243360    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> anova(fitlogit, type = "I")
Analysis of Deviance Table (Type I tests: terms added sequentially from
first to last)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: pain

         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                       500     343.06              
x2        1  15.9378       499     327.12 6.546e-05 ***
x3        2   4.5430       497     322.58   0.10315    
x4        1   6.1761       496     316.40   0.01295 *  
x2:x3     2   3.1601       494     313.24   0.20597    
x2:x4     1   0.4467       493     312.79   0.50390    
x3:x4     2   0.3554       491     312.44   0.83720    
x2:x3:x4  2   1.2912       489     311.15   0.52434    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> anova(fitlogit, type = "III")
Analysis of Deviance Table (Type III tests: each term added last)

Model: 'cumulative', 'VGAMordinal', 'VGAMcategorical'

Links: 'logitlink'

Response: pain

         Df Deviance Resid. Df Resid. Dev Pr(>Chi)
x2        1   2.6030       490     313.75   0.1067
x3        2   3.7380       491     314.88   0.1543
x4        1   1.7479       490     312.89   0.1861
x2:x3     2   3.9591       491     315.11   0.1381
x2:x4     1   0.8115       490     311.96   0.3677
x3:x4     2   0.4165       491     311.56   0.8120
x2:x3:x4  2   1.2912       491     312.44   0.5243
> 
> 
> 
> cleanEx()
> nameEx("asinlink")
> ### * asinlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: asinlink
> ### Title: Arcsine Link Function
> ### Aliases: asinlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, length= 10)
> asinlink(p)
 [1] -2.7409230 -1.7334784 -1.1514533 -0.6655492 -0.2182104  0.2182104
 [7]  0.6655492  1.1514533  1.7334784  2.7409230
> max(abs(asinlink(asinlink(p), inv = TRUE) - p))  # 0?
[1] 1.110223e-16
> 
> ## Not run: 
> ##D par(mfrow = c(2, 2), lwd = (mylwd <- 2))
> ##D y <- seq(-4, 4, length = 100)
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D 
> ##D for (d in 0:1) {
> ##D   matplot(p, cbind(logitlink(p, deriv = d), probitlink(p, deriv = d)),
> ##D           type = "n", col = "blue", ylab = "transformation",
> ##D           log = ifelse(d == 1, "y", ""),
> ##D           las = 1, main = if (d == 0) "Some probability link functions"
> ##D           else "First derivative")
> ##D   lines(p,   logitlink(p, deriv = d), col = "green")
> ##D   lines(p,  probitlink(p, deriv = d), col = "blue")
> ##D   lines(p, clogloglink(p, deriv = d), col = "tan")
> ##D   lines(p,    asinlink(p, deriv = d), col = "red3")
> ##D   if (d == 0) {
> ##D     abline(v = 0.5, h = 0, lty = "dashed")
> ##D     legend(0, 4.5, c("logitlink", "probitlink", "clogloglink",
> ##D            "asinlink"), lwd = mylwd,
> ##D            col = c("green", "blue", "tan", "red3"))
> ##D   } else
> ##D     abline(v = 0.5, lwd = 0.5, col = "gray")
> ##D }
> ##D 
> ##D for (d in 0) {
> ##D   matplot(y, cbind( logitlink(y, deriv = d, inverse = TRUE),
> ##D                    probitlink(y, deriv = d, inverse = TRUE)),
> ##D           type  = "n", col = "blue", xlab = "transformation", ylab = "p",
> ##D           main = if (d == 0) "Some inverse probability link functions"
> ##D           else "First derivative", las=1)
> ##D   lines(y,   logitlink(y, deriv = d, inverse = TRUE), col = "green")
> ##D   lines(y,  probitlink(y, deriv = d, inverse = TRUE), col = "blue")
> ##D   lines(y, clogloglink(y, deriv = d, inverse = TRUE), col = "tan")
> ##D   lines(y,    asinlink(y, deriv = d, inverse = TRUE), col = "red3")
> ##D   if (d == 0) {
> ##D       abline(h = 0.5, v = 0, lwd = 0.5, col = "gray")
> ##D       legend(-4, 1, c("logitlink", "probitlink", "clogloglink",
> ##D              "asinlink"), lwd = mylwd,
> ##D              col = c("green", "blue", "tan", "red3"))
> ##D   }
> ##D }
> ##D par(lwd = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("auuc")
> ### * auuc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: auuc
> ### Title: Auckland University Undergraduate Counts Data
> ### Aliases: auuc
> ### Keywords: datasets
> 
> ### ** Examples
> 
> auuc
     Commerce Arts SciEng Law Medicine
SES1      446  895    496 170      184
SES2      937 1834    994 246      198
SES3      311  805    430  95       48
SES4       49  157     62  15        9
> ## Not run: 
> ##D round(fitted(grc(auuc)))
> ##D round(fitted(grc(auuc, Rank = 2)))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("auxposbernoulli.t")
> ### * auxposbernoulli.t
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: aux.posbernoulli.t
> ### Title: Auxiliary Function for the Positive Bernoulli Family Function
> ###   with Time Effects
> ### Aliases: aux.posbernoulli.t
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Fit a M_tbh model to the deermice data:
> (pdata <- aux.posbernoulli.t(with(deermice,
+                                   cbind(y1, y2, y3, y4, y5, y6))))
$cap.hist1
      bei1 bei2 bei3 bei4 bei5 bei6
 [1,]    0    1    1    1    1    1
 [2,]    0    1    1    1    1    1
 [3,]    0    1    1    1    1    1
 [4,]    0    1    1    1    1    1
 [5,]    0    1    1    1    1    1
 [6,]    0    1    1    1    1    1
 [7,]    0    1    1    1    1    1
 [8,]    0    1    1    1    1    1
 [9,]    0    1    1    1    1    1
[10,]    0    1    1    1    1    1
[11,]    0    1    1    1    1    1
[12,]    0    1    1    1    1    1
[13,]    0    1    1    1    1    1
[14,]    0    1    1    1    1    1
[15,]    0    1    1    1    1    1
[16,]    0    0    1    1    1    1
[17,]    0    0    1    1    1    1
[18,]    0    0    1    1    1    1
[19,]    0    0    1    1    1    1
[20,]    0    0    1    1    1    1
[21,]    0    0    1    1    1    1
[22,]    0    0    1    1    1    1
[23,]    0    0    1    1    1    1
[24,]    0    0    0    1    1    1
[25,]    0    0    0    1    1    1
[26,]    0    0    0    1    1    1
[27,]    0    0    0    1    1    1
[28,]    0    0    0    1    1    1
[29,]    0    0    0    1    1    1
[30,]    0    0    0    0    1    1
[31,]    0    0    0    0    1    1
[32,]    0    0    0    0    1    1
[33,]    0    0    0    0    0    1
[34,]    0    0    0    0    0    1
[35,]    0    0    0    0    0    1
[36,]    0    0    0    0    0    0
[37,]    0    0    0    0    0    0
[38,]    0    0    0    0    0    0

$cap1
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 5 5 5 6 6 6

$y0i
 [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 4 4 4 5 5 5

$yr0i
 [1] 0 2 2 1 0 1 1 2 0 1 1 1 0 2 4 3 2 3 2 2 2 3 2 3 0 1 0 2 3 2 0 1 1 1 1 0 0 0

$yr1i
 [1] 5 3 3 4 5 4 4 3 5 4 4 4 5 3 1 1 2 1 2 2 2 1 2 0 3 2 3 1 0 0 2 1 0 0 0 0 0 0

> 
> deermice <- data.frame(deermice,
+                     bei = 0,  # Add this
+                     pdata$cap.hist1)  # Incorporate these
> head(deermice)  # Augmented with behavioural effect indicator variables
  y1 y2 y3 y4 y5 y6 sex age weight bei bei1 bei2 bei3 bei4 bei5 bei6
1  1  1  1  1  1  1   0   y     12   0    0    1    1    1    1    1
2  1  0  0  1  1  1   1   y     15   0    0    1    1    1    1    1
3  1  1  0  0  1  1   0   y     15   0    0    1    1    1    1    1
4  1  1  0  1  1  1   0   y     15   0    0    1    1    1    1    1
5  1  1  1  1  1  1   0   y     13   0    0    1    1    1    1    1
6  1  1  0  1  1  1   0   a     21   0    0    1    1    1    1    1
> tail(deermice)
   y1 y2 y3 y4 y5 y6 sex age weight bei bei1 bei2 bei3 bei4 bei5 bei6
33  0  0  0  0  1  0   0   y     14   0    0    0    0    0    0    1
34  0  0  0  0  1  0   1   y     11   0    0    0    0    0    0    1
35  0  0  0  0  1  0   0   a     24   0    0    0    0    0    0    1
36  0  0  0  0  0  1   0   y      9   0    0    0    0    0    0    0
37  0  0  0  0  0  1   0   a     16   0    0    0    0    0    0    0
38  0  0  0  0  0  1   1   a     19   0    0    0    0    0    0    0
> 
> 
> 
> cleanEx()
> nameEx("backPain")
> ### * backPain
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: backPain
> ### Title: Data on Back Pain Prognosis, from Anderson (1984)
> ### Aliases: backPain backPain2
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(backPain)
       x1              x2              x3                          pain   
 Min.   :1.000   Min.   :1.000   Min.   :1.000   worse               : 5  
 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000   same                :14  
 Median :2.000   Median :2.000   Median :1.000   slight.improvement  :18  
 Mean   :1.614   Mean   :2.069   Mean   :1.366   moderate.improvement:20  
 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   marked.improvement  :28  
 Max.   :2.000   Max.   :3.000   Max.   :2.000   complete.relief     :16  
> summary(backPain2)
 x2     x3     x4                       pain   
 1:39   1:21   1:64   worse               : 5  
 2:62   2:52   2:37   same                :14  
        3:28          slight.improvement  :18  
                      moderate.improvement:20  
                      marked.improvement  :28  
                      complete.relief     :16  
> 
> 
> 
> cleanEx()
> nameEx("beggs")
> ### * beggs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: beggs
> ### Title: Bacon and Eggs Data
> ### Aliases: beggs
> ### Keywords: datasets
> 
> ### ** Examples
> 
> beggs
    e0  e1 e2 e3 e4
b0 254 115 42 13  6
b1  34  29 16  6  1
b2   8   8  3  3  1
b3   0   0  4  1  1
b4   1   1  1  0  0
> colSums(beggs)
 e0  e1  e2  e3  e4 
297 153  66  23   9 
> rowSums(beggs)
 b0  b1  b2  b3  b4 
430  86  23   6   3 
> 
> 
> 
> cleanEx()
> nameEx("bell")
> ### * bell
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bell
> ### Title: The Bell Series of Integers
> ### Aliases: bell
> ### Keywords: math
> 
> ### ** Examples
>  ## Not run: 
> ##D plot(0:10, bell(0:10), log = "y", type = "h", col = "blue")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("benfUC")
> ### * benfUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Benford
> ### Title: Benford's Distribution
> ### Aliases: Benford dbenf pbenf qbenf rbenf
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dbenf(x <- c(0:10, NA, NaN, -Inf, Inf))
 [1] 0.00000000 0.30103000 0.17609126 0.12493874 0.09691001 0.07918125
 [7] 0.06694679 0.05799195 0.05115252 0.04575749 0.00000000         NA
[13]        NaN 0.00000000 0.00000000
> pbenf(x)
 [1] 0.0000000 0.3010300 0.4771213 0.6020600 0.6989700 0.7781513 0.8450980
 [8] 0.9030900 0.9542425 1.0000000 1.0000000        NA       NaN 0.0000000
[15] 1.0000000
> 
> ## Not run: 
> ##D xx <- 1:9
> ##D barplot(dbenf(xx), col = "lightblue", xlab = "Leading digit",
> ##D         ylab = "Probability", names.arg = as.character(xx),
> ##D         main = "Benford's distribution", las = 1)
> ##D 
> ##D hist(rbenf(1000), border = "blue", prob = TRUE,
> ##D      main = "1000 random variates from Benford's distribution",
> ##D      xlab = "Leading digit", sub="Red is the true probability",
> ##D      breaks = 0:9 + 0.5, ylim = c(0, 0.35), xlim = c(0, 10.0))
> ##D lines(xx, dbenf(xx), col = "red", type = "h")
> ##D points(xx, dbenf(xx), col = "red")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("benini")
> ### * benini
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: benini1
> ### Title: Benini Distribution Family Function
> ### Aliases: benini1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> y0 <- 1; nn <- 3000
> bdata <- data.frame(y  = rbenini(nn, y0 = y0, shape = exp(2)))
> fit <- vglm(y ~ 1, benini1(y0 = y0), data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 180.851727
VGLM    linear loop  2 :  loglikelihood = 180.851727
> coef(fit, matrix = TRUE)
            loglink(shape)
(Intercept)       1.977232
> Coef(fit)
   shape 
7.222725 
> fit@extra$y0
[1] 1
> c(head(fitted(fit), 1), with(bdata, median(y)))  # Should be equal
[1] 1.363134 1.369999
> 
> 
> 
> cleanEx()
> nameEx("beniniUC")
> ### * beniniUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Benini
> ### Title: The Benini Distribution
> ### Aliases: Benini dbenini pbenini qbenini rbenini
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D y0 <- 1; shape <- exp(1)
> ##D xx <- seq(0.0, 4, len = 101)
> ##D plot(xx, dbenini(xx, y0 = y0, shape = shape), col = "blue",
> ##D      main = "Blue is density, orange is the CDF", type = "l",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      ylim = 0:1, las = 1, ylab = "", xlab = "x")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(xx, pbenini(xx, y0 = y0, shape = shape), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qbenini(probs, y0 = y0, shape = shape)
> ##D lines(Q, dbenini(Q, y0 = y0, shape = shape),
> ##D       col = "purple", lty = 3, type = "h")
> ##D pbenini(Q, y0 = y0, shape = shape) - probs  # Should be all zero
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betaII")
> ### * betaII
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betaII
> ### Title: Beta Distribution of the Second Kind
> ### Aliases: betaII
> ### Keywords: models regression
> 
> ### ** Examples
> 
> bdata <- data.frame(y = rsinmad(2000, shape1.a = 1,
+          shape3.q = exp(2), scale = exp(1)))  # Not genuine data!
> # fit <- vglm(y ~ 1, betaII, data = bdata, trace = TRUE)
> fit <- vglm(y ~ 1, betaII(ishape2.p = 0.7, ishape3.q = 0.7),
+             data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -331.984745
VGLM    linear loop  2 :  loglikelihood = -277.017157
VGLM    linear loop  3 :  loglikelihood = -268.86177
VGLM    linear loop  4 :  loglikelihood = -267.185375
VGLM    linear loop  5 :  loglikelihood = -267.092132
VGLM    linear loop  6 :  loglikelihood = -267.091032
VGLM    linear loop  7 :  loglikelihood = -267.091028
VGLM    linear loop  8 :  loglikelihood = -267.091028
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(shape2.p) loglink(shape3.q)
(Intercept)       0.915111       -0.02563288          1.903107
> Coef(fit)
    scale  shape2.p  shape3.q 
2.4970524 0.9746929 6.7067015 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = betaII(ishape2.p = 0.7, ishape3.q = 0.7), 
    data = bdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.91511    0.26867   3.406 0.000659 ***
(Intercept):2 -0.02563    0.03745  -0.684 0.493724    
(Intercept):3  1.90311    0.21324   8.925  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), loglink(shape2.p), 
loglink(shape3.q)

Log-likelihood: -267.091 on 5997 degrees of freedom

Number of Fisher scoring iterations: 8 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):3'

> 
> 
> 
> cleanEx()
> nameEx("betaR")
> ### * betaR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betaR
> ### Title: The Two-parameter Beta Distribution Family Function
> ### Aliases: betaR
> ### Keywords: models regression
> 
> ### ** Examples
> 
> bdata <- data.frame(y = rbeta(1000, shape1 = exp(0), shape2 = exp(1)))
> fit <- vglm(y ~ 1, betaR(lshape1 = "identitylink",
+             lshape2 = "identitylink"), bdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.9507808, 2.6283258
VGLM    linear loop  2 :  coefficients = 0.95876445, 2.64306529
VGLM    linear loop  3 :  coefficients = 0.95883953, 2.64321113
VGLM    linear loop  4 :  coefficients = 0.95883954, 2.64321114
> fit <- vglm(y ~ 1, betaR, data = bdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -0.047513835,  0.966560761
VGLM    linear loop  2 :  coefficients = -0.042049445,  0.971976778
VGLM    linear loop  3 :  coefficients = -0.04203154,  0.97199452
VGLM    linear loop  4 :  coefficients = -0.04203154,  0.97199452
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)     -0.04203154       0.9719945
> Coef(fit)  # Useful for intercept-only models
   shape1    shape2 
0.9588395 2.6432111 
> 
> bdata <- transform(bdata, Y = 5 + 8 * y)  # From 5 to 13, not 0 to 1
> fit <- vglm(Y ~ 1, betaR(A = 5, B = 13), data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1706.4661
VGLM    linear loop  2 :  loglikelihood = -1706.4561
VGLM    linear loop  3 :  loglikelihood = -1706.4561
> Coef(fit)
   shape1    shape2 
0.9588395 2.6432111 
> c(meanY = with(bdata, mean(Y)), head(fitted(fit),2))
   meanY                   
7.140081 7.129541 7.129541 
> 
> 
> 
> cleanEx()
> nameEx("betabinomUC")
> ### * betabinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Betabinom
> ### Title: The Beta-Binomial Distribution
> ### Aliases: Betabinom dbetabinom pbetabinom rbetabinom dbetabinom.ab
> ###   pbetabinom.ab rbetabinom.ab dzoibetabinom pzoibetabinom rzoibetabinom
> ###   dzoibetabinom.ab pzoibetabinom.ab rzoibetabinom.ab
> ### Keywords: distribution
> 
> ### ** Examples
> 
> set.seed(1); rbetabinom(10, 100, prob = 0.5)
 [1] 52 46 60 49 52 51 63 52 47 38
> set.seed(1);     rbinom(10, 100, prob = 0.5)  # The same as rho = 0
 [1] 52 46 60 49 52 51 63 52 47 38
> 
> ## Not run: 
> ##D  N <- 9; xx <- 0:N; s1 <- 2; s2 <- 3
> ##D dy <- dbetabinom.ab(xx, size = N, shape1 = s1, shape2 = s2)
> ##D barplot(rbind(dy, dbinom(xx, size = N, prob = s1 / (s1+s2))),
> ##D         beside = TRUE, col = c("blue","green"), las = 1,
> ##D         main = paste("Beta-binomial (size=",N,", shape1=", s1,
> ##D                    ", shape2=", s2, ") (blue) vs\n",
> ##D         " Binomial(size=", N, ", prob=", s1/(s1+s2), ") (green)",
> ##D                      sep = ""),
> ##D         names.arg = as.character(xx), cex.main = 0.8)
> ##D sum(dy * xx)  # Check expected values are equal
> ##D sum(dbinom(xx, size = N, prob = s1 / (s1+s2)) * xx)
> ##D # Should be all 0:
> ##D cumsum(dy) - pbetabinom.ab(xx, N, shape1 = s1, shape2 = s2)
> ##D 
> ##D y <- rbetabinom.ab(n = 1e4, size = N, shape1 = s1, shape2 = s2)
> ##D ty <- table(y)
> ##D barplot(rbind(dy, ty / sum(ty)),
> ##D         beside = TRUE, col = c("blue", "orange"), las = 1,
> ##D         main = paste("Beta-binomial (size=", N, ", shape1=", s1,
> ##D                      ", shape2=", s2, ") (blue) vs\n",
> ##D         " Random generated beta-binomial(size=", N, ", prob=",
> ##D         s1/(s1+s2), ") (orange)", sep = ""), cex.main = 0.8,
> ##D         names.arg = as.character(xx))
> ##D 
> ##D N <- 1e5; size <- 20; pstr0 <- 0.2; pstrsize <- 0.2
> ##D kk <- rzoibetabinom.ab(N, size, s1, s2, pstr0, pstrsize)
> ##D hist(kk, probability = TRUE, border = "blue", ylim = c(0, 0.25),
> ##D      main = "Blue/green = inflated; orange = ordinary beta-binomial",
> ##D      breaks = -0.5 : (size + 0.5))
> ##D sum(kk == 0) / N  # Proportion of 0
> ##D sum(kk == size) / N  # Proportion of size
> ##D lines(0 : size,
> ##D       dbetabinom.ab(0 : size, size, s1, s2), col = "orange")
> ##D lines(0 : size, col = "green", type = "b",
> ##D       dzoibetabinom.ab(0 : size, size, s1, s2, pstr0, pstrsize))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betabinomial")
> ### * betabinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betabinomial
> ### Title: Beta-binomial Distribution Family Function
> ### Aliases: betabinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1
> bdata <- data.frame(N = 10, mu = 0.5, rho = 0.8)
> bdata <- transform(bdata,
+             y = rbetabinom(100, size = N, prob = mu, rho = rho))
> fit <- vglm(cbind(y, N-y) ~ 1, betabinomial, bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -177.56046
VGLM    linear loop  2 :  loglikelihood = -177.56043
VGLM    linear loop  3 :  loglikelihood = -177.56043
> coef(fit, matrix = TRUE)
            logitlink(mu) logitlink(rho)
(Intercept)    0.08693693       1.212346
> Coef(fit)
       mu       rho 
0.5217206 0.7707137 
> head(cbind(depvar(fit), weights(fit, type = "prior")))
  [,1] [,2]
1    1   10
2    0   10
3    1   10
4    0   10
5    0   10
6    0   10
> 
> 
> # Example 2
> fit <- vglm(cbind(R, N-R) ~ 1, betabinomial, lirat,
+             trace = TRUE, subset = N > 1)
VGLM    linear loop  1 :  loglikelihood = -122.69536
VGLM    linear loop  2 :  loglikelihood = -122.69531
VGLM    linear loop  3 :  loglikelihood = -122.69531
> coef(fit, matrix = TRUE)
            logitlink(mu) logitlink(rho)
(Intercept)    -0.1190049      0.4069599
> Coef(fit)
       mu       rho 
0.4702838 0.6003587 
> t(fitted(fit))
          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,50]     [,51]     [,52]     [,53]     [,54]     [,55]     [,56]
[1,] 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838 0.4702838
         [,57]
[1,] 0.4702838
> t(depvar(fit))
       1         2    3 4 5         6 7 8 9  10 11  12 13        14        15
[1,] 0.1 0.3636364 0.75 1 1 0.8181818 1 1 1 0.7  1 0.9  1 0.8181818 0.6666667
            16 17        18        19        20        21 22        23
[1,] 0.7777778  1 0.5833333 0.8181818 0.6153846 0.3571429  1 0.8333333
            24 25        26 27   28 29        30 31  32        33         34 35
[1,] 0.6153846  1 0.2142857  1 0.75  1 0.3846154  1 0.1 0.3333333 0.07692308  0
            36        37        38     39 40 41 43 44         45 46         47
[1,] 0.2857143 0.2222222 0.1538462 0.0625  0  0  0  0 0.09090909  0 0.07142857
     48 49 50        51        52 53 54         55 56 57 58
[1,]  0  0  0 0.2222222 0.1176471  0  0 0.07142857  0  0  0
> t(weights(fit, type = "prior"))
      1  2  3 4  5  6 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[1,] 10 11 12 4 10 11 9 11 10 10 12 10  8 11  6  9 14 12 11 13 14 10 12 13 10
     26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 43 44 45 46 47 48 49 50 51
[1,] 14 13  4  8 13 12 10  3 13 12 14  9 13 16 11  4 12  8 11 14 14 11  3 13  9
     52 53 54 55 56 57 58
[1,] 17 15  2 14  8  6 17
> 
> 
> # Example 3, which is more complicated
> lirat <- transform(lirat, fgrp = factor(grp))
> summary(lirat)  # Only 5 litters in group 3
       N               R                hb              grp        fgrp  
 Min.   : 1.00   Min.   : 0.000   Min.   : 3.100   Min.   :1.000   1:31  
 1st Qu.: 9.00   1st Qu.: 0.250   1st Qu.: 4.325   1st Qu.:1.000   2:12  
 Median :11.00   Median : 3.500   Median : 6.500   Median :1.000   3: 5  
 Mean   :10.47   Mean   : 4.603   Mean   : 7.798   Mean   :1.897   4:10  
 3rd Qu.:13.00   3rd Qu.: 9.000   3rd Qu.:10.775   3rd Qu.:2.750         
 Max.   :17.00   Max.   :14.000   Max.   :16.600   Max.   :4.000         
> fit2 <- vglm(cbind(R, N-R) ~ fgrp + hb, betabinomial(zero = 2),
+              data = lirat, trace = TRUE, subset = N > 1)
VGLM    linear loop  1 :  loglikelihood = -104.03084
VGLM    linear loop  2 :  loglikelihood = -94.204685
VGLM    linear loop  3 :  loglikelihood = -93.00179
VGLM    linear loop  4 :  loglikelihood = -92.911711
VGLM    linear loop  5 :  loglikelihood = -92.908006
VGLM    linear loop  6 :  loglikelihood = -92.907857
VGLM    linear loop  7 :  loglikelihood = -92.907851
VGLM    linear loop  8 :  loglikelihood = -92.907851
> coef(fit2, matrix = TRUE)
            logitlink(mu) logitlink(rho)
(Intercept)     2.0895865      -1.171752
fgrp2          -2.4529617       0.000000
fgrp3          -2.8840242       0.000000
fgrp4          -2.3637974       0.000000
hb             -0.1609718       0.000000
> ## Not run: 
> ##D  with(lirat, plot(hb[N > 1], fit2@misc$rho,
> ##D          xlab = "Hemoglobin", ylab = "Estimated rho",
> ##D          pch = as.character(grp[N > 1]), col = grp[N > 1])) 
> ## End(Not run)
> ## Not run: 
> ##D   # cf. Figure 3 of Moore and Tsiatis (1991)
> ##D with(lirat, plot(hb, R / N, pch = as.character(grp), col = grp,
> ##D          xlab = "Hemoglobin level", ylab = "Proportion Dead",
> ##D          main = "Fitted values (lines)", las = 1))
> ##D smalldf <- with(lirat, lirat[N > 1, ])
> ##D for (gp in 1:4) {
> ##D   xx <- with(smalldf, hb[grp == gp])
> ##D   yy <- with(smalldf, fitted(fit2)[grp == gp])
> ##D   ooo <- order(xx)
> ##D   lines(xx[ooo], yy[ooo], col = gp, lwd = 2)
> ##D } 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betabinomialff")
> ### * betabinomialff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betabinomialff
> ### Title: Beta-binomial Distribution Family Function
> ### Aliases: betabinomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1
> N <- 10; s1 <- exp(1); s2 <- exp(2)
> y <- rbetabinom.ab(n = 100, size = N, shape1 = s1, shape2 = s2)
> fit <- vglm(cbind(y, N-y) ~ 1, betabinomialff, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -191.83583
VGLM    linear loop  2 :  loglikelihood = -191.26973
VGLM    linear loop  3 :  loglikelihood = -191.26429
VGLM    linear loop  4 :  loglikelihood = -191.26429
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)         1.18108        2.274247
> Coef(fit)
  shape1   shape2 
3.257890 9.720592 
> head(fit@misc$rho)  # The correlation parameter
[1] 0.07153853 0.07153853 0.07153853 0.07153853 0.07153853 0.07153853
> head(cbind(depvar(fit), weights(fit, type = "prior")))
  [,1] [,2]
1  0.1   10
2  0.3   10
3  0.3   10
4  0.3   10
5  0.1   10
6  0.1   10
> 
> 
> # Example 2
> fit <- vglm(cbind(R, N-R) ~ 1, betabinomialff, data = lirat,
+             trace = TRUE, subset = N > 1)
VGLM    linear loop  1 :  loglikelihood = -123.51975
VGLM    linear loop  2 :  loglikelihood = -122.69831
VGLM    linear loop  3 :  loglikelihood = -122.69531
VGLM    linear loop  4 :  loglikelihood = -122.69531
VGLM    linear loop  5 :  loglikelihood = -122.69531
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)       -1.161384       -1.042375
> Coef(fit)
   shape1    shape2 
0.3130526 0.3526163 
> fit@misc$rho  # The correlation parameter
 [1] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
 [8] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[15] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[22] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[29] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[36] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[43] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[50] 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594 0.6003594
[57] 0.6003594
> t(fitted(fit))
          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
          [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,50]     [,51]     [,52]     [,53]     [,54]     [,55]     [,56]
[1,] 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828 0.4702828
         [,57]
[1,] 0.4702828
> t(depvar(fit))
       1         2    3 4 5         6 7 8 9  10 11  12 13        14        15
[1,] 0.1 0.3636364 0.75 1 1 0.8181818 1 1 1 0.7  1 0.9  1 0.8181818 0.6666667
            16 17        18        19        20        21 22        23
[1,] 0.7777778  1 0.5833333 0.8181818 0.6153846 0.3571429  1 0.8333333
            24 25        26 27   28 29        30 31  32        33         34 35
[1,] 0.6153846  1 0.2142857  1 0.75  1 0.3846154  1 0.1 0.3333333 0.07692308  0
            36        37        38     39 40 41 43 44         45 46         47
[1,] 0.2857143 0.2222222 0.1538462 0.0625  0  0  0  0 0.09090909  0 0.07142857
     48 49 50        51        52 53 54         55 56 57 58
[1,]  0  0  0 0.2222222 0.1176471  0  0 0.07142857  0  0  0
> t(weights(fit, type = "prior"))
      1  2  3 4  5  6 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[1,] 10 11 12 4 10 11 9 11 10 10 12 10  8 11  6  9 14 12 11 13 14 10 12 13 10
     26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 43 44 45 46 47 48 49 50 51
[1,] 14 13  4  8 13 12 10  3 13 12 14  9 13 16 11  4 12  8 11 14 14 11  3 13  9
     52 53 54 55 56 57 58
[1,] 17 15  2 14  8  6 17
> # A "loglink" link for the 2 shape params is a logistic regression:
> all.equal(c(fitted(fit)),
+           as.vector(logitlink(predict(fit)[, 1] -
+                           predict(fit)[, 2], inverse = TRUE)))
[1] TRUE
> 
> 
> # Example 3, which is more complicated
> lirat <- transform(lirat, fgrp = factor(grp))
> summary(lirat)  # Only 5 litters in group 3
       N               R                hb              grp        fgrp  
 Min.   : 1.00   Min.   : 0.000   Min.   : 3.100   Min.   :1.000   1:31  
 1st Qu.: 9.00   1st Qu.: 0.250   1st Qu.: 4.325   1st Qu.:1.000   2:12  
 Median :11.00   Median : 3.500   Median : 6.500   Median :1.000   3: 5  
 Mean   :10.47   Mean   : 4.603   Mean   : 7.798   Mean   :1.897   4:10  
 3rd Qu.:13.00   3rd Qu.: 9.000   3rd Qu.:10.775   3rd Qu.:2.750         
 Max.   :17.00   Max.   :14.000   Max.   :16.600   Max.   :4.000         
> fit2 <- vglm(cbind(R, N-R) ~ fgrp + hb, betabinomialff(zero = 2),
+            data = lirat, trace = TRUE, subset = N > 1)
VGLM    linear loop  1 :  loglikelihood = -107.09354
VGLM    linear loop  2 :  loglikelihood = -99.955137
VGLM    linear loop  3 :  loglikelihood = -98.976074
VGLM    linear loop  4 :  loglikelihood = -98.873897
VGLM    linear loop  5 :  loglikelihood = -98.863768
VGLM    linear loop  6 :  loglikelihood = -98.862731
VGLM    linear loop  7 :  loglikelihood = -98.862626
VGLM    linear loop  8 :  loglikelihood = -98.862615
VGLM    linear loop  9 :  loglikelihood = -98.862614
> coef(fit2, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)        1.532068       -0.130058
fgrp2             -1.955610        0.000000
fgrp3             -2.412000        0.000000
fgrp4             -2.176017        0.000000
hb                -0.105682        0.000000
> coef(fit2, matrix = TRUE)[, 1] -
+ coef(fit2, matrix = TRUE)[, 2]  # logitlink(p)
(Intercept)       fgrp2       fgrp3       fgrp4          hb 
   1.662126   -1.955610   -2.412000   -2.176017   -0.105682 
> ## Not run: 
> ##D  with(lirat, plot(hb[N > 1], fit2@misc$rho,
> ##D    xlab = "Hemoglobin", ylab = "Estimated rho",
> ##D    pch = as.character(grp[N > 1]), col = grp[N > 1])) 
> ## End(Not run)
> ## Not run: 
> ##D   # cf. Figure 3 of Moore and Tsiatis (1991)
> ##D with(lirat, plot(hb, R / N, pch = as.character(grp), col = grp,
> ##D    xlab = "Hemoglobin level", ylab = "Proportion Dead", las = 1,
> ##D    main = "Fitted values (lines)"))
> ##D 
> ##D smalldf <- with(lirat, lirat[N > 1, ])
> ##D for (gp in 1:4) {
> ##D   xx <- with(smalldf, hb[grp == gp])
> ##D   yy <- with(smalldf, fitted(fit2)[grp == gp])
> ##D   ooo <- order(xx)
> ##D   lines(xx[ooo], yy[ooo], col = gp, lwd = 2)
> ##D } 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betaff")
> ### * betaff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betaff
> ### Title: The Two-parameter Beta Distribution Family Function
> ### Aliases: betaff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> bdata <- data.frame(y = rbeta(nn <- 1000, shape1 = exp(0),
+                               shape2 = exp(1)))
> fit1 <- vglm(y ~ 1, betaff, data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 370.77992
VGLM    linear loop  2 :  loglikelihood = 372.98027
VGLM    linear loop  3 :  loglikelihood = 372.98541
VGLM    linear loop  4 :  loglikelihood = 372.98541
> coef(fit1, matrix = TRUE)
            logitlink(mu) loglink(phi)
(Intercept)     -1.014026     1.281503
> Coef(fit1)  # Useful for intercept-only models
       mu       phi 
0.2661927 3.6020507 
> 
> # General A and B, and with a covariate
> bdata <- transform(bdata, x2 = runif(nn))
> bdata <- transform(bdata, mu = logitlink(0.5 - x2, inverse = TRUE),
+                           prec = exp(3.0 + x2))  # prec == phi
> bdata <- transform(bdata, shape2 = prec * (1 - mu),
+                           shape1 = mu * prec)
> bdata <- transform(bdata,
+                    y = rbeta(nn, shape1 = shape1, shape2 = shape2))
> bdata <- transform(bdata, Y = 5 + 8 * y)  # From 5--13, not 0--1
> fit <- vglm(Y ~ x2, data = bdata, trace = TRUE,
+    betaff(A = 5, B = 13, lmu = extlogitlink(min = 5, max = 13)))
VGLM    linear loop  1 :  loglikelihood = -1132.9927
VGLM    linear loop  2 :  loglikelihood = -1058.4458
VGLM    linear loop  3 :  loglikelihood = -1049.5817
VGLM    linear loop  4 :  loglikelihood = -1049.451
VGLM    linear loop  5 :  loglikelihood = -1049.451
VGLM    linear loop  6 :  loglikelihood = -1049.451
> coef(fit, matrix = TRUE)
            extlogitlink(mu, min = 5, max = 13) loglink(phi)
(Intercept)                           0.5239043    3.0326195
x2                                   -0.9944360    0.8432138
> 
> 
> 
> cleanEx()
> nameEx("betageomUC")
> ### * betageomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Betageom
> ### Title: The Beta-Geometric Distribution
> ### Aliases: Betageom dbetageom pbetageom rbetageom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape1 <- 1; shape2 <- 2; y <- 0:30
> ##D proby <- dbetageom(y, shape1, shape2, log = FALSE)
> ##D plot(y, proby, type = "h", col = "blue", ylab = "P[Y=y]", main = paste0(
> ##D      "Y ~ Beta-geometric(shape1=", shape1,", shape2=", shape2, ")"))
> ##D sum(proby)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betageometric")
> ### * betageometric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betageometric
> ### Title: Beta-geometric Distribution Family Function
> ### Aliases: betageometric
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D bdata <- data.frame(y = 0:11,
> ##D                     wts = c(227,123,72,42,21,31,11,14,6,4,7,28))
> ##D fitb <- vglm(y ~ 1, betageometric, bdata, weight = wts, trace = TRUE)
> ##D fitg <- vglm(y ~ 1,     geometric, bdata, weight = wts, trace = TRUE)
> ##D coef(fitb, matrix = TRUE)
> ##D Coef(fitb)
> ##D sqrt(diag(vcov(fitb, untransform = TRUE)))
> ##D fitb@misc$shape1
> ##D fitb@misc$shape2
> ##D # Very strong evidence of a beta-geometric:
> ##D pchisq(2 * (logLik(fitb) - logLik(fitg)), df = 1, lower.tail = FALSE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betanormUC")
> ### * betanormUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Betanorm
> ### Title: The Beta-Normal Distribution
> ### Aliases: Betanorm dbetanorm pbetanorm qbetanorm rbetanorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape1 <- 0.1; shape2 <- 4; m <- 1
> ##D x <- seq(-10, 2, len = 501)
> ##D plot(x, dbetanorm(x, shape1, shape2, m = m), type = "l",
> ##D      ylim = 0:1, las = 1,
> ##D      ylab = paste0("betanorm(",shape1,", ",shape2,", m=",m, ", sd=1)"),
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Gray lines are the 10,20,...,90 percentiles", col = "blue")
> ##D lines(x, pbetanorm(x, shape1, shape2, m = m), col = "orange")
> ##D abline(h = 0, col = "black")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qbetanorm(probs, shape1, shape2, m = m)
> ##D lines(Q, dbetanorm(Q, shape1, shape2, m = m),
> ##D       col = "gray50", lty = 2, type = "h")
> ##D lines(Q, pbetanorm(Q, shape1, shape2, m = m),
> ##D       col = "gray50", lty = 2, type = "h")
> ##D abline(h = probs, col = "gray50", lty = 2)
> ##D pbetanorm(Q, shape1, shape2, m = m) - probs  # Should be all 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("betaprime")
> ### * betaprime
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betaprime
> ### Title: The Beta-Prime Distribution
> ### Aliases: betaprime
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> bdata <- data.frame(shape1 = exp(1), shape2 = exp(3))
> bdata <- transform(bdata, yb = rbeta(nn, shape1, shape2))
> bdata <- transform(bdata, y1 = (1-yb) /    yb,
+                           y2 =    yb  / (1-yb),
+                           y3 = rgamma(nn, exp(3)) / rgamma(nn, exp(2)))
> 
> fit1 <- vglm(y1 ~ 1, betaprime, data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -4775.2581
VGLM    linear loop  2 :  loglikelihood = -4169.8419
VGLM    linear loop  3 :  loglikelihood = -3711.6884
VGLM    linear loop  4 :  loglikelihood = -3410.9574
VGLM    linear loop  5 :  loglikelihood = -3282.2622
VGLM    linear loop  6 :  loglikelihood = -3261.6451
VGLM    linear loop  7 :  loglikelihood = -3261.1751
VGLM    linear loop  8 :  loglikelihood = -3261.1748
VGLM    linear loop  9 :  loglikelihood = -3261.1748
> coef(fit1, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)        2.952206       0.9456458
> 
> fit2 <- vglm(y2 ~ 1, betaprime, data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -8918.846
VGLM    linear loop  2 :  loglikelihood = -7919.0013
VGLM    linear loop  3 :  loglikelihood = -6919.423
VGLM    linear loop  4 :  loglikelihood = -5920.5673
VGLM    linear loop  5 :  loglikelihood = -4923.6634
VGLM    linear loop  6 :  loglikelihood = -3931.9751
VGLM    linear loop  7 :  loglikelihood = -2953.8393
VGLM    linear loop  8 :  loglikelihood = -2008.6142
VGLM    linear loop  9 :  loglikelihood = -1132.7418
VGLM    linear loop  10 :  loglikelihood = -372.90291
VGLM    linear loop  11 :  loglikelihood = 238.36709
VGLM    linear loop  12 :  loglikelihood = 691.8902
VGLM    linear loop  13 :  loglikelihood = 978.86838
VGLM    linear loop  14 :  loglikelihood = 1095.4435
VGLM    linear loop  15 :  loglikelihood = 1112.1767
VGLM    linear loop  16 :  loglikelihood = 1112.4851
VGLM    linear loop  17 :  loglikelihood = 1112.4852
VGLM    linear loop  18 :  loglikelihood = 1112.4852
> coef(fit2, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)       0.9456458        2.952206
> 
> fit3 <- vglm(y3 ~ 1, betaprime, data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2184.7505
VGLM    linear loop  2 :  loglikelihood = -1848.4631
VGLM    linear loop  3 :  loglikelihood = -1662.0869
VGLM    linear loop  4 :  loglikelihood = -1615.1423
VGLM    linear loop  5 :  loglikelihood = -1612.691
VGLM    linear loop  6 :  loglikelihood = -1612.6847
VGLM    linear loop  7 :  loglikelihood = -1612.6847
> coef(fit3, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)        3.005983        2.025456
> 
> # Compare the fitted values
> with(bdata, mean(y3))
[1] 3.090932
> head(fitted(fit3))
         [,1]
[1,] 3.071035
[2,] 3.071035
[3,] 3.071035
[4,] 3.071035
[5,] 3.071035
[6,] 3.071035
> Coef(fit3)  # Useful for intercept-only models
   shape1    shape2 
20.206068  7.579564 
> 
> 
> 
> cleanEx()
> nameEx("biamhcop")
> ### * biamhcop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biamhcop
> ### Title: Ali-Mikhail-Haq Distribution Family Function
> ### Aliases: biamhcop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- rbiamhcop(1000, apar = rhobitlink(2, inverse = TRUE))
> fit <- vglm(ymat ~ 1, biamhcop, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 45.260874
VGLM    linear loop  2 :  loglikelihood = 45.44503
VGLM    linear loop  3 :  loglikelihood = 45.445164
VGLM    linear loop  4 :  loglikelihood = 45.445164
> coef(fit, matrix = TRUE)
            rhobitlink(apar)
(Intercept)         1.692639
> Coef(fit)
     apar 
0.6891419 
> 
> 
> 
> cleanEx()
> nameEx("biamhcopUC")
> ### * biamhcopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Biamhcop
> ### Title: Ali-Mikhail-Haq Bivariate Distribution
> ### Aliases: Biamhcop dbiamhcop pbiamhcop rbiamhcop
> ### Keywords: distribution
> 
> ### ** Examples
>  x <- seq(0, 1, len = (N <- 101)); apar <- 0.7
> ox <- expand.grid(x, x)
> zedd <- dbiamhcop(ox[, 1], ox[, 2], apar = apar)
> ## Not run: 
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D zedd <- pbiamhcop(ox[, 1], ox[, 2], apar = apar)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D 
> ##D plot(r <- rbiamhcop(n = 1000, apar = apar), col = "blue")
> ##D par(mfrow = c(1, 2))
> ##D hist(r[, 1])  # Should be uniform
> ##D hist(r[, 2])  # Should be uniform
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("biclaytoncop")
> ### * biclaytoncop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biclaytoncop
> ### Title: Clayton Copula (Bivariate) Family Function
> ### Aliases: biclaytoncop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- rbiclaytoncop(n = (nn <- 1000), apar = exp(2))
> bdata <- data.frame(y1 = ymat[, 1], y2 = ymat[, 2],
+                     y3 = ymat[, 1], y4 = ymat[, 2], x2 = runif(nn))
> summary(bdata)
       y1                 y2                 y3                 y4          
 Min.   :0.001315   Min.   :0.001606   Min.   :0.001315   Min.   :0.001606  
 1st Qu.:0.258129   1st Qu.:0.251737   1st Qu.:0.258129   1st Qu.:0.251737  
 Median :0.483260   Median :0.488585   Median :0.483260   Median :0.488585  
 Mean   :0.499692   Mean   :0.495656   Mean   :0.499692   Mean   :0.495656  
 3rd Qu.:0.746932   3rd Qu.:0.758858   3rd Qu.:0.746932   3rd Qu.:0.758858  
 Max.   :0.999931   Max.   :0.999234   Max.   :0.999931   Max.   :0.999234  
       x2           
 Min.   :0.0005705  
 1st Qu.:0.2355822  
 Median :0.4788345  
 Mean   :0.4896972  
 3rd Qu.:0.7537982  
 Max.   :0.9998635  
> ## Not run:  plot(ymat, col = "blue") 
> fit1 <-
+   vglm(cbind(y1, y2, y3, y4) ~ 1,  # 2 responses, e.g., (y1,y2) is the 1st
+        biclaytoncop, data = bdata,
+        trace = TRUE, crit = "coef")  # Sometimes a good idea
VGLM    linear loop  1 :  coefficients = 1.9539254, 1.9521817
VGLM    linear loop  2 :  coefficients = 1.9539027, 1.9539289
VGLM    linear loop  3 :  coefficients = 1.9539030, 1.9539026
VGLM    linear loop  4 :  coefficients = 1.953903, 1.953903
VGLM    linear loop  5 :  coefficients = 1.953903, 1.953903
> coef(fit1, matrix = TRUE)
            loglink(apar1) loglink(apar2)
(Intercept)       1.953903       1.953903
> Coef(fit1)
   apar1    apar2 
7.056174 7.056174 
> head(fitted(fit1))
   y1  y2  y3  y4
1 0.5 0.5 0.5 0.5
2 0.5 0.5 0.5 0.5
3 0.5 0.5 0.5 0.5
4 0.5 0.5 0.5 0.5
5 0.5 0.5 0.5 0.5
6 0.5 0.5 0.5 0.5
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2, y3, y4) ~ 1, family = biclaytoncop, 
    data = bdata, trace = TRUE, crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.95390    0.03117   62.68   <2e-16 ***
(Intercept):2  1.95390    0.03117   62.68   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(apar1), loglink(apar2)

Log-likelihood: 2400.956 on 1998 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> # Another example; apar is a function of x2
> bdata <- transform(bdata, apar = exp(-0.5 + x2))
> ymat <- rbiclaytoncop(n = nn, apar = with(bdata, apar))
> bdata <- transform(bdata, y5 = ymat[, 1], y6 = ymat[, 2])
> fit2 <- vgam(cbind(y5, y6) ~ s(x2), data = bdata,
+              biclaytoncop(lapar = "loglink"), trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = 171.60695
VGAM  s.vam  loop  2 :  loglikelihood = 172.14865
VGAM  s.vam  loop  3 :  loglikelihood = 172.18263
VGAM  s.vam  loop  4 :  loglikelihood = 172.18652
VGAM  s.vam  loop  5 :  loglikelihood = 172.18721
VGAM  s.vam  loop  6 :  loglikelihood = 172.18733
VGAM  s.vam  loop  7 :  loglikelihood = 172.18735
VGAM  s.vam  loop  8 :  loglikelihood = 172.18736
VGAM  s.vam  loop  9 :  loglikelihood = 172.18736
> ## Not run: plot(fit2, lcol = "blue", scol = "orange", se = TRUE) 
> 
> 
> cleanEx()
> nameEx("biclaytoncopUC")
> ### * biclaytoncopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Biclaytoncop
> ### Title: Clayton Copula (Bivariate) Distribution
> ### Aliases: dbiclaytoncop rbiclaytoncop
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  edge <- 0.01  # A small positive value
> ##D N <- 101; x <- seq(edge, 1.0 - edge, len = N); Rho <- 0.7
> ##D ox <- expand.grid(x, x)
> ##D zedd <- dbiclaytoncop(ox[, 1], ox[, 2], apar = Rho, log = TRUE)
> ##D par(mfrow = c(1, 2))
> ##D contour(x, x, matrix(zedd, N, N), col = 4, labcex = 1.5, las = 1)
> ##D plot(rbiclaytoncop(1000, 2), col = 4, las = 1)  
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("bifgmcop")
> ### * bifgmcop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bifgmcop
> ### Title: Farlie-Gumbel-Morgenstern's Bivariate Distribution Family
> ###   Function
> ### Aliases: bifgmcop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ymat <- rbifgmcop(1000, apar = rhobitlink(3, inverse = TRUE))
> ## Not run: plot(ymat, col = "blue")
> fit <- vglm(ymat ~ 1, fam = bifgmcop, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 39.134225
VGLM    linear loop  2 :  loglikelihood = 39.134229
VGLM    linear loop  3 :  loglikelihood = 39.134229
> coef(fit, matrix = TRUE)
            rhobitlink(apar)
(Intercept)         2.303892
> Coef(fit)
     apar 
0.8183977 
> head(fitted(fit))
  [,1] [,2]
1  0.5  0.5
2  0.5  0.5
3  0.5  0.5
4  0.5  0.5
5  0.5  0.5
6  0.5  0.5
> 
> 
> 
> cleanEx()
> nameEx("bifgmcopUC")
> ### * bifgmcopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Bifgmcop
> ### Title: Farlie-Gumbel-Morgenstern's Bivariate Distribution
> ### Aliases: Bifgmcop dbifgmcop pbifgmcop rbifgmcop
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  N <- 101; x <- seq(0.0, 1.0, len = N); apar <- 0.7
> ##D ox <- expand.grid(x, x)
> ##D zedd <- dbifgmcop(ox[, 1], ox[, 2], apar = apar)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D zedd <- pbifgmcop(ox[, 1], ox[, 2], apar = apar)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D 
> ##D plot(r <- rbifgmcop(n = 3000, apar = apar), col = "blue")
> ##D par(mfrow = c(1, 2))
> ##D hist(r[, 1])  # Should be uniform
> ##D hist(r[, 2])  # Should be uniform
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bifgmexp")
> ### * bifgmexp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bifgmexp
> ### Title: Bivariate Farlie-Gumbel-Morgenstern Exponential Distribution
> ###   Family Function
> ### Aliases: bifgmexp
> ### Keywords: models regression
> 
> ### ** Examples
> 
> N <- 1000; mdata <- data.frame(y1 = rexp(N), y2 = rexp(N))
> ## Not run: plot(ymat)
> fit <- vglm(cbind(y1, y2) ~ 1, bifgmexp, data = mdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2011.9139
VGLM    linear loop  2 :  loglikelihood = -2011.9088
VGLM    linear loop  3 :  loglikelihood = -2011.9088
> fit <- vglm(cbind(y1, y2) ~ 1, bifgmexp, data = mdata, # May fail
+             trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.44688429
VGLM    linear loop  2 :  coefficients = 0.46682892
VGLM    linear loop  3 :  coefficients = 0.46692873
VGLM    linear loop  4 :  coefficients = 0.46692873
> coef(fit, matrix = TRUE)
            rhobitlink(apar)
(Intercept)        0.4669287
> Coef(fit)
     apar 
0.2293131 
> head(fitted(fit))
  y1 y2
1  1  1
2  1  1
3  1  1
4  1  1
5  1  1
6  1  1
> 
> 
> 
> cleanEx()
> nameEx("bifrankcop")
> ### * bifrankcop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bifrankcop
> ### Title: Frank's Bivariate Distribution Family Function
> ### Aliases: bifrankcop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ymat <- rbifrankcop(n = 2000, apar = exp(4))
> ##D plot(ymat, col = "blue")
> ##D fit <- vglm(ymat ~ 1, fam = bifrankcop, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D vcov(fit)
> ##D head(fitted(fit))
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bifrankcopUC")
> ### * bifrankcopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Frank
> ### Title: Frank's Bivariate Distribution
> ### Aliases: Frank dbifrankcop pbifrankcop rbifrankcop
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D N <- 100; apar <- exp(2)
> ##D xx <- seq(-0.30, 1.30, len = N)
> ##D ox <- expand.grid(xx, xx)
> ##D zedd <- dbifrankcop(ox[, 1], ox[, 2], apar = apar)
> ##D contour(xx, xx, matrix(zedd, N, N))
> ##D zedd <- pbifrankcop(ox[, 1], ox[, 2], apar = apar)
> ##D contour(xx, xx, matrix(zedd, N, N))
> ##D 
> ##D plot(rr <- rbifrankcop(n = 3000, apar = exp(4)))
> ##D par(mfrow = c(1, 2))
> ##D hist(rr[, 1]); hist(rr[, 2])  # Should be uniform
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bigumbelIexp")
> ### * bigumbelIexp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bigumbelIexp
> ### Title: Gumbel's Type I Bivariate Distribution Family Function
> ### Aliases: bigumbelIexp
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> gdata <- data.frame(y1 = rexp(nn), y2 = rexp(nn))
> ## Not run:  with(gdata, plot(cbind(y1, y2))) 
> fit <- vglm(cbind(y1, y2) ~ 1, bigumbelIexp, gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2026.4121
VGLM    linear loop  2 :  loglikelihood = -2023.0304
VGLM    linear loop  3 :  loglikelihood = -2018.1566
VGLM    linear loop  4 :  loglikelihood = -2014.347
VGLM    linear loop  5 :  loglikelihood = -2013.5177
VGLM    linear loop  6 :  loglikelihood = -2013.5017
VGLM    linear loop  7 :  loglikelihood = -2013.5017
> coef(fit, matrix = TRUE)
                  apar
(Intercept) 0.04944346
> Coef(fit)
      apar 
0.04944346 
> head(fitted(fit))
  y1 y2
1  1  1
2  1  1
3  1  1
4  1  1
5  1  1
6  1  1
> 
> 
> 
> cleanEx()
> nameEx("bilogisUC")
> ### * bilogisUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bilogis
> ### Title: Bivariate Logistic Distribution
> ### Aliases: bilogis dbilogis pbilogis rbilogis
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 3))
> ##D ymat <- rbilogis(n = 2000, loc1 = 5, loc2 = 7, scale2 = exp(1))
> ##D myxlim <- c(-2, 15); myylim <- c(-10, 30)
> ##D plot(ymat, xlim = myxlim, ylim = myylim)
> ##D 
> ##D N <- 100
> ##D x1 <- seq(myxlim[1], myxlim[2], len = N)
> ##D x2 <- seq(myylim[1], myylim[2], len = N)
> ##D ox <- expand.grid(x1, x2)
> ##D z <- dbilogis(ox[,1], ox[,2], loc1 = 5, loc2 = 7, scale2 = exp(1))
> ##D contour(x1, x2, matrix(z, N, N), main = "density")
> ##D z <- pbilogis(ox[,1], ox[,2], loc1 = 5, loc2 = 7, scale2 = exp(1))
> ##D contour(x1, x2, matrix(z, N, N), main = "cdf") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bilogistic")
> ### * bilogistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bilogistic
> ### Title: Bivariate Logistic Distribution Family Function
> ### Aliases: bilogistic
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ymat <- rbilogis(n <- 50, loc1 = 5, loc2 = 7, scale2 = exp(1))
> ##D plot(ymat)
> ##D bfit <- vglm(ymat ~ 1, family = bilogistic, trace = TRUE)
> ##D coef(bfit, matrix = TRUE)
> ##D Coef(bfit)
> ##D head(fitted(bfit))
> ##D vcov(bfit)
> ##D head(weights(bfit, type = "work"))
> ##D summary(bfit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binom2.or")
> ### * binom2.or
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binom2.or
> ### Title: Bivariate Binary Regression with an Odds Ratio (Family Function)
> ### Aliases: binom2.or
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Fit the model in Table 6.7 in McCullagh and Nelder (1989)
> coalminers <- transform(coalminers, Age = (age - 42) / 5)
> fit <- vglm(cbind(nBnW, nBW, BnW, BW) ~ Age,
+             binom2.or(zero = NULL), data = coalminers)
> fitted(fit)
         00         01          10          11
1 0.9374725 0.04940880 0.004635613 0.008483108
2 0.9146106 0.06363617 0.006975651 0.014777591
3 0.8841071 0.08002862 0.010496469 0.025367860
4 0.8439351 0.09748397 0.015813823 0.042767142
5 0.7918821 0.11383853 0.023859791 0.070419602
6 0.7257847 0.12591033 0.035968350 0.112336597
7 0.6440431 0.13037828 0.053762296 0.171816282
8 0.5467652 0.12560781 0.078375013 0.249252018
9 0.4378039 0.11312565 0.108557133 0.340513348
> summary(fit)
Call:
vglm(formula = cbind(nBnW, nBW, BnW, BW) ~ Age, family = binom2.or(zero = NULL), 
    data = coalminers)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -2.262468   0.029892 -75.688  < 2e-16 ***
(Intercept):2 -1.487760   0.020559 -72.364  < 2e-16 ***
(Intercept):3  3.021908   0.069732  43.336  < 2e-16 ***
Age:1          0.514510   0.012071  42.623  < 2e-16 ***
Age:2          0.325446   0.008869  36.697  < 2e-16 ***
Age:3         -0.131365   0.028442  -4.619 3.86e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(mu1), logitlink(mu2), loglink(oratio)

Residual deviance: 30.3939 on 21 degrees of freedom

Log-likelihood: -100.5292 on 21 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)     -2.2624682     -1.4877603       3.0219085
Age              0.5145103      0.3254455      -0.1313647
> c(weights(fit, type = "prior")) * fitted(fit)  # Table 6.8
         00        01         10        11
1 1829.9463  96.44597   9.048717  16.55903
2 1638.0676 113.97238  12.493391  26.46667
3 1868.1182 169.10047  22.179039  53.60229
4 2348.6713 271.29790  44.009870 119.02096
5 1800.7399 258.86881  54.257166 160.13417
6 1736.8028 301.30342  86.072262 268.82148
7 1346.0502 272.49061 112.363199 359.09603
8  956.8390 219.81367 137.156273 436.19103
9  497.3452 128.51074 123.320903 386.82316
> 
> ## Not run: 
> ##D  with(coalminers, matplot(Age, fitted(fit), type = "l", las = 1,
> ##D                          xlab = "(age - 42) / 5", lwd = 2))
> ##D with(coalminers, matpoints(Age, depvar(fit), col=1:4))
> ##D legend(x = -4, y = 0.5, lty = 1:4, col = 1:4, lwd = 2,
> ##D        legend = c("1 = (Breathlessness=0, Wheeze=0)",
> ##D                   "2 = (Breathlessness=0, Wheeze=1)",
> ##D                   "3 = (Breathlessness=1, Wheeze=0)",
> ##D                   "4 = (Breathlessness=1, Wheeze=1)")) 
> ## End(Not run)
> 
> 
> # Another model: pet ownership
> ## Not run: 
> ##D  data(xs.nz, package = "VGAMdata")
> ##D # More homogeneous:
> ##D petdata <- subset(xs.nz, ethnicity == "European" & age < 70 &
> ##D                          sex == "M")
> ##D petdata <- na.omit(petdata[, c("cat", "dog", "age")])
> ##D summary(petdata)
> ##D with(petdata, table(cat, dog))  # Can compute the odds ratio
> ##D 
> ##D fit <- vgam(cbind((1-cat) * (1-dog), (1-cat) * dog,
> ##D                      cat  * (1-dog),    cat  * dog) ~ s(age, df = 5),
> ##D             binom2.or(zero =    3), data = petdata, trace = TRUE)
> ##D colSums(depvar(fit))
> ##D coef(fit, matrix = TRUE)
> ## End(Not run)
> 
> ## Not run: 
> ##D  # Plot the estimated probabilities
> ##D ooo <- order(with(petdata, age))
> ##D matplot(with(petdata, age)[ooo], fitted(fit)[ooo, ], type = "l",
> ##D         xlab = "Age", ylab = "Probability", main = "Pet ownership",
> ##D         ylim = c(0, max(fitted(fit))), las = 1, lwd = 1.5)
> ##D legend("topleft", col=1:4, lty = 1:4, leg = c("no cat or dog ",
> ##D        "dog only", "cat only", "cat and dog"), lwd = 1.5) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binom2.orUC")
> ### * binom2.orUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Binom2.or
> ### Title: Bivariate Odds Ratio Model
> ### Aliases: Binom2.or dbinom2.or rbinom2.or
> ### Keywords: distribution
> 
> ### ** Examples
> 
> nn <- 1000  # Example 1
> ymat <- rbinom2.or(nn, mu1 = logitlink(1, inv = TRUE),
+                    oratio = exp(2), exch = TRUE)
> (mytab <- table(ymat[, 1], ymat[, 2], dnn = c("Y1", "Y2")))
   Y2
Y1    0   1
  0 152 113
  1 120 615
> (myor <- mytab["0","0"] * mytab["1","1"] / (mytab["1","0"] *
+          mytab["0","1"]))
[1] 6.893805
> fit <- vglm(ymat ~ 1, binom2.or(exch = TRUE))
> coef(fit, matrix = TRUE)
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)       1.002246       1.002246         1.92972
> 
> bdata <- data.frame(x2 = sort(runif(nn)))  # Example 2
> bdata <- transform(bdata,
+            mu1 = logitlink(-2 + 4 * x2, inverse = TRUE),
+            mu2 = logitlink(-1 + 3 * x2, inverse = TRUE))
> dmat <- with(bdata, dbinom2.or(mu1 = mu1, mu2 = mu2,
+                                oratio = exp(2)))
> ymat <- with(bdata, rbinom2.or(n = nn, mu1 = mu1, mu2 = mu2,
+                                oratio = exp(2)))
> fit2 <- vglm(ymat ~ x2, binom2.or, data = bdata)
> coef(fit2, matrix = TRUE)
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)      -2.064740      -1.093951        2.045445
x2                3.931656       3.218274        0.000000
> ## Not run: 
> ##D matplot(with(bdata, x2), dmat, lty = 1:4, col = 1:4,
> ##D         main = "Joint probabilities", ylim = 0:1, type = "l",
> ##D         ylab = "Probabilities", xlab = "x2", las = 1)
> ##D legend("top", lty = 1:4, col = 1:4,
> ##D        legend = c("1 = (y1=0, y2=0)", "2 = (y1=0, y2=1)",
> ##D                   "3 = (y1=1, y2=0)", "4 = (y1=1, y2=1)"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binom2.rho")
> ### * binom2.rho
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binom2.rho
> ### Title: Bivariate Probit Regression
> ### Aliases: binom2.rho binom2.Rho
> ### Keywords: models regression
> 
> ### ** Examples
> 
> coalminers <- transform(coalminers, Age = (age - 42) / 5)
> fit <- vglm(cbind(nBnW, nBW, BnW, BW) ~ Age,
+             binom2.rho, data = coalminers, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -95.59942
VGLM    linear loop  2 :  loglikelihood = -95.59848
VGLM    linear loop  3 :  loglikelihood = -95.59848
> summary(fit)
Call:
vglm(formula = cbind(nBnW, nBW, BnW, BW) ~ Age, family = binom2.rho, 
    data = coalminers, trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.279160   0.014435  -88.61   <2e-16 ***
(Intercept):2 -0.881462   0.011242  -78.41   <2e-16 ***
(Intercept):3  2.044267   0.043254   47.26   <2e-16 ***
Age:1          0.273350   0.006178   44.25   <2e-16 ***
Age:2          0.184643   0.004901   37.68   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: probitlink(mu1), probitlink(mu2), rhobitlink(rho)

Log-likelihood: -95.5985 on 22 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            probitlink(mu1) probitlink(mu2) rhobitlink(rho)
(Intercept)      -1.2791601      -0.8814624        2.044267
Age               0.2733501       0.1846432        0.000000
> 
> 
> 
> cleanEx()
> nameEx("binom2.rhoUC")
> ### * binom2.rhoUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Binom2.rho
> ### Title: Bivariate Probit Model
> ### Aliases: Binom2.rho dbinom2.rho rbinom2.rho
> ### Keywords: distribution
> 
> ### ** Examples
> 
> (myrho <- rhobitlink(2, inverse = TRUE))  # Example 1
[1] 0.7615942
> nn <- 2000
> ymat <- rbinom2.rho(nn, mu1 = 0.8, rho = myrho, exch = TRUE)
> (mytab <- table(ymat[, 1], ymat[, 2], dnn = c("Y1", "Y2")))
   Y2
Y1     0    1
  0  253  175
  1  145 1427
> fit <- vglm(ymat ~ 1, binom2.rho(exch = TRUE))
> coef(fit, matrix = TRUE)
            probitlink(mu1) probitlink(mu2) rhobitlink(rho)
(Intercept)       0.8186257       0.8186257        1.974697
> 
> bdata <- data.frame(x2 = sort(runif(nn)))  # Example 2
> bdata <- transform(bdata, mu1 = probitlink(-2+4*x2, inv = TRUE),
+                           mu2 = probitlink(-1+3*x2, inv = TRUE))
> dmat <- with(bdata, dbinom2.rho(mu1, mu2, myrho))
> ymat <- with(bdata, rbinom2.rho(nn, mu1, mu2, myrho))
> fit2 <- vglm(ymat ~ x2, binom2.rho, data = bdata)
> coef(fit2, matrix = TRUE)
            probitlink(mu1) probitlink(mu2) rhobitlink(rho)
(Intercept)       -1.872344       -1.030801        1.811207
x2                 3.838750        3.172657        0.000000
> ## Not run: 
> ##D  matplot(with(bdata, x2), dmat, lty = 1:4, col = 1:4,
> ##D         type = "l", main = "Joint probabilities",
> ##D         ylim = 0:1, lwd = 2, ylab = "Probability")
> ##D legend(x = 0.25, y = 0.9, lty = 1:4, col = 1:4, lwd = 2,
> ##D        legend = c("1 = (y1=0, y2=0)", "2 = (y1=0, y2=1)",
> ##D                   "3 = (y1=1, y2=0)", "4 = (y1=1, y2=1)")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binomialff")
> ### * binomialff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binomialff
> ### Title: Binomial Family Function
> ### Aliases: binomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> shunua <- hunua[sort.list(with(hunua, altitude)), ]  # Sort by altitude
> fit <- vglm(agaaus ~ poly(altitude, 2), binomialff(link = clogloglink),
+             data = shunua)
> ## Not run: 
> ##D plot(agaaus ~ jitter(altitude), shunua, ylab = "Pr(Agaaus = 1)",
> ##D      main = "Presence/absence of Agathis australis", col = 4, las = 1)
> ##D with(shunua, lines(altitude, fitted(fit), col = "orange", lwd = 2)) 
> ## End(Not run)
> 
> # Fit two species simultaneously
> fit2 <- vgam(cbind(agaaus, kniexc) ~ s(altitude),
+              binomialff(multiple.responses = TRUE), data = shunua)
> ## Not run: 
> ##D with(shunua, matplot(altitude, fitted(fit2), type = "l",
> ##D      main = "Two species response curves", las = 1)) 
> ## End(Not run)
> 
> # Shows that Fisher scoring can sometime fail. See Ridout (1990).
> ridout <- data.frame(v = c(1000, 100, 10), r = c(4, 3, 3), n = rep(5, 3))
> (ridout <- transform(ridout, logv = log(v)))
     v r n     logv
1 1000 4 5 6.907755
2  100 3 5 4.605170
3   10 3 5 2.302585
> # The iterations oscillates between two local solutions:
> glm.fail <- glm(r / n ~ offset(logv) + 1, weight = n,
+                binomial(link = 'cloglog'), ridout, trace = TRUE)
Deviance = 22.72791 Iterations - 1
Deviance = 28.29819 Iterations - 2
Deviance = 19.01799 Iterations - 3
Deviance = 21.07849 Iterations - 4
Deviance = 17.97632 Iterations - 5
Deviance = 18.59905 Iterations - 6
Deviance = 17.90326 Iterations - 7
Deviance = 18.42591 Iterations - 8
Deviance = 17.88036 Iterations - 9
Deviance = 18.37207 Iterations - 10
Deviance = 17.87175 Iterations - 11
Deviance = 18.3519 Iterations - 12
Deviance = 17.86832 Iterations - 13
Deviance = 18.34386 Iterations - 14
Deviance = 17.86692 Iterations - 15
Deviance = 18.34059 Iterations - 16
Deviance = 17.86634 Iterations - 17
Deviance = 18.33924 Iterations - 18
Deviance = 17.8661 Iterations - 19
Deviance = 18.33868 Iterations - 20
Deviance = 17.866 Iterations - 21
Deviance = 18.33845 Iterations - 22
Deviance = 17.86596 Iterations - 23
Deviance = 18.33835 Iterations - 24
Deviance = 17.86595 Iterations - 25
Warning: glm.fit: algorithm did not converge
Deviance = 18.33831 Iterations - 1
Deviance = 17.86594 Iterations - 2
Deviance = 18.3383 Iterations - 3
Deviance = 17.86594 Iterations - 4
Deviance = 18.33829 Iterations - 5
Deviance = 17.86594 Iterations - 6
Deviance = 18.33829 Iterations - 7
Deviance = 17.86593 Iterations - 8
Deviance = 18.33829 Iterations - 9
Deviance = 17.86593 Iterations - 10
Deviance = 18.33829 Iterations - 11
Deviance = 17.86593 Iterations - 12
Deviance = 18.33829 Iterations - 13
Deviance = 17.86593 Iterations - 14
Deviance = 18.33829 Iterations - 15
Deviance = 17.86593 Iterations - 16
Deviance = 18.33829 Iterations - 17
Deviance = 17.86593 Iterations - 18
Deviance = 18.33829 Iterations - 19
Deviance = 17.86593 Iterations - 20
Deviance = 18.33829 Iterations - 21
Deviance = 17.86593 Iterations - 22
Deviance = 18.33829 Iterations - 23
Deviance = 17.86593 Iterations - 24
Deviance = 18.33829 Iterations - 25
Warning: glm.fit: algorithm did not converge
Warning in glm(r/n ~ offset(logv) + 1, weight = n, binomial(link = "cloglog"),  :
  fitting to calculate the null deviance did not converge -- increase 'maxit'?
> coef(glm.fail)
(Intercept) 
  -5.157362 
> # vglm()'s half-stepping ensures the MLE of -5.4007 is obtained:
> vglm.ok <- vglm(cbind(r, n-r) ~ offset(logv) + 1,
+                binomialff(link = clogloglink), ridout, trace = TRUE)
VGLM    linear loop  1 :  deviance = 22.72791
VGLM    linear loop  2 :  deviance = 28.29819
Taking a modified step.
VGLM    linear loop  2 :  deviance = 18.08526
VGLM    linear loop  3 :  deviance = 17.80972
VGLM    linear loop  4 :  deviance = 18.20771
Taking a modified step.
VGLM    linear loop  4 :  deviance = 17.46081
VGLM    linear loop  5 :  deviance = 17.46624
Taking a modified step.
VGLM    linear loop  5 :  deviance = 17.43666
VGLM    linear loop  6 :  deviance = 17.43668
Taking a modified step.
VGLM    linear loop  6 :  deviance = 17.43661
VGLM    linear loop  7 :  deviance = 17.43661
Taking a modified step.
VGLM    linear loop  7 :  deviance = 17.43661
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
> coef(vglm.ok)
(Intercept) 
  -5.400638 
> 
> # Separable data
> set.seed(123)
> threshold <- 0
> bdata <- data.frame(x2 = sort(rnorm(nn <- 100)))
> bdata <- transform(bdata, y1 = ifelse(x2 < threshold, 0, 1))
> fit <- vglm(y1 ~ x2, binomialff(bred = TRUE),
+             data = bdata, criter = "coef", trace = TRUE)
VGLM    linear loop  1 :  coefficients = -0.10015601,  2.10784287
VGLM    linear loop  2 :  coefficients = -0.11063311,  3.70413974
VGLM    linear loop  3 :  coefficients = -0.10881416,  5.58548660
VGLM    linear loop  4 :  coefficients = -0.11736263,  7.34812525
VGLM    linear loop  5 :  coefficients = -0.13796225,  8.32167765
VGLM    linear loop  6 :  coefficients = -0.14943194,  8.49745083
VGLM    linear loop  7 :  coefficients = -0.15084587,  8.49588971
VGLM    linear loop  8 :  coefficients = -0.15083084,  8.49572183
VGLM    linear loop  9 :  coefficients = -0.15082982,  8.49572911
VGLM    linear loop  10 :  coefficients = -0.15082987,  8.49572905
VGLM    linear loop  11 :  coefficients = -0.15082987,  8.49572904
> coef(fit, matrix = TRUE)  # Finite!!
            logitlink(prob)
(Intercept)      -0.1508299
x2                8.4957290
> summary(fit)
Warning in hdeff.vglm(object, ...) :
  NAs detected. Setting 'fd.only = TRUE' and making a full recursive call
Call:
vglm(formula = y1 ~ x2, family = binomialff(bred = TRUE), data = bdata, 
    criter = "coef", trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -0.1508     0.4626  -0.326    0.744    
x2            8.4957     2.0883   4.068 4.74e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logitlink(prob) 

Residual deviance: 15.0176 on 98 degrees of freedom

Log-likelihood: -7.5088 on 98 degrees of freedom

Number of Fisher scoring iterations: 11 

No Hauck-Donner effect found in any of the estimates

> ## Not run: 
> ##D  plot(depvar(fit) ~ x2, data = bdata, col = "blue", las = 1)
> ##D lines(fitted(fit) ~ x2, data = bdata, col = "orange")
> ##D abline(v = threshold, col = "gray", lty = "dashed") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binormal")
> ### * binormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binormal
> ### Title: Bivariate Normal Distribution Family Function
> ### Aliases: binormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(123); nn <- 1000
> bdata <- data.frame(x2 = runif(nn), x3 = runif(nn))
> bdata <- transform(bdata, y1 = rnorm(nn, 1 + 2 * x2),
+                           y2 = rnorm(nn, 3 + 4 * x2))
> fit1 <- vglm(cbind(y1, y2) ~ x2,
+              binormal(eq.sd = TRUE), bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2982.0414
VGLM    linear loop  2 :  loglikelihood = -2842.6316
VGLM    linear loop  3 :  loglikelihood = -2824.4788
VGLM    linear loop  4 :  loglikelihood = -2824.2708
VGLM    linear loop  5 :  loglikelihood = -2824.2708
VGLM    linear loop  6 :  loglikelihood = -2824.2708
> coef(fit1, matrix = TRUE)
               mean1    mean2 loglink(sd1) loglink(sd2) rhobitlink(rho)
(Intercept) 1.021178 2.929393 -0.006632272 -0.006632272      0.05229185
x2          2.042807 4.101541  0.000000000  0.000000000      0.00000000
> constraints(fit1)
$`(Intercept)`
     [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]    0    1    0    0
[3,]    0    0    1    0
[4,]    0    0    1    0
[5,]    0    0    0    1

$x2
     [,1] [,2]
[1,]    1    0
[2,]    0    1
[3,]    0    0
[4,]    0    0
[5,]    0    0

> summary(fit1)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = binormal(eq.sd = TRUE), 
    data = bdata, trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.021178   0.062789  16.264   <2e-16 ***
(Intercept):2  2.929393   0.062789  46.655   <2e-16 ***
(Intercept):3 -0.006632   0.015817  -0.419    0.675    
(Intercept):4  0.052292   0.063246   0.827    0.408    
x2:1           2.042807   0.109326  18.685   <2e-16 ***
x2:2           4.101541   0.109326  37.517   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  5 

Names of linear predictors: mean1, mean2, loglink(sd1), loglink(sd2), 
rhobitlink(rho)

Log-likelihood: -2824.271 on 4994 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> # Estimated P(Y1 <= y1, Y2 <= y2) under the fitted model
> var1  <- loglink(2 * predict(fit1)[, "loglink(sd1)"], inv = TRUE)
> var2  <- loglink(2 * predict(fit1)[, "loglink(sd2)"], inv = TRUE)
> cov12 <- rhobitlink(predict(fit1)[, "rhobitlink(rho)"], inv = TRUE)
> head(with(bdata, pbinorm(y1, y2,
+                          mean1 = predict(fit1)[, "mean1"],
+                          mean2 = predict(fit1)[, "mean2"],
+                          var1 = var1, var2 = var2, cov12 = cov12)))
[1] 0.049938306 0.082072022 0.148273155 0.377605648 0.002533645 0.247922216
> 
> 
> 
> cleanEx()
> nameEx("binormalUC")
> ### * binormalUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Binorm
> ### Title: Bivariate Normal Distribution Cumulative Distribution Function
> ### Aliases: Binorm pnorm2 dbinorm pbinorm rbinorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> yvec <- c(-5, -1.96, 0, 1.96, 5)
> ymat <- expand.grid(yvec, yvec)
> cbind(ymat, pbinorm(ymat[, 1], ymat[, 2]))
    Var1  Var2 pbinorm(ymat[, 1], ymat[, 2])
1  -5.00 -5.00                  8.216912e-14
2  -1.96 -5.00                  7.165686e-09
3   0.00 -5.00                  1.433258e-07
4   1.96 -5.00                  2.794859e-07
5   5.00 -5.00                  2.866515e-07
6  -5.00 -1.96                  7.165686e-09
7  -1.96 -1.96                  6.248948e-04
8   0.00 -1.96                  1.249895e-02
9   1.96 -1.96                  2.437300e-02
10  5.00 -1.96                  2.499789e-02
11 -5.00  0.00                  1.433258e-07
12 -1.96  0.00                  1.249895e-02
13  0.00  0.00                  2.500000e-01
14  1.96  0.00                  4.875011e-01
15  5.00  0.00                  4.999999e-01
16 -5.00  1.96                  2.794859e-07
17 -1.96  1.96                  2.437300e-02
18  0.00  1.96                  4.875011e-01
19  1.96  1.96                  9.506291e-01
20  5.00  1.96                  9.750018e-01
21 -5.00  5.00                  2.866515e-07
22 -1.96  5.00                  2.499789e-02
23  0.00  5.00                  4.999999e-01
24  1.96  5.00                  9.750018e-01
25  5.00  5.00                  9.999994e-01
> 
> ## Not run: 
> ##D  rhovec <- seq(-0.95, 0.95, by = 0.01)
> ##D plot(rhovec, pbinorm(0, 0, cov12 = rhovec),
> ##D      xlab = expression(rho), lwd = 2,
> ##D      type = "l", col = "blue", las = 1)
> ##D abline(v = 0, h = 0.25, col = "gray", lty = "dashed") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("binormalcop")
> ### * binormalcop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binormalcop
> ### Title: Gaussian Copula (Bivariate) Family Function
> ### Aliases: binormalcop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> ymat <- rbinormcop(nn, rho = rhobitlink(-0.9, inverse = TRUE))
> bdata <- data.frame(y1 = ymat[, 1], y2 = ymat[, 2],
+                     y3 = ymat[, 1], y4 = ymat[, 2],
+                     x2 = runif(nn))
> summary(bdata)
       y1                 y2                  y3                 y4           
 Min.   :0.001315   Min.   :0.0001089   Min.   :0.001315   Min.   :0.0001089  
 1st Qu.:0.242785   1st Qu.:0.2462124   1st Qu.:0.242785   1st Qu.:0.2462124  
 Median :0.485911   Median :0.4904584   Median :0.485911   Median :0.4904584  
 Mean   :0.497408   Mean   :0.4959616   Mean   :0.497408   Mean   :0.4959616  
 3rd Qu.:0.754408   3rd Qu.:0.7527392   3rd Qu.:0.754408   3rd Qu.:0.7527392  
 Max.   :0.999931   Max.   :0.9980033   Max.   :0.999931   Max.   :0.9980033  
       x2           
 Min.   :0.0002004  
 1st Qu.:0.2564958  
 Median :0.5022080  
 Mean   :0.5072109  
 3rd Qu.:0.7732091  
 Max.   :0.9987511  
> ## Not run:  plot(ymat, col = "blue") 
> fit1 <-  # 2 responses, e.g., (y1,y2) is the 1st
+   vglm(cbind(y1, y2, y3, y4) ~ 1, fam = binormalcop,
+        crit = "coef",  # Sometimes a good idea
+        data = bdata, trace = TRUE)
VGLM    linear loop  1 :  coefficients = -0.78123724, -0.80292338
VGLM    linear loop  2 :  coefficients = -0.84221918, -0.84057220
VGLM    linear loop  3 :  coefficients = -0.83721312, -0.83736426
VGLM    linear loop  4 :  coefficients = -0.83766961, -0.83765595
VGLM    linear loop  5 :  coefficients = -0.83762834, -0.83762957
VGLM    linear loop  6 :  coefficients = -0.83763207, -0.83763196
VGLM    linear loop  7 :  coefficients = -0.83763173, -0.83763174
VGLM    linear loop  8 :  coefficients = -0.83763177, -0.83763176
> coef(fit1, matrix = TRUE)
            rhobitlink(rho1) rhobitlink(rho2)
(Intercept)       -0.8376318       -0.8376318
> Coef(fit1)
      rho1       rho2 
-0.3959324 -0.3959324 
> head(fitted(fit1))
   y1  y2  y3  y4
1 0.5 0.5 0.5 0.5
2 0.5 0.5 0.5 0.5
3 0.5 0.5 0.5 0.5
4 0.5 0.5 0.5 0.5
5 0.5 0.5 0.5 0.5
6 0.5 0.5 0.5 0.5
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2, y3, y4) ~ 1, family = binormalcop, 
    data = bdata, crit = "coef", trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -0.8376     0.0588  -14.24   <2e-16 ***
(Intercept):2  -0.8376     0.0588  -14.24   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: rhobitlink(rho1), rhobitlink(rho2)

Log-likelihood: 189.9613 on 1998 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> # Another example; rho is a linear function of x2
> bdata <- transform(bdata, rho = -0.5 + x2)
> ymat <- rbinormcop(n = nn, rho = with(bdata, rho))
> bdata <- transform(bdata, y5 = ymat[, 1], y6 = ymat[, 2])
> fit2 <- vgam(cbind(y5, y6) ~ s(x2), data = bdata,
+              binormalcop(lrho = "identitylink"), trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = 63.656657
VGAM  s.vam  loop  2 :  loglikelihood = 67.062845
VGAM  s.vam  loop  3 :  loglikelihood = 67.102085
VGAM  s.vam  loop  4 :  loglikelihood = 67.10237
VGAM  s.vam  loop  5 :  loglikelihood = 67.102449
VGAM  s.vam  loop  6 :  loglikelihood = 67.102439
VGAM  s.vam  loop  7 :  loglikelihood = 67.102441
VGAM  s.vam  loop  8 :  loglikelihood = 67.10244
> ## Not run: plot(fit2, lcol = "blue", scol = "orange", se = TRUE)
> 
> 
> 
> cleanEx()
> nameEx("binormcopUC")
> ### * binormcopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Binormcop
> ### Title: Gaussian Copula (Bivariate) Distribution
> ### Aliases: Binormcop dbinormcop pbinormcop rbinormcop
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  edge <- 0.01  # A small positive value
> ##D N <- 101; x <- seq(edge, 1.0 - edge, len = N); Rho <- 0.7
> ##D ox <- expand.grid(x, x)
> ##D zedd <- dbinormcop(ox[, 1], ox[, 2], rho = Rho, log = TRUE)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue", labcex = 1.5)
> ##D zedd <- pbinormcop(ox[, 1], ox[, 2], rho = Rho)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue", labcex = 1.5)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("biplackettcop")
> ### * biplackettcop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biplackettcop
> ### Title: Plackett's Bivariate Copula Family Function
> ### Aliases: biplackettcop
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ymat <- rbiplackcop(n = 2000, oratio = exp(2))
> ##D plot(ymat, col = "blue")
> ##D fit <- vglm(ymat ~ 1, fam = biplackettcop, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D vcov(fit)
> ##D head(fitted(fit))
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("biplackettcopUC")
> ### * biplackettcopUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Biplackett
> ### Title: Plackett's Bivariate Copula
> ### Aliases: Biplackett dbiplackcop pbiplackcop rbiplackcop
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  N <- 101; oratio <- exp(1)
> ##D x <- seq(0.0, 1.0, len = N)
> ##D ox <- expand.grid(x, x)
> ##D zedd <- dbiplackcop(ox[, 1], ox[, 2], oratio = oratio)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D zedd <- pbiplackcop(ox[, 1], ox[, 2], oratio = oratio)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue")
> ##D 
> ##D plot(rr <- rbiplackcop(n = 3000, oratio = oratio))
> ##D par(mfrow = c(1, 2))
> ##D hist(rr[, 1])  # Should be uniform
> ##D hist(rr[, 2])  # Should be uniform
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bisa")
> ### * bisa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bisa
> ### Title: Birnbaum-Saunders Regression Family Function
> ### Aliases: bisa
> ### Keywords: models regression
> 
> ### ** Examples
> 
> bdata1 <- data.frame(x2 = runif(nn <- 1000))
> bdata1 <- transform(bdata1, shape = exp(-0.5 + x2),
+                             scale = exp(1.5))
> bdata1 <- transform(bdata1, y = rbisa(nn, scale, shape))
> fit1 <- vglm(y ~ x2, bisa(zero = 1), data = bdata1, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3275.1413
VGLM    linear loop  2 :  loglikelihood = -3094.3244
VGLM    linear loop  3 :  loglikelihood = -2931.4672
VGLM    linear loop  4 :  loglikelihood = -2835.7287
VGLM    linear loop  5 :  loglikelihood = -2816.7466
VGLM    linear loop  6 :  loglikelihood = -2816.1925
VGLM    linear loop  7 :  loglikelihood = -2816.192
VGLM    linear loop  8 :  loglikelihood = -2816.192
> coef(fit1, matrix = TRUE)
            loglink(scale) loglink(shape)
(Intercept)       1.480685     -0.4527655
x2                0.000000      0.9700815
> 
> ## Not run: 
> ##D bdata2 <- data.frame(shape = exp(-0.5), scale = exp(0.5))
> ##D bdata2 <- transform(bdata2, y = rbisa(nn, scale, shape))
> ##D fit <- vglm(y ~ 1, bisa, data = bdata2, trace = TRUE)
> ##D with(bdata2, hist(y, prob = TRUE, ylim = c(0, 0.5),
> ##D                   col = "lightblue"))
> ##D coef(fit, matrix = TRUE)
> ##D with(bdata2, mean(y))
> ##D head(fitted(fit))
> ##D x <- with(bdata2, seq(0, max(y), len = 200))
> ##D lines(dbisa(x, Coef(fit)[1], Coef(fit)[2]) ~ x, data = bdata2,
> ##D       col = "orange", lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bisaUC")
> ### * bisaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Bisa
> ### Title: The Birnbaum-Saunders Distribution
> ### Aliases: Bisa dbisa pbisa qbisa rbisa
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- seq(0, 6, len = 400)
> ##D plot(x, dbisa(x, shape = 1), type = "l", col = "blue",
> ##D      ylab = "Density", lwd = 2, ylim = c(0,1.3), lty = 3,
> ##D      main = "X ~ Birnbaum-Saunders(shape, scale = 1)")
> ##D lines(x, dbisa(x, shape = 2), col = "orange", lty = 2, lwd = 2)
> ##D lines(x, dbisa(x, shape = 0.5), col = "green", lty = 1, lwd = 2)
> ##D legend(x = 3, y = 0.9, legend = paste("shape  = ",c(0.5, 1,2)),
> ##D        col = c("green","blue","orange"), lty = 1:3, lwd = 2)
> ##D 
> ##D shape <- 1; x <- seq(0.0, 4, len = 401)
> ##D plot(x, dbisa(x, shape = shape), type = "l", col = "blue",
> ##D      main = "Blue is density, orange is the CDF", las = 1,
> ##D      sub = "Red lines are the 10,20,...,90 percentiles",
> ##D      ylab = "", ylim = 0:1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pbisa(x, shape = shape), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qbisa(probs, shape = shape)
> ##D lines(Q, dbisa(Q, shape = shape), col = "red", lty = 3, type = "h")
> ##D pbisa(Q, shape = shape) - probs  # Should be all zero
> ##D abline(h = probs, col = "red", lty = 3)
> ##D lines(Q, pbisa(Q, shape = shape), col = "red", lty = 3, type = "h")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bistudentt")
> ### * bistudentt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bistudentt
> ### Title: Bivariate Student-t Family Function
> ### Aliases: bistudentt
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> mydof <- logloglink(1, inverse = TRUE)
> ymat <- cbind(rt(nn, df = mydof), rt(nn, df = mydof))
> bdata <- data.frame(y1 = ymat[, 1], y2 = ymat[, 2],
+                     y3 = ymat[, 1], y4 = ymat[, 2],
+                     x2 = runif(nn))
> summary(bdata)
       y1                  y2                 y3                  y4          
 Min.   :-4.373254   Min.   :-4.22060   Min.   :-4.373254   Min.   :-4.22060  
 1st Qu.:-0.695238   1st Qu.:-0.59607   1st Qu.:-0.695238   1st Qu.:-0.59607  
 Median : 0.005109   Median : 0.04509   Median : 0.005109   Median : 0.04509  
 Mean   :-0.010725   Mean   : 0.03431   Mean   :-0.010725   Mean   : 0.03431  
 3rd Qu.: 0.698110   3rd Qu.: 0.71332   3rd Qu.: 0.698110   3rd Qu.: 0.71332  
 Max.   : 4.980639   Max.   : 3.10536   Max.   : 4.980639   Max.   : 3.10536  
       x2          
 Min.   :0.002058  
 1st Qu.:0.236358  
 Median :0.486391  
 Mean   :0.490733  
 3rd Qu.:0.752908  
 Max.   :0.996861  
> ## Not run:  plot(ymat, col = "blue") 
> fit1 <-    # 2 responses, e.g., (y1,y2) is the 1st
+   vglm(cbind(y1, y2, y3, y4) ~ 1,
+        bistudentt,  # crit = "coef",  # Sometimes a good idea
+        data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -5957.0814
VGLM    linear loop  2 :  loglikelihood = -5955.3644
VGLM    linear loop  3 :  loglikelihood = -5955.333
VGLM    linear loop  4 :  loglikelihood = -5955.3325
VGLM    linear loop  5 :  loglikelihood = -5955.3324
VGLM    linear loop  6 :  loglikelihood = -5955.3323
VGLM    linear loop  7 :  loglikelihood = -5955.3323
> coef(fit1, matrix = TRUE)
            logloglink(df1) rhobitlink(rho1) logloglink(df2) rhobitlink(rho2)
(Intercept)        1.054191       0.03832434        1.054192       0.03832806
> Coef(fit1)
        df1        rho1         df2        rho2 
17.63091341  0.01915983 17.63091991  0.01916169 
> head(fitted(fit1))
  y1 y2 y3 y4
1  0  0  0  0
2  0  0  0  0
3  0  0  0  0
4  0  0  0  0
5  0  0  0  0
6  0  0  0  0
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2, y3, y4) ~ 1, family = bistudentt, 
    data = bdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.05419    0.08029  13.130   <2e-16 ***
(Intercept):2  0.03832    0.04371   0.877    0.381    
(Intercept):3  1.05419    0.08029  13.130   <2e-16 ***
(Intercept):4  0.03833    0.04371   0.877    0.381    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logloglink(df1), rhobitlink(rho1), 
logloglink(df2), rhobitlink(rho2)

Log-likelihood: -5955.332 on 3996 degrees of freedom

Number of Fisher scoring iterations: 7 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1', '(Intercept):3'

> 
> 
> 
> cleanEx()
> nameEx("bistudenttUC")
> ### * bistudenttUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Bistudentt
> ### Title: Bivariate Student-t Distribution Density Function
> ### Aliases: Bistudentt dbistudentt
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  N <- 101; x <- seq(-4, 4, len = N); Rho <- 0.7
> ##D mydf <- 10; ox <- expand.grid(x, x)
> ##D zedd <- dbistudentt(ox[, 1], ox[, 2], df = mydf,
> ##D                     rho = Rho, log = TRUE)
> ##D contour(x, x, matrix(zedd, N, N), col = "blue", labcex = 1.5)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("bmi.nz")
> ### * bmi.nz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bmi.nz
> ### Title: Body Mass Index of New Zealand Adults Data
> ### Aliases: bmi.nz
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  with(bmi.nz, plot(age, BMI, col = "blue"))
> ##D fit <- vgam(BMI ~ s(age, df = c(2, 4, 2)), lms.yjn,
> ##D             data = bmi.nz, trace = TRUE)
> ##D qtplot(fit, pcol = "blue", tcol = "brown", lcol = "brown") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("borel.tanner")
> ### * borel.tanner
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: borel.tanner
> ### Title: Borel-Tanner Distribution Family Function
> ### Aliases: borel.tanner
> ### Keywords: models regression
> 
> ### ** Examples
> 
> bdata <- data.frame(y = rbort(n <- 200))
> fit <- vglm(y ~ 1, borel.tanner, bdata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = -1.0363935
VGLM    linear loop  2 :  coefficients = 0.44435457
VGLM    linear loop  3 :  coefficients = 0.0086440233
VGLM    linear loop  4 :  coefficients = -0.11892994
VGLM    linear loop  5 :  coefficients = -0.12779385
VGLM    linear loop  6 :  coefficients = -0.12783337
VGLM    linear loop  7 :  coefficients = -0.12783337
> coef(fit, matrix = TRUE)
            logitlink(a)
(Intercept)   -0.1278334
> Coef(fit)
        a 
0.4680851 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = borel.tanner, data = bdata, trace = TRUE, 
    crit = "c")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.1278     0.1417  -0.902    0.367

Name of linear predictor: logitlink(a) 

Log-likelihood: -251.9834 on 199 degrees of freedom

Number of Fisher scoring iterations: 7 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("bortUC")
> ### * bortUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Bort
> ### Title: The Borel-Tanner Distribution
> ### Aliases: Bort dbort rbort
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  qsize <- 1; a <- 0.5; x <- qsize:(qsize+10)
> ##D plot(x, dbort(x, qsize, a), type = "h", las = 1, col = "blue",
> ##D      ylab = paste("fbort(qsize=", qsize, ", a=", a, ")"),
> ##D      log = "y", main = "Borel-Tanner density function") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("brat")
> ### * brat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: brat
> ### Title: Bradley Terry Model
> ### Aliases: brat
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Citation statistics: being cited is a 'win'; citing is a 'loss'
> journal <- c("Biometrika", "Comm.Statist", "JASA", "JRSS-B")
> mat <- matrix(c( NA, 33, 320, 284,
+                 730, NA, 813, 276,
+                 498, 68,  NA, 325,
+                 221, 17, 142,  NA), 4, 4)
> dimnames(mat) <- list(winner = journal, loser = journal)
> fit <- vglm(Brat(mat) ~ 1, brat(refgp = 1), trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -20597.93
VGLM    linear loop  2 :  loglikelihood = -20525.99
VGLM    linear loop  3 :  loglikelihood = -20520.45
VGLM    linear loop  4 :  loglikelihood = -20520.38
VGLM    linear loop  5 :  loglikelihood = -20520.38
> fit <- vglm(Brat(mat) ~ 1, brat(refgp = 1), trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
-1.88777996, -0.36440031,  0.22664468
VGLM    linear loop  2 :  coefficients = 
-2.63602167, -0.46252561,  0.26673847
VGLM    linear loop  3 :  coefficients = 
-2.91276757, -0.47880148,  0.26897129
VGLM    linear loop  4 :  coefficients = 
-2.94851677, -0.47956414,  0.26895435
VGLM    linear loop  5 :  coefficients = 
-2.94907236, -0.47956977,  0.26895406
VGLM    linear loop  6 :  coefficients = 
-2.94907250, -0.47956977,  0.26895406
> summary(fit)
Call:
vglm(formula = Brat(mat) ~ 1, family = brat(refgp = 1), trace = TRUE, 
    crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -2.94907    0.10255 -28.759  < 2e-16 ***
(Intercept):2 -0.47957    0.06059  -7.915 2.47e-15 ***
(Intercept):3  0.26895    0.07083   3.797 0.000146 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(alpha2), loglink(alpha3), loglink(alpha4)

Log-likelihood: -20520.38 on 0 degrees of freedom

Number of Fisher scoring iterations: 6 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'

> c(0, coef(fit))  # Log-abilities (in order of "journal")
              (Intercept):1 (Intercept):2 (Intercept):3 
    0.0000000    -2.9490725    -0.4795698     0.2689541 
> c(1, Coef(fit))  # Abilities (in order of "journal")
               alpha2     alpha3     alpha4 
1.00000000 0.05238827 0.61904967 1.30859502 
> fitted(fit)     # Probabilities of winning in awkward form
  Comm.Statist>Biometrika JASA>Biometrika JRSS-B>Biometrika
1              0.04978037       0.3823537         0.5668361
  Biometrika>Comm.Statist JASA>Comm.Statist JRSS-B>Comm.Statist Biometrika>JASA
1               0.9502196          0.921976            0.961507       0.6176463
  Comm.Statist>JASA JRSS-B>JASA Biometrika>JRSS-B Comm.Statist>JRSS-B
1          0.078024    0.678857         0.4331639          0.03849296
  JASA>JRSS-B
1    0.321143
> (check <- InverseBrat(fitted(fit)))  # Probabilities of winning
             Biometrika Comm.Statist      JASA     JRSS-B
Biometrika           NA    0.9502196 0.6176463 0.43316389
Comm.Statist 0.04978037           NA 0.0780240 0.03849296
JASA         0.38235372    0.9219760        NA 0.32114304
JRSS-B       0.56683611    0.9615070 0.6788570         NA
> check + t(check)  # Should be 1's in the off-diagonals
             Biometrika Comm.Statist JASA JRSS-B
Biometrika           NA            1    1      1
Comm.Statist          1           NA    1      1
JASA                  1            1   NA      1
JRSS-B                1            1    1     NA
> 
> 
> 
> cleanEx()
> nameEx("bratUC")
> ### * bratUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Brat
> ### Title: Inputting Data to fit a Bradley Terry Model
> ### Aliases: Brat
> ### Keywords: models regression
> 
> ### ** Examples
> 
> journal <- c("Biometrika", "Comm Statist", "JASA", "JRSS-B")
> mat <- matrix(c( NA, 33, 320, 284,   730, NA, 813, 276,
+                 498, 68,  NA, 325,   221, 17, 142, NA), 4, 4)
> dimnames(mat) <- list(winner = journal, loser = journal)
> Brat(mat)  # Less readable
  Comm Statist>Biometrika JASA>Biometrika JRSS-B>Biometrika
1                      33             320               284
  Biometrika>Comm Statist JASA>Comm Statist JRSS-B>Comm Statist Biometrika>JASA
1                     730               813                 276             498
  Comm Statist>JASA JRSS-B>JASA Biometrika>JRSS-B Comm Statist>JRSS-B
1                68         325               221                  17
  JASA>JRSS-B
1         142
attr(,"ties")
  Comm Statist==Biometrika JASA==Biometrika JRSS-B==Biometrika
1                        0                0                  0
  Biometrika==Comm Statist JASA==Comm Statist JRSS-B==Comm Statist
1                        0                  0                    0
  Biometrika==JASA Comm Statist==JASA JRSS-B==JASA Biometrika==JRSS-B
1                0                  0            0                  0
  Comm Statist==JRSS-B JASA==JRSS-B
1                    0            0
attr(,"are.ties")
[1] FALSE
> Brat(mat, whitespace = TRUE)  # More readable
  Comm Statist > Biometrika JASA > Biometrika JRSS-B > Biometrika
1                        33               320                 284
  Biometrika > Comm Statist JASA > Comm Statist JRSS-B > Comm Statist
1                       730                 813                   276
  Biometrika > JASA Comm Statist > JASA JRSS-B > JASA Biometrika > JRSS-B
1               498                  68           325                 221
  Comm Statist > JRSS-B JASA > JRSS-B
1                    17           142
attr(,"ties")
  Comm Statist == Biometrika JASA == Biometrika JRSS-B == Biometrika
1                          0                  0                    0
  Biometrika == Comm Statist JASA == Comm Statist JRSS-B == Comm Statist
1                          0                    0                      0
  Biometrika == JASA Comm Statist == JASA JRSS-B == JASA Biometrika == JRSS-B
1                  0                    0              0                    0
  Comm Statist == JRSS-B JASA == JRSS-B
1                      0              0
attr(,"are.ties")
[1] FALSE
> vglm(Brat(mat, whitespace = TRUE) ~ 1, brat, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -20597.93
VGLM    linear loop  2 :  loglikelihood = -20525.99
VGLM    linear loop  3 :  loglikelihood = -20520.45
VGLM    linear loop  4 :  loglikelihood = -20520.38
VGLM    linear loop  5 :  loglikelihood = -20520.38

Call:
vglm(formula = Brat(mat, whitespace = TRUE) ~ 1, family = brat, 
    trace = TRUE)


Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 
   -0.2689541    -3.2180264    -0.7485238 

Degrees of Freedom: 3 Total; 0 Residual
Log-likelihood: -20520.38 
> 
> 
> 
> cleanEx()
> nameEx("bratt")
> ### * bratt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bratt
> ### Title: Bradley Terry Model With Ties
> ### Aliases: bratt
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # citation statistics: being cited is a 'win'; citing is a 'loss'
> journal <- c("Biometrika", "Comm.Statist", "JASA", "JRSS-B")
> mat <- matrix(c( NA, 33, 320, 284,
+                 730, NA, 813, 276,
+                 498, 68,  NA, 325,
+                 221, 17, 142,  NA), 4, 4)
> dimnames(mat) <- list(winner = journal, loser = journal)
> 
> # Add some ties. This is fictitional data.
> ties <- 5 + 0 * mat
> ties[2, 1] <- ties[1,2] <- 9
> 
> # Now fit the model
> fit <- vglm(Brat(mat, ties) ~ 1, bratt(refgp = 1), trace = TRUE,
+             crit = "coef")
VGLM    linear loop  1 :  coefficients = 
-1.87315508, -0.35603686,  0.21647979, -4.33749983
VGLM    linear loop  2 :  coefficients = 
-2.62102078, -0.44597533,  0.25173061, -4.53044084
VGLM    linear loop  3 :  coefficients = 
-2.89870009, -0.46059635,  0.25373904, -4.54107239
VGLM    linear loop  4 :  coefficients = 
-2.93502254, -0.46126674,  0.25372840, -4.54144520
VGLM    linear loop  5 :  coefficients = 
-2.93559920, -0.46127181,  0.25372820, -4.54144817
VGLM    linear loop  6 :  coefficients = 
-2.93559935, -0.46127181,  0.25372820, -4.54144817
> 
> summary(fit)
Call:
vglm(formula = Brat(mat, ties) ~ 1, family = bratt(refgp = 1), 
    trace = TRUE, crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -2.93560    0.10221 -28.721  < 2e-16 ***
(Intercept):2 -0.46127    0.05990  -7.701 1.35e-14 ***
(Intercept):3  0.25373    0.07003   3.623 0.000291 ***
(Intercept):4 -4.54145    0.17595 -25.810  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(alpha2), loglink(alpha3), loglink(alpha4), 
loglink(alpha0)

Log-likelihood: -1821.474 on 0 degrees of freedom

Number of Fisher scoring iterations: 6 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1', '(Intercept):4'

> c(0, coef(fit))  # Log-abilities (last is log(alpha0))
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 
    0.0000000    -2.9355993    -0.4612718     0.2537282    -4.5414482 
> c(1, Coef(fit))  #     Abilities (last is alpha0)
               alpha2     alpha3     alpha4     alpha0 
1.00000000 0.05309889 0.63048128 1.28882145 0.01065796 
> 
> fit@misc$alpha   # alpha_1,...,alpha_M
[1] 1.00000000 0.05309889 0.63048128 1.28882145
> fit@misc$alpha0  # alpha_0
[1] 0.01065796
> 
> fitted(fit)  # Probabilities of winning and tying, in awkward form
  Comm.Statist>Biometrika JASA>Biometrika JRSS-B>Biometrika
1              0.04991637       0.3841729          0.560484
  Biometrika>Comm.Statist JASA>Comm.Statist JRSS-B>Comm.Statist Biometrika>JASA
1               0.9400645         0.9081629           0.9528627       0.6093328
  Comm.Statist>JASA JRSS-B>JASA Biometrika>JRSS-B Comm.Statist>JRSS-B
1        0.07648512   0.6677967          0.434881          0.03925753
  JASA>JRSS-B
1   0.3266809
attr(,"probtie")
  Comm.Statist==Biometrika JASA==Biometrika JRSS-B==Biometrika
1               0.01001917      0.006494245        0.004634945
  Biometrika==Comm.Statist JASA==Comm.Statist JRSS-B==Comm.Statist
1               0.01001917         0.01535202          0.007879737
  Biometrika==JASA Comm.Statist==JASA JRSS-B==JASA Biometrika==JRSS-B
1      0.006494245         0.01535202  0.005522372        0.004634945
  Comm.Statist==JRSS-B JASA==JRSS-B
1          0.007879737  0.005522372
> predict(fit)
     loglink(alpha2) loglink(alpha3) loglink(alpha4) loglink(alpha0)
[1,]       -2.935599      -0.4612718       0.2537282       -4.541448
> (check <- InverseBrat(fitted(fit)))    # Probabilities of winning
             Biometrika Comm.Statist       JASA     JRSS-B
Biometrika           NA    0.9400645 0.60933282 0.43488104
Comm.Statist 0.04991637           NA 0.07648512 0.03925753
JASA         0.38417294    0.9081629         NA 0.32668089
JRSS-B       0.56048401    0.9528627 0.66779674         NA
> qprob <- attr(fitted(fit), "probtie")  # Probabilities of a tie
> qprobmat <- InverseBrat(c(qprob), NCo = nrow(ties))  # Pr(tie)
> check + t(check) + qprobmat  # Should be 1s in the off-diagonals
             Biometrika Comm.Statist JASA JRSS-B
Biometrika           NA            1    1      1
Comm.Statist          1           NA    1      1
JASA                  1            1   NA      1
JRSS-B                1            1    1     NA
> 
> 
> 
> cleanEx()
> nameEx("budworm")
> ### * budworm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: budworm
> ### Title: Western Spuce Budworm
> ### Aliases: budworm
> ### Keywords: datasets
> 
> ### ** Examples
> 
> budworm
   ddeg total stage1 stage2 stage3 stage4 stage5 stage6 stage7
1    58    16     16      0      0      0      0      0      0
2    82    10     10      0      0      0      0      0      0
3   107    30     23      7      0      0      0      0      0
4   155    47      3     44      0      0      0      0      0
5   237    64      0      6     45     13      0      0      0
6   307    74      0      2      9     48     15      0      0
7   342    72      0      0      1     34     37      0      0
8   388   103      0      0      1     10     87      5      0
9   442    81      0      0      0      7     53     21      0
10  518    76      0      0      0      0     10     65      1
11  609    40      0      0      0      0      0     14     26
12  685    42      0      0      0      0      0      0     42
> summary(budworm)
      ddeg           total            stage1           stage2      
 Min.   : 58.0   Min.   : 10.00   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:143.0   1st Qu.: 37.50   1st Qu.: 0.000   1st Qu.: 0.000  
 Median :324.5   Median : 55.50   Median : 0.000   Median : 0.000  
 Mean   :327.5   Mean   : 54.58   Mean   : 4.333   Mean   : 4.917  
 3rd Qu.:461.0   3rd Qu.: 74.50   3rd Qu.: 4.750   3rd Qu.: 3.000  
 Max.   :685.0   Max.   :103.00   Max.   :23.000   Max.   :44.000  
     stage3           stage4           stage5          stage6     
 Min.   : 0.000   Min.   : 0.000   Min.   : 0.00   Min.   : 0.00  
 1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.00   1st Qu.: 0.00  
 Median : 0.000   Median : 0.000   Median : 0.00   Median : 0.00  
 Mean   : 4.667   Mean   : 9.333   Mean   :16.83   Mean   : 8.75  
 3rd Qu.: 1.000   3rd Qu.:10.750   3rd Qu.:20.50   3rd Qu.: 7.25  
 Max.   :45.000   Max.   :48.000   Max.   :87.00   Max.   :65.00  
     stage7     
 Min.   : 0.00  
 1st Qu.: 0.00  
 Median : 0.00  
 Mean   : 5.75  
 3rd Qu.: 0.25  
 Max.   :42.00  
> 
> 
> 
> cleanEx()
> nameEx("calibrate")
> ### * calibrate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate
> ### Title: Model Calibrations
> ### Aliases: calibrate
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Stdzed environmental vars
> ##D set.seed(123)
> ##D pcao1 <- cao(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull, Zoraspin) ~
> ##D          WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D          family = poissonff, data = hspider, Rank = 1, Bestof = 3,
> ##D          df1.nl = c(Zoraspin = 2, 1.9), Crow1positive = TRUE)
> ##D 
> ##D siteNos <- 1:2  # Calibrate these sites
> ##D cpcao1 <- calibrate(pcao1, trace = TRUE,
> ##D                     newdata = data.frame(depvar(pcao1)[siteNos, ],
> ##D                                          model.matrix(pcao1)[siteNos, ]))
> ##D 
> ##D # Graphically compare the actual site scores with their calibrated values
> ##D persp(pcao1, main = "Site scores: solid=actual, dashed=calibrated",
> ##D       label = TRUE, col = "blue", las = 1)
> ##D abline(v = latvar(pcao1)[siteNos], col = seq(siteNos))  # Actual scores
> ##D abline(v = cpcao1, lty = 2, col = seq(siteNos))  # Calibrated values
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("calibrate.qrrvglm")
> ### * calibrate.qrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate.qrrvglm
> ### Title: Calibration for CQO and CAO models
> ### Aliases: calibrate.qrrvglm
> ### Keywords: models nonlinear regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Stdze environmental variables
> ##D set.seed(123)
> ##D siteNos <- c(1, 5)  # Calibrate these sites
> ##D pet1 <- cqo(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull, Zoraspin) ~
> ##D         WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D         trace = FALSE,
> ##D         data = hspider[-siteNos, ],  # Sites not in fitted model
> ##D         family = poissonff, I.toler = TRUE, Crow1positive = TRUE)
> ##D y0 <- hspider[siteNos, colnames(depvar(pet1))]  # Species counts
> ##D (cpet1 <- calibrate(pet1, trace = TRUE, newdata = data.frame(y0)))
> ##D (clrpet1 <- calibrate(pet1, lr.confint = TRUE, newdata = data.frame(y0)))
> ##D (ccfpet1 <- calibrate(pet1, cf.confint = TRUE, newdata = data.frame(y0)))
> ##D (cp1wald <- calibrate(pet1, newdata = y0, type = "everything"))
> ## End(Not run)
> 
> ## Not run: 
> ##D # Graphically compare the actual site scores with their calibrated
> ##D # values. 95 percent likelihood-based confidence intervals in green.
> ##D persp(pet1, main = "Site scores: solid=actual, dashed=calibrated",
> ##D       label = TRUE, col = "gray50", las = 1)
> ##D # Actual site scores:
> ##D xvars <- rownames(concoef(pet1))  # Variables comprising the latvar
> ##D est.latvar <- as.matrix(hspider[siteNos, xvars]) %*% concoef(pet1)
> ##D abline(v = est.latvar, col = seq(siteNos))
> ##D abline(v = cpet1, lty = 2, col = seq(siteNos))  # Calibrated values
> ##D arrows(clrpet1[,  3], c(60, 60), clrpet1[,  4], c(60, 60),  # Add CIs
> ##D        length = 0.08, col = "orange", angle = 90, code = 3, lwd = 2)
> ##D arrows(ccfpet1[,  3], c(70, 70), ccfpet1[,  4], c(70, 70),  # Add CIs
> ##D        length = 0.08, col = "limegreen", angle = 90, code = 3, lwd = 2)
> ##D arrows(cp1wald$latvar - 1.96 * sqrt(cp1wald$vcov), c(65, 65),
> ##D        cp1wald$latvar + 1.96 * sqrt(cp1wald$vcov), c(65, 65),  # Wald CIs
> ##D        length = 0.08, col = "blue", angle = 90, code = 3, lwd = 2)
> ##D legend("topright", lwd = 2,
> ##D        leg = c("CF interval", "Wald  interval", "LR interval"),
> ##D        col = c("limegreen", "blue", "orange"), lty = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("calibrate.qrrvglm.control")
> ### * calibrate.qrrvglm.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate.qrrvglm.control
> ### Title: Control Function for CQO/CAO Calibration
> ### Aliases: calibrate.qrrvglm.control
> ### Keywords: optimize models nonlinear regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  hspider[, 1:6] <- scale(hspider[, 1:6])  # Needed for I.tol=TRUE
> ##D set.seed(123)
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Pardlugu, Pardnigr,
> ##D                 Pardpull, Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           family = poissonff, data = hspider, I.tol = TRUE)
> ##D sort(deviance(p1, history = TRUE))  # A history of all the iterations
> ##D siteNos <- 3:4  # Calibrate these sites
> ##D cp1 <- calibrate(p1, trace = TRUE,
> ##D                  new = data.frame(depvar(p1)[siteNos, ]))
> ## End(Not run)
> ## Not run: 
> ##D # Graphically compare the actual site scores with their calibrated values
> ##D persp(p1, main = "Site scores: solid=actual, dashed=calibrated",
> ##D       label = TRUE, col = "blue", las = 1)
> ##D abline(v = latvar(p1)[siteNos], col = seq(siteNos))  # Actual site scores
> ##D abline(v = cp1, lty = 2, col = seq(siteNos))  # Calibrated values
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("calibrate.rrvglm")
> ### * calibrate.rrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibrate.rrvglm
> ### Title: Calibration for CLO models (RR-VGLMs)
> ### Aliases: calibrate.rrvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D   # Example 1
> ##D nona.xs.nz <- na.omit(xs.nz)  # Overkill!! (Data in VGAMdata package)
> ##D nona.xs.nz$dmd     <- with(nona.xs.nz, round(drinkmaxday))
> ##D nona.xs.nz$feethr  <- with(nona.xs.nz, round(feethour))
> ##D nona.xs.nz$sleephr <- with(nona.xs.nz, round(sleep))
> ##D nona.xs.nz$beats   <- with(nona.xs.nz, round(pulse))
> ##D 
> ##D p2 <- rrvglm(cbind(dmd, feethr, sleephr, beats) ~ age + smokenow +
> ##D   depressed + embarrassed + fedup + hurt + miserable +  # 11 psychological
> ##D   nofriend + moody + nervous + tense + worry + worrier, # variables
> ##D   noRRR = ~ age + smokenow, trace = FALSE, poissonff, data = nona.xs.nz,
> ##D   Rank = 2)
> ##D cp2 <- calibrate(p2, newdata = head(nona.xs.nz, 9), trace = TRUE)
> ##D cp2
> ##D 
> ##D two.cases <- nona.xs.nz[1:2, ]  # Another calibration example
> ##D two.cases$dmd       <- c(4, 10)
> ##D two.cases$feethr    <- c(4, 7)
> ##D two.cases$sleephr   <- c(7, 8)
> ##D two.cases$beats     <- c(62, 71)
> ##D (cp2b <- calibrate(p2, newdata = two.cases))
> ##D 
> ##D # Example 2
> ##D p1 <- rrvglm(cbind(dmd, feethr, sleephr, beats) ~ age + smokenow +
> ##D   depressed + embarrassed + fedup + hurt + miserable +  # 11 psychological
> ##D   nofriend + moody + nervous + tense + worry + worrier, # variables
> ##D   noRRR = ~ age + smokenow, trace = FALSE, poissonff, data = nona.xs.nz,
> ##D   Rank = 1)
> ##D (cp1c <- calibrate(p1, newdata = two.cases, lr.confint = TRUE))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cao")
> ### * cao
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cao
> ### Title: Fitting Constrained Additive Ordination (CAO)
> ### Aliases: cao
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Stdzd environmental vars
> ##D set.seed(149)  # For reproducible results
> ##D ap1 <- cao(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull) ~
> ##D            WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D            family = poissonff, data = hspider, Rank = 1,
> ##D            df1.nl = c(Pardpull= 2.7, 2.5),
> ##D            Bestof = 7, Crow1positive = FALSE)
> ##D sort(deviance(ap1, history = TRUE))  # A history of all the iterations
> ##D 
> ##D Coef(ap1)
> ##D concoef(ap1)
> ##D 
> ##D par(mfrow = c(2, 2))
> ##D plot(ap1)  # All the curves are unimodal; some quite symmetric
> ##D 
> ##D par(mfrow = c(1, 1), las = 1)
> ##D index <- 1:ncol(depvar(ap1))
> ##D lvplot(ap1, lcol = index, pcol = index, y = TRUE)
> ##D 
> ##D trplot(ap1, label = TRUE, col = index)
> ##D abline(a = 0, b = 1, lty = 2)
> ##D 
> ##D trplot(ap1, label = TRUE, col = "blue", log = "xy", which.sp = c(1, 3))
> ##D abline(a = 0, b = 1, lty = 2)
> ##D 
> ##D persp(ap1, col = index, lwd = 2, label = TRUE)
> ##D abline(v = Opt(ap1), lty = 2, col = index)
> ##D abline(h = Max(ap1), lty = 2, col = index)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cao.control")
> ### * cao.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cao.control
> ### Title: Control Function for RR-VGAMs (CAO)
> ### Aliases: cao.control
> ### Keywords: optimize models regression
> 
> ### ** Examples
> ## Not run: 
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Standardized environmental vars
> ##D set.seed(123)
> ##D ap1 <- cao(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull, Zoraspin) ~
> ##D            WaterCon + BareSand + FallTwig +
> ##D            CoveMoss + CoveHerb + ReflLux,
> ##D            family = poissonff, data = hspider,
> ##D            df1.nl = c(Zoraspin = 2.3, 2.1),
> ##D            Bestof = 10, Crow1positive = FALSE)
> ##D sort(deviance(ap1, history = TRUE))  # A history of all the iterations
> ##D 
> ##D Coef(ap1)
> ##D 
> ##D par(mfrow = c(2, 3))  # All or most of the curves are unimodal; some are
> ##D plot(ap1, lcol = "blue")  # quite symmetric. Hence a CQO model should be ok
> ##D 
> ##D par(mfrow = c(1, 1), las = 1)
> ##D index <- 1:ncol(depvar(ap1))  # lvplot is jagged because only 28 sites
> ##D lvplot(ap1, lcol = index, pcol = index, y = TRUE)
> ##D 
> ##D trplot(ap1, label = TRUE, col = index)
> ##D abline(a = 0, b = 1, lty = 2)
> ##D 
> ##D persp(ap1, label = TRUE, col = 1:4)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cardUC")
> ### * cardUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Card
> ### Title: Cardioid Distribution
> ### Aliases: Card dcard pcard qcard rcard
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mu <- 4; rho <- 0.4; x <- seq(0, 2*pi, len = 501)
> ##D plot(x, dcard(x, mu, rho), type = "l", las = 1, ylim = c(0, 1),
> ##D      ylab = paste("[dp]card(mu=", mu, ", rho=", rho, ")"),
> ##D      main = "Blue is density, orange is the CDF", col = "blue",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D lines(x, pcard(x, mu, rho), col = "orange")
> ##D 
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qcard(probs, mu, rho)
> ##D lines(Q, dcard(Q, mu, rho), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pcard(Q, mu, rho), col = "purple", lty = 3, type = "h")
> ##D abline(h = c(0,probs, 1), v = c(0, 2*pi), col = "purple", lty = 3)
> ##D max(abs(pcard(Q, mu, rho) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cardioid")
> ### * cardioid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cardioid
> ### Title: Cardioid Distribution Family Function
> ### Aliases: cardioid
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D cdata <- data.frame(y = rcard(n = 1000, mu = 4, rho = 0.45))
> ##D fit <- vglm(y ~ 1, cardioid, data = cdata, trace = TRUE)
> ##D coef(fit, matrix=TRUE)
> ##D Coef(fit)
> ##D c(with(cdata, mean(y)), head(fitted(fit), 1))
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cauchitlink")
> ### * cauchitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cauchitlink
> ### Title: Cauchit Link Function
> ### Aliases: cauchitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, by = 0.01)
> cauchitlink(p)
 [1] -31.82051595 -15.89454484 -10.57889499  -7.91581509  -6.31375151
 [6]  -5.24218358  -4.47374283  -3.89474285  -3.44202258  -3.07768354
[11]  -2.77760685  -2.52571169  -2.31086365  -2.12510817  -1.96261051
[16]  -1.81899325  -1.69090766  -1.57574786  -1.47145532  -1.37638192
[21]  -1.28919223  -1.20879235  -1.13427735  -1.06489184  -1.00000000
[26]  -0.93906251  -0.88161859  -0.82727195  -0.77567951  -0.72654253
[31]  -0.67959930  -0.63461930  -0.59139835  -0.54975465  -0.50952545
[36]  -0.47056428  -0.43273864  -0.39592801  -0.36002215  -0.32491970
[41]  -0.29052686  -0.25675636  -0.22352648  -0.19076020  -0.15838444
[46]  -0.12632938  -0.09452783  -0.06291467  -0.03142627   0.00000000
[51]   0.03142627   0.06291467   0.09452783   0.12632938   0.15838444
[56]   0.19076020   0.22352648   0.25675636   0.29052686   0.32491970
[61]   0.36002215   0.39592801   0.43273864   0.47056428   0.50952545
[66]   0.54975465   0.59139835   0.63461930   0.67959930   0.72654253
[71]   0.77567951   0.82727195   0.88161859   0.93906251   1.00000000
[76]   1.06489184   1.13427735   1.20879235   1.28919223   1.37638192
[81]   1.47145532   1.57574786   1.69090766   1.81899325   1.96261051
[86]   2.12510817   2.31086365   2.52571169   2.77760685   3.07768354
[91]   3.44202258   3.89474285   4.47374283   5.24218358   6.31375151
[96]   7.91581509  10.57889499  15.89454484  31.82051595
> max(abs(cauchitlink(cauchitlink(p), inverse = TRUE) - p))  # Should be 0
[1] 1.110223e-16
> 
> p <- c(seq(-0.02, 0.02, by=0.01), seq(0.97, 1.02, by = 0.01))
> cauchitlink(p)  # Has no NAs
 [1] -1.374823e+15 -1.374823e+15 -1.374823e+15 -3.182052e+01 -1.589454e+01
 [6]  1.057889e+01  1.589454e+01  3.182052e+01  1.374823e+15  1.374823e+15
[11]  1.374823e+15
> 
> ## Not run: 
> ##D par(mfrow = c(2, 2), lwd = (mylwd <- 2))
> ##D y <- seq(-4, 4, length = 100)
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D 
> ##D for (d in 0:1) {
> ##D   matplot(p, cbind(logitlink(p, deriv = d), probitlink(p, deriv = d)),
> ##D           type = "n", col = "purple", ylab = "transformation",
> ##D           las = 1, main = if (d == 0) "Some probability link functions"
> ##D           else "First derivative")
> ##D   lines(p,   logitlink(p, deriv = d), col = "limegreen")
> ##D   lines(p,  probitlink(p, deriv = d), col = "purple")
> ##D   lines(p, clogloglink(p, deriv = d), col = "chocolate")
> ##D   lines(p, cauchitlink(p, deriv = d), col = "tan")
> ##D   if (d == 0) {
> ##D     abline(v = 0.5, h = 0, lty = "dashed")
> ##D     legend(0, 4.5, c("logitlink", "probitlink", "clogloglink",
> ##D            "cauchitlink"), lwd = mylwd,
> ##D            col = c("limegreen", "purple", "chocolate", "tan"))
> ##D   } else
> ##D     abline(v = 0.5, lty = "dashed")
> ##D }
> ##D 
> ##D for (d in 0) {
> ##D   matplot(y, cbind( logitlink(y, deriv = d, inverse = TRUE),
> ##D                    probitlink(y, deriv = d, inverse = TRUE)),
> ##D           type  = "n", col = "purple", xlab = "transformation", ylab = "p",
> ##D           main = if (d == 0) "Some inverse probability link functions"
> ##D           else "First derivative", las=1)
> ##D   lines(y,   logitlink(y, deriv = d, inverse = TRUE), col = "limegreen")
> ##D   lines(y,  probitlink(y, deriv = d, inverse = TRUE), col = "purple")
> ##D   lines(y, clogloglink(y, deriv = d, inverse = TRUE), col = "chocolate")
> ##D   lines(y, cauchitlink(y, deriv = d, inverse = TRUE), col = "tan")
> ##D   if (d == 0) {
> ##D       abline(h = 0.5, v = 0, lty = "dashed")
> ##D       legend(-4, 1, c("logitlink", "probitlink", "clogloglink",
> ##D              "cauchitlink"), lwd = mylwd,
> ##D              col = c("limegreen", "purple", "chocolate", "tan"))
> ##D   }
> ##D }
> ##D par(lwd = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cauchy")
> ### * cauchy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cauchy
> ### Title: Cauchy Distribution Family Function
> ### Aliases: cauchy cauchy1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Both location and scale parameters unknown
> set.seed(123)
> cdata <- data.frame(x2 = runif(nn <- 1000))
> cdata <- transform(cdata, loc = exp(1 + 0.5 * x2), scale = exp(1))
> cdata <- transform(cdata, y2 = rcauchy(nn, loc, scale))
> fit2 <- vglm(y2 ~ x2, cauchy(lloc = "loglink"), data = cdata)
> coef(fit2, matrix = TRUE)
            loglink(location) loglink(scale)
(Intercept)         0.9251979       1.047886
x2                  0.6149455       0.000000
> head(fitted(fit2))  # Location estimates
      [,1]
1 3.010308
2 4.095802
3 3.243641
4 4.341437
5 4.497555
6 2.594030
> summary(fit2)
Call:
vglm(formula = y2 ~ x2, family = cauchy(lloc = "loglink"), data = cdata)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.92520    0.08558  10.811  < 2e-16 ***
(Intercept):2  1.04789    0.04472  23.431  < 2e-16 ***
x2             0.61495    0.13006   4.728 2.26e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(location), loglink(scale)

Log-likelihood: -3608.661 on 1997 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> # Location parameter unknown
> cdata <- transform(cdata, scale1 = 0.4)
> cdata <- transform(cdata, y1 = rcauchy(nn, loc, scale1))
> fit1 <- vglm(y1 ~ x2, cauchy1(scale = 0.4), data = cdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1562.2176
VGLM    linear loop  2 :  loglikelihood = -1527.4069
VGLM    linear loop  3 :  loglikelihood = -1527.342
VGLM    linear loop  4 :  loglikelihood = -1527.3415
VGLM    linear loop  5 :  loglikelihood = -1527.3415
> coef(fit1, matrix = TRUE)
            location
(Intercept) 2.678798
x2          1.680406
> 
> 
> 
> cleanEx()
> nameEx("cdf.lmscreg")
> ### * cdf.lmscreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cdf.lmscreg
> ### Title: Cumulative Distribution Function for LMS Quantile Regression
> ### Aliases: cdf.lmscreg
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fit <- vgam(BMI ~ s(age, df=c(4, 2)), lms.bcn(zero = 1), data = bmi.nz)
VGAM  s.vam  loop  1 :  loglikelihood = -6429.7568
VGAM  s.vam  loop  2 :  loglikelihood = -6327.3502
VGAM  s.vam  loop  3 :  loglikelihood = -6313.2224
VGAM  s.vam  loop  4 :  loglikelihood = -6312.8069
VGAM  s.vam  loop  5 :  loglikelihood = -6312.8166
VGAM  s.vam  loop  6 :  loglikelihood = -6312.8032
VGAM  s.vam  loop  7 :  loglikelihood = -6312.8088
VGAM  s.vam  loop  8 :  loglikelihood = -6312.8062
VGAM  s.vam  loop  9 :  loglikelihood = -6312.8074
VGAM  s.vam  loop  10 :  loglikelihood = -6312.8068
VGAM  s.vam  loop  11 :  loglikelihood = -6312.8071
VGAM  s.vam  loop  12 :  loglikelihood = -6312.807
> head(fit@post$cdf)
        1         2         3         4         5         6 
0.2280309 0.6365499 0.6356761 0.4321450 0.4321311 0.9686738 
> head(cdf(fit))  # Same
        1         2         3         4         5         6 
0.2280309 0.6365499 0.6356761 0.4321450 0.4321311 0.9686738 
> head(depvar(fit))
      [,1]
1 22.77107
2 27.70033
3 28.18127
4 25.08380
5 26.46388
6 36.19648
> head(fitted(fit))
       25%      50%      75%
1 23.00836 25.48922 28.44767
2 23.65211 26.19783 29.23269
3 24.07328 26.66334 29.75085
4 23.25503 25.75937 28.74518
5 24.53531 27.17650 30.32525
6 23.63164 26.17517 29.20742
> 
> cdf(fit, data.frame(age = c(31.5, 39), BMI = c(28.4, 24)))
        1         2 
0.7469759 0.2861046 
> 
> 
> 
> cleanEx()
> nameEx("cens.gumbel")
> ### * cens.gumbel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cens.gumbel
> ### Title: Censored Gumbel Distribution
> ### Aliases: cens.gumbel
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1
> ystar <- venice[["r1"]]  # Use the first order statistic as the response
> nn <- length(ystar)
> L <- runif(nn, 100, 104)  # Lower censoring points
> U <- runif(nn, 130, 135)  # Upper censoring points
> y <- pmax(L, ystar)  # Left  censored
> y <- pmin(U, y)      # Right censored
> extra <- list(leftcensored = ystar < L, rightcensored = ystar > U)
> fit <- vglm(y ~ scale(year), data = venice, trace = TRUE, extra = extra,
+             fam = cens.gumbel(mean = FALSE, perc = c(5, 25, 50, 75, 95)))
Warning in any(y) : coercing argument of type 'double' to logical
VGLM    linear loop  1 :  loglikelihood = -162.11775
VGLM    linear loop  2 :  loglikelihood = -148.27488
VGLM    linear loop  3 :  loglikelihood = -147.51752
VGLM    linear loop  4 :  loglikelihood = -147.46981
VGLM    linear loop  5 :  loglikelihood = -147.45962
VGLM    linear loop  6 :  loglikelihood = -147.45707
VGLM    linear loop  7 :  loglikelihood = -147.45645
VGLM    linear loop  8 :  loglikelihood = -147.4563
VGLM    linear loop  9 :  loglikelihood = -147.45626
VGLM    linear loop  10 :  loglikelihood = -147.45625
VGLM    linear loop  11 :  loglikelihood = -147.45625
> coef(fit, matrix = TRUE)
              location loglink(scale)
(Intercept) 112.274743       2.615653
scale(year)   7.523294       0.000000
> head(fitted(fit))
        5%      25%      50%      75%      95%
1 84.61764 95.15586 104.6354 116.6621 140.2438
2 85.12372 95.66193 105.1415 117.1681 140.7498
3 85.62979 96.16800 105.6476 117.6742 141.2559
4 86.13586 96.67407 106.1537 118.1803 141.7620
5 86.64193 97.18014 106.6597 118.6863 142.2681
6 87.14800 97.68621 107.1658 119.1924 142.7741
> fit@extra
$leftcensored
 [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE
[13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[25]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE
[49] FALSE FALSE FALSE

$rightcensored
 [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
[13] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE
[25] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE
[37]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[49]  TRUE  TRUE  TRUE

> 
> # Example 2: simulated data
> nn <- 1000
> ystar <- rgumbel(nn, loc = 1, scale = exp(0.5))  # The uncensored data
> L <- runif(nn, -1, 1)  # Lower censoring points
> U <- runif(nn,  2, 5)  # Upper censoring points
> y <- pmax(L, ystar)  # Left  censored
> y <- pmin(U, y)      # Right censored
> ## Not run: par(mfrow = c(1, 2)); hist(ystar); hist(y);
> extra <- list(leftcensored = ystar < L, rightcensored = ystar > U)
> fit <- vglm(y ~ 1, trace = TRUE, extra = extra, fam = cens.gumbel)
Warning in any(y) : coercing argument of type 'double' to logical
VGLM    linear loop  1 :  loglikelihood = -1931.7685
VGLM    linear loop  2 :  loglikelihood = -1635.2656
VGLM    linear loop  3 :  loglikelihood = -1632.141
VGLM    linear loop  4 :  loglikelihood = -1631.5218
VGLM    linear loop  5 :  loglikelihood = -1631.3795
VGLM    linear loop  6 :  loglikelihood = -1631.3485
VGLM    linear loop  7 :  loglikelihood = -1631.3419
VGLM    linear loop  8 :  loglikelihood = -1631.3406
VGLM    linear loop  9 :  loglikelihood = -1631.3403
VGLM    linear loop  10 :  loglikelihood = -1631.3402
VGLM    linear loop  11 :  loglikelihood = -1631.3402
VGLM    linear loop  12 :  loglikelihood = -1631.3402
> coef(fit, matrix = TRUE)
             location loglink(scale)
(Intercept) 0.9891276      0.4912966
> 
> 
> 
> cleanEx()
> nameEx("cens.normal")
> ### * cens.normal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cens.normal
> ### Title: Censored Normal Distribution
> ### Aliases: cens.normal cennormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D cdata <- data.frame(x2 = runif(nn <- 1000))  # ystar are true values
> ##D cdata <- transform(cdata, ystar = rnorm(nn, m = 100 + 15 * x2, sd = exp(3)))
> ##D with(cdata, hist(ystar))
> ##D cdata <- transform(cdata, L = runif(nn,  80,  90),  # Lower censoring points
> ##D                           U = runif(nn, 130, 140))  # Upper censoring points
> ##D cdata <- transform(cdata, y = pmax(L, ystar))  # Left  censored
> ##D cdata <- transform(cdata, y = pmin(U, y))      # Right censored
> ##D with(cdata, hist(y))
> ##D Extra <- list(leftcensored  = with(cdata, ystar < L),
> ##D               rightcensored = with(cdata, ystar > U))
> ##D fit1 <- vglm(y ~ x2, cens.normal, data = cdata, crit = "c", extra = Extra)
> ##D fit2 <- vglm(y ~ x2, tobit(Lower = with(cdata, L), Upper = with(cdata, U)),
> ##D             data = cdata, crit = "c", trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D max(abs(coef(fit1, matrix = TRUE) -
> ##D         coef(fit2, matrix = TRUE)))  # Should be 0
> ##D names(fit1@extra)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cens.poisson")
> ### * cens.poisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cens.poisson
> ### Title: Censored Poisson Family Function
> ### Aliases: cens.poisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: right censored data
> set.seed(123); U <- 20
> cdata <- data.frame(y = rpois(N <- 100, exp(3)))
> cdata <- transform(cdata, cy = pmin(U, y),
+                           rcensored = (y >= U))
> cdata <- transform(cdata, status = ifelse(rcensored, 0, 1))
> with(cdata, table(cy))
cy
12 13 14 15 16 17 18 19 20 
 2  1  6  4  5  9  9 12 52 
> with(cdata, table(rcensored))
rcensored
FALSE  TRUE 
   48    52 
> with(cdata, table(print(SurvS4(cy, status))))  # Check; U+ means >= U
       time status
  [1,]   17      1
  [2,]   20      0
  [3,]   12      1
  [4,]   20      0
  [5,]   20      0
  [6,]   20      0
  [7,]   14      1
  [8,]   12      1
  [9,]   20      0
 [10,]   20      0
 [11,]   20      0
 [12,]   20      0
 [13,]   17      1
 [14,]   20      0
 [15,]   20      0
 [16,]   19      1
 [17,]   16      1
 [18,]   15      1
 [19,]   18      1
 [20,]   15      1
 [21,]   17      1
 [22,]   19      1
 [23,]   14      1
 [24,]   14      1
 [25,]   18      1
 [26,]   14      1
 [27,]   20      0
 [28,]   20      0
 [29,]   20      0
 [30,]   20      0
 [31,]   20      0
 [32,]   19      1
 [33,]   18      1
 [34,]   18      1
 [35,]   16      1
 [36,]   20      0
 [37,]   19      1
 [38,]   20      0
 [39,]   15      1
 [40,]   18      1
 [41,]   17      1
 [42,]   14      1
 [43,]   20      0
 [44,]   19      1
 [45,]   19      1
 [46,]   20      0
 [47,]   19      1
 [48,]   20      0
 [49,]   13      1
 [50,]   15      1
 [51,]   20      0
 [52,]   20      0
 [53,]   17      1
 [54,]   20      0
 [55,]   14      1
 [56,]   20      0
 [57,]   20      0
 [58,]   20      0
 [59,]   20      0
 [60,]   20      0
 [61,]   17      1
 [62,]   16      1
 [63,]   16      1
 [64,]   20      0
 [65,]   20      0
 [66,]   17      1
 [67,]   20      0
 [68,]   19      1
 [69,]   20      0
 [70,]   20      0
 [71,]   18      1
 [72,]   20      0
 [73,]   19      1
 [74,]   20      0
 [75,]   20      0
 [76,]   20      0
 [77,]   18      1
 [78,]   20      0
 [79,]   20      0
 [80,]   20      0
 [81,]   20      0
 [82,]   17      1
 [83,]   20      0
 [84,]   18      1
 [85,]   20      0
 [86,]   19      1
 [87,]   20      0
 [88,]   20      0
 [89,]   20      0
 [90,]   20      0
 [91,]   20      0
 [92,]   17      1
 [93,]   19      1
 [94,]   16      1
 [95,]   20      0
 [96,]   19      1
 [97,]   20      0
 [98,]   20      0
 [99,]   20      0
[100,]   18      1
attr(,"type")
[1] "right"
attr(,"class")
[1] "SurvS4"

 0  1 12 13 14 15 16 17 18 19 20 
 0  0  0  0  0  0  0  0  0  0  0 
> fit <- vglm(SurvS4(cy, status) ~ 1, cens.poisson, data = cdata,
+             trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -164.29209
VGLM    linear loop  2 :  loglikelihood = -164.17535
VGLM    linear loop  3 :  loglikelihood = -164.1743
VGLM    linear loop  4 :  loglikelihood = -164.17429
VGLM    linear loop  5 :  loglikelihood = -164.17429
> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)    3.007622
> table(print(depvar(fit)))  # Another check; U+ means >= U
    time status
1     17      1
2     20      0
3     12      1
4     20      0
5     20      0
6     20      0
7     14      1
8     12      1
9     20      0
10    20      0
11    20      0
12    20      0
13    17      1
14    20      0
15    20      0
16    19      1
17    16      1
18    15      1
19    18      1
20    15      1
21    17      1
22    19      1
23    14      1
24    14      1
25    18      1
26    14      1
27    20      0
28    20      0
29    20      0
30    20      0
31    20      0
32    19      1
33    18      1
34    18      1
35    16      1
36    20      0
37    19      1
38    20      0
39    15      1
40    18      1
41    17      1
42    14      1
43    20      0
44    19      1
45    19      1
46    20      0
47    19      1
48    20      0
49    13      1
50    15      1
51    20      0
52    20      0
53    17      1
54    20      0
55    14      1
56    20      0
57    20      0
58    20      0
59    20      0
60    20      0
61    17      1
62    16      1
63    16      1
64    20      0
65    20      0
66    17      1
67    20      0
68    19      1
69    20      0
70    20      0
71    18      1
72    20      0
73    19      1
74    20      0
75    20      0
76    20      0
77    18      1
78    20      0
79    20      0
80    20      0
81    20      0
82    17      1
83    20      0
84    18      1
85    20      0
86    19      1
87    20      0
88    20      0
89    20      0
90    20      0
91    20      0
92    17      1
93    19      1
94    16      1
95    20      0
96    19      1
97    20      0
98    20      0
99    20      0
100   18      1
attr(,"class")
[1] "SurvS4"
attr(,"type")
[1] "right"

 0  1 12 13 14 15 16 17 18 19 20 
 0  0  0  0  0  0  0  0  0  0  0 
> 
> # Example 2: left censored data
> L <- 15
> cdata <- transform(cdata,
+                cY = pmax(L, y),
+                lcensored = y <  L)  # Note y < L, not cY == L or y <= L
> cdata <- transform(cdata, status = ifelse(lcensored, 0, 1))
> with(cdata, table(cY))
cY
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 
13  5  9  9 12  8 11  9  4  4  8  3  2  1  2 
> with(cdata, table(lcensored))
lcensored
FALSE  TRUE 
   91     9 
> with(cdata, table(print(SurvS4(cY, status, type = "left"))))  # Check
       time status
  [1,]   17      1
  [2,]   25      1
  [3,]   15      0
  [4,]   20      1
  [5,]   27      1
  [6,]   22      1
  [7,]   15      0
  [8,]   15      0
  [9,]   25      1
 [10,]   21      1
 [11,]   21      1
 [12,]   20      1
 [13,]   17      1
 [14,]   25      1
 [15,]   23      1
 [16,]   19      1
 [17,]   16      1
 [18,]   15      1
 [19,]   18      1
 [20,]   15      1
 [21,]   17      1
 [22,]   19      1
 [23,]   15      0
 [24,]   15      0
 [25,]   18      1
 [26,]   15      0
 [27,]   24      1
 [28,]   24      1
 [29,]   23      1
 [30,]   23      1
 [31,]   22      1
 [32,]   19      1
 [33,]   18      1
 [34,]   18      1
 [35,]   16      1
 [36,]   23      1
 [37,]   19      1
 [38,]   25      1
 [39,]   15      1
 [40,]   18      1
 [41,]   17      1
 [42,]   15      0
 [43,]   21      1
 [44,]   19      1
 [45,]   19      1
 [46,]   26      1
 [47,]   19      1
 [48,]   26      1
 [49,]   15      0
 [50,]   15      1
 [51,]   21      1
 [52,]   21      1
 [53,]   17      1
 [54,]   29      1
 [55,]   15      0
 [56,]   21      1
 [57,]   22      1
 [58,]   20      1
 [59,]   24      1
 [60,]   29      1
 [61,]   17      1
 [62,]   16      1
 [63,]   16      1
 [64,]   22      1
 [65,]   20      1
 [66,]   17      1
 [67,]   20      1
 [68,]   19      1
 [69,]   20      1
 [70,]   21      1
 [71,]   18      1
 [72,]   22      1
 [73,]   19      1
 [74,]   21      1
 [75,]   25      1
 [76,]   22      1
 [77,]   18      1
 [78,]   25      1
 [79,]   24      1
 [80,]   22      1
 [81,]   21      1
 [82,]   17      1
 [83,]   25      1
 [84,]   18      1
 [85,]   21      1
 [86,]   19      1
 [87,]   21      1
 [88,]   20      1
 [89,]   28      1
 [90,]   20      1
 [91,]   25      1
 [92,]   17      1
 [93,]   19      1
 [94,]   16      1
 [95,]   22      1
 [96,]   19      1
 [97,]   26      1
 [98,]   27      1
 [99,]   22      1
[100,]   18      1
attr(,"type")
[1] "left"
attr(,"class")
[1] "SurvS4"

 0  1 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 
> fit <- vglm(SurvS4(cY, status, type = "left") ~ 1, cens.poisson,
+             data = cdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -267.92333
VGLM    linear loop  2 :  loglikelihood = -267.43783
VGLM    linear loop  3 :  loglikelihood = -267.43756
VGLM    linear loop  4 :  loglikelihood = -267.43756
> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)    2.991431
> 
> # Example 3: interval censored data
> cdata <- transform(cdata, Lvec = rep(L, len = N),
+                           Uvec = rep(U, len = N))
> cdata <-
+   transform(cdata,
+         icensored = Lvec <= y & y < Uvec)  # Not lcensored or rcensored
> with(cdata, table(icensored))
icensored
FALSE  TRUE 
   61    39 
> cdata <- transform(cdata, status = rep(3, N))  # 3 == interval censored
> cdata <- transform(cdata,
+          status = ifelse(rcensored, 0, status))  # 0 means right censored
> cdata <- transform(cdata,
+          status = ifelse(lcensored, 2, status))  # 2 means left  censored
> # Have to adjust Lvec and Uvec because of the (start, end] format:
> cdata$Lvec[with(cdata,icensored)] <- cdata$Lvec[with(cdata,icensored)]-1
> cdata$Uvec[with(cdata,icensored)] <- cdata$Uvec[with(cdata,icensored)]-1
> # Unchanged:
> cdata$Lvec[with(cdata, lcensored)] <- cdata$Lvec[with(cdata, lcensored)]
> cdata$Lvec[with(cdata, rcensored)] <- cdata$Uvec[with(cdata, rcensored)]
> with(cdata,  # Check
+  table(ii <- print(SurvS4(Lvec, Uvec, status, type = "interval"))))
       time    status
  [1,]   14 19      3
  [2,]   20  1      0
  [3,]   15  1      2
  [4,]   20  1      0
  [5,]   20  1      0
  [6,]   20  1      0
  [7,]   15  1      2
  [8,]   15  1      2
  [9,]   20  1      0
 [10,]   20  1      0
 [11,]   20  1      0
 [12,]   20  1      0
 [13,]   14 19      3
 [14,]   20  1      0
 [15,]   20  1      0
 [16,]   14 19      3
 [17,]   14 19      3
 [18,]   14 19      3
 [19,]   14 19      3
 [20,]   14 19      3
 [21,]   14 19      3
 [22,]   14 19      3
 [23,]   15  1      2
 [24,]   15  1      2
 [25,]   14 19      3
 [26,]   15  1      2
 [27,]   20  1      0
 [28,]   20  1      0
 [29,]   20  1      0
 [30,]   20  1      0
 [31,]   20  1      0
 [32,]   14 19      3
 [33,]   14 19      3
 [34,]   14 19      3
 [35,]   14 19      3
 [36,]   20  1      0
 [37,]   14 19      3
 [38,]   20  1      0
 [39,]   14 19      3
 [40,]   14 19      3
 [41,]   14 19      3
 [42,]   15  1      2
 [43,]   20  1      0
 [44,]   14 19      3
 [45,]   14 19      3
 [46,]   20  1      0
 [47,]   14 19      3
 [48,]   20  1      0
 [49,]   15  1      2
 [50,]   14 19      3
 [51,]   20  1      0
 [52,]   20  1      0
 [53,]   14 19      3
 [54,]   20  1      0
 [55,]   15  1      2
 [56,]   20  1      0
 [57,]   20  1      0
 [58,]   20  1      0
 [59,]   20  1      0
 [60,]   20  1      0
 [61,]   14 19      3
 [62,]   14 19      3
 [63,]   14 19      3
 [64,]   20  1      0
 [65,]   20  1      0
 [66,]   14 19      3
 [67,]   20  1      0
 [68,]   14 19      3
 [69,]   20  1      0
 [70,]   20  1      0
 [71,]   14 19      3
 [72,]   20  1      0
 [73,]   14 19      3
 [74,]   20  1      0
 [75,]   20  1      0
 [76,]   20  1      0
 [77,]   14 19      3
 [78,]   20  1      0
 [79,]   20  1      0
 [80,]   20  1      0
 [81,]   20  1      0
 [82,]   14 19      3
 [83,]   20  1      0
 [84,]   14 19      3
 [85,]   20  1      0
 [86,]   14 19      3
 [87,]   20  1      0
 [88,]   20  1      0
 [89,]   20  1      0
 [90,]   20  1      0
 [91,]   20  1      0
 [92,]   14 19      3
 [93,]   14 19      3
 [94,]   14 19      3
 [95,]   20  1      0
 [96,]   14 19      3
 [97,]   20  1      0
 [98,]   20  1      0
 [99,]   20  1      0
[100,]   14 19      3
attr(,"type")
[1] "interval"
attr(,"class")
[1] "SurvS4"

 0  1  2  3 14 15 19 20 
 0  0  0  0  0  0  0  0 
> fit <- vglm(SurvS4(Lvec, Uvec, status, type = "interval") ~ 1,
+             cens.poisson, data = cdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -92.864355
VGLM    linear loop  2 :  loglikelihood = -92.598948
VGLM    linear loop  3 :  loglikelihood = -92.598882
VGLM    linear loop  4 :  loglikelihood = -92.598882
> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)    2.996398
> table(print(depvar(fit)))  # Another check
    time    status
1     14 19      3
2     20  1      0
3     15  1      2
4     20  1      0
5     20  1      0
6     20  1      0
7     15  1      2
8     15  1      2
9     20  1      0
10    20  1      0
11    20  1      0
12    20  1      0
13    14 19      3
14    20  1      0
15    20  1      0
16    14 19      3
17    14 19      3
18    14 19      3
19    14 19      3
20    14 19      3
21    14 19      3
22    14 19      3
23    15  1      2
24    15  1      2
25    14 19      3
26    15  1      2
27    20  1      0
28    20  1      0
29    20  1      0
30    20  1      0
31    20  1      0
32    14 19      3
33    14 19      3
34    14 19      3
35    14 19      3
36    20  1      0
37    14 19      3
38    20  1      0
39    14 19      3
40    14 19      3
41    14 19      3
42    15  1      2
43    20  1      0
44    14 19      3
45    14 19      3
46    20  1      0
47    14 19      3
48    20  1      0
49    15  1      2
50    14 19      3
51    20  1      0
52    20  1      0
53    14 19      3
54    20  1      0
55    15  1      2
56    20  1      0
57    20  1      0
58    20  1      0
59    20  1      0
60    20  1      0
61    14 19      3
62    14 19      3
63    14 19      3
64    20  1      0
65    20  1      0
66    14 19      3
67    20  1      0
68    14 19      3
69    20  1      0
70    20  1      0
71    14 19      3
72    20  1      0
73    14 19      3
74    20  1      0
75    20  1      0
76    20  1      0
77    14 19      3
78    20  1      0
79    20  1      0
80    20  1      0
81    20  1      0
82    14 19      3
83    20  1      0
84    14 19      3
85    20  1      0
86    14 19      3
87    20  1      0
88    20  1      0
89    20  1      0
90    20  1      0
91    20  1      0
92    14 19      3
93    14 19      3
94    14 19      3
95    20  1      0
96    14 19      3
97    20  1      0
98    20  1      0
99    20  1      0
100   14 19      3
attr(,"class")
[1] "SurvS4"
attr(,"type")
[1] "interval"

 0  1  2  3 14 15 19 20 
 0  0  0  0  0  0  0  0 
> 
> # Example 4: Add in some uncensored observations
> index <- (1:N)[with(cdata, icensored)]
> index <- head(index, 4)
> cdata$status[index] <- 1  # actual or uncensored value
> cdata$Lvec[index] <- cdata$y[index]
> with(cdata, table(ii <- print(SurvS4(Lvec, Uvec, status,
+                                      type = "interval"))))  # Check
       time    status
  [1,]   17  1      1
  [2,]   20  1      0
  [3,]   15  1      2
  [4,]   20  1      0
  [5,]   20  1      0
  [6,]   20  1      0
  [7,]   15  1      2
  [8,]   15  1      2
  [9,]   20  1      0
 [10,]   20  1      0
 [11,]   20  1      0
 [12,]   20  1      0
 [13,]   17  1      1
 [14,]   20  1      0
 [15,]   20  1      0
 [16,]   19  1      1
 [17,]   16  1      1
 [18,]   14 19      3
 [19,]   14 19      3
 [20,]   14 19      3
 [21,]   14 19      3
 [22,]   14 19      3
 [23,]   15  1      2
 [24,]   15  1      2
 [25,]   14 19      3
 [26,]   15  1      2
 [27,]   20  1      0
 [28,]   20  1      0
 [29,]   20  1      0
 [30,]   20  1      0
 [31,]   20  1      0
 [32,]   14 19      3
 [33,]   14 19      3
 [34,]   14 19      3
 [35,]   14 19      3
 [36,]   20  1      0
 [37,]   14 19      3
 [38,]   20  1      0
 [39,]   14 19      3
 [40,]   14 19      3
 [41,]   14 19      3
 [42,]   15  1      2
 [43,]   20  1      0
 [44,]   14 19      3
 [45,]   14 19      3
 [46,]   20  1      0
 [47,]   14 19      3
 [48,]   20  1      0
 [49,]   15  1      2
 [50,]   14 19      3
 [51,]   20  1      0
 [52,]   20  1      0
 [53,]   14 19      3
 [54,]   20  1      0
 [55,]   15  1      2
 [56,]   20  1      0
 [57,]   20  1      0
 [58,]   20  1      0
 [59,]   20  1      0
 [60,]   20  1      0
 [61,]   14 19      3
 [62,]   14 19      3
 [63,]   14 19      3
 [64,]   20  1      0
 [65,]   20  1      0
 [66,]   14 19      3
 [67,]   20  1      0
 [68,]   14 19      3
 [69,]   20  1      0
 [70,]   20  1      0
 [71,]   14 19      3
 [72,]   20  1      0
 [73,]   14 19      3
 [74,]   20  1      0
 [75,]   20  1      0
 [76,]   20  1      0
 [77,]   14 19      3
 [78,]   20  1      0
 [79,]   20  1      0
 [80,]   20  1      0
 [81,]   20  1      0
 [82,]   14 19      3
 [83,]   20  1      0
 [84,]   14 19      3
 [85,]   20  1      0
 [86,]   14 19      3
 [87,]   20  1      0
 [88,]   20  1      0
 [89,]   20  1      0
 [90,]   20  1      0
 [91,]   20  1      0
 [92,]   14 19      3
 [93,]   14 19      3
 [94,]   14 19      3
 [95,]   20  1      0
 [96,]   14 19      3
 [97,]   20  1      0
 [98,]   20  1      0
 [99,]   20  1      0
[100,]   14 19      3
attr(,"type")
[1] "interval"
attr(,"class")
[1] "SurvS4"

 0  1  2  3 14 15 16 17 19 20 
 0  0  0  0  0  0  1  2  1  0 
> fit <- vglm(SurvS4(Lvec, Uvec, status, type = "interval") ~ 1,
+             cens.poisson, data = cdata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 3.0153027
VGLM    linear loop  2 :  coefficients = 2.9962444
VGLM    linear loop  3 :  coefficients = 2.9963726
VGLM    linear loop  4 :  coefficients = 2.9963736
VGLM    linear loop  5 :  coefficients = 2.9963736
> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)    2.996374
> table(print(depvar(fit)))  # Another check
    time    status
1     17  1      1
2     20  1      0
3     15  1      2
4     20  1      0
5     20  1      0
6     20  1      0
7     15  1      2
8     15  1      2
9     20  1      0
10    20  1      0
11    20  1      0
12    20  1      0
13    17  1      1
14    20  1      0
15    20  1      0
16    19  1      1
17    16  1      1
18    14 19      3
19    14 19      3
20    14 19      3
21    14 19      3
22    14 19      3
23    15  1      2
24    15  1      2
25    14 19      3
26    15  1      2
27    20  1      0
28    20  1      0
29    20  1      0
30    20  1      0
31    20  1      0
32    14 19      3
33    14 19      3
34    14 19      3
35    14 19      3
36    20  1      0
37    14 19      3
38    20  1      0
39    14 19      3
40    14 19      3
41    14 19      3
42    15  1      2
43    20  1      0
44    14 19      3
45    14 19      3
46    20  1      0
47    14 19      3
48    20  1      0
49    15  1      2
50    14 19      3
51    20  1      0
52    20  1      0
53    14 19      3
54    20  1      0
55    15  1      2
56    20  1      0
57    20  1      0
58    20  1      0
59    20  1      0
60    20  1      0
61    14 19      3
62    14 19      3
63    14 19      3
64    20  1      0
65    20  1      0
66    14 19      3
67    20  1      0
68    14 19      3
69    20  1      0
70    20  1      0
71    14 19      3
72    20  1      0
73    14 19      3
74    20  1      0
75    20  1      0
76    20  1      0
77    14 19      3
78    20  1      0
79    20  1      0
80    20  1      0
81    20  1      0
82    14 19      3
83    20  1      0
84    14 19      3
85    20  1      0
86    14 19      3
87    20  1      0
88    20  1      0
89    20  1      0
90    20  1      0
91    20  1      0
92    14 19      3
93    14 19      3
94    14 19      3
95    20  1      0
96    14 19      3
97    20  1      0
98    20  1      0
99    20  1      0
100   14 19      3
attr(,"class")
[1] "SurvS4"
attr(,"type")
[1] "interval"

 0  1  2  3 14 15 16 17 19 20 
 0  0  0  0  0  0  1  2  1  0 
> 
> 
> 
> cleanEx()
> nameEx("cfibrosis")
> ### * cfibrosis
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cfibrosis
> ### Title: Cystic Fibrosis Data
> ### Aliases: cfibrosis
> ### Keywords: datasets
> 
> ### ** Examples
> 
> cfibrosis
   siblings affected ascertained families
1        10        3           1        1
2         9        3           1        1
3         8        4           1        1
4         7        3           2        1
5         7        3           1        1
6         7        2           1        1
7         7        1           1        1
8         6        2           1        1
9         6        1           1        1
10        5        3           3        1
11        5        3           2        1
12        5        2           1        5
13        5        1           1        2
14        4        3           2        1
15        4        3           1        2
16        4        2           1        4
17        4        1           1        6
18        3        2           2        3
19        3        2           1        3
20        3        1           1       10
21        2        2           2        2
22        2        2           1        4
23        2        1           1       18
24        1        1           1        9
> summary(cfibrosis)
    siblings         affected      ascertained       families     
 Min.   : 1.000   Min.   :1.000   Min.   :1.000   Min.   : 1.000  
 1st Qu.: 3.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 1.000  
 Median : 5.000   Median :2.000   Median :1.000   Median : 1.500  
 Mean   : 4.958   Mean   :2.125   Mean   :1.292   Mean   : 3.333  
 3rd Qu.: 7.000   3rd Qu.:3.000   3rd Qu.:1.250   3rd Qu.: 4.000  
 Max.   :10.000   Max.   :4.000   Max.   :3.000   Max.   :18.000  
> 
> 
> 
> cleanEx()
> nameEx("cgo")
> ### * cgo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cgo
> ### Title: Redirects the user to cqo
> ### Aliases: cgo
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D cgo()
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("chest.nz")
> ### * chest.nz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: chest.nz
> ### Title: Chest Pain in NZ Adults Data
> ### Aliases: chest.nz
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D fit <- vgam(cbind(nolnor, nolr, lnor, lr) ~ s(age, c(4, 3)),
> ##D             binom2.or(exchan = TRUE, zero = NULL), data = chest.nz)
> ##D coef(fit, matrix = TRUE)
> ## End(Not run)
> ## Not run:  plot(fit, which.cf = 2, se = TRUE) 
> 
> 
> 
> cleanEx()
> nameEx("chinese.nz")
> ### * chinese.nz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: chinese.nz
> ### Title: Chinese Population in New Zealand 1867-2001 Data
> ### Aliases: chinese.nz
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D plot(female / (male + female) ~ year, chinese.nz, type = "b",
> ##D      ylab = "Proportion", col = "blue", las = 1,
> ##D      cex = 0.015 * sqrt(male + female),
> ##D #    cex = 0.10 * sqrt((male + female)^1.5 / sqrt(female) / sqrt(male)),
> ##D      main = "Proportion of NZ Chinese that are female")
> ##D abline(h = 0.5, lty = "dashed", col = "gray")
> ##D 
> ##D fit1.cnz <- vglm(cbind(female, male) ~ year,             binomialff,
> ##D                  data = chinese.nz)
> ##D fit2.cnz <- vglm(cbind(female, male) ~ sm.poly(year, 2), binomialff,
> ##D                  data = chinese.nz)
> ##D fit4.cnz <- vglm(cbind(female, male) ~   sm.bs(year, 5), binomialff,
> ##D                  data = chinese.nz)
> ##D 
> ##D lines(fitted(fit1.cnz) ~ year, chinese.nz, col = "purple", lty = 1)
> ##D lines(fitted(fit2.cnz) ~ year, chinese.nz, col = "green", lty = 2)
> ##D lines(fitted(fit4.cnz) ~ year, chinese.nz, col = "orange", lwd = 2, lty = 1)
> ##D legend("bottomright", col = c("purple", "green", "orange"),
> ##D        lty = c(1, 2, 1), leg = c("linear", "quadratic", "B-spline"))
> ##D 
> ##D plot(100*(male+female)/nz ~ year, chinese.nz, type = "b", ylab = "Percent",
> ##D      ylim = c(0, max(100*(male+female)/nz)), col = "blue", las = 1,
> ##D      main = "Percent of NZers that are Chinese")
> ##D abline(h = 0, lty = "dashed", col = "gray") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("chisq")
> ### * chisq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: chisq
> ### Title: Chi-squared and Chi Distributions
> ### Aliases: chisq
> ### Keywords: models regression
> 
> ### ** Examples
> 
> cdata <- data.frame(x2 = runif(nn <- 1000))
> cdata <- transform(cdata, y1 = rchisq(nn, df = exp(1 - 1 * x2)),
+                           y2 = rchisq(nn, df = exp(2 - 2 * x2)))
> fit <- vglm(cbind(y1, y2) ~ x2, chisq, data = cdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3478.6278
VGLM    linear loop  2 :  loglikelihood = -3355.1204
VGLM    linear loop  3 :  loglikelihood = -3354.9371
VGLM    linear loop  4 :  loglikelihood = -3354.9371
VGLM    linear loop  5 :  loglikelihood = -3354.9371
> coef(fit, matrix = TRUE)
            loglink(df1) loglink(df2)
(Intercept)    0.9094868     1.991739
x2            -0.9266441    -1.908473
> 
> 
> 
> cleanEx()
> nameEx("clo")
> ### * clo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clo
> ### Title: Redirects the User to rrvglm()
> ### Aliases: clo
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D clo()
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("clogloglink")
> ### * clogloglink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clogloglink
> ### Title: Complementary Log-log Link Function
> ### Aliases: clogloglink cloglink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, by = 0.01)
> clogloglink(p)
 [1] -4.600149227 -3.901938658 -3.491366950 -3.198534261 -2.970195249
 [6] -2.782632533 -2.623194119 -2.484327510 -2.361160846 -2.250367327
[11] -2.149573780 -2.057027648 -1.971397744 -1.891649046 -1.816960795
[16] -1.746671079 -1.680238248 -1.617213369 -1.557220147 -1.499939987
[21] -1.445100720 -1.392467941 -1.341838284 -1.293034115 -1.245899324
[26] -1.200295930 -1.156101332 -1.113206061 -1.071511917 -1.030930433
[31] -0.991381583 -0.952792694 -0.915097528 -0.878235496 -0.842150991
[36] -0.806792806 -0.772113638 -0.738069652 -0.704620100 -0.671726992
[41] -0.639354802 -0.607470205 -0.576041853 -0.545040164 -0.514437136
[46] -0.484206187 -0.454321995 -0.424760369 -0.395498114 -0.366512921
[51] -0.337783253 -0.309288247 -0.281007617 -0.252921561 -0.225010673
[56] -0.197255858 -0.169638247 -0.142139113 -0.114739787 -0.087421572
[61] -0.060165654 -0.032953009 -0.005764308  0.021420188  0.048620745
[66]  0.075858268  0.103154446  0.130531896  0.158014333  0.185626759
[71]  0.213395680  0.241349356  0.269518092  0.297934571  0.326634260
[76]  0.355655874  0.385041948  0.414839519  0.445100958  0.475884995
[81]  0.507257991  0.539295539  0.572084496  0.605725609  0.640336939
[86]  0.676058424  0.713058051  0.751540390  0.791758684  0.834032445
[91]  0.878773939  0.926529593  0.978047902  1.034397526  1.097188700
[96]  1.169032176  1.254634900  1.364054633  1.527179626
> max(abs(clogloglink(clogloglink(p), inverse = TRUE) - p))  # Should be 0
[1] 1.110223e-16
> 
> p <- c(seq(-0.02, 0.02, by = 0.01), seq(0.97, 1.02, by = 0.01))
> clogloglink(p)  # Has NAs
Warning in log1p(-theta) : NaNs produced
Warning in log(-log1p(-theta)) : NaNs produced
 [1]       NaN       NaN      -Inf -4.600149 -3.901939  1.254635  1.364055
 [8]  1.527180       Inf       NaN       NaN
> clogloglink(p, bvalue = .Machine$double.eps)  # Has no NAs
 [1] -36.043653 -36.043653 -36.043653  -4.600149  -3.901939   1.254635
 [7]   1.364055   1.527180   3.584731   3.584731   3.584731
> 
> ## Not run: 
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D plot(p, logitlink(p), type = "l", col = "limegreen", lwd = 2, las = 1,
> ##D      main = "Some probability link functions", ylab = "transformation")
> ##D lines(p, probitlink(p), col = "purple", lwd = 2)
> ##D lines(p, clogloglink(p), col = "chocolate", lwd = 2)
> ##D lines(p, cauchitlink(p), col = "tan", lwd = 2)
> ##D abline(v = 0.5, h = 0, lty = "dashed")
> ##D legend(0.1, 4, c("logitlink", "probitlink", "clogloglink", "cauchitlink"),
> ##D        col = c("limegreen", "purple", "chocolate", "tan"), lwd = 2)
> ## End(Not run)
> 
> ## Not run: 
> ##D # This example shows that clogloglink is preferred over logitlink
> ##D n <- 500; p <- 5; S <- 3; Rank <- 1  # Species packing model:
> ##D mydata <- rcqo(n, p, S, eq.tol = TRUE, es.opt = TRUE, eq.max = TRUE,
> ##D                family = "binomial", hi.abundance = 5, seed = 123,
> ##D                Rank = Rank)
> ##D fitc <- cqo(attr(mydata, "formula"), I.tol = TRUE, data = mydata,
> ##D             fam = binomialff(multiple.responses = TRUE, link = "cloglog"),
> ##D             Rank = Rank)
> ##D fitl <- cqo(attr(mydata, "formula"), I.tol = TRUE, data = mydata,
> ##D             fam = binomialff(multiple.responses = TRUE, link = "logitlink"),
> ##D             Rank = Rank)
> ##D 
> ##D # Compare the fitted models (cols 1 and 3) with the truth (col 2)
> ##D cbind(concoef(fitc), attr(mydata, "concoefficients"), concoef(fitl))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("coalminers")
> ### * coalminers
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coalminers
> ### Title: Breathlessness and Wheeze Amongst Coalminers Data
> ### Aliases: coalminers
> ### Keywords: datasets
> 
> ### ** Examples
> 
> str(coalminers)
'data.frame':	9 obs. of  5 variables:
 $ BW  : int  9 23 54 121 169 269 404 406 372
 $ BnW : int  7 9 19 48 54 88 117 152 106
 $ nBW : int  95 105 177 257 273 324 245 225 132
 $ nBnW: int  1841 1654 1863 2357 1778 1712 1324 967 526
 $ age : int  22 27 32 37 42 47 52 57 62
> 
> 
> 
> cleanEx()
> nameEx("coefvgam")
> ### * coefvgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coefvgam
> ### Title: Extract Model Coefficients of a vgam() Object
> ### Aliases: coefvgam coef,vgam-method coefficients,vgam-method
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fit <- vgam(agaaus ~ s(altitude, df = 2), binomialff, data = hunua)
> coef(fit)  # Same as coef(fit, type = "linear")
        (Intercept) s(altitude, df = 2) 
      -1.301849e+00        3.919178e-05 
> (ii <- coef(fit, type = "nonlinear"))
$`s(altitude, df = 2)`
An object of class "vsmooth.spline.fit"
Slot "Bcoefficients":
              [,1]
 [1,] -0.453131264
 [2,] -0.451322597
 [3,] -0.442279265
 [4,] -0.427810243
 [5,] -0.402498437
 [6,] -0.375402178
 [7,] -0.335771519
 [8,] -0.299930993
 [9,] -0.255644000
[10,] -0.220742389
[11,] -0.169548167
[12,] -0.128439200
[13,] -0.084801002
[14,] -0.062303539
[15,] -0.033305655
[16,] -0.002757622
[17,]  0.029051673
[18,]  0.058376346
[19,]  0.079808659
[20,]  0.108794279
[21,]  0.129387573
[22,]  0.156436270
[23,]  0.173520062
[24,]  0.186318882
[25,]  0.190391025
[26,]  0.191789712
[27,]  0.188788943
[28,]  0.177870024
[29,]  0.163726970
[30,]  0.144508534
[31,]  0.121863793
[32,]  0.093749308
[33,]  0.044210464
[34,] -0.014026052
[35,] -0.102520326
[36,] -0.175954116
[37,] -0.254379233
[38,] -0.322914456
[39,] -0.424639054
[40,] -0.548255268
[41,] -0.844790483
[42,] -1.223243055
[43,] -1.548915695
[44,] -1.657494172

Slot "knots":
 [1] 0.000000000 0.000000000 0.000000000 0.000000000 0.001515152 0.007575758
 [7] 0.012121212 0.022727273 0.030303030 0.045454545 0.053030303 0.068181818
[13] 0.075757576 0.098484848 0.106060606 0.118181818 0.121212121 0.136363636
[19] 0.151515152 0.159090909 0.174242424 0.181818182 0.204545455 0.212121212
[25] 0.242424242 0.257575758 0.280303030 0.287878788 0.318181818 0.333333333
[31] 0.363636364 0.378787879 0.393939394 0.424242424 0.439393939 0.484848485
[37] 0.515151515 0.560606061 0.575757576 0.606060606 0.636363636 0.681818182
[43] 0.727272727 0.909090909 1.000000000 1.000000000 1.000000000 1.000000000

Slot "xmin":
s(altitude, df = 2) 
                  0 

Slot "xmax":
s(altitude, df = 2) 
                660 


> is.list(ii)
[1] TRUE
> names(ii)
[1] "s(altitude, df = 2)"
> slotNames(ii[[1]])
[1] "Bcoefficients" "knots"         "xmin"          "xmax"         
> 
> 
> 
> cleanEx()
> nameEx("coefvlm")
> ### * coefvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: coefvlm
> ### Title: Extract Model Coefficients
> ### Aliases: coefvlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(x2 = runif(nn <- 200))
> zdata <- transform(zdata, pstr0  = logitlink(-0.5 + 1*x2, inverse = TRUE),
+                           lambda =   loglink( 0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata, y2 = rzipois(nn, lambda, pstr0 = pstr0))
> 
> fit2 <- vglm(y2 ~ x2, zipoisson(zero = 1), data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -288.70377
VGLM    linear loop  2 :  loglikelihood = -287.69243
VGLM    linear loop  3 :  loglikelihood = -287.67327
VGLM    linear loop  4 :  loglikelihood = -287.67251
VGLM    linear loop  5 :  loglikelihood = -287.67248
VGLM    linear loop  6 :  loglikelihood = -287.67247
> coef(fit2, matrix = TRUE)  # Always a good idea
            logitlink(pstr0) loglink(lambda)
(Intercept)        0.5028711       0.7636501
x2                 0.0000000       1.7466258
> coef(fit2)
(Intercept):1 (Intercept):2            x2 
    0.5028711     0.7636501     1.7466258 
> coef(fit2, colon = TRUE)
(Intercept):1 (Intercept):2          x2:1 
    0.5028711     0.7636501     1.7466258 
> 
> 
> 
> cleanEx()
> nameEx("concoef")
> ### * concoef
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: concoef
> ### Title: Extract Model Constrained/Canonical Coefficients
> ### Aliases: concoef
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  set.seed(111)  # This leads to the global solution
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Standardized environmental vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           family = poissonff, data = hspider, Crow1positive = FALSE)
> ##D concoef(p1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("confintvglm")
> ### * confintvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confintvglm
> ### Title: Confidence Intervals for Parameters of VGLMs
> ### Aliases: confintvglm confintrrvglm confintvgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: this is based on a glm example
> counts <- c(18,17,15,20,10,20,25,13,12)
> outcome <- gl(3, 1, 9); treatment <- gl(3, 3)
>  glm.D93 <-  glm(counts ~ outcome + treatment, family = poisson())
> vglm.D93 <- vglm(counts ~ outcome + treatment, family = poissonff)
> confint(glm.D93) # needs MASS to be present on the system
Waiting for profiling to be done...
                 2.5 %      97.5 %
(Intercept)  2.6958215  3.36655581
outcome2    -0.8577018 -0.06255840
outcome3    -0.6753696  0.08244089
treatment2  -0.3932548  0.39325483
treatment3  -0.3932548  0.39325483
> confint.default(glm.D93)  # based on asymptotic normality
                 2.5 %      97.5 %
(Intercept)  2.7095672  3.37947764
outcome2    -0.8505027 -0.05800787
outcome3    -0.6707552  0.08478093
treatment2  -0.3919928  0.39199279
treatment3  -0.3919928  0.39199279
> confint(vglm.D93)
                 2.5 %      97.5 %
(Intercept)  2.7095672  3.37947764
outcome2    -0.8505027 -0.05800787
outcome3    -0.6707552  0.08478093
treatment2  -0.3919928  0.39199279
treatment3  -0.3919928  0.39199279
> confint(vglm.D93) - confint(glm.D93)    # Should be all 0s
Waiting for profiling to be done...
                  2.5 %       97.5 %
(Intercept) 0.013745734  0.012921826
outcome2    0.007199164  0.004550529
outcome3    0.004614421  0.002340033
treatment2  0.001262037 -0.001262037
treatment3  0.001262036 -0.001262036
> confint(vglm.D93) - confint.default(glm.D93)  # based on asympt. normality
                   2.5 %        97.5 %
(Intercept) 9.864554e-12 -9.863665e-12
outcome2    3.233238e-10 -3.233241e-10
outcome3    1.765685e-10 -1.765695e-10
treatment2  2.162413e-10 -2.162420e-10
treatment3  1.785772e-10 -1.785784e-10
> 
> # Example 2: simulated negative binomial data with multiple responses
> ndata <- data.frame(x2 = runif(nn <- 100))
> ndata <- transform(ndata, y1 = rnbinom(nn, mu = exp(3+x2), size = exp(1)),
+                           y2 = rnbinom(nn, mu = exp(2-x2), size = exp(0)))
> fit1 <- vglm(cbind(y1, y2) ~ x2, negbinomial, data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -704.01737
VGLM    linear loop  2 :  loglikelihood = -702.86917
VGLM    linear loop  3 :  loglikelihood = -702.86153
VGLM    linear loop  4 :  loglikelihood = -702.86151
VGLM    linear loop  5 :  loglikelihood = -702.86151
> coef(fit1)
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4          x2:1 
   2.84968717    0.87603665    2.10434023   -0.07638607    1.12710592 
         x2:2 
  -0.89343242 
> coef(fit1, matrix = TRUE)
            loglink(mu1) loglink(size1) loglink(mu2) loglink(size2)
(Intercept)     2.849687      0.8760367    2.1043402    -0.07638607
x2              1.127106      0.0000000   -0.8934324     0.00000000
> confint(fit1)
                   2.5 %      97.5 %
(Intercept):1  2.5589911  3.14038319
(Intercept):2  0.5859113  1.16616203
(Intercept):3  1.6269309  2.58174956
(Intercept):4 -0.4219548  0.26918267
x2:1           0.6321060  1.62210587
x2:2          -1.7256162 -0.06124867
> confint(fit1, "x2:1")  #  This might be improved to "x2" some day...
        2.5 %   97.5 %
x2:1 0.632106 1.622106
> ## Not run: 
> ##D confint(fit1, method = "profile")  # Computationally expensive
> ##D confint(fit1, "x2:1", method = "profile", trace = FALSE)
> ## End(Not run)
> 
> fit2 <- rrvglm(y1 ~ x2, negbinomial(zero = NULL), data = ndata)
> confint(as(fit2, "vglm"))  # Too narrow (SEs are biased downwards)
                  2.5 %   97.5 %
(Intercept):1 2.5835620 3.068215
(Intercept):2 1.1063014 1.841983
x2            0.7259983 1.627963
> 
> 
> 
> cleanEx()
> nameEx("constraints")
> ### * constraints
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: constraints
> ### Title: Constraint Matrices
> ### Aliases: constraints constraints.vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Fit the proportional odds model:
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ sm.bs(let, 3),
+               cumulative(parallel = TRUE, reverse = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ sm.bs(let, 3), family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
 (Intercept):1  (Intercept):2 sm.bs(let, 3)1 sm.bs(let, 3)2 sm.bs(let, 3)3 
     -19.10457      -20.00722       22.83129       15.95020       19.82301 

Degrees of Freedom: 16 Total; 11 Residual
Residual deviance: 2.231924 
Log-likelihood: -23.69281 
> coef(fit1, matrix = TRUE)
               logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)             -19.10457          -20.00722
sm.bs(let, 3)1           22.83129           22.83129
sm.bs(let, 3)2           15.95020           15.95020
sm.bs(let, 3)3           19.82301           19.82301
> constraints(fit1)  # Parallel assumption results in this
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$`sm.bs(let, 3)1`
     [,1]
[1,]    1
[2,]    1

$`sm.bs(let, 3)2`
     [,1]
[1,]    1
[2,]    1

$`sm.bs(let, 3)3`
     [,1]
[1,]    1
[2,]    1

> constraints(fit1, type = "term")  # Same as the default ("vlm"-type)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$`sm.bs(let, 3)`
     [,1]
[1,]    1
[2,]    1

> is.parallel(fit1)
  (Intercept) sm.bs(let, 3) 
        FALSE          TRUE 
> 
> # An equivalent model to fit1 (needs the type "term" constraints):
> clist.term <- constraints(fit1, type = "term")  # "term"-type constraints
> # cumulative() has no 'zero' argument to set to NULL (a good idea
> # when using the 'constraints' argument):
> (fit2 <- vglm(cbind(normal, mild, severe) ~ sm.bs(let, 3), data = pneumo,
+               cumulative(reverse = TRUE), constraints = clist.term))

Call:
vglm(formula = cbind(normal, mild, severe) ~ sm.bs(let, 3), family = cumulative(reverse = TRUE), 
    data = pneumo, constraints = clist.term)


Coefficients:
 (Intercept):1  (Intercept):2 sm.bs(let, 3)1 sm.bs(let, 3)2 sm.bs(let, 3)3 
     -19.10457      -20.00722       22.83129       15.95020       19.82301 

Degrees of Freedom: 16 Total; 11 Residual
Residual deviance: 2.231924 
Log-likelihood: -23.69281 
> abs(max(coef(fit1, matrix = TRUE) -
+         coef(fit2, matrix = TRUE)))  # Should be zero
[1] 0
> 
> # Fit a rank-1 stereotype (RR-multinomial logit) model:
> fit <- rrvglm(Country ~ Width + Height + HP, multinomial, data = car.all)
> constraints(fit)  # All except the first are the estimated A matrix
$`(Intercept)`
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
 [1,]    1    0    0    0    0    0    0    0    0
 [2,]    0    1    0    0    0    0    0    0    0
 [3,]    0    0    1    0    0    0    0    0    0
 [4,]    0    0    0    1    0    0    0    0    0
 [5,]    0    0    0    0    1    0    0    0    0
 [6,]    0    0    0    0    0    1    0    0    0
 [7,]    0    0    0    0    0    0    1    0    0
 [8,]    0    0    0    0    0    0    0    1    0
 [9,]    0    0    0    0    0    0    0    0    1

$Width
           [,1]
 [1,] 1.0000000
 [2,] 0.3941479
 [3,] 0.2926137
 [4,] 0.3737671
 [5,] 0.4650090
 [6,] 0.5202978
 [7,] 0.7834294
 [8,] 0.6651159
 [9,] 0.2945319

$Height
           [,1]
 [1,] 1.0000000
 [2,] 0.3941479
 [3,] 0.2926137
 [4,] 0.3737671
 [5,] 0.4650090
 [6,] 0.5202978
 [7,] 0.7834294
 [8,] 0.6651159
 [9,] 0.2945319

$HP
           [,1]
 [1,] 1.0000000
 [2,] 0.3941479
 [3,] 0.2926137
 [4,] 0.3737671
 [5,] 0.4650090
 [6,] 0.5202978
 [7,] 0.7834294
 [8,] 0.6651159
 [9,] 0.2945319

> 
> 
> 
> cleanEx()
> nameEx("cops")
> ### * cops
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cops
> ### Title: Centre of the Parameter Space
> ### Aliases: cops copsvglm cops,vglm-method
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data("xs.nz", package = "VGAMdata")
> ##D data1 <- na.omit(xs.nz[, c("age", "cancer", "sex")])
> ##D fit1 <- vglm(cancer ~ age + sex, binomialff, data1)
> ##D cops(fit1)  # 'beta.range' is okay here
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("corbet")
> ### * corbet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: corbet
> ### Title: Corbet's Butterfly Data
> ### Aliases: corbet
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(corbet)
     ofreq          species      
 Min.   : 1.00   Min.   :  3.00  
 1st Qu.: 6.75   1st Qu.:  8.25  
 Median :12.50   Median : 12.00  
 Mean   :12.50   Mean   : 20.88  
 3rd Qu.:18.25   3rd Qu.: 20.50  
 Max.   :24.00   Max.   :118.00  
> 
> 
> 
> cleanEx()
> nameEx("cqo")
> ### * cqo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cqo
> ### Title: Fitting Constrained Quadratic Ordination (CQO)
> ### Aliases: cqo
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1; Fit an unequal tolerances model to the hunting spiders data
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Standardized environmental variables
> ##D set.seed(1234)  # For reproducibility of the results
> ##D p1ut <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                   Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                   Trocterr, Zoraspin) ~
> ##D             WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D             fam = poissonff, data = hspider, Crow1positive = FALSE,
> ##D             eq.tol = FALSE)
> ##D sort(deviance(p1ut, history = TRUE))  # A history of all the iterations
> ##D if (deviance(p1ut) > 1177) warning("suboptimal fit obtained")
> ##D 
> ##D S <- ncol(depvar(p1ut))  # Number of species
> ##D clr <- (1:(S+1))[-7]  # Omits yellow
> ##D lvplot(p1ut, y = TRUE, lcol = clr, pch = 1:S, pcol = clr,
> ##D        las = 1)  # Ordination diagram
> ##D legend("topright", leg = colnames(depvar(p1ut)), col = clr,
> ##D        pch = 1:S, merge = TRUE, bty = "n", lty = 1:S, lwd = 2)
> ##D (cp <- Coef(p1ut))
> ##D 
> ##D (a <- latvar(cp)[cp@latvar.order])  # Ordered site scores along the gradient
> ##D # Names of the ordered sites along the gradient:
> ##D rownames(latvar(cp))[cp@latvar.order]
> ##D (aa <- Opt(cp)[, cp@Optimum.order])  # Ordered optimums along the gradient
> ##D aa <- aa[!is.na(aa)]  # Delete the species that is not unimodal
> ##D names(aa)  # Names of the ordered optimums along the gradient
> ##D 
> ##D trplot(p1ut, which.species = 1:3, log = "xy", type = "b", lty = 1, lwd = 2,
> ##D        col = c("blue","red","green"), label = TRUE) -> ii  # Trajectory plot
> ##D legend(0.00005, 0.3, paste(ii$species[, 1], ii$species[, 2], sep = " and "),
> ##D        lwd = 2, lty = 1, col = c("blue", "red", "green"))
> ##D abline(a = 0, b = 1, lty = "dashed")
> ##D 
> ##D S <- ncol(depvar(p1ut))  # Number of species
> ##D clr <- (1:(S+1))[-7]  # Omits yellow
> ##D persp(p1ut, col = clr, label = TRUE, las = 1)  # Perspective plot
> ##D 
> ##D 
> ##D # Example 2; Fit an equal tolerances model. Less numerically fraught.
> ##D set.seed(1234)
> ##D p1et <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                   Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                   Trocterr, Zoraspin) ~
> ##D             WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D             poissonff, data = hspider, Crow1positive = FALSE)
> ##D sort(deviance(p1et, history = TRUE))  # A history of all the iterations
> ##D if (deviance(p1et) > 1586) warning("suboptimal fit obtained")
> ##D S <- ncol(depvar(p1et))  # Number of species
> ##D clr <- (1:(S+1))[-7]  # Omits yellow
> ##D persp(p1et, col = clr, label = TRUE, las = 1)
> ##D 
> ##D 
> ##D # Example 3: A rank-2 equal tolerances CQO model with Poisson data
> ##D # This example is numerically fraught... need I.toler = TRUE too.
> ##D set.seed(555)
> ##D p2 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, Crow1positive = FALSE,
> ##D           I.toler = TRUE, Rank = 2, Bestof = 3, isd.latvar = c(2.1, 0.9))
> ##D sort(deviance(p2, history = TRUE))  # A history of all the iterations
> ##D if (deviance(p2) > 1127) warning("suboptimal fit obtained")
> ##D lvplot(p2, ellips = FALSE, label = TRUE, xlim = c(-3,4),
> ##D        C = TRUE, Ccol = "brown", sites = TRUE, scol = "grey",
> ##D        pcol = "blue", pch = "+", chull = TRUE, ccol = "grey")
> ##D 
> ##D 
> ##D # Example 4: species packing model with presence/absence data
> ##D set.seed(2345)
> ##D n <- 200; p <- 5; S <- 5
> ##D mydata <- rcqo(n, p, S, fam = "binomial", hi.abundance = 4,
> ##D                eq.tol = TRUE, es.opt = TRUE, eq.max = TRUE)
> ##D myform <- attr(mydata, "formula")
> ##D set.seed(1234)
> ##D b1et <- cqo(myform, binomialff(multiple.responses = TRUE, link = "clogloglink"),
> ##D             data = mydata)
> ##D sort(deviance(b1et, history = TRUE))  # A history of all the iterations
> ##D lvplot(b1et, y = TRUE, lcol = 1:S, pch = 1:S, pcol = 1:S, las = 1)
> ##D Coef(b1et)
> ##D 
> ##D # Compare the fitted model with the 'truth'
> ##D cbind(truth = attr(mydata, "concoefficients"), fitted = concoef(b1et))
> ##D 
> ##D 
> ##D # Example 5: Plot the deviance residuals for diagnostic purposes
> ##D set.seed(1234)
> ##D p1et <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                   Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                   Trocterr, Zoraspin) ~
> ##D             WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D             poissonff, data = hspider, eq.tol = TRUE, trace = FALSE)
> ##D sort(deviance(p1et, history = TRUE))  # A history of all the iterations
> ##D if (deviance(p1et) > 1586) warning("suboptimal fit obtained")
> ##D S <- ncol(depvar(p1et))
> ##D par(mfrow = c(3, 4))
> ##D for (ii in 1:S) {
> ##D   tempdata <- data.frame(latvar1 = c(latvar(p1et)),
> ##D                          sppCounts = depvar(p1et)[, ii])
> ##D   tempdata <- transform(tempdata, myOffset = -0.5 * latvar1^2)
> ##D 
> ##D # For species ii, refit the model to get the deviance residuals
> ##D   fit1 <- vglm(sppCounts ~ offset(myOffset) + latvar1, poissonff,
> ##D                data = tempdata, trace = FALSE)
> ##D 
> ##D # For checking: this should be 0
> ##D # print("max(abs(c(Coef(p1et)@B1[1,ii],Coef(p1et)@A[ii,1])-coef(fit1)))")
> ##D # print( max(abs(c(Coef(p1et)@B1[1,ii],Coef(p1et)@A[ii,1])-coef(fit1))) )
> ##D 
> ##D # Plot the deviance residuals
> ##D   devresid <- resid(fit1, type = "deviance")
> ##D   predvalues <- predict(fit1) + fit1@offset
> ##D   ooo <- with(tempdata, order(latvar1))
> ##D   plot(predvalues + devresid ~ latvar1, data = tempdata, col = "red",
> ##D        xlab = "latvar1", ylab = "", main = colnames(depvar(p1et))[ii])
> ##D   with(tempdata, lines(latvar1[ooo], predvalues[ooo], col = "blue"))
> ##D }
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("crashes")
> ### * crashes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: crashes
> ### Title: Crashes on New Zealand Roads in 2009
> ### Aliases: crashi crashf crashtr crashmc crashbc crashp alcoff alclevels
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  plot(unlist(alcoff), type = "l", frame.plot = TRUE,
> ##D      axes = FALSE, col = "blue", bty = "o",
> ##D      main = "Alcoholic offenders on NZ roads, aggregated over 2009",
> ##D      sub  = "Vertical lines at midnight (purple) and noon (orange)",
> ##D      xlab = "Day/hour", ylab = "Number of offenders")
> ##D axis(1, at = 1 + (0:6) * 24 + 12, labels = colnames(alcoff))
> ##D axis(2, las = 1)
> ##D axis(3:4, labels = FALSE, tick = FALSE)
> ##D abline(v = sort(1 + c((0:7) * 24, (0:6) * 24 + 12)), lty = "dashed",
> ##D        col = c("purple", "orange")) 
> ## End(Not run)
> 
> # Goodmans RC models
> ## Not run: 
> ##D fitgrc1 <- grc(alcoff)  # Rank-1 model
> ##D fitgrc2 <- grc(alcoff, Rank = 2, Corner = FALSE, Uncor = TRUE)
> ##D Coef(fitgrc2)
> ## End(Not run)
> ## Not run: 
> ##D  biplot(fitgrc2, scaleA = 2.3, Ccol = "blue", Acol = "orange",
> ##D        Clabels = as.character(1:23), xlim = c(-1.3, 2.3),
> ##D        ylim = c(-1.2, 1)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cratio")
> ### * cratio
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cratio
> ### Title: Ordinal Regression with Continuation Ratios
> ### Aliases: cratio
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let,
+              cratio(parallel = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cratio(parallel = TRUE), 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -8.733797     -8.051302      2.321359 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 7.626763 
Log-likelihood: -26.39023 
> coef(fit, matrix = TRUE)
            logitlink(P[Y>1|Y>=1]) logitlink(P[Y>2|Y>=2])
(Intercept)              -8.733797              -8.051302
let                       2.321359               2.321359
> constraints(fit)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> predict(fit)
  logitlink(P[Y>1|Y>=1]) logitlink(P[Y>2|Y>=2])
1             -4.6531774             -3.9706824
2             -2.4474398             -1.7649448
3             -1.6117442             -0.9292491
4             -1.0403809             -0.3578859
5             -0.5822388              0.1002563
6             -0.1997827              0.4827124
7              0.1538548              0.8363499
8              0.4160301              1.0985252
> predict(fit, untransform = TRUE)
  P[Y>1|Y>=1] P[Y>2|Y>=2]
1 0.009441281  0.01851142
2 0.079625970  0.14617212
3 0.166346597  0.28307708
4 0.261076501  0.41147144
5 0.358417612  0.52504310
6 0.450219786  0.61838815
7 0.538388014  0.69769591
8 0.602532898  0.74998366
> margeff(fit)
, , 1

                 normal        mild        severe
(Intercept)  0.08167972 -0.07878663 -0.0028930983
let         -0.02170968  0.02090961  0.0008000745

, , 2

                normal       mild      severe
(Intercept)  0.6400622 -0.4664909 -0.17357136
let         -0.1701224  0.1221861  0.04793632

, , 3

                normal       mild     severe
(Intercept)  1.2111629 -0.5965056 -0.6146573
let         -0.3219154  0.1524215  0.1694939

, , 4

                normal       mild    severe
(Intercept)  1.6848854 -0.4825758 -1.202310
let         -0.4478263  0.1167953  0.331031

, , 5

                normal        mild     severe
(Intercept)  2.0083753 -0.23426941 -1.7741059
let         -0.5338068  0.04605304  0.4877538

, , 6

                normal        mild     severe
(Intercept)  2.1618063  0.03043788 -2.1922442
let         -0.5745873 -0.02736296  0.6019503

, , 7

               normal        mild     severe
(Intercept)  2.170579  0.25808932 -2.4286681
let         -0.576919 -0.08919657  0.6661155

, , 8

                normal       mild     severe
(Intercept)  2.0916309  0.3866929 -2.4783238
let         -0.5559354 -0.1232739  0.6792092

> 
> 
> 
> cleanEx()
> nameEx("cumulative")
> ### * cumulative
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cumulative
> ### Title: Ordinal Regression with Cumulative Probabilities
> ### Aliases: cumulative
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Proportional odds model (p.179) of McCullagh and Nelder (1989)
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let,
+              cumulative(parallel = TRUE, reverse = TRUE), pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> depvar(fit)  # Sample proportions (good technique)
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> fit@y        # Sample proportions (bad technique)
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> weights(fit, type = "prior")  # Number of observations
  [,1]
1   98
2   54
3   43
4   48
5   51
6   38
7   28
8   11
> coef(fit, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> constraints(fit)  # Constraint matrices
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> apply(fitted(fit), 1, which.max)  # Classification
1 2 3 4 5 6 7 8 
1 1 1 1 1 1 1 3 
> apply(predict(fit, newdata = pneumo, type = "response"),
+       1, which.max)  # Classification
1 2 3 4 5 6 7 8 
1 1 1 1 1 1 1 3 
> R2latvar(fit)
[1] 0.5142885
> 
> # Check that the model is linear in let ----------------------
> fit2 <- vgam(cbind(normal, mild, severe) ~ s(let, df = 2),
+              cumulative(reverse = TRUE), data = pneumo)
> ## Not run: 
> ##D  plot(fit2, se = TRUE, overlay = TRUE, lcol = 1:2, scol = 1:2) 
> ## End(Not run)
> 
> # Check the proportional odds assumption with a LRT ----------
> (fit3 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = FALSE, reverse = TRUE), pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = FALSE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
    -9.593308    -11.104791      2.571300      2.743550 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.884404 
Log-likelihood: -25.01905 
> pchisq(2 * (logLik(fit3) - logLik(fit)), df =
+        length(coef(fit3)) - length(coef(fit)), lower.tail = FALSE)
[1] 0.7058849
> lrtest(fit3, fit)  # More elegant
Likelihood ratio test

Model 1: cbind(normal, mild, severe) ~ let
Model 2: cbind(normal, mild, severe) ~ let
  #Df  LogLik Df  Chisq Pr(>Chisq)
1  12 -25.019                     
2  13 -25.090  1 0.1424     0.7059
> 
> # A factor() version of fit ----------------------------------
> # This is in long format (cf. wide format above)
> Nobs <- round(depvar(fit) * c(weights(fit, type = "prior")))
> sumNobs <- colSums(Nobs)  # apply(Nobs, 2, sum)
> 
> pneumo.long <-
+   data.frame(symptoms = ordered(rep(rep(colnames(Nobs), nrow(Nobs)),
+                                         times = c(t(Nobs))),
+                                 levels = colnames(Nobs)),
+              let = rep(rep(with(pneumo, let), each = ncol(Nobs)),
+                        times = c(t(Nobs))))
> with(pneumo.long, table(let, symptoms))  # Should be same as pneumo
                  symptoms
let                normal mild severe
  1.75785791755237     98    0      0
  2.70805020110221     51    2      1
  3.06805293513362     34    6      3
  3.31418600467253     35    5      8
  3.51154543883102     32   10      9
  3.67630067190708     23    7      8
  3.8286413964891      12    6     10
  3.94158180766969      4    2      5
> 
> 
> (fit.long1 <- vglm(symptoms ~ let, data = pneumo.long, trace = TRUE,
+                    cumulative(parallel = TRUE, reverse = TRUE)))
VGLM    linear loop  1 :  deviance = 428.98474
VGLM    linear loop  2 :  deviance = 411.73439
VGLM    linear loop  3 :  deviance = 408.69065
VGLM    linear loop  4 :  deviance = 408.54867
VGLM    linear loop  5 :  deviance = 408.54833
VGLM    linear loop  6 :  deviance = 408.54833

Call:
vglm(formula = symptoms ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo.long, trace = TRUE)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676092    -10.581724      2.596806 

Degrees of Freedom: 742 Total; 739 Residual
Residual deviance: 408.5483 
Log-likelihood: -204.2742 
> coef(fit.long1, matrix = TRUE)  # cf. coef(fit, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676092         -10.581724
let                   2.596806           2.596806
> # Could try using mustart if fit.long1 failed to converge.
> mymustart <- matrix(sumNobs / sum(sumNobs),
+                     nrow(pneumo.long), ncol(Nobs), byrow = TRUE)
> fit.long2 <- vglm(symptoms ~ let, mustart = mymustart,
+                   cumulative(parallel = TRUE, reverse = TRUE),
+                   data = pneumo.long, trace = TRUE)
VGLM    linear loop  1 :  deviance = 428.98474
VGLM    linear loop  2 :  deviance = 411.73439
VGLM    linear loop  3 :  deviance = 408.69065
VGLM    linear loop  4 :  deviance = 408.54867
VGLM    linear loop  5 :  deviance = 408.54833
VGLM    linear loop  6 :  deviance = 408.54833
> coef(fit.long2, matrix = TRUE)  # cf. coef(fit, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676092         -10.581724
let                   2.596806           2.596806
> 
> 
> 
> cleanEx()
> nameEx("dagum")
> ### * dagum
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dagum
> ### Title: Dagum Distribution Family Function
> ### Aliases: dagum
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ddata <- data.frame(y = rdagum(n = 3000, scale = exp(2),
> ##D                                shape1 = exp(1), shape2 = exp(1)))
> ##D fit <- vglm(y ~ 1, dagum(lss = FALSE), data = ddata, trace = TRUE)
> ##D fit <- vglm(y ~ 1, dagum(lss = FALSE, ishape1.a = exp(1)),
> ##D             data = ddata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("dagumUC")
> ### * dagumUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Dagum
> ### Title: The Dagum Distribution
> ### Aliases: Dagum ddagum pdagum qdagum rdagum
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.1, 0.9, by = 0.1)
> shape1.a <- 1; shape2.p <- 2
> # Should be 0:
> max(abs(pdagum(qdagum(probs, shape1.a = shape1.a, shape2.p =
+   shape2.p), shape1.a = shape1.a, shape2.p = shape2.p) - probs))
[1] 5.551115e-17
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D x <- seq(-0.01, 5, len = 401)
> ##D plot(x, dexp(x), type = "l", col = "black",
> ##D      ylab = "", las = 1, ylim = c(0, 1),
> ##D      main = "Black is std exponential, others are ddagum(x, ...)")
> ##D lines(x, ddagum(x, shape1.a = shape1.a, shape2.p = 1), col = "orange")
> ##D lines(x, ddagum(x, shape1.a = shape1.a, shape2.p = 2), col = "blue")
> ##D lines(x, ddagum(x, shape1.a = shape1.a, shape2.p = 5), col = "green")
> ##D legend("topright", col = c("orange","blue","green"),
> ##D        lty = rep(1, len = 3), legend = paste("shape1.a =", shape1.a,
> ##D        ", shape2.p =", c(1, 2, 5)))
> ##D 
> ##D plot(x, pexp(x), type = "l", col = "black", ylab = "", las = 1,
> ##D      main = "Black is std exponential, others are pdagum(x, ...)")
> ##D lines(x, pdagum(x, shape1.a = shape1.a, shape2.p = 1), col = "orange")
> ##D lines(x, pdagum(x, shape1.a = shape1.a, shape2.p = 2), col = "blue")
> ##D lines(x, pdagum(x, shape1.a = shape1.a, shape2.p = 5), col = "green")
> ##D legend("bottomright", col = c("orange", "blue", "green"),
> ##D        lty = rep(1, len = 3), legend = paste("shape1.a =", shape1.a,
> ##D        ", shape2.p =", c(1, 2, 5)))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("deermice")
> ### * deermice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deermice
> ### Title: Captures of Peromyscus maniculatus (Also Known as Deer Mice).
> ### Aliases: deermice
> ### Keywords: datasets
> 
> ### ** Examples
> 
> head(deermice)
  y1 y2 y3 y4 y5 y6 sex age weight
1  1  1  1  1  1  1   0   y     12
2  1  0  0  1  1  1   1   y     15
3  1  1  0  0  1  1   0   y     15
4  1  1  0  1  1  1   0   y     15
5  1  1  1  1  1  1   0   y     13
6  1  1  0  1  1  1   0   a     21
> ## Not run: 
> ##D fit1 <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + age,
> ##D              posbernoulli.t(parallel.t = TRUE), deermice, trace = TRUE)
> ##D coef(fit1)
> ##D coef(fit1, matrix = TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("deplot.lmscreg")
> ### * deplot.lmscreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deplot.lmscreg
> ### Title: Density Plot for LMS Quantile Regression
> ### Aliases: deplot.lmscreg
> ### Keywords: hplot
> 
> ### ** Examples
> ## Not run: 
> ##D fit <- vgam(BMI ~ s(age, df = c(4, 2)), lms.bcn(zero = 1), bmi.nz)
> ##D ygrid <- seq(15, 43, by = 0.25)
> ##D deplot(fit, x0 = 20, y = ygrid, xlab = "BMI", col = "green", llwd = 2,
> ##D     main = "BMI distribution at ages 20 (green), 40 (blue), 60 (red)")
> ##D deplot(fit, x0 = 40, y = ygrid, add = TRUE, col = "blue", llwd = 2)
> ##D deplot(fit, x0 = 60, y = ygrid, add = TRUE, col = "red", llwd = 2) -> a
> ##D 
> ##D names(a@post$deplot)
> ##D a@post$deplot$newdata
> ##D head(a@post$deplot$y)
> ##D head(a@post$deplot$density)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("depvar")
> ### * depvar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: depvar
> ### Title: Response Variable Extracted
> ### Aliases: depvar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> fit@y        # Sample proportions (not recommended)
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> depvar(fit)  # Better than using fit@y
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> weights(fit, type = "prior")  # Number of observations
  [,1]
1   98
2   54
3   43
4   48
5   51
6   38
7   28
8   11
> 
> 
> 
> cleanEx()
> nameEx("df.residual")
> ### * df.residual
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: df.residual
> ### Title: Residual Degrees-of-Freedom
> ### Aliases: df.residual df.residual_vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> head(model.matrix(fit, type = "vlm"))
    (Intercept):1 (Intercept):2      let
1:1             1             0 1.757858
1:2             0             1 1.757858
2:1             1             0 2.708050
2:2             0             1 2.708050
3:1             1             0 3.068053
3:2             0             1 3.068053
> head(model.matrix(fit, type = "lm"))
  (Intercept)      let
1           1 1.757858
2           1 2.708050
3           1 3.068053
4           1 3.314186
5           1 3.511545
6           1 3.676301
> 
> df.residual(fit, type = "vlm")  # n * M - p_VLM
[1] 13
> nobs(fit, type = "vlm")  # n * M
[1] 16
> nvar(fit, type = "vlm")  # p_VLM
[1] 3
> 
> df.residual(fit, type = "lm")  # n - p_LM(j)
logitlink(P[Y>=2]) logitlink(P[Y>=3]) 
                 6                  6 
> nobs(fit, type = "lm")  # n
[1] 8
> nvar(fit, type = "lm")  # p_LM
[1] 2
> nvar_vlm(fit, type = "lm")  # p_LM(j) (<= p_LM elementwise)
logitlink(P[Y>=2]) logitlink(P[Y>=3]) 
                 2                  2 
> 
> 
> 
> cleanEx()
> nameEx("dgaitdplot")
> ### * dgaitdplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dgaitdplot
> ### Title: Plotting the GAITD Combo Density
> ### Aliases: dgaitdplot
> ### Keywords: dplot hplot distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D   i.mix <- seq(0, 25, by = 5)
> ##D mean.p <- 10; size.p <- 8
> ##D dgaitdplot(c(size.p, mean.p), fam = "nbinom", xlim = c(0, 25),
> ##D      a.mix = i.mix + 1, i.mix = i.mix, pobs.mix = 0.1,
> ##D      pstr.mix = 0.1, lwd.i = 2,lwd.p = 2, lwd.a = 2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("diffzeta")
> ### * diffzeta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diffzeta
> ### Title: Differenced Zeta Distribution Family Function
> ### Aliases: diffzeta
> ### Keywords: models regression
> 
> ### ** Examples
> 
> odata <- data.frame(x2 = runif(nn <- 1000))  # Artificial data
> odata <- transform(odata, shape = loglink(-0.25 + x2, inv = TRUE))
> odata <- transform(odata, y1 = rdiffzeta(nn, shape))
> with(odata, table(y1))
y1
  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20  21 
593 140  79  41  25  22  20   9   8   7   4   5   6   5   4   1   2   5   1   1 
 22  23  26  27  29  33  34  38  40  56  57  60  62  94  98 103 113 121 132 
  1   2   1   2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   1 
> ofit <- vglm(y1 ~ x2, diffzeta, odata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1644.2242
VGLM    linear loop  2 :  loglikelihood = -1641.7683
VGLM    linear loop  3 :  loglikelihood = -1641.7349
VGLM    linear loop  4 :  loglikelihood = -1641.7348
VGLM    linear loop  5 :  loglikelihood = -1641.7348
> coef(ofit, matrix = TRUE)
            loglink(shape)
(Intercept)     -0.1928807
x2               0.8699937
> 
> 
> 
> cleanEx()
> nameEx("diffzetaUC")
> ### * diffzetaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Diffzeta
> ### Title: Differenced Zeta Distribution
> ### Aliases: Diffzeta ddiffzeta pdiffzeta qdiffzeta rdiffzeta
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ddiffzeta(1:20, 0.5, start = 2)
 [1] 0.000000000 0.183503419 0.109389800 0.074651249 0.055105263 0.042827785
 [7] 0.034522484 0.028595479 0.024190925 0.020812163 0.018153142 0.016016020
[13] 0.014267797 0.012816101 0.011594981 0.010556220 0.009663837 0.008890491
[19] 0.008215076 0.007621066
> rdiffzeta(20, 0.5)
 [1]     1     2     5   118     1    96   326     8     7     1     1     1
[13]    10     2    18     3    12 15264     2    20
> 
> ## Not run: 
> ##D  shape <- 0.8; x <- 1:10
> ##D plot(x, ddiffzeta(x, sh = shape), type = "h", ylim = 0:1, las = 1,
> ##D      sub = "shape=0.8", col = "blue", ylab = "Probability",
> ##D      main = "Differenced zeta distribution: blue=PMF; orange=CDF")
> ##D lines(x + 0.1, pdiffzeta(x, shape = shape), col = "orange",
> ##D       lty = 3, type = "h") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("dirichlet")
> ### * dirichlet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dirichlet
> ### Title: Fitting a Dirichlet Distribution
> ### Aliases: dirichlet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ddata <- data.frame(rdiric(1000,
+                     shape = exp(c(y1 = -1, y2 = 1, y3 = 0))))
> fit <- vglm(cbind(y1, y2, y3)  ~ 1, dirichlet,
+             data = ddata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
-1.77277540,  0.25371824, -0.76449505
VGLM    linear loop  2 :  coefficients = 
-1.25874692,  0.76375635, -0.27683963
VGLM    linear loop  3 :  coefficients = 
-1.035191161,  0.989054092, -0.058538968
VGLM    linear loop  4 :  coefficients = 
-1.000397547,  1.024019556, -0.023943731
VGLM    linear loop  5 :  coefficients = 
-0.99966405,  1.02475476, -0.02320716
VGLM    linear loop  6 :  coefficients = 
-0.999663730,  1.024755078, -0.023206837
VGLM    linear loop  7 :  coefficients = 
-0.999663730,  1.024755078, -0.023206837
> Coef(fit)
   shape1    shape2    shape3 
0.3680032 2.7864129 0.9770604 
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2) loglink(shape3)
(Intercept)      -0.9996637        1.024755     -0.02320684
> head(fitted(fit))
          y1        y2        y3
1 0.08907304 0.6744351 0.2364918
2 0.08907304 0.6744351 0.2364918
3 0.08907304 0.6744351 0.2364918
4 0.08907304 0.6744351 0.2364918
5 0.08907304 0.6744351 0.2364918
6 0.08907304 0.6744351 0.2364918
> 
> 
> 
> cleanEx()
> nameEx("dirmul.old")
> ### * dirmul.old
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dirmul.old
> ### Title: Fitting a Dirichlet-Multinomial Distribution
> ### Aliases: dirmul.old
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Data from p.50 of Lange (2002)
> alleleCounts <- c(2, 84, 59, 41, 53, 131, 2, 0,
+        0, 50, 137, 78, 54, 51, 0, 0,
+        0, 80, 128, 26, 55, 95, 0, 0,
+        0, 16, 40, 8, 68, 14, 7, 1)
> dim(alleleCounts) <- c(8, 4)
> alleleCounts <- data.frame(t(alleleCounts))
> dimnames(alleleCounts) <- list(c("White","Black","Chicano","Asian"),
+                     paste("Allele", 5:12, sep = ""))
> 
> set.seed(123)  # @initialize uses random numbers
> fit <- vglm(cbind(Allele5,Allele6,Allele7,Allele8,Allele9,
+                   Allele10,Allele11,Allele12) ~ 1, dirmul.old,
+              trace = TRUE, crit = "c", data = alleleCounts)
VGLM    linear loop  1 :  coefficients = 
-3.8910869, -3.6350411, -3.6298790, -3.6403794, -3.6332751, -3.6337880, 
-3.7360239, -3.9009179
VGLM    linear loop  2 :  coefficients = 
-3.4223445, -2.6980446, -2.6795530, -2.7170128, -2.6917385, -2.6935653, 
-3.0178262, -3.4466109
VGLM    linear loop  3 :  coefficients = 
-3.1638955, -1.8107720, -1.7595230, -1.8622977, -1.7934316, -1.7984234, 
-2.5168153, -3.2009785
VGLM    linear loop  4 :  coefficients = 
-2.98347967, -0.98412783, -0.85974357, -1.10397597, -0.94276612, -0.95448708, 
-2.20846822, -3.02951199
VGLM    linear loop  5 :  coefficients = 
-2.80473181, -0.21810317,  0.03623887, -0.44935744, -0.13578181, -0.15814199, 
-1.97869438, -2.85943102
VGLM    linear loop  6 :  coefficients = 
-2.61021035,  0.48663962,  0.87825275,  0.13503442,  0.61101386,  0.58101766, 
-1.75021500, -2.67546526
VGLM    linear loop  7 :  coefficients = 
-2.41891431,  1.07169370,  1.52295278,  0.64663206,  1.21293124,  1.18749143, 
-1.52499339, -2.49639970
VGLM    linear loop  8 :  coefficients = 
-2.28369573,  1.41960674,  1.87829275,  0.97499879,  1.55889636,  1.54374568, 
-1.36193004, -2.37151446
VGLM    linear loop  9 :  coefficients = 
-2.2358175,  1.5253024,  1.9836330,  1.0793649,  1.6622051,  1.6517404, 
-1.3026311, -2.3279204
VGLM    linear loop  10 :  coefficients = 
-2.2314875,  1.5338999,  1.9921631,  1.0879989,  1.6705571,  1.6605351, 
-1.2971389, -2.3240277
VGLM    linear loop  11 :  coefficients = 
-2.2314584,  1.5339538,  1.9922165,  1.0880533,  1.6706093,  1.6605903, 
-1.2971013, -2.3240019
VGLM    linear loop  12 :  coefficients = 
-2.2314584,  1.5339538,  1.9922165,  1.0880533,  1.6706093,  1.6605903, 
-1.2971013, -2.3240019
> 
> (sfit <- summary(fit))
Call:
vglm(formula = cbind(Allele5, Allele6, Allele7, Allele8, Allele9, 
    Allele10, Allele11, Allele12) ~ 1, family = dirmul.old, data = alleleCounts, 
    trace = TRUE, crit = "c")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -2.2315     1.0060  -2.218 0.026546 *  
(Intercept):2   1.5340     0.4113   3.729 0.000192 ***
(Intercept):3   1.9922     0.3978   5.008 5.51e-07 ***
(Intercept):4   1.0881     0.4283   2.540 0.011079 *  
(Intercept):5   1.6706     0.4015   4.161 3.17e-05 ***
(Intercept):6   1.6606     0.4124   4.027 5.65e-05 ***
(Intercept):7  -1.2971     0.7089  -1.830 0.067284 .  
(Intercept):8  -2.3240     1.0090  -2.303 0.021264 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  8 

Names of linear predictors: loglink(shape1), loglink(shape2), loglink(shape3), 
loglink(shape4), loglink(shape5), loglink(shape6), loglink(shape7), 
loglink(shape8)

Log-likelihood: -47692.08 on 24 degrees of freedom

Number of Fisher scoring iterations: 12 

No Hauck-Donner effect found in any of the estimates

> vcov(sfit)
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4
(Intercept):1    1.01205315    0.04973858    0.05134910    0.04720810
(Intercept):2    0.04973858    0.16918837    0.11953160    0.10989208
(Intercept):3    0.05134910    0.11953160    0.15826836    0.11345035
(Intercept):4    0.04720810    0.10989208    0.11345035    0.18347177
(Intercept):5    0.04953016    0.11529742    0.11903071    0.10943159
(Intercept):6    0.05101272    0.11874857    0.12259362    0.11270716
(Intercept):7    0.02586997    0.06022070    0.06217063    0.05715694
(Intercept):8    0.01966148    0.04576844    0.04725041    0.04343995
              (Intercept):5 (Intercept):6 (Intercept):7 (Intercept):8
(Intercept):1    0.04953016    0.05101272    0.02586997    0.01966148
(Intercept):2    0.11529742    0.11874857    0.06022070    0.04576844
(Intercept):3    0.11903071    0.12259362    0.06217063    0.04725041
(Intercept):4    0.10943159    0.11270716    0.05715694    0.04343995
(Intercept):5    0.16120336    0.11825097    0.05996835    0.04557666
(Intercept):6    0.11825097    0.17004926    0.06176336    0.04694089
(Intercept):7    0.05996835    0.06176336    0.50251874    0.02380503
(Intercept):8    0.04557666    0.04694089    0.02380503    1.01809210
> round(eta2theta(coef(fit),
+                 fit@misc$link,
+                 fit@misc$earg), digits = 2)  # not preferred
      shape1 shape2 shape3 shape4 shape5 shape6 shape7 shape8
theta   0.11   4.64   7.33   2.97   5.32   5.26   0.27    0.1
> round(Coef(fit), digits = 2)  # preferred
shape1 shape2 shape3 shape4 shape5 shape6 shape7 shape8 
  0.11   4.64   7.33   2.97   5.32   5.26   0.27   0.10 
> round(t(fitted(fit)), digits = 4)  # 2nd row of Lange (2002, Table 3.5)
          White  Black Chicano  Asian
Allele5  0.0053 0.0003  0.0003 0.0006
Allele6  0.2227 0.1380  0.2064 0.1147
Allele7  0.1667 0.3645  0.3301 0.2630
Allele8  0.1105 0.2045  0.0707 0.0609
Allele9  0.1465 0.1498  0.1471 0.4073
Allele10 0.3424 0.1421  0.2445 0.1070
Allele11 0.0057 0.0007  0.0007 0.0404
Allele12 0.0002 0.0002  0.0002 0.0061
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2) loglink(shape3) loglink(shape4)
(Intercept)       -2.231458        1.533954        1.992217        1.088053
            loglink(shape5) loglink(shape6) loglink(shape7) loglink(shape8)
(Intercept)        1.670609         1.66059       -1.297101       -2.324002
> 
> 
> pfit <- vglm(cbind(Allele5,Allele6,Allele7,Allele8,Allele9,
+                    Allele10,Allele11,Allele12) ~ 1,
+              dirmul.old(parallel = TRUE), trace = TRUE,
+              data = alleleCounts)
VGLM    linear loop  1 :  loglikelihood = -45179.1
VGLM    linear loop  2 :  loglikelihood = -45237.27
Taking a modified step....................
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  iterations terminated because half-step sizes are very small
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
> round(eta2theta(coef(pfit, matrix = TRUE), pfit@misc$link,
+                 pfit@misc$earg), digits = 2)  # 'Right' answer
            shape1 shape2 shape3 shape4 shape5 shape6 shape7 shape8
(Intercept)   0.03   0.03   0.03   0.03   0.03   0.03   0.03   0.03
> round(Coef(pfit), digits = 2)  # 'Wrong' due to parallelism constraint
(Intercept) 
      -3.66 
> 
> 
> 
> cleanEx()
> nameEx("dirmultinomial")
> ### * dirmultinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dirmultinomial
> ### Title: Fitting a Dirichlet-Multinomial Distribution
> ### Aliases: dirmultinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 5; M <- 4; set.seed(1)
> ydata <- data.frame(round(matrix(runif(nn * M, max = 100), nn, M)))
> colnames(ydata) <- paste("y", 1:M, sep = "")  # Integer counts
> 
> fit <- vglm(cbind(y1, y2, y3, y4) ~ 1, dirmultinomial,
+             data = ydata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1428.784
VGLM    linear loop  2 :  loglikelihood = -1428.75
VGLM    linear loop  3 :  loglikelihood = -1428.744
VGLM    linear loop  4 :  loglikelihood = -1428.743
VGLM    linear loop  5 :  loglikelihood = -1428.742
VGLM    linear loop  6 :  loglikelihood = -1428.742
VGLM    linear loop  7 :  loglikelihood = -1428.742
> head(fitted(fit))
         y1        y2        y3        y4
1 0.2180462 0.2530048 0.2077817 0.3211673
2 0.2180462 0.2530048 0.2077817 0.3211673
3 0.2180462 0.2530048 0.2077817 0.3211673
4 0.2180462 0.2530048 0.2077817 0.3211673
5 0.2180462 0.2530048 0.2077817 0.3211673
> depvar(fit)  # Sample proportions
         y1         y2         y3        y4
1 0.1436170 0.47872340 0.11170213 0.2659574
2 0.1674208 0.42533937 0.08144796 0.3257919
3 0.1958763 0.22680412 0.23711340 0.3402062
4 0.3956522 0.27391304 0.16521739 0.1652174
5 0.1104972 0.03314917 0.42541436 0.4309392
> weights(fit, type = "prior", matrix = FALSE)  # Total counts per row
[1] 188 221 291 230 181
> 
> ## Not run: 
> ##D ydata <- transform(ydata, x2 = runif(nn))
> ##D fit <- vglm(cbind(y1, y2, y3, y4) ~ x2, dirmultinomial,
> ##D             data = ydata, trace = TRUE)
> ##D Coef(fit)
> ##D coef(fit, matrix = TRUE)
> ##D (sfit <- summary(fit))
> ##D vcov(sfit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("double.cens.normal")
> ### * double.cens.normal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: double.cens.normal
> ### Title: Univariate Normal Distribution with Double Censoring
> ### Aliases: double.cens.normal
> ### Keywords: models regression
> 
> ### ** Examples
> ## Not run: 
> ##D  # Repeat the simulations of Harter & Moore (1966)
> ##D SIMS <- 100  # Number of simulations (change this to 1000)
> ##D mu.save <- sd.save <- rep(NA, len = SIMS)
> ##D r1 <- 0; r2 <- 4; nn <- 20
> ##D for (sim in 1:SIMS) {
> ##D   y <- sort(rnorm(nn))
> ##D   y <- y[(1+r1):(nn-r2)]  # Delete r1 smallest and r2 largest
> ##D   fit <- vglm(y ~ 1, double.cens.normal(r1 = r1, r2 = r2))
> ##D   mu.save[sim] <- predict(fit)[1, 1]
> ##D   sd.save[sim] <- exp(predict(fit)[1, 2])  # Assumes a log link & ~ 1
> ##D }
> ##D c(mean(mu.save), mean(sd.save))  # Should be c(0,1)
> ##D c(sd(mu.save), sd(sd.save))
> ## End(Not run)
> 
> # Data from Sarhan & Greenberg (1962); MLEs are mu=9.2606, sd=1.3754
> strontium90 <- data.frame(y = c(8.2, 8.4, 9.1, 9.8, 9.9))
> fit <- vglm(y ~ 1, double.cens.normal(r1 = 2, r2 = 3, isd = 6),
+             data = strontium90, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -15.69275
VGLM    linear loop  2 :  loglikelihood = -14.25529
VGLM    linear loop  3 :  loglikelihood = -13.50897
VGLM    linear loop  4 :  loglikelihood = -13.32076
VGLM    linear loop  5 :  loglikelihood = -13.30763
VGLM    linear loop  6 :  loglikelihood = -13.3075
VGLM    linear loop  7 :  loglikelihood = -13.3075
VGLM    linear loop  8 :  loglikelihood = -13.3075
> coef(fit, matrix = TRUE)
                  mu loglink(sd)
(Intercept) 9.260564   0.3187177
> Coef(fit)
      mu       sd 
9.260564 1.375363 
> 
> 
> 
> cleanEx()
> nameEx("double.expbinomial")
> ### * double.expbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: double.expbinomial
> ### Title: Double Exponential Binomial Distribution Family Function
> ### Aliases: double.expbinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # This example mimics the example in Efron (1986).
> # The results here differ slightly.
> 
> # Scale the variables
> toxop <- transform(toxop,
+                    phat = positive / ssize,
+                    srainfall = scale(rainfall),  # (6.1)
+                    sN = scale(ssize))            # (6.2)
> 
> # A fit similar (should be identical) to Sec.6 of Efron (1986).
> # But does not use poly(), and M = 1.25 here, as in (5.3)
> cmlist <- list("(Intercept)"    = diag(2),
+                "I(srainfall)"   = rbind(1, 0),
+                "I(srainfall^2)" = rbind(1, 0),
+                "I(srainfall^3)" = rbind(1, 0),
+                "I(sN)" = rbind(0, 1),
+                "I(sN^2)" = rbind(0, 1))
> fit <-
+   vglm(cbind(phat, 1 - phat) * ssize ~
+        I(srainfall) + I(srainfall^2) + I(srainfall^3) +
+        I(sN) + I(sN^2),
+        double.expbinomial(ldisp = extlogitlink(min = 0, max = 1.25),
+                           idisp = 0.2, zero = NULL),
+        toxop, trace = TRUE, constraints = cmlist)
VGLM    linear loop  1 :  loglikelihood = -473.41023
VGLM    linear loop  2 :  loglikelihood = -472.665
VGLM    linear loop  3 :  loglikelihood = -472.00972
VGLM    linear loop  4 :  loglikelihood = -471.81556
VGLM    linear loop  5 :  loglikelihood = -471.6183
VGLM    linear loop  6 :  loglikelihood = -471.59167
VGLM    linear loop  7 :  loglikelihood = -471.57862
VGLM    linear loop  8 :  loglikelihood = -471.57585
VGLM    linear loop  9 :  loglikelihood = -471.57487
VGLM    linear loop  10 :  loglikelihood = -471.5746
VGLM    linear loop  11 :  loglikelihood = -471.57452
VGLM    linear loop  12 :  loglikelihood = -471.57449
VGLM    linear loop  13 :  loglikelihood = -471.57449
> 
> # Now look at the results
> coef(fit, matrix = TRUE)
               logitlink(mu) extlogitlink(dispersion, min = 0, max = 1.25)
(Intercept)       -0.0876428                                     0.6291300
I(srainfall)      -0.6867008                                     0.0000000
I(srainfall^2)    -0.1671289                                     0.0000000
I(srainfall^3)     0.2766119                                     0.0000000
I(sN)              0.0000000                                     0.4529937
I(sN^2)            0.0000000                                    -0.5863660
> head(fitted(fit))
          [,1]
[1,] 0.5406318
[2,] 0.4499732
[3,] 0.3884616
[4,] 0.4131199
[5,] 0.5483156
[6,] 0.5515432
> summary(fit)
Call:
vglm(formula = cbind(phat, 1 - phat) * ssize ~ I(srainfall) + 
    I(srainfall^2) + I(srainfall^3) + I(sN) + I(sN^2), family = double.expbinomial(ldisp = extlogitlink(min = 0, 
    max = 1.25), idisp = 0.2, zero = NULL), data = toxop, constraints = cmlist, 
    trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)   
(Intercept):1  -0.08764    0.14177  -0.618  0.53643   
(Intercept):2   0.62913    0.87260   0.721  0.47092   
I(srainfall)   -0.68670    0.23225  -2.957  0.00311 **
I(srainfall^2) -0.16713    0.10930  -1.529  0.12624   
I(srainfall^3)  0.27661    0.08644   3.200  0.00137 **
I(sN)           0.45299    1.03977   0.436  0.66308   
I(sN^2)        -0.58637    0.53422  -1.098  0.27238   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(mu), 
extlogitlink(dispersion, min = 0, max = 1.25)

Log-likelihood: -471.5745 on 61 degrees of freedom

Number of Fisher scoring iterations: 13 

No Hauck-Donner effect found in any of the estimates

> vcov(fit)
               (Intercept):1 (Intercept):2  I(srainfall) I(srainfall^2)
(Intercept):1   2.009784e-02 -8.858886e-20  1.575495e-03  -9.186354e-03
(Intercept):2  -8.858886e-20  7.614306e-01  1.074103e-18   3.624557e-19
I(srainfall)    1.575495e-03  1.074103e-18  5.394044e-02   8.584228e-03
I(srainfall^2) -9.186354e-03  3.624557e-19  8.584228e-03   1.194605e-02
I(srainfall^3)  1.440713e-03 -4.594651e-19 -1.746804e-02  -5.894585e-03
I(sN)          -8.607136e-20  6.334727e-01  1.043579e-18   3.521555e-19
I(sN^2)         5.931708e-20 -3.763901e-01 -7.191948e-19  -2.426920e-19
               I(srainfall^3)         I(sN)       I(sN^2)
(Intercept):1    1.440713e-03 -8.607136e-20  5.931708e-20
(Intercept):2   -4.594651e-19  6.334727e-01 -3.763901e-01
I(srainfall)    -1.746804e-02  1.043579e-18 -7.191948e-19
I(srainfall^2)  -5.894585e-03  3.521555e-19 -2.426920e-19
I(srainfall^3)   7.472242e-03 -4.464081e-19  3.076474e-19
I(sN)           -4.464081e-19  1.081116e+00 -5.229643e-01
I(sN^2)          3.076474e-19 -5.229643e-01  2.853935e-01
> sqrt(diag(vcov(fit)))  # Standard errors
 (Intercept):1  (Intercept):2   I(srainfall) I(srainfall^2) I(srainfall^3) 
    0.14176683     0.87259991     0.23225082     0.10929800     0.08644213 
         I(sN)        I(sN^2) 
    1.03976738     0.53422232 
> 
> # Effective sample size (not quite the last column of Table 1)
> head(predict(fit))
  logitlink(mu) extlogitlink(dispersion, min = 0, max = 1.25)
1     0.1628865                                  -0.008940524
2    -0.2007791                                   0.294623793
3    -0.4537834                                   0.047331202
4    -0.3510827                                   0.294623793
5     0.1938674                                  -0.128297183
6     0.2069076                                   0.047331202
> Dispersion <- extlogitlink(predict(fit)[,2], min = 0, max = 1.25,
+                            inverse = TRUE)
> c(round(weights(fit, type = "prior") * Dispersion, digits = 1))
 [1]  2.5  7.2  3.2  7.2  1.2  3.2  5.5 15.3  3.9  7.2 19.9  0.6 25.2 18.1  0.6
[16]  8.0  0.6 34.1  6.3 14.4  8.9  0.6  8.0 12.9 34.7 12.5  8.2  9.8 33.5 15.0
[31]  9.8  7.2  3.9 30.4
> 
> 
> # Ordinary logistic regression (gives same results as (6.5))
> ofit <- vglm(cbind(phat, 1 - phat) * ssize ~
+              I(srainfall) + I(srainfall^2) + I(srainfall^3),
+              binomialff, toxop, trace = TRUE)
VGLM    linear loop  1 :  deviance = 62.708824
VGLM    linear loop  2 :  deviance = 62.634603
VGLM    linear loop  3 :  deviance = 62.634602
> 
> 
> # Same as fit but it uses poly(), and can be plotted (cf. Fig.1)
> cmlist2 <- list("(Intercept)"                 = diag(2),
+                 "poly(srainfall, degree = 3)" = rbind(1, 0),
+                 "poly(sN, degree = 2)"        = rbind(0, 1))
> fit2 <-
+   vglm(cbind(phat, 1 - phat) * ssize ~
+        poly(srainfall, degree = 3) + poly(sN, degree = 2),
+        double.expbinomial(ldisp = extlogitlink(min = 0, max = 1.25),
+                           idisp = 0.2, zero = NULL),
+        toxop, trace = TRUE, constraints = cmlist2)
VGLM    linear loop  1 :  loglikelihood = -473.41023
VGLM    linear loop  2 :  loglikelihood = -472.665
VGLM    linear loop  3 :  loglikelihood = -472.00972
VGLM    linear loop  4 :  loglikelihood = -471.81556
VGLM    linear loop  5 :  loglikelihood = -471.6183
VGLM    linear loop  6 :  loglikelihood = -471.59167
VGLM    linear loop  7 :  loglikelihood = -471.57862
VGLM    linear loop  8 :  loglikelihood = -471.57585
VGLM    linear loop  9 :  loglikelihood = -471.57487
VGLM    linear loop  10 :  loglikelihood = -471.5746
VGLM    linear loop  11 :  loglikelihood = -471.57452
VGLM    linear loop  12 :  loglikelihood = -471.57449
VGLM    linear loop  13 :  loglikelihood = -471.57449
> ## Not run: 
> ##D  par(mfrow = c(1, 2))  # Cf. Fig.1
> ##D plot(as(fit2, "vgam"), se = TRUE, lcol = "blue", scol = "orange")
> ##D 
> ##D # Cf. Figure 1(a)
> ##D par(mfrow = c(1,2))
> ##D ooo <- with(toxop, sort.list(rainfall))
> ##D with(toxop, plot(rainfall[ooo], fitted(fit2)[ooo], type = "l",
> ##D                  col = "blue", las = 1, ylim = c(0.3, 0.65)))
> ##D with(toxop, points(rainfall[ooo], fitted(ofit)[ooo],
> ##D                    col = "orange", type = "b", pch = 19))
> ##D 
> ##D # Cf. Figure 1(b)
> ##D ooo <- with(toxop, sort.list(ssize))
> ##D with(toxop, plot(ssize[ooo], Dispersion[ooo], type = "l",
> ##D                  col = "blue", las = 1, xlim = c(0, 100))) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("drop1")
> ### * drop1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: add1.vglm
> ### Title: Add or Drop All Possible Single Terms to/from a Model
> ### Aliases: add1.vglm drop1.vglm
> ### Keywords: models
> 
> ### ** Examples
> 
> data("backPain2", package = "VGAM")
> summary(backPain2)
 x2     x3     x4                       pain   
 1:39   1:21   1:64   worse               : 5  
 2:62   2:52   2:37   same                :14  
        3:28          slight.improvement  :18  
                      moderate.improvement:20  
                      marked.improvement  :28  
                      complete.relief     :16  
> fit1 <- vglm(pain ~ x2 + x3 + x4, propodds, data = backPain2)
> coef(fit1)
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5 
   5.41024201    3.83654247    2.83868976    1.85978224    0.09680069 
          x22           x32           x33           x42 
  -1.46570383   -1.03178249   -1.10212111   -0.92407971 
> add1(fit1, scope = ~ x2 * x3 * x4, test = "LRT")
Single term additions

Model:
pain ~ x2 + x3 + x4
       Df Deviance    AIC     LRT Pr(>Chi)
<none>      316.40 334.40                 
x2:x3   2   313.24 335.24 3.16009   0.2060
x2:x4   1   316.28 336.28 0.12455   0.7242
x3:x4   2   316.19 338.19 0.21529   0.8979
> drop1(fit1, test = "LRT")
Single term deletions

Model:
pain ~ x2 + x3 + x4
       Df Deviance    AIC     LRT  Pr(>Chi)    
<none>      316.40 334.40                      
x2      1   330.48 346.48 14.0793 0.0001753 ***
x3      2   321.53 335.53  5.1257 0.0770836 .  
x4      1   322.58 338.58  6.1761 0.0129486 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> fit2 <- vglm(pain ~ x2 * x3 * x4, propodds, data = backPain2)
> drop1(fit2)
Single term deletions

Model:
pain ~ x2 * x3 * x4
         Df Deviance    AIC
<none>        311.15 343.15
x2:x3:x4  2   312.44 340.44
> 
> 
> 
> cleanEx()
> nameEx("ducklings")
> ### * ducklings
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ducklings
> ### Title: Relative Frequencies of Serum Proteins in White Pekin Ducklings
> ### Aliases: ducklings
> ### Keywords: datasets
> 
> ### ** Examples
> 
> print(ducklings)
       p1    p2     p3
1  0.1780 0.346 0.4760
2  0.1620 0.307 0.5310
3  0.0830 0.448 0.4690
4  0.0870 0.474 0.4390
5  0.0780 0.503 0.4190
6  0.0400 0.456 0.5040
7  0.0490 0.363 0.5880
8  0.1000 0.317 0.5830
9  0.0750 0.394 0.5310
10 0.0840 0.445 0.4710
11 0.0600 0.435 0.5050
12 0.0890 0.418 0.4930
13 0.0500 0.485 0.4650
14 0.0730 0.378 0.5490
15 0.0640 0.562 0.3740
16 0.0850 0.465 0.4500
17 0.0940 0.388 0.5180
18 0.0140 0.449 0.5370
19 0.0600 0.544 0.3960
20 0.0310 0.569 0.4000
21 0.0250 0.491 0.4840
22 0.0450 0.613 0.3420
23 0.0195 0.526 0.4545
> 
> 
> 
> cleanEx()
> nameEx("eCDF")
> ### * eCDF
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: eCDF
> ### Title: Empirical Cumulative Distribution Function
> ### Aliases: eCDF eCDF.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fit1 <- vglm(BMI ~ ns(age, 4), extlogF1, data = bmi.nz)  # trace = TRUE
> eCDF(fit1)
(tau = 0.25)  (tau = 0.5) (tau = 0.75) 
   0.2500000    0.5042857    0.7500000 
> eCDF(fit1, all = TRUE)
       ecdf  tau
  0.2500000 0.25
  0.5042857 0.50
  0.7500000 0.75
> 
> 
> 
> cleanEx()
> nameEx("eexpUC")
> ### * eexpUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Expectiles-Exponential
> ### Title: Expectiles of the Exponential Distribution
> ### Aliases: Expectiles-Exponential eexp deexp peexp qeexp reexp
> ### Keywords: distribution
> 
> ### ** Examples
> 
> my.p <- 0.25; y <- rexp(nn <- 1000)
> (myexp <- qeexp(my.p))
[1] 0.6530184
> sum(myexp - y[y <= myexp]) / sum(abs(myexp - y))  # Should be my.p
[1] 0.2281489
> 
> ## Not run: 
> ##D  par(mfrow = c(2,1))
> ##D yy <- seq(-0, 4, len = nn)
> ##D plot(yy, deexp(yy),  col = "blue", ylim = 0:1, xlab = "y", ylab = "g(y)",
> ##D      type = "l", main = "g(y) for Exp(1); dotted green is f(y) = dexp(y)")
> ##D lines(yy, dexp(yy), col = "green", lty = "dotted", lwd = 2)  # 'original'
> ##D 
> ##D plot(yy, peexp(yy), type = "l", col = "blue", ylim = 0:1,
> ##D      xlab = "y", ylab = "G(y)", main = "G(y) for Exp(1)")
> ##D abline(v = 1, h = 0.5, col = "red", lty = "dashed")
> ##D lines(yy, pexp(yy), col = "green", lty = "dotted", lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("enormUC")
> ### * enormUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Expectiles-Normal
> ### Title: Expectiles of the Normal Distribution
> ### Aliases: Expectiles-Normal enorm denorm penorm qenorm renorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> my.p <- 0.25; y <- rnorm(nn <- 1000)
> (myexp <- qenorm(my.p))
[1] -0.4363266
> sum(myexp - y[y <= myexp]) / sum(abs(myexp - y))  # Should be my.p
[1] 0.2615782
> 
> # Non-standard normal
> mymean <- 1; mysd <- 2
> yy <- rnorm(nn, mymean, mysd)
> (myexp <- qenorm(my.p, mymean, mysd))
[1] 0.1273469
> sum(myexp - yy[yy <= myexp]) / sum(abs(myexp - yy))  # Should be my.p
[1] 0.2646361
> penorm(-Inf, mymean, mysd)      #  Should be 0
[1] 0
> penorm( Inf, mymean, mysd)      #  Should be 1
[1] 1
> penorm(mean(yy), mymean, mysd)  #  Should be 0.5
[1] 0.4898107
> abs(qenorm(0.5, mymean, mysd) - mean(yy))  #  Should be 0
[1] 0.03252382
> abs(penorm(myexp, mymean, mysd) - my.p)    #  Should be 0
[1] 2.775558e-17
> integrate(f = denorm, lower = -Inf, upper = Inf,
+           mymean, mysd)  #  Should be 1
1 with absolute error < 0.00011
> 
> ## Not run: 
> ##D par(mfrow = c(2, 1))
> ##D yy <- seq(-3, 3, len = nn)
> ##D plot(yy, denorm(yy), type = "l", col="blue", xlab = "y", ylab = "g(y)",
> ##D      main = "g(y) for N(0,1); dotted green is f(y) = dnorm(y)")
> ##D lines(yy, dnorm(yy), col = "green", lty = "dotted", lwd = 2)  # 'original'
> ##D 
> ##D plot(yy, penorm(yy), type = "l", col = "blue", ylim = 0:1,
> ##D      xlab = "y", ylab = "G(y)", main = "G(y) for N(0,1)")
> ##D abline(v = 0, h = 0.5, col = "red", lty = "dashed")
> ##D lines(yy, pnorm(yy), col = "green", lty = "dotted", lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("enzyme")
> ### * enzyme
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: enzyme
> ### Title: Enzyme Data
> ### Aliases: enzyme
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D fit <- vglm(velocity ~ 1, micmen, data = enzyme, trace = TRUE,
> ##D             form2 = ~ conc - 1, crit = "crit")
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("erf")
> ### * erf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: erf
> ### Title: Error Function, and variants
> ### Aliases: erf erfc
> ### Keywords: math
> 
> ### ** Examples
> 
> ## Not run: 
> ##D curve(erf,   -3, 3, col = "orange", ylab = "", las = 1)
> ##D curve(pnorm, -3, 3, add = TRUE, col = "blue", lty = "dotted", lwd = 2)
> ##D abline(v = 0, h = 0, lty = "dashed")
> ##D legend("topleft", c("erf(x)", "pnorm(x)"), col = c("orange", "blue"),
> ##D        lty = c("solid", "dotted"), lwd = 1:2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("erlang")
> ### * erlang
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: erlang
> ### Title: Erlang Distribution
> ### Aliases: erlang
> ### Keywords: models regression
> 
> ### ** Examples
> 
> rate <- exp(2); myshape <- 3
> edata <- data.frame(y = rep(0, nn <- 1000))
> for (ii in 1:myshape)
+   edata <- transform(edata, y = y + rexp(nn, rate = rate))
> fit <- vglm(y ~ 1, erlang(shape = myshape), edata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 53.435763
VGLM    linear loop  2 :  loglikelihood = 106.28906
VGLM    linear loop  3 :  loglikelihood = 106.76014
VGLM    linear loop  4 :  loglikelihood = 106.76018
VGLM    linear loop  5 :  loglikelihood = 106.76018
> coef(fit, matrix = TRUE)
            loglink(scale)
(Intercept)      -1.968288
> Coef(fit)  # Answer = 1/rate
    scale 
0.1396958 
> 1/rate
[1] 0.1353353
> summary(fit)
Call:
vglm(formula = y ~ 1, family = erlang(shape = myshape), data = edata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.96829    0.01826  -107.8   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(scale) 

Log-likelihood: 106.7602 on 999 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("eunifUC")
> ### * eunifUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Expectiles-Uniform
> ### Title: Expectiles of the Uniform Distribution
> ### Aliases: Expectiles-Uniform eunif deunif peunif qeunif reunif
> ### Keywords: distribution
> 
> ### ** Examples
> 
> my.p <- 0.25; y <- runif(nn <- 1000)
> (myexp <- qeunif(my.p))
[1] 0.3660254
> sum(myexp - y[y <= myexp]) / sum(abs(myexp - y))  # Should be my.p
[1] 0.2473423
> # Equivalently:
> I1 <- mean(y <= myexp) * mean( myexp - y[y <= myexp])
> I2 <- mean(y >  myexp) * mean(-myexp + y[y >  myexp])
> I1 / (I1 + I2)  # Should be my.p
[1] 0.2473423
> # Or:
> I1 <- sum( myexp - y[y <= myexp])
> I2 <- sum(-myexp + y[y >  myexp])
> 
> # Non-standard uniform
> mymin <- 1; mymax <- 8
> yy <- runif(nn, mymin, mymax)
> (myexp <- qeunif(my.p, mymin, mymax))
[1] 3.562178
> sum(myexp - yy[yy <= myexp]) / sum(abs(myexp - yy))  # Should be my.p
[1] 0.270428
> peunif(mymin, mymin, mymax)     #  Should be 0
[1] 0
> peunif(mymax, mymin, mymax)     #  Should be 1
[1] 1
> peunif(mean(yy), mymin, mymax)  #  Should be 0.5
[1] 0.4805996
> abs(qeunif(0.5, mymin, mymax) - mean(yy))  #  Should be 0
[1] 0.06792681
> abs(qeunif(0.5, mymin, mymax) - (mymin+mymax)/2)  #  Should be 0
[1] 0
> abs(peunif(myexp, mymin, mymax) - my.p)  #  Should be 0
[1] 2.220446e-16
> integrate(f = deunif, lower = mymin - 3, upper = mymax + 3,
+           min = mymin, max = mymax)  # Should be 1
1.000003 with absolute error < 1e-04
> 
> ## Not run: 
> ##D par(mfrow = c(2,1))
> ##D yy <- seq(0.0, 1.0, len = nn)
> ##D plot(yy, deunif(yy), type = "l", col = "blue", ylim = c(0, 2),
> ##D      xlab = "y", ylab = "g(y)", main = "g(y) for Uniform(0,1)")
> ##D lines(yy, dunif(yy), col = "green", lty = "dotted", lwd = 2)  # 'original'
> ##D 
> ##D plot(yy, peunif(yy), type = "l", col = "blue", ylim = 0:1,
> ##D      xlab = "y", ylab = "G(y)", main = "G(y) for Uniform(0,1)")
> ##D abline(a = 0.0, b = 1.0, col = "green", lty = "dotted", lwd = 2)
> ##D abline(v = 0.5, h = 0.5, col = "red", lty = "dashed") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("expexpff")
> ### * expexpff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expexpff
> ### Title: Exponentiated Exponential Distribution
> ### Aliases: expexpff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # A special case: exponential data
> ##D edata <- data.frame(y = rexp(n <- 1000))
> ##D fit <- vglm(y ~ 1, fam = expexpff, data = edata, trace = TRUE, maxit = 99)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D 
> ##D 
> ##D # Ball bearings data (number of million revolutions before failure)
> ##D edata <- data.frame(bbearings = c(17.88, 28.92, 33.00, 41.52, 42.12, 45.60,
> ##D 48.80, 51.84, 51.96, 54.12, 55.56, 67.80, 68.64, 68.64,
> ##D 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92,
> ##D 128.04, 173.40))
> ##D fit <- vglm(bbearings ~ 1, fam = expexpff(irate = 0.05, ish = 5),
> ##D             trace = TRUE, maxit = 300, data = edata)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)    # Authors get c(rate=0.0314, shape=5.2589)
> ##D logLik(fit)  # Authors get -112.9763
> ##D 
> ##D 
> ##D # Failure times of the airconditioning system of an airplane
> ##D eedata <- data.frame(acplane = c(23, 261, 87, 7, 120, 14, 62, 47,
> ##D 225, 71, 246, 21, 42, 20, 5, 12, 120, 11, 3, 14,
> ##D 71, 11, 14, 11, 16, 90, 1, 16, 52, 95))
> ##D fit <- vglm(acplane ~ 1, fam = expexpff(ishape = 0.8, irate = 0.15),
> ##D             trace = TRUE, maxit = 99, data = eedata)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)    # Authors get c(rate=0.0145, shape=0.8130)
> ##D logLik(fit)  # Authors get log-lik -152.264
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("expexpff1")
> ### * expexpff1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expexpff1
> ### Title: Exponentiated Exponential Distribution
> ### Aliases: expexpff1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Ball bearings data (number of million revolutions before failure)
> edata <- data.frame(bbearings = c(17.88, 28.92, 33.00, 41.52, 42.12, 45.60,
+ 48.80, 51.84, 51.96, 54.12, 55.56, 67.80, 68.64, 68.64,
+ 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92,
+ 128.04, 173.40))
> fit <- vglm(bbearings ~ 1, expexpff1(ishape = 4), trace = TRUE,
+             maxit = 250, checkwz = FALSE, data = edata)
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  1 :  loglikelihood = -142.86097
Applying Greenstadt modification to 19 matrices
VGLM    linear loop  2 :  loglikelihood = -138.55752
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  3 :  loglikelihood = -134.61146
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  4 :  loglikelihood = -131.02887
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  5 :  loglikelihood = -127.81272
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  6 :  loglikelihood = -124.96291
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  7 :  loglikelihood = -122.47572
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  8 :  loglikelihood = -120.34414
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  9 :  loglikelihood = -118.55463
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  10 :  loglikelihood = -117.08958
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  11 :  loglikelihood = -115.9242
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  12 :  loglikelihood = -115.02504
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  13 :  loglikelihood = -114.35458
Applying Greenstadt modification to 19 matrices
VGLM    linear loop  14 :  loglikelihood = -113.87332
Applying Greenstadt modification to 20 matrices
VGLM    linear loop  15 :  loglikelihood = -113.5415
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  16 :  loglikelihood = -113.32227
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  17 :  loglikelihood = -113.18287
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  18 :  loglikelihood = -113.09699
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  19 :  loglikelihood = -113.04553
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  20 :  loglikelihood = -113.01541
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  21 :  loglikelihood = -112.99811
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  22 :  loglikelihood = -112.98834
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  23 :  loglikelihood = -112.98288
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  24 :  loglikelihood = -112.97986
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  25 :  loglikelihood = -112.9782
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  26 :  loglikelihood = -112.9773
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  27 :  loglikelihood = -112.9768
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  28 :  loglikelihood = -112.97654
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  29 :  loglikelihood = -112.97639
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  30 :  loglikelihood = -112.97631
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  31 :  loglikelihood = -112.97627
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  32 :  loglikelihood = -112.97625
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  33 :  loglikelihood = -112.97624
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  34 :  loglikelihood = -112.97623
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  35 :  loglikelihood = -112.97623
Applying Greenstadt modification to 23 matrices
VGLM    linear loop  36 :  loglikelihood = -112.97622
> coef(fit, matrix = TRUE)
            loglink(rate)
(Intercept)      -3.43239
> Coef(fit)  # Authors get c(0.0314, 5.2589) with log-lik -112.9763
      rate 
0.03230964 
> logLik(fit)
[1] -112.9762
> fit@misc$shape  # Estimate of shape
[1] 5.288181
> 
> 
> # Failure times of the airconditioning system of an airplane
> eedata <- data.frame(acplane = c(23, 261, 87, 7, 120, 14, 62, 47,
+ 225, 71, 246, 21, 42, 20, 5, 12, 120, 11, 3, 14,
+ 71, 11, 14, 11, 16, 90, 1, 16, 52, 95))
> fit <- vglm(acplane ~ 1, expexpff1(ishape = 0.8), trace = TRUE,
+             maxit = 50, checkwz = FALSE, data = eedata)
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  1 :  loglikelihood = -229.62305
Applying Greenstadt modification to 29 matrices
VGLM    linear loop  2 :  loglikelihood = -223.12728
Applying Greenstadt modification to 29 matrices
VGLM    linear loop  3 :  loglikelihood = -216.82414
Applying Greenstadt modification to 29 matrices
VGLM    linear loop  4 :  loglikelihood = -210.72536
Applying Greenstadt modification to 29 matrices
VGLM    linear loop  5 :  loglikelihood = -204.84383
Applying Greenstadt modification to 29 matrices
VGLM    linear loop  6 :  loglikelihood = -199.19363
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  7 :  loglikelihood = -193.79011
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  8 :  loglikelihood = -188.64997
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  9 :  loglikelihood = -183.79108
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  10 :  loglikelihood = -179.2324
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  11 :  loglikelihood = -174.99364
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  12 :  loglikelihood = -171.0948
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  13 :  loglikelihood = -167.55531
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  14 :  loglikelihood = -164.39285
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  15 :  loglikelihood = -161.62175
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  16 :  loglikelihood = -159.25086
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  17 :  loglikelihood = -157.28105
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  18 :  loglikelihood = -155.70247
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  19 :  loglikelihood = -154.4922
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  20 :  loglikelihood = -153.61294
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  21 :  loglikelihood = -153.01404
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  22 :  loglikelihood = -152.6357
Applying Greenstadt modification to 30 matrices
VGLM    linear loop  23 :  loglikelihood = -152.41607
Applying Greenstadt modification to 27 matrices
VGLM    linear loop  24 :  loglikelihood = -152.29971
Applying Greenstadt modification to 20 matrices
VGLM    linear loop  25 :  loglikelihood = -152.24343
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  26 :  loglikelihood = -152.21827
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  27 :  loglikelihood = -152.20765
Applying Greenstadt modification to 18 matrices
VGLM    linear loop  28 :  loglikelihood = -152.20337
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  29 :  loglikelihood = -152.20168
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  30 :  loglikelihood = -152.20104
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  31 :  loglikelihood = -152.20079
Applying Greenstadt modification to 17 matrices
VGLM    linear loop  32 :  loglikelihood = -152.2007
Applying Greenstadt modification to 16 matrices
VGLM    linear loop  33 :  loglikelihood = -152.20066
Applying Greenstadt modification to 16 matrices
VGLM    linear loop  34 :  loglikelihood = -152.20065
Applying Greenstadt modification to 16 matrices
VGLM    linear loop  35 :  loglikelihood = -152.20065
Applying Greenstadt modification to 16 matrices
VGLM    linear loop  36 :  loglikelihood = -152.20064
> coef(fit, matrix = TRUE)
            loglink(rate)
(Intercept)     -4.230272
> Coef(fit)  # Authors get c(0.0145, 0.8130) with log-lik -152.264
      rate 
0.01454843 
> logLik(fit)
[1] -152.2006
> fit@misc$shape  # Estimate of shape
[1] 0.8095687
> 
> 
> 
> cleanEx()
> nameEx("expgeometric")
> ### * expgeometric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expgeometric
> ### Title: Exponential Geometric Distribution Family Function
> ### Aliases: expgeometric
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D Scale <- exp(2); shape = logitlink(-1, inverse = TRUE);
> ##D edata <- data.frame(y = rexpgeom(n = 2000, scale = Scale, shape = shape))
> ##D fit <- vglm(y ~ 1, expgeometric, edata, trace = TRUE)
> ##D c(with(edata, mean(y)), head(fitted(fit), 1))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("expgeometricUC")
> ### * expgeometricUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expgeom
> ### Title: The Exponential Geometric Distribution
> ### Aliases: expgeom dexpgeom pexpgeom qexpgeom rexpgeom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape <- 0.5; scale <- 1; nn <- 501
> ##D x <- seq(-0.10, 3.0, len = nn)
> ##D plot(x, dexpgeom(x, scale, shape), type = "l", las = 1, ylim = c(0, 2),
> ##D      ylab = paste("[dp]expgeom(shape = ", shape, ", scale = ", scale, ")"),
> ##D      col = "blue", cex.main = 0.8,
> ##D      main = "Blue is density, red is cumulative distribution function",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D lines(x, pexpgeom(x, scale, shape), col = "red")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qexpgeom(probs, scale, shape)
> ##D lines(Q, dexpgeom(Q, scale, shape), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pexpgeom(Q, scale, shape), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pexpgeom(Q, scale, shape) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("expint3")
> ### * expint3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expint
> ### Title: The Exponential Integral and Variants
> ### Aliases: expint expexpint expint.E1
> ### Keywords: math
> 
> ### ** Examples
>  ## Not run: 
> ##D par(mfrow = c(2, 2))
> ##D curve(expint, 0.01, 2, xlim = c(0, 2), ylim = c(-3, 5),
> ##D       las = 1, col = "orange")
> ##D abline(v = (-3):5, h = (-4):5, lwd = 2, lty = "dotted", col = "gray")
> ##D abline(h = 0, v = 0, lty = "dashed", col = "blue")
> ##D 
> ##D curve(expexpint, 0.01, 2, xlim = c(0, 2), ylim = c(-3, 2),
> ##D       las = 1, col = "orange")
> ##D abline(v = (-3):2, h = (-4):5, lwd = 2, lty = "dotted", col = "gray")
> ##D abline(h = 0, v = 0, lty = "dashed", col = "blue")
> ##D 
> ##D curve(expint.E1, 0.01, 2, xlim = c(0, 2), ylim = c(0, 5),
> ##D       las = 1, col = "orange")
> ##D abline(v = (-3):2, h = (-4):5, lwd = 2, lty = "dotted", col = "gray")
> ##D abline(h = 0, v = 0, lty = "dashed", col = "blue")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("explink")
> ### * explink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: explink
> ### Title: Exponential Link Function
> ### Aliases: explink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> theta <- rnorm(30)
> explink(theta)
 [1] 0.5344838 1.2015872 0.4336018 4.9297132 1.3902836 0.4402254 1.6281250
 [8] 2.0924271 1.7785196 0.7368371 4.5348008 1.4767493 0.5372775 0.1091863
[15] 3.0800041 0.9560610 0.9839401 2.5698209 2.2732743 1.8110401 2.5067256
[22] 2.1861375 1.0774154 0.1367841 1.8586041 0.9454174 0.8557342 0.2297526
[29] 0.6199292 1.5188319
> max(abs(explink(explink(theta), inverse = TRUE) - theta))  # 0?
[1] 1.110223e-16
> 
> 
> 
> cleanEx()
> nameEx("explogUC")
> ### * explogUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: explog
> ### Title: The Exponential Logarithmic Distribution
> ### Aliases: explog dexplog pexplog qexplog rexplog
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape <- 0.5; scale <- 2; nn <- 501
> ##D x <- seq(-0.50, 6.0, len = nn)
> ##D plot(x, dexplog(x, scale, shape), type = "l", las = 1, ylim = c(0, 1.1),
> ##D      ylab = paste("[dp]explog(shape = ", shape, ", scale = ", scale, ")"),
> ##D      col = "blue", cex.main = 0.8,
> ##D      main = "Blue is density, orange is cumulative distribution function",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D lines(x, pexplog(x, scale, shape), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qexplog(probs, scale, shape = shape)
> ##D lines(Q, dexplog(Q, scale, shape = shape), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pexplog(Q, scale, shape = shape), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pexplog(Q, scale, shape = shape) - probs)) # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("explogff")
> ### * explogff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: explogff
> ### Title: Exponential Logarithmic Distribution Family Function
> ### Aliases: explogff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  Scale <- exp(2); shape <- logitlink(-1, inverse = TRUE)
> ##D edata <- data.frame(y = rexplog(n = 2000, scale = Scale, shape = shape))
> ##D fit <- vglm(y ~ 1, explogff, data = edata, trace = TRUE)
> ##D c(with(edata, median(y)), head(fitted(fit), 1))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("exponential")
> ### * exponential
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: exponential
> ### Title: Exponential Distribution
> ### Aliases: exponential
> ### Keywords: models regression
> 
> ### ** Examples
> 
> edata <- data.frame(x2 = runif(nn <- 100) - 0.5)
> edata <- transform(edata, x3 = runif(nn) - 0.5)
> edata <- transform(edata, eta = 0.2 - 0.7 * x2 + 1.9 * x3)
> edata <- transform(edata, rate = exp(eta))
> edata <- transform(edata, y = rexp(nn, rate = rate))
> with(edata, stem(y))

  The decimal point is at the |

  0 | 000000111111111111122222222333333334444444444444555566667789999
  1 | 00011111222334555566679
  2 | 0001346678
  3 | 0
  4 | 3
  5 | 0
  6 | 
  7 | 
  8 | 0

> 
> fit.slow <- vglm(y ~ x2 + x3, exponential, data = edata, trace = TRUE)
VGLM    linear loop  1 :  deviance = 91.133506
VGLM    linear loop  2 :  deviance = 85.620092
VGLM    linear loop  3 :  deviance = 85.459653
VGLM    linear loop  4 :  deviance = 85.458748
VGLM    linear loop  5 :  deviance = 85.458739
VGLM    linear loop  6 :  deviance = 85.458739
> fit.fast <- vglm(y ~ x2 + x3, exponential(exp = FALSE), data = edata,
+                  trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
 0.63209278, -1.17449602,  3.17559323
VGLM    linear loop  2 :  coefficients = 
 0.35187414, -1.09437737,  2.64491625
VGLM    linear loop  3 :  coefficients = 
 0.28775344, -1.08130323,  2.43449332
VGLM    linear loop  4 :  coefficients = 
 0.28389156, -1.08243112,  2.41914493
VGLM    linear loop  5 :  coefficients = 
 0.28387609, -1.08244560,  2.41907985
VGLM    linear loop  6 :  coefficients = 
 0.28387609, -1.08244560,  2.41907985
> coef(fit.slow, mat = TRUE)
            loglink(rate)
(Intercept)     0.2838764
x2             -1.0824548
x3              2.4190716
> summary(fit.slow)
Call:
vglm(formula = y ~ x2 + x3, family = exponential, data = edata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   0.2839     0.1004   2.827  0.00470 ** 
x2           -1.0825     0.3757  -2.882  0.00396 ** 
x3            2.4191     0.3698   6.542 6.09e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(rate) 

Residual deviance: 85.4587 on 97 degrees of freedom

Log-likelihood: -69.3177 on 97 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> # Compare results with a GPD. Has a threshold.
> threshold <- 0.5
> gdata <- data.frame(y1 = threshold + rexp(n = 3000, rate = exp(1.5)))
> 
> fit.exp <- vglm(y1 ~ 1, exponential(location = threshold), data = gdata)
> coef(fit.exp, matrix = TRUE)
            loglink(rate)
(Intercept)      1.476264
> Coef(fit.exp)
    rate 
4.376564 
> logLik(fit.exp)
[1] 1428.792
> 
> fit.gpd <- vglm(y1 ~ 1, gpd(threshold =  threshold), data = gdata)
> coef(fit.gpd, matrix = TRUE)
            loglink(scale) logofflink(shape, offset = 0.5)
(Intercept)      -1.486959                      -0.6720914
> Coef(fit.gpd)
     scale      shape 
0.22605898 0.01063953 
> logLik(fit.gpd)
[1] 1428.959
> 
> 
> 
> cleanEx()
> nameEx("exppoisson")
> ### * exppoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: exppoisson
> ### Title: Exponential Poisson Distribution Family Function
> ### Aliases: exppoisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape <- exp(1); rate <- exp(2)
> ##D rdata <- data.frame(y = rexppois(n = 1000, rate = rate, shape = shape))
> ##D library("hypergeo")  # Required!
> ##D fit <- vglm(y ~ 1, exppoisson, data = rdata, trace = FALSE, maxit = 1200)
> ##D c(with(rdata, median(y)), head(fitted(fit), 1))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("exppoissonUC")
> ### * exppoissonUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: exppois
> ### Title: The Exponential Poisson Distribution
> ### Aliases: exppois dexppois pexppois qexppois rexppois
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  rate <- 2; shape <- 0.5; nn <- 201
> ##D x <- seq(-0.05, 1.05, len = nn)
> ##D plot(x, dexppois(x, rate = rate, shape), type = "l", las = 1, ylim = c(0, 3),
> ##D      ylab = paste("fexppoisson(rate = ", rate, ", shape = ", shape, ")"),
> ##D      col = "blue", cex.main = 0.8,
> ##D      main = "Blue is the density, orange the cumulative distribution function",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D lines(x, pexppois(x, rate = rate, shape), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qexppois(probs, rate = rate, shape)
> ##D lines(Q, dexppois(Q, rate = rate, shape), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pexppois(Q, rate = rate, shape), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3); abline(h = 0, col = "gray50")
> ##D max(abs(pexppois(Q, rate = rate, shape) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("extbetabinomUC")
> ### * extbetabinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Extbetabinom
> ### Title: The Beta-Binomial Distribution
> ### Aliases: Extbetabinom dextbetabinom pextbetabinom qextbetabinom
> ###   rextbetabinom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> set.seed(1); rextbetabinom(10, 100, 0.5)
 [1] 52 46 60 49 52 51 63 52 47 38
> set.seed(1);        rbinom(10, 100, 0.5)  # Same
 [1] 52 46 60 49 52 51 63 52 47 38
> 
> ## Not run: 
> ##D N <- 9; xx <- 0:N; prob <- 0.5; rho <- -0.02
> ##D dy <- dextbetabinom(xx, N, prob, rho)
> ##D barplot(rbind(dy, dbinom(xx, size = N, prob)),
> ##D   beside = TRUE, col = c("blue","green"), las = 1,
> ##D   main = paste0("Beta-binom(size=", N, 
> ##D   ", prob=", prob, ", rho=", rho, ") (blue) vs\n",
> ##D   " Binom(size=", N, ", prob=", prob, ") (green)"),
> ##D   names.arg = as.character(xx), cex.main = 0.8)
> ##D sum(dy * xx)  # Check expected values are equal
> ##D sum(dbinom(xx, size = N, prob = prob) * xx)
> ##D cumsum(dy) - pextbetabinom(xx, N, prob, rho)  # 0?
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("extbetabinomial")
> ### * extbetabinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extbetabinomial
> ### Title: Extended Beta-binomial Distribution Family Function
> ### Aliases: extbetabinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1
> edata <- data.frame(N = 10, mu = 0.5, rho = 0.1)
> edata <- transform(edata,
+       y = rextbetabinom(100, N, mu, rho = rho))
> fit1 <- vglm(cbind(y, N-y) ~ 1, extbetabinomial, edata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -206.77572
VGLM    linear loop  2 :  loglikelihood = -206.77443
VGLM    linear loop  3 :  loglikelihood = -206.77443
> coef(fit1, matrix = TRUE)
            logitlink(mu) cloglink(rho)
(Intercept)    0.04824012    0.05656821
> Coef(fit1)
        mu        rho 
0.51205769 0.05499797 
> head(cbind(depvar(fit1), weights(fit1, type = "prior")))
  [,1] [,2]
1  0.4   10
2  0.4   10
3  0.5   10
4  0.8   10
5  0.3   10
6  0.8   10
> 
> # Example 2: VFL model
> ## Not run: 
> ##D N <- size1 <- 10; nn <- 2000; set.seed(1)
> ##D edata <-  # Generate the data set. Expensive.
> ##D     data.frame(x2 = runif(nn),
> ##D                ooo =  log(size1 / (size1 - 1)))
> ##D edata <- transform(edata, x1copy = 1, x2copy = x2,
> ##D   y2 = rextbetabinom(nn, size1,  # Expensive
> ##D          logitlink(1 + x2, inverse = TRUE),
> ##D          cloglink(ooo + 1 - 0.5 * x2, inv = TRUE)))
> ##D fit2 <- vglm(data = edata,
> ##D         cbind(y2, N - y2) ~ x2 + x1copy + x2copy,
> ##D         extbetabinomial(zero = NULL, vfl = TRUE,
> ##D                  Form2 = ~ x1copy + x2copy - 1),
> ##D         offset = cbind(0, ooo), trace = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ##D wald.stat(fit2, values0 = c(1, 1, -0.5))
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("extlogF.UC")
> ### * extlogF.UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dextlogF
> ### Title: Extended log-F Distribution
> ### Aliases: dextlogF
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(-2, 8, by = 0.1); mytau <- 0.25; mylambda <- 0.2
> ##D plot(x, dextlogF(x, mylambda, tau = mytau), type = "l",
> ##D      las = 1, col = "blue", ylab = "PDF (log-scale)", log = "y",
> ##D      main = "Extended log-F density function is blue",
> ##D      sub = "Asymmetric Laplace is orange dashed")
> ##D lines(x, dalap(x, tau = mytau, scale = 3.5), col = "orange", lty = 2)
> ##D abline(v = 0, col = "gray", lty = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("extlogF1")
> ### * extlogF1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extlogF1
> ### Title: Extended log-F Distribution Family Function
> ### Aliases: extlogF1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D nn <- 1000; mytau <- c(0.25, 0.75)
> ##D edata <- data.frame(x2 = sort(rnorm(nn)))
> ##D edata <- transform(edata, y1 = 1 + x2  + rnorm(nn, sd = exp(-1)),
> ##D           y2 = cos(x2) / (1 + abs(x2)) + rnorm(nn, sd = exp(-1)))
> ##D fit1 <- vglm(y1 ~ x2, extlogF1(tau = mytau), data = edata)  # trace = TRUE
> ##D fit2 <- vglm(y2 ~ bs(x2, 6), extlogF1(tau = mytau), data = edata)
> ##D coef(fit1, matrix = TRUE)
> ##D fit2@extra$percentile  # Empirical percentiles here
> ##D summary(fit2)
> ##D c(is.crossing(fit1), is.crossing(fit2))
> ##D head(fitted(fit1))
> ##D plot(y2 ~ x2, edata, col = "blue")
> ##D matlines(with(edata, x2), fitted(fit2), col="orange", lty = 1, lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("familyname")
> ### * familyname
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: familyname
> ### Title: Family Function Name
> ### Aliases: familyname familyname.vlm familyname.vglmff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = TRUE, reverse = TRUE), data = pneumo)
> familyname(fit1)
[1] "cumulative"
> familyname(fit1, all = TRUE)
[1] "cumulative"      "VGAMordinal"     "VGAMcategorical"
> familyname(propodds())  # "cumulative"
[1] "cumulative"
> 
> 
> 
> cleanEx()
> nameEx("felix")
> ### * felix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: felix
> ### Title: Felix Distribution Family Function
> ### Aliases: felix
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fdata <- data.frame(y = 2 * rpois(n = 200, 1) + 1)  # Not real data!
> fit <- vglm(y ~ 1, felix, data = fdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.17229273
VGLM    linear loop  2 :  coefficients = 0.88101019
VGLM    linear loop  3 :  coefficients = 0.72216934
VGLM    linear loop  4 :  coefficients = 0.7081352
VGLM    linear loop  5 :  coefficients = 0.7080358
VGLM    linear loop  6 :  coefficients = 0.70803579
> coef(fit, matrix = TRUE)
            extlogitlink(rate, min = 0, max = 0.5)
(Intercept)                              0.7080358
> Coef(fit)
     rate 
0.3349835 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = felix, data = fdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   0.7080     0.2127   3.329  0.00087 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: extlogitlink(rate, min = 0, max = 0.5) 

Log-likelihood: -358.8255 on 199 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("felixUC")
> ### * felixUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Felix
> ### Title: The Felix Distribution
> ### Aliases: Felix dfelix
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D rate <- 0.25; x <- 1:15
> ##D plot(x, dfelix(x, rate), type = "h", las = 1, col = "blue",
> ##D      ylab = paste("dfelix(rate=", rate, ")"),
> ##D      main = "Felix density function")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("fff")
> ### * fff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fff
> ### Title: F Distribution Family Function
> ### Aliases: fff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D fdata <- data.frame(x2 = runif(nn <- 2000))
> ##D fdata <- transform(fdata, df1 = exp(2+0.5*x2),
> ##D                           df2 = exp(2-0.5*x2))
> ##D fdata <- transform(fdata, y   = rf(nn, df1, df2))
> ##D fit <- vglm(y  ~ x2, fff, data = fdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("fill1")
> ### * fill1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fill1
> ### Title: Creates a Matrix of Appropriate Dimension
> ### Aliases: fill1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fill1(runif(5))
     [,1]
[1,]    0
[2,]    0
[3,]    0
[4,]    0
[5,]    0
> fill1(runif(5), ncol = 3)
     [,1] [,2] [,3]
[1,]    0    0    0
[2,]    0    0    0
[3,]    0    0    0
[4,]    0    0    0
[5,]    0    0    0
> fill1(runif(5), val = 1, ncol = 3)
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    1    1    1
[4,]    1    1    1
[5,]    1    1    1
> 
> # Generate (independent) eyes data for the examples below; OR=1.
> ## Not run: 
> ##D  nn <- 1000  # Number of people
> ##D eyesdata <- data.frame(lop = round(runif(nn), 2),
> ##D                        rop = round(runif(nn), 2),
> ##D                        age = round(rnorm(nn, 40, 10)))
> ##D eyesdata <- transform(eyesdata,
> ##D   mop = (lop + rop) / 2,        # Mean ocular pressure
> ##D   op  = (lop + rop) / 2,        # Value unimportant unless plotting
> ##D # op  =  lop,                   # Choose this if plotting
> ##D   eta1 = 0 - 2*lop + 0.04*age,  # Linear predictor for left eye
> ##D   eta2 = 0 - 2*rop + 0.04*age)  # Linear predictor for right eye
> ##D eyesdata <- transform(eyesdata,
> ##D   leye = rbinom(nn, size=1, prob = logitlink(eta1, inverse = TRUE)),
> ##D   reye = rbinom(nn, size=1, prob = logitlink(eta2, inverse = TRUE)))
> ##D 
> ##D # Example 1. All effects are linear.
> ##D fit1 <- vglm(cbind(leye,reye) ~ op + age,
> ##D              family = binom2.or(exchangeable = TRUE, zero = 3),
> ##D              data = eyesdata, trace = TRUE,
> ##D              xij = list(op ~ lop + rop + fill1(lop)),
> ##D              form2 =  ~ op + lop + rop + fill1(lop) + age)
> ##D head(model.matrix(fit1, type = "lm"))   # LM model matrix
> ##D head(model.matrix(fit1, type = "vlm"))  # Big VLM model matrix
> ##D coef(fit1)
> ##D coef(fit1, matrix = TRUE)  # Unchanged with 'xij'
> ##D constraints(fit1)
> ##D max(abs(predict(fit1)-predict(fit1, new = eyesdata)))  # Okay
> ##D summary(fit1)
> ##D plotvgam(fit1,
> ##D      se = TRUE)  # Wrong, e.g., coz it plots against op, not lop.
> ##D # So set op = lop in the above for a correct plot.
> ##D 
> ##D # Example 2. This uses regression splines on ocular pressure.
> ##D # It uses a trick to ensure common basis functions.
> ##D BS <- function(x, ...)
> ##D   sm.bs(c(x,...), df = 3)[1:length(x), , drop = FALSE]  # trick
> ##D 
> ##D fit2 <-
> ##D   vglm(cbind(leye,reye) ~ BS(lop,rop) + age,
> ##D        family = binom2.or(exchangeable = TRUE, zero = 3),
> ##D        data = eyesdata, trace = TRUE,
> ##D        xij = list(BS(lop,rop) ~ BS(lop,rop) +
> ##D                                 BS(rop,lop) +
> ##D                                 fill1(BS(lop,rop))),
> ##D        form2 = ~  BS(lop,rop) + BS(rop,lop) + fill1(BS(lop,rop)) +
> ##D                         lop + rop + age)
> ##D head(model.matrix(fit2, type =  "lm"))  # LM model matrix
> ##D head(model.matrix(fit2, type = "vlm"))  # Big VLM model matrix
> ##D coef(fit2)
> ##D coef(fit2, matrix = TRUE)
> ##D summary(fit2)
> ##D fit2@smart.prediction
> ##D max(abs(predict(fit2) - predict(fit2, new = eyesdata)))  # Okay
> ##D predict(fit2, new = head(eyesdata))  # OR is 'scalar' as zero=3
> ##D max(abs(head(predict(fit2)) -
> ##D              predict(fit2, new = head(eyesdata))))  # Should be 0
> ##D plotvgam(fit2, se = TRUE, xlab = "lop")  # Correct
> ##D 
> ##D # Example 3. Capture-recapture model with ephemeral and enduring
> ##D # memory effects. Similar to Yang and Chao (2005), Biometrics.
> ##D deermice <- transform(deermice, Lag1 = y1)
> ##D M.tbh.lag1 <-
> ##D   vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight + Lag1,
> ##D        posbernoulli.tb(parallel.t = FALSE ~ 0,
> ##D                        parallel.b = FALSE ~ 0,
> ##D                        drop.b = FALSE ~ 1),
> ##D        xij = list(Lag1 ~ fill1(y1) + fill1(y2) + fill1(y3) +
> ##D                          fill1(y4) + fill1(y5) + fill1(y6) +
> ##D                          y1 + y2 + y3 + y4 + y5),
> ##D        form2 = ~ sex + weight + Lag1 +
> ##D                  fill1(y1) + fill1(y2) + fill1(y3) + fill1(y4) +
> ##D                  fill1(y5) + fill1(y6) +
> ##D                  y1 + y2 + y3 + y4 + y5 + y6,
> ##D        data = deermice, trace = TRUE)
> ##D coef(M.tbh.lag1)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("finney44")
> ### * finney44
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: finney44
> ### Title: Toxicity trial for insects
> ### Aliases: finney44
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(finney44)
> transform(finney44, mortality = unhatched / (hatched + unhatched))
   pconc hatched unhatched mortality
1 0.0000      44        55 0.5555556
2 0.0025      19        32 0.6274510
3 0.0050      17        32 0.6530612
4 0.0100      13        38 0.7450980
5 0.0250       2        46 0.9583333
6 0.0500       0        50 1.0000000
> 
> 
> 
> cleanEx()
> nameEx("fisherzlink")
> ### * fisherzlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fisherzlink
> ### Title: Fisher's Z Link Function
> ### Aliases: fisherzlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> theta <- seq(-0.99, 0.99, by = 0.01)
> y <- fisherzlink(theta)
> ## Not run: 
> ##D  plot(theta, y, type = "l", las = 1, ylab = "",
> ##D    main = "fisherzlink(theta)", col = "blue")
> ##D abline(v = (-1):1, h = 0, lty = 2, col = "gray") 
> ## End(Not run)
> 
> x <- c(seq(-1.02, -0.98, by = 0.01), seq(0.97, 1.02, by = 0.01))
> fisherzlink(x)  # Has NAs
Warning in atanh(theta) : NaNs produced
 [1]       NaN       NaN      -Inf -2.646652 -2.297560  2.092296  2.297560
 [8]  2.646652       Inf       NaN       NaN
> fisherzlink(x, bminvalue = -1 + .Machine$double.eps,
+                bmaxvalue =  1 - .Machine$double.eps)  # Has no NAs
 [1] -18.368400 -18.368400 -18.368400  -2.646652  -2.297560   2.092296
 [7]   2.297560   2.646652  18.368400  18.368400  18.368400
> 
> 
> 
> cleanEx()
> nameEx("fisk")
> ### * fisk
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fisk
> ### Title: Fisk Distribution family function
> ### Aliases: fisk
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fdata <- data.frame(y = rfisk(200, shape = exp(1), exp(2)))
> fit <- vglm(y ~ 1, fisk(lss = FALSE), data = fdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -579.78935
VGLM    linear loop  2 :  loglikelihood = -579.78783
VGLM    linear loop  3 :  loglikelihood = -579.78783
> fit <- vglm(y ~ 1, fisk(ishape1.a = exp(2)), fdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -614.56816
VGLM    linear loop  2 :  loglikelihood = -582.75843
VGLM    linear loop  3 :  loglikelihood = -579.80746
VGLM    linear loop  4 :  loglikelihood = -579.78783
VGLM    linear loop  5 :  loglikelihood = -579.78783
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(shape1.a)
(Intercept)       2.036029          1.141519
> Coef(fit)
   scale shape1.a 
7.660129 3.131521 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = fisk(ishape1.a = exp(2)), data = fdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  2.03603    0.03911   52.06   <2e-16 ***
(Intercept):2  1.14152    0.05913   19.30   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), loglink(shape1.a)

Log-likelihood: -579.7878 on 398 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("fiskUC")
> ### * fiskUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Fisk
> ### Title: The Fisk Distribution
> ### Aliases: Fisk dfisk pfisk qfisk rfisk
> ### Keywords: distribution
> 
> ### ** Examples
> 
> fdata <- data.frame(y = rfisk(1000, shape = exp(1), scale = exp(2)))
> fit <- vglm(y ~ 1, fisk(lss = FALSE), data = fdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3008.2046
VGLM    linear loop  2 :  loglikelihood = -3008.2046
VGLM    linear loop  3 :  loglikelihood = -3008.2046
> coef(fit, matrix = TRUE)
            loglink(shape1.a) loglink(scale)
(Intercept)          1.001235       1.999311
> Coef(fit)
shape1.a    scale 
2.721640 7.383966 
> 
> 
> 
> cleanEx()
> nameEx("fittedvlm")
> ### * fittedvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fittedvlm
> ### Title: Fitted Values of a VLM object
> ### Aliases: fittedvlm fitted.values.vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Categorical regression example 1
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> fitted(fit1)
     normal        mild      severe
1 0.9940077 0.003560995 0.002431267
2 0.9336285 0.038433830 0.027937700
3 0.8467004 0.085093969 0.068205607
4 0.7445575 0.133635126 0.121807334
5 0.6358250 0.176154091 0.188020946
6 0.5323177 0.205582603 0.262099709
7 0.4338530 0.220783923 0.345363119
8 0.3636788 0.222017003 0.414304232
> 
> # LMS quantile regression example 2
> fit2 <- vgam(BMI ~ s(age, df = c(4, 2)),
+              lms.bcn(zero = 1), data = bmi.nz, trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = -6429.7568
VGAM  s.vam  loop  2 :  loglikelihood = -6327.3502
VGAM  s.vam  loop  3 :  loglikelihood = -6313.2224
VGAM  s.vam  loop  4 :  loglikelihood = -6312.8069
VGAM  s.vam  loop  5 :  loglikelihood = -6312.8166
VGAM  s.vam  loop  6 :  loglikelihood = -6312.8032
VGAM  s.vam  loop  7 :  loglikelihood = -6312.8088
VGAM  s.vam  loop  8 :  loglikelihood = -6312.8062
VGAM  s.vam  loop  9 :  loglikelihood = -6312.8074
VGAM  s.vam  loop  10 :  loglikelihood = -6312.8068
VGAM  s.vam  loop  11 :  loglikelihood = -6312.8071
VGAM  s.vam  loop  12 :  loglikelihood = -6312.807
> head(predict(fit2, type = "response"))  # Equals to both these:
       25%      50%      75%
1 23.00836 25.48922 28.44767
2 23.65211 26.19783 29.23269
3 24.07328 26.66334 29.75085
4 23.25503 25.75937 28.74518
5 24.53531 27.17650 30.32525
6 23.63164 26.17517 29.20742
> head(fitted(fit2))
       25%      50%      75%
1 23.00836 25.48922 28.44767
2 23.65211 26.19783 29.23269
3 24.07328 26.66334 29.75085
4 23.25503 25.75937 28.74518
5 24.53531 27.17650 30.32525
6 23.63164 26.17517 29.20742
> predict(fit2, type = "response", newdata = head(bmi.nz))
       25%      50%      75%
1 23.00853 25.48943 28.44792
2 23.65207 26.19779 29.23264
3 24.07313 26.66317 29.75064
4 23.25511 25.75947 28.74530
5 24.53487 27.17599 30.32464
6 23.63160 26.17513 29.20738
> 
> # Zero-inflated example 3
> zdata <- data.frame(x2 = runif(nn <- 1000))
> zdata <- transform(zdata,
+                    pstr0.3  = logitlink(-0.5       , inverse = TRUE),
+                    lambda.3 =   loglink(-0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata,
+          y1 = rzipois(nn, lambda = lambda.3, pstr0 = pstr0.3))
> fit3 <- vglm(y1 ~ x2, zipoisson(zero = NULL), zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1392.9897
VGLM    linear loop  2 :  loglikelihood = -1367.21
VGLM    linear loop  3 :  loglikelihood = -1365.2291
VGLM    linear loop  4 :  loglikelihood = -1365.1936
VGLM    linear loop  5 :  loglikelihood = -1365.1936
VGLM    linear loop  6 :  loglikelihood = -1365.1936
> head(fitted(fit3, type.fitted = "mean" ))  # E(Y) (the default)
       [,1]
1 0.6426275
2 0.7926570
3 1.1695210
4 2.1982194
5 0.5661845
6 2.1587251
> head(fitted(fit3, type.fitted = "pobs0"))  # Pr(Y = 0)
       [,1]
1 0.5807540
2 0.5342554
3 0.4683846
4 0.4565671
5 0.6103503
6 0.4552838
> head(fitted(fit3, type.fitted = "pstr0"))  # Prob of a structural 0
       [,1]
1 0.3052809
2 0.3270020
3 0.3699194
4 0.4460985
5 0.2926724
6 0.4438129
> head(fitted(fit3, type.fitted = "onempstr0"))  # 1 - Pr(structural 0)
       [,1]
1 0.6947191
2 0.6729980
3 0.6300806
4 0.5539015
5 0.7073276
6 0.5561871
> 
> 
> 
> cleanEx()
> nameEx("fix.crossing")
> ### * fix.crossing
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fix.crossing
> ### Title: Fixing a Quantile Regression having Crossing
> ### Aliases: fix.crossing fix.crossing.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  ooo <- with(bmi.nz, order(age))
> ##D bmi.nz <- bmi.nz[ooo, ]  # Sort by age
> ##D with(bmi.nz, plot(age, BMI, col = "blue"))
> ##D mytau <- c(50, 93, 95, 97) / 100  # Some quantiles are quite close
> ##D fit1 <- vglm(BMI ~ ns(age, 7), extlogF1(mytau), bmi.nz, trace = TRUE)
> ##D plot(BMI ~ age, bmi.nz, col = "blue", las = 1,
> ##D      main = "Partially parallel (darkgreen) & nonparallel quantiles",
> ##D      sub = "Crossing quantiles are orange")
> ##D fix.crossing(fit1)
> ##D matlines(with(bmi.nz, age), fitted(fit1), lty = 1, col = "orange")
> ##D fit2 <- fix.crossing(fit1)  # Some quantiles have been fixed
> ##D constraints(fit2)
> ##D matlines(with(bmi.nz, age), fitted(fit2), lty = "dashed",
> ##D          col = "darkgreen", lwd = 2)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("flourbeetle")
> ### * flourbeetle
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: flourbeetle
> ### Title: Mortality of Flour Beetles from Carbon Disulphide
> ### Aliases: flourbeetle
> ### Keywords: datasets
> 
> ### ** Examples
> 
> fit1 <- vglm(cbind(killed, exposed - killed) ~ logdose,
+              binomialff(link = probitlink), flourbeetle, trace = TRUE)
VGLM    linear loop  1 :  deviance = 10.13646
VGLM    linear loop  2 :  deviance = 10.10674
VGLM    linear loop  3 :  deviance = 10.10671
VGLM    linear loop  4 :  deviance = 10.10671
> summary(fit1)
Call:
vglm(formula = cbind(killed, exposed - killed) ~ logdose, family = binomialff(link = probitlink), 
    data = flourbeetle, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -34.939      2.648  -13.20   <2e-16 ***
logdose       19.730      1.487   13.27   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: probitlink(prob) 

Residual deviance: 10.1067 on 6 degrees of freedom

Log-likelihood: -18.1524 on 6 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'logdose'

> 
> 
> 
> cleanEx()
> nameEx("foldnormUC")
> ### * foldnormUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Foldnorm
> ### Title: The Folded-Normal Distribution
> ### Aliases: Foldnorm dfoldnorm pfoldnorm qfoldnorm rfoldnorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D m <- 1.5; SD <- exp(0)
> ##D x <- seq(-1, 4, len = 501)
> ##D plot(x, dfoldnorm(x, m = m, sd = SD), type = "l", ylim = 0:1,
> ##D      ylab = paste("foldnorm(m = ", m, ", sd = ",
> ##D                   round(SD, digits = 3), ")"), las = 1,
> ##D      main = "Blue is density, orange is CDF", col = "blue",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0, col = "gray50")
> ##D lines(x, pfoldnorm(x, m = m, sd = SD), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qfoldnorm(probs, m = m, sd = SD)
> ##D lines(Q, dfoldnorm(Q, m, SD), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pfoldnorm(Q, m, SD), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pfoldnorm(Q, m = m, sd = SD) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("foldnormal")
> ### * foldnormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: foldnormal
> ### Title: Folded Normal Distribution Family Function
> ### Aliases: foldnormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  m <-  2; SD <- exp(1)
> ##D fdata <- data.frame(y = rfoldnorm(n <- 1000, m = m, sd = SD))
> ##D hist(with(fdata, y), prob = TRUE, main = paste("foldnormal(m = ",
> ##D      m, ", sd = ", round(SD, 2), ")"))
> ##D fit <- vglm(y ~ 1, foldnormal, data = fdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D (Cfit <- Coef(fit))
> ##D # Add the fit to the histogram:
> ##D mygrid <- with(fdata, seq(min(y), max(y), len = 200))
> ##D lines(mygrid, dfoldnorm(mygrid, Cfit[1], Cfit[2]), col = "orange")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("foldsqrtlink")
> ### * foldsqrtlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sqrtlink
> ### Title: Square Root and Folded Square Root Link Functions
> ### Aliases: foldsqrtlink sqrtlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, by = 0.01)
> foldsqrtlink(p)
 [1] -1.26570337 -1.20000000 -1.14788985 -1.10279793 -1.06217711 -1.02472076
 [7] -0.98965243 -0.95646600 -0.92480969 -0.89442719 -0.86512483 -0.83675197
[13] -0.80918864 -0.78233744 -0.75611792 -0.73046271 -0.70531468 -0.68062485
[19] -0.65635081 -0.63245553 -0.60890644 -0.58567464 -0.56273437 -0.54006248
[25] -0.51763809 -0.49544225 -0.47345767 -0.45166852 -0.43006022 -0.40861929
[31] -0.38733323 -0.36619038 -0.34517985 -0.32429140 -0.30351540 -0.28284271
[37] -0.26226469 -0.24177308 -0.22136002 -0.20101792 -0.18073954 -0.16051782
[43] -0.14034598 -0.12021737 -0.10012555 -0.08006418 -0.06002704 -0.04000801
[49] -0.02000100  0.00000000  0.02000100  0.04000801  0.06002704  0.08006418
[55]  0.10012555  0.12021737  0.14034598  0.16051782  0.18073954  0.20101792
[61]  0.22136002  0.24177308  0.26226469  0.28284271  0.30351540  0.32429140
[67]  0.34517985  0.36619038  0.38733323  0.40861929  0.43006022  0.45166852
[73]  0.47345767  0.49544225  0.51763809  0.54006248  0.56273437  0.58567464
[79]  0.60890644  0.63245553  0.65635081  0.68062485  0.70531468  0.73046271
[85]  0.75611792  0.78233744  0.80918864  0.83675197  0.86512483  0.89442719
[91]  0.92480969  0.95646600  0.98965243  1.02472076  1.06217711  1.10279793
[97]  1.14788985  1.20000000  1.26570337
> max(abs(foldsqrtlink(foldsqrtlink(p), inverse = TRUE) - p))  # 0
[1] 1.110223e-16
> 
> p <- c(seq(-0.02, 0.02, by = 0.01), seq(0.97, 1.02, by = 0.01))
> foldsqrtlink(p)  # Has NAs
Warning in sqrt(theta - min) : NaNs produced
Warning in sqrt(max - theta) : NaNs produced
 [1]       NaN       NaN -1.414214 -1.265703 -1.200000  1.147890  1.200000
 [8]  1.265703  1.414214       NaN       NaN
> 
> ## Not run: 
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D par(mfrow = c(2, 2), lwd = (mylwd <- 2))
> ##D y <- seq(-4, 4, length = 100)
> ##D for (d in 0:1) {
> ##D   matplot(p, cbind(   logitlink(p, deriv = d),
> ##D                    foldsqrtlink(p, deriv = d)),
> ##D           col = "blue", ylab = "transformation",
> ##D           main = ifelse(d == 0, "Some probability links",
> ##D           "First derivative"), type = "n", las = 1)
> ##D   lines(p,    logitlink(p, deriv = d), col = "green")
> ##D   lines(p,   probitlink(p, deriv = d), col = "blue")
> ##D   lines(p,  clogloglink(p, deriv = d), col = "red")
> ##D   lines(p, foldsqrtlink(p, deriv = d), col = "tan")
> ##D   if (d == 0) {
> ##D     abline(v = 0.5, h = 0, lty = "dashed")
> ##D     legend(0, 4.5, c("logitlink", "probitlink",
> ##D                      "clogloglink", "foldsqrtlink"),
> ##D            lwd = 2, col = c("green", "blue",
> ##D                             "red", "tan"))
> ##D   } else
> ##D     abline(v = 0.5, lty = "dashed")
> ##D }
> ##D 
> ##D for (d in 0) {
> ##D   matplot(y,
> ##D           cbind(   logitlink(y, deriv = d, inverse = TRUE),
> ##D                 foldsqrtlink(y, deriv = d, inverse = TRUE)),
> ##D           type = "n", col = "blue", xlab = "transformation",
> ##D           ylab = "p", lwd = 2, las = 1, main = if (d == 0)
> ##D           "Some inverse probability link functions" else
> ##D           "First derivative")
> ##D   lines(y,    logitlink(y, deriv=d, inverse=TRUE), col="green")
> ##D   lines(y,   probitlink(y, deriv=d, inverse=TRUE), col="blue")
> ##D   lines(y,  clogloglink(y, deriv=d, inverse=TRUE), col="red")
> ##D   lines(y, foldsqrtlink(y, deriv=d, inverse=TRUE), col="tan")
> ##D   if (d == 0) {
> ##D     abline(h = 0.5, v = 0, lty = "dashed")
> ##D     legend(-4, 1, c("logitlink", "probitlink",
> ##D                     "clogloglink", "foldsqrtlink"), lwd = 2, 
> ##D            col = c("green", "blue", "red", "tan"))
> ##D   }
> ##D }
> ##D par(lwd = 1)
> ## End(Not run)
> 
> # This is lucky to converge
> fit.h <- vglm(agaaus ~ sm.bs(altitude),
+               binomialff(foldsqrtlink(mux = 5)),
+               hunua, trace = TRUE)
VGLM    linear loop  1 :  deviance = 390.01966
VGLM    linear loop  2 :  deviance = 388.89021
VGLM    linear loop  3 :  deviance = 388.78756
VGLM    linear loop  4 :  deviance = 388.77251
VGLM    linear loop  5 :  deviance = 388.76941
VGLM    linear loop  6 :  deviance = 388.76865
VGLM    linear loop  7 :  deviance = 388.76845
VGLM    linear loop  8 :  deviance = 388.76839
VGLM    linear loop  9 :  deviance = 388.76838
VGLM    linear loop  10 :  deviance = 388.76837
VGLM    linear loop  11 :  deviance = 388.76837
> ## Not run: 
> ##D plotvgam(fit.h, se = TRUE, lcol = "orange", scol = "orange",
> ##D          main = "Orange is Hunua, Blue is Waitakere") 
> ## End(Not run)
> head(predict(fit.h, hunua, type = "response"))
[1] 0.2205809 0.2033924 0.1429702 0.1206420 0.1206420 0.1429702
> 
> ## Not run: 
> ##D # The following fails.
> ##D pneumo <- transform(pneumo, let = log(exposure.time))
> ##D fit <- vglm(cbind(normal, mild, severe) ~ let,
> ##D        cumulative(foldsqrtlink(mux = 10), par = TRUE, rev = TRUE),
> ##D        data = pneumo, trace = TRUE, maxit = 200) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("formulavlm")
> ### * formulavlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: formulavlm
> ### Title: Model Formulae and Term Names for VGLMs
> ### Aliases: formula.vlm formulavlm term.names term.namesvlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example: this is based on a glm example
> counts <- c(18,17,15,20,10,20,25,13,12)
> outcome <- gl(3, 1, 9); treatment <- gl(3, 3)
> vglm.D93 <- vglm(counts ~ outcome + treatment, family = poissonff)
> formula(vglm.D93)
counts ~ outcome + treatment
> pdata <- data.frame(counts, outcome, treatment)  # Better style
> vglm.D93 <- vglm(counts ~ outcome + treatment, poissonff, data = pdata)
> formula(vglm.D93)
counts ~ outcome + treatment
> term.names(vglm.D93)
[1] "(Intercept)" "outcome"     "treatment"  
> responseName(vglm.D93)
[1] "counts"
> has.intercept(vglm.D93)
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("frechet")
> ### * frechet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: frechet
> ### Title: Frechet Distribution Family Function
> ### Aliases: frechet
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(123)
> ##D fdata <- data.frame(y1 = rfrechet(1000, shape = 2 + exp(1)))
> ##D with(fdata, hist(y1))
> ##D fit2 <- vglm(y1 ~ 1, frechet, data = fdata, trace = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ##D Coef(fit2)
> ##D head(fitted(fit2))
> ##D with(fdata, mean(y1))
> ##D head(weights(fit2, type = "working"))
> ##D vcov(fit2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("frechetUC")
> ### * frechetUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Frechet
> ### Title: The Frechet Distribution
> ### Aliases: Frechet dfrechet pfrechet qfrechet rfrechet
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  shape <- 5
> ##D x <- seq(-0.1, 3.5, length = 401)
> ##D plot(x, dfrechet(x, shape = shape), type = "l", ylab = "",
> ##D   main = "Frechet density divided into 10 equal areas",
> ##D   sub = "Orange = CDF", las = 1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D qq <- qfrechet(seq(0.1, 0.9, by = 0.1), shape = shape)
> ##D lines(qq, dfrechet(qq, shape = shape), col = 2, lty = 2, type = "h")
> ##D lines(x, pfrechet(q = x, shape = shape), col = "orange")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("freund61")
> ### * freund61
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: freund61
> ### Title: Freund's (1961) Bivariate Extension of the Exponential
> ###   Distribution
> ### Aliases: freund61
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fdata <- data.frame(y1 = rexp(nn <- 1000, rate = exp(1)))
> fdata <- transform(fdata, y2 = rexp(nn, rate = exp(2)))
> fit1 <- vglm(cbind(y1, y2) ~ 1, freund61, fdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 988.6378
> coef(fit1, matrix = TRUE)
            loglink(a) loglink(ap) loglink(b) loglink(bp)
(Intercept)  0.8452949    1.011479   2.003465    2.059584
> Coef(fit1)
       a       ap        b       bp 
2.328664 2.749664 7.414701 7.842703 
> vcov(fit1)
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4
(Intercept):1     0.0041841    0.00000000    0.00000000     0.0000000
(Intercept):2     0.0000000    0.00131406    0.00000000     0.0000000
(Intercept):3     0.0000000    0.00000000    0.00131406     0.0000000
(Intercept):4     0.0000000    0.00000000    0.00000000     0.0041841
> head(fitted(fit1))
         y1        y2
1 0.3793951 0.1331081
2 0.3793951 0.1331081
3 0.3793951 0.1331081
4 0.3793951 0.1331081
5 0.3793951 0.1331081
6 0.3793951 0.1331081
> summary(fit1)
Call:
vglm(formula = cbind(y1, y2) ~ 1, family = freund61, data = fdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.84529    0.06468   13.07   <2e-16 ***
(Intercept):2  1.01148    0.03625   27.90   <2e-16 ***
(Intercept):3  2.00346    0.03625   55.27   <2e-16 ***
(Intercept):4  2.05958    0.06468   31.84   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(a), loglink(ap), loglink(b), loglink(bp)

Log-likelihood: 988.6378 on 3996 degrees of freedom

Number of Fisher scoring iterations: 1 

No Hauck-Donner effect found in any of the estimates

> 
> # y1 and y2 are independent, so fit an independence model
> fit2 <- vglm(cbind(y1, y2) ~ 1, freund61(indep = TRUE),
+              data = fdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 985.76737
VGLM    linear loop  2 :  loglikelihood = 985.77075
VGLM    linear loop  3 :  loglikelihood = 985.77075
> coef(fit2, matrix = TRUE)
            loglink(a) loglink(ap) loglink(b) loglink(bp)
(Intercept)  0.9691773   0.9691773   2.016593    2.016593
> constraints(fit2)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    1    0
[3,]    0    1
[4,]    0    1

> pchisq(2 * (logLik(fit1) - logLik(fit2)),  # p-value
+        df = df.residual(fit2) - df.residual(fit1),
+        lower.tail = FALSE)
[1] 0.05686656
> lrtest(fit1, fit2)  # Better alternative
Likelihood ratio test

Model 1: cbind(y1, y2) ~ 1
Model 2: cbind(y1, y2) ~ 1
   #Df LogLik Df  Chisq Pr(>Chisq)  
1 3996 988.64                       
2 3998 985.77  2 5.7341    0.05687 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> 
> 
> cleanEx()
> nameEx("gaitdbinomUC")
> ### * gaitdbinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gaitdbinom
> ### Title: Generally Altered, Inflated, Truncated and Deflated Binomial
> ###   Distribution
> ### Aliases: Gaitdbinom dgaitdbinom pgaitdbinom qgaitdbinom rgaitdbinom
> ### Keywords: distribution
> 
> ### ** Examples
>  size <- 20
> ivec <- c(6, 10); avec <- c(8, 11); prob <- 0.25; xgrid <- 0:25
> tvec <- 14; pobs.a <- 0.05; pstr.i <- 0.15
> dvec <- 5; pdip.mlm <- 0.05
> (ddd <- dgaitdbinom(xgrid, size, prob.p = prob,
+    prob.a = prob + 0.05, truncate = tvec, pobs.mix = pobs.a,
+    pdip.mlm = pdip.mlm, d.mlm = dvec,
+    pobs.mlm = pobs.a, a.mlm = avec,
+    pstr.mix = pstr.i, i.mix = ivec))
 [1] 2.710203e-03 1.806802e-02 5.721541e-02 1.144308e-01 1.621103e-01
 [6] 1.229177e-01 2.827010e-01 9.606537e-02 5.000000e-02 2.312685e-02
[11] 1.663631e-02 5.247166e-02 1.260327e-03 2.585286e-04 2.112528e-05
[16] 5.745080e-06 5.984458e-07 4.693692e-08 2.607607e-09 9.149498e-11
[21] 1.524916e-12 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
[26] 0.000000e+00
> ## Not run: 
> ##D  dgaitdplot(c(size, prob), ylab = "Probability",
> ##D    xlab = "x", pobs.mix = pobs.mix,
> ##D    pobs.mlm = pobs.a, a.mlm = avec, all.lwd = 3,
> ##D    pdip.mlm = pdip.mlm, d.mlm = dvec, fam = "binom",
> ##D    pstr.mix = pstr.i, i.mix = ivec, deflation = TRUE,
> ##D    main = "GAITD Combo PMF---Binomial Parent")   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdlog")
> ### * gaitdlog
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaitdlog
> ### Title: Generally Altered, Inflated, Truncated and Deflated Logarithmic
> ###   Regression
> ### Aliases: gaitdlog
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  avec <- c(5, 10)  # Alter these values parametrically
> ##D ivec <- c(3, 15)  # Inflate these values
> ##D tvec <- c(6, 7)   # Truncate these values
> ##D max.support <- 20; set.seed(1)
> ##D pobs.a <- pstr.i <- 0.1
> ##D gdata <- data.frame(x2 = runif(nn <- 1000))
> ##D gdata <- transform(gdata, shape.p = logitlink(2+0.5*x2, inverse = TRUE))
> ##D gdata <- transform(gdata,
> ##D   y1 = rgaitdlog(nn, shape.p, a.mix = avec, pobs.mix = pobs.a,
> ##D                 i.mix = ivec, pstr.mix = pstr.i, truncate = tvec,
> ##D                 max.support = max.support))
> ##D gaitdlog(a.mix = avec, i.mix = ivec, max.support = max.support)
> ##D with(gdata, table(y1))
> ##D spikeplot(with(gdata, y1), las = 1)
> ##D fit7 <- vglm(y1 ~ x2, trace = TRUE, data = gdata,
> ##D              gaitdlog(i.mix = ivec, truncate = tvec,
> ##D                       max.support = max.support, a.mix = avec,
> ##D                       eq.ap = TRUE, eq.ip = TRUE))
> ##D head(fitted(fit7, type.fitted = "Pstr.mix"))
> ##D head(predict(fit7))
> ##D t(coef(fit7, matrix = TRUE))  # Easier to see with t()
> ##D summary(fit7)
> ##D spikeplot(with(gdata, y1), lwd = 2, ylim = c(0, 0.4))
> ##D plotdgaitd(fit7, new.plot = FALSE, offset.x = 0.2, all.lwd = 2)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdlogUC")
> ### * gaitdlogUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gaitdlog
> ### Title: Generally Altered, Inflated, Truncated and Deflated Logarithmic
> ###   Distribution
> ### Aliases: Gaitdlog dgaitdlog pgaitdlog qgaitdlog rgaitdlog
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ivec <- c(2, 10); avec <- ivec + 1; shape <- 0.995; xgrid <- 0:15
> max.support <- 15; pobs.a <- 0.10; pstr.i <- 0.15
> dvec <- 1; pdip.mlm <- 0.05
> (ddd <- dgaitdlog(xgrid, shape,
+    max.support = max.support, pobs.mix = pobs.a,
+    pdip.mlm = pdip.mlm, d.mlm = dvec,
+    a.mix = avec, pstr.mix = pstr.i, i.mix = ivec))
 [1] 0.00000000 0.23125894 0.26575062 0.07923885 0.06926528 0.05513516
 [7] 0.04571624 0.03898942 0.03394516 0.03002261 0.05106095 0.02076115
[13] 0.02218089 0.02037230 0.01882255 0.01747987
> ## Not run: 
> ##D  dgaitdplot(shape, ylab = "Probability", xlab = "x",
> ##D    max.support = max.support, pobs.mix = 0,
> ##D    pobs.mlm = 0, a.mlm = avec, all.lwd = 3,
> ##D    pdip.mlm = pdip.mlm, d.mlm = dvec, fam = "log",
> ##D    pstr.mix = pstr.i, i.mix = ivec, deflation = TRUE,
> ##D    main = "GAITD Combo PMF---Logarithmic Parent")   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdnbinomUC")
> ### * gaitdnbinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gaitdnbinom
> ### Title: Generally Altered, Inflated, Truncated and Deflated Negative
> ###   Binomial Distribution
> ### Aliases: Gaitdnbinom dgaitdnbinom pgaitdnbinom qgaitdnbinom
> ###   rgaitdnbinom
> ### Keywords: distribution
> 
> ### ** Examples
> size <- 10; xgrid <- 0:25
> ivec <- c(5, 6, 10, 14); avec <- c(8, 11); munb <- 10
> tvec <- 15; pobs.a <- 0.05; pstr.i <- 0.25
> dvec <- 13; pdip.mlm <- 0.03; pobs.mlm <- 0.05
> (ddd <- dgaitdnbinom(xgrid, size, munb.p = munb, munb.a = munb + 5,
+    truncate = tvec, pobs.mix = pobs.a,
+    pdip.mlm = pdip.mlm, d.mlm = dvec,
+    pobs.mlm = pobs.a, a.mlm = avec,
+    pstr.mix = pstr.i, i.mix = ivec))
 [1] 0.000842496 0.004212480 0.011584320 0.023168640 0.037649040 0.108397814
 [7] 0.135497268 0.075298081 0.050000000 0.080004211 0.156305777 0.050000000
[13] 0.060457727 0.021156538 0.086419059 0.000000000 0.026263401 0.020083778
[19] 0.015062833 0.011098930 0.008046724 0.005747660 0.004049488 0.002817035
[25] 0.001936712 0.001316964
> ## Not run: 
> ##D dgaitdplot(c(size, munb), fam = "nbinom",
> ##D   ylab = "Probability", xlab = "x", xlim = c(0, 25),
> ##D   truncate = tvec, pobs.mix = pobs.mix,
> ##D   pobs.mlm = pobs.mlm, a.mlm = avec, all.lwd = 3,
> ##D   pdip.mlm = pdip.mlm, d.mlm = dvec,
> ##D   pstr.mix = pstr.i, i.mix = ivec, deflation = TRUE,
> ##D   main = "GAITD Combo PMF---NB Parent")   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdnbinomial")
> ### * gaitdnbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaitdnbinomial
> ### Title: Generally Altered, Inflated, Truncated and Deflated Negative
> ###   Binomial Regression
> ### Aliases: gaitdnbinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D i.mix <- c(5, 10, 12, 16)  # Inflate these values parametrically
> ##D i.mlm <- c(14, 15)  # Inflate these values
> ##D a.mix <- c(1, 6, 13, 20)  # Alter these values
> ##D tvec <- c(3, 11)   # Truncate these values
> ##D pstr.mlm <- 0.1  # So parallel.i = TRUE
> ##D pobs.mix <- pstr.mix <- 0.1; set.seed(1)
> ##D gdata <- data.frame(x2 = runif(nn <- 1000))
> ##D gdata <- transform(gdata, munb.p = exp(2 + 0.0 * x2),
> ##D                    size.p = exp(1))
> ##D gdata <- transform(gdata,
> ##D   y1 = rgaitdnbinom(nn, size.p, munb.p, a.mix = a.mix,
> ##D                     i.mix = i.mix,
> ##D                     pobs.mix = pobs.mix, pstr.mix = pstr.mix,
> ##D                     i.mlm = i.mlm, pstr.mlm = pstr.mlm,
> ##D                     truncate = tvec))
> ##D gaitdnbinomial(a.mix = a.mix, i.mix = i.mix, i.mlm = i.mlm)
> ##D with(gdata, table(y1))
> ##D fit1 <- vglm(y1 ~ 1, crit = "coef", trace = TRUE, data = gdata,
> ##D              gaitdnbinomial(a.mix = a.mix, i.mix = i.mix,
> ##D                             i.mlm = i.mlm,
> ##D                             parallel.i = TRUE, eq.ap = TRUE,
> ##D                             eq.ip = TRUE, truncate = tvec))
> ##D head(fitted(fit1, type.fitted = "Pstr.mix"))
> ##D head(predict(fit1))
> ##D t(coef(fit1, matrix = TRUE))  # Easier to see with t()
> ##D summary(fit1)
> ##D spikeplot(with(gdata, y1), lwd = 2)
> ##D plotdgaitd(fit1, new.plot = FALSE, offset.x = 0.2, all.lwd = 2)  
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdpoisUC")
> ### * gaitdpoisUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gaitdpois
> ### Title: Generally Altered, Inflated, Truncated and Deflated Poisson
> ###   Distribution
> ### Aliases: Gaitdpois dgaitdpois pgaitdpois qgaitdpois rgaitdpois
> ### Keywords: distribution
> 
> ### ** Examples
>  # Example 1
> ivec <- c(6, 14); avec <- c(8, 11); lambda <- 10; xgrid <- 0:25
> tvec <- 15; max.support <- 20; pobs.mix <- 0.05; pstr.i <- 0.25
> dvec <- 13; pdip.mlm <- 0.05; pobs.mlm <- 0.05
> (ddd <- dgaitdpois(xgrid, lambda, lambda.a = lambda + 5,
+    truncate = tvec, max.support = max.support, pobs.mix = pobs.mix,
+    pobs.mlm = pobs.mlm, a.mlm = avec,
+    pdip.mlm = pdip.mlm, d.mlm = dvec,
+    pstr.mix = pstr.i, i.mix = ivec))
 [1] 4.309974e-05 4.309974e-04 2.154987e-03 7.183291e-03 1.795823e-02
 [6] 3.591645e-02 1.967800e-01 8.551536e-02 5.000000e-02 1.187713e-01
[11] 1.187713e-01 5.000000e-02 8.997829e-02 1.921407e-02 1.625194e-01
[16] 0.000000e+00 2.059942e-02 1.211731e-02 6.731838e-03 3.543073e-03
[21] 1.771536e-03 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
[26] 0.000000e+00
> ## Not run: 
> ##D  dgaitdplot(lambda, ylab = "Probability", xlab = "x",
> ##D    truncate = tvec, max.support = max.support, pobs.mix = pobs.mix,
> ##D    pobs.mlm = pobs.mlm, a.mlm = avec, all.lwd = 3,
> ##D    pdip.mlm = pdip.mlm, d.mlm = dvec,
> ##D    pstr.mix = pstr.i, i.mix = ivec, deflation = TRUE,
> ##D    main = "GAITD Combo PMF---Poisson Parent")   
> ## End(Not run)
> 
> # Example 2: detection of an invalid PMF
> xgrid <- 1:3  # Does not cover the special values purposely
> (ddd <- dgaitdpois(xgrid, 1, pdip.mlm = 0.1, d.mlm = 5,
+                   pstr.mix = 0.95, i.mix = 0))  # Undetected
[1] 0.055181916 0.027590958 0.009196986
> xgrid <- 0:13  # Wider range so this detects the problem
> (ddd <- dgaitdpois(xgrid, 1, pdip.mlm = 0.1, d.mlm = 5,
+                    pstr.mix = 0.95, i.mix = 0))  # Detected
Warning in dgaitdpois(xgrid, 1, pdip.mlm = 0.1, d.mlm = 5, pstr.mix = 0.95,  :
  PMF < 0; too much deflation? NaNs produced
Warning in dgaitdpois(xgrid, 1, pdip.mlm = 0.1, d.mlm = 5, pstr.mix = 0.95,  :
  PMF > 1: too much inflation? NaNs produced
 [1]          NaN 5.518192e-02 2.759096e-02 9.196986e-03 2.299247e-03
 [6]          NaN 7.664155e-05 1.094879e-05 1.368599e-06 1.520666e-07
[11] 1.520666e-08 1.382423e-09 1.152019e-10 8.861688e-12
> sum(ddd, na.rm = TRUE)  # Something gone awry
[1] 0.09435823
> 
> 
> 
> cleanEx()
> nameEx("gaitdpoisson")
> ### * gaitdpoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaitdpoisson
> ### Title: Generally Altered, Inflated, Truncated and Deflated Poisson
> ###   Regression
> ### Aliases: gaitdpoisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D i.mix <- c(5, 10)  # Inflate these values parametrically
> ##D i.mlm <- c(14, 15)  # Inflate these values
> ##D a.mix <- c(1, 13)  # Alter these values
> ##D tvec <- c(3, 11)   # Truncate these values
> ##D pstr.mlm <- 0.1  # So parallel.i = TRUE
> ##D pobs.mix <- pstr.mix <- 0.1
> ##D max.support <- 20; set.seed(1)
> ##D gdata <- data.frame(x2 = runif(nn <- 1000))
> ##D gdata <- transform(gdata, lambda.p = exp(2 + 0.0 * x2))
> ##D gdata <- transform(gdata,
> ##D   y1 = rgaitdpois(nn, lambda.p, a.mix = a.mix, i.mix = i.mix,
> ##D                   pobs.mix = pobs.mix, pstr.mix = pstr.mix,
> ##D                   i.mlm = i.mlm, pstr.mlm = pstr.mlm,
> ##D                   truncate = tvec, max.support = max.support))
> ##D gaitdpoisson(a.mix = a.mix, i.mix = i.mix, i.mlm = i.mlm)
> ##D with(gdata, table(y1))
> ##D fit1 <- vglm(y1 ~ 1, crit = "coef", trace = TRUE, data = gdata,
> ##D              gaitdpoisson(a.mix = a.mix, i.mix = i.mix,
> ##D                           i.mlm = i.mlm, parallel.i = TRUE,
> ##D                           eq.ap = TRUE, eq.ip = TRUE, truncate =
> ##D                           tvec, max.support = max.support))
> ##D head(fitted(fit1, type.fitted = "Pstr.mix"))
> ##D head(predict(fit1))
> ##D t(coef(fit1, matrix = TRUE))  # Easier to see with t()
> ##D summary(fit1)  # No HDE test by default but HDEtest = TRUE is ideal
> ##D spikeplot(with(gdata, y1), lwd = 2)
> ##D plotdgaitd(fit1, new.plot = FALSE, offset.x = 0.2, all.lwd = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gaitdzeta")
> ### * gaitdzeta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaitdzeta
> ### Title: Generally Altered, Inflated, Truncated and Deflated Zeta
> ###   Regression
> ### Aliases: gaitdzeta
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D avec <- c(5, 10)  # Alter these values parametrically
> ##D ivec <- c(3, 15)  # Inflate these values
> ##D tvec <- c(6, 7)   # Truncate these values
> ##D set.seed(1); pobs.a <- pstr.i <- 0.1
> ##D gdata <- data.frame(x2 = runif(nn <- 1000))
> ##D gdata <- transform(gdata, shape.p = logitlink(2, inverse = TRUE))
> ##D gdata <- transform(gdata,
> ##D   y1 = rgaitdzeta(nn, shape.p, a.mix = avec, pobs.mix = pobs.a,
> ##D                   i.mix = ivec, pstr.mix = pstr.i, truncate = tvec))
> ##D gaitdzeta(a.mix = avec, i.mix = ivec)
> ##D with(gdata, table(y1))
> ##D spikeplot(with(gdata, y1), las = 1)
> ##D fit7 <- vglm(y1 ~ 1, trace = TRUE, data = gdata, crit = "coef",
> ##D              gaitdzeta(i.mix = ivec, truncate = tvec,
> ##D                        a.mix = avec, eq.ap = TRUE, eq.ip = TRUE))
> ##D head(fitted(fit7, type.fitted = "Pstr.mix"))
> ##D head(predict(fit7))
> ##D t(coef(fit7, matrix = TRUE))  # Easier to see with t()
> ##D summary(fit7)
> ##D spikeplot(with(gdata, y1), lwd = 2, ylim = c(0, 0.6), xlim = c(0, 20))
> ##D plotdgaitd(fit7, new.plot = FALSE, offset.x = 0.2, all.lwd = 2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("gaitdzetaUC")
> ### * gaitdzetaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gaitdzeta
> ### Title: Generally Altered, Inflated and Truncated and Deflated Zeta
> ###   Distribution
> ### Aliases: Gaitdzeta dgaitdzeta pgaitdzeta qgaitdzeta rgaitdzeta
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ivec <- c(2, 10); avec <- ivec + 4; shape <- 0.95; xgrid <- 0:29
> tvec <- 15; max.support <- 25; pobs.a <- 0.10; pstr.i <- 0.15
> (ddd <- dgaitdzeta(xgrid, shape, truncate = tvec,
+    max.support = max.support, pobs.mix = pobs.a,
+    a.mix = avec, pstr.mix = pstr.i, i.mix = ivec))
 [1] 0.0000000000 0.4674039610 0.2647391363 0.0548663339 0.0313094476
 [6] 0.0202628664 0.0839192325 0.0105135942 0.0081033932 0.0064404987
[11] 0.0114769540 0.0043548846 0.0036752675 0.0031441471 0.0160807675
[16] 0.0000000000 0.0020972897 0.0018634468 0.0016669056 0.0015001092
[21] 0.0013573252 0.0012341410 0.0011271148 0.0010335300 0.0009512189
[26] 0.0008784345 0.0000000000 0.0000000000 0.0000000000 0.0000000000
> ## Not run: 
> ##D plot(xgrid, ddd, type = "n", ylab = "Probability",
> ##D               xlab = "x", main = "GAIT PMF---Zeta Parent")
> ##D mylwd <- 0.5
> ##D abline(v = avec, col = 'blue', lwd = mylwd)
> ##D abline(v = ivec, col = 'purple', lwd = mylwd)
> ##D abline(v = tvec, col = 'tan', lwd = mylwd)
> ##D abline(v = max.support, col = 'magenta', lwd = mylwd)
> ##D abline(h = c(pobs.a, pstr.i, 0:1), col = 'gray', lty = "dashed")
> ##D lines(xgrid, dzeta(xgrid, shape), col='gray', lty="dashed")  # f_{\pi}
> ##D lines(xgrid, ddd, type = "h", col = "pink", lwd = 3)  # GAIT PMF
> ##D points(xgrid[ddd == 0], ddd[ddd == 0], pch = 16, col = 'tan', cex = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gamma1")
> ### * gamma1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamma1
> ### Title: 1-parameter Gamma Regression Family Function
> ### Aliases: gamma1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(y = rgamma(n = 100, shape = exp(3)))
> fit <- vglm(y ~ 1, gamma1, data = gdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 3.0504968
VGLM    linear loop  2 :  coefficients = 3.0165385
VGLM    linear loop  3 :  coefficients = 3.0165524
VGLM    linear loop  4 :  coefficients = 3.0165524
> coef(fit, matrix = TRUE)
            loglink(shape)
(Intercept)       3.016552
> Coef(fit)
   shape 
20.42077 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = gamma1, data = gdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.01655    0.02186     138   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(shape) 

Log-likelihood: -277.4002 on 99 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("gamma2")
> ### * gamma2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamma2
> ### Title: 2-parameter Gamma Regression Family Function
> ### Aliases: gamma2
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Essentially a 1-parameter gamma
> gdata <- data.frame(y = rgamma(n = 100, shape = exp(1)))
> fit1 <- vglm(y ~ 1, gamma1, data = gdata)
> fit2 <- vglm(y ~ 1, gamma2, data = gdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.2769188, 1.7879856
VGLM    linear loop  2 :  coefficients = 1.00691138, 0.49222431
VGLM    linear loop  3 :  coefficients = 0.96318280, 0.98414614
VGLM    linear loop  4 :  coefficients = 0.96219837, 1.18793106
VGLM    linear loop  5 :  coefficients = 0.96219788, 1.21442034
VGLM    linear loop  6 :  coefficients = 0.96219788, 1.21480914
VGLM    linear loop  7 :  coefficients = 0.96219788, 1.21480922
> coef(fit2, matrix = TRUE)
            loglink(mu) loglink(shape)
(Intercept)   0.9621979       1.214809
> c(Coef(fit2), colMeans(gdata))
      mu    shape        y 
2.617443 3.369651 2.617443 
> 
> # Essentially a 2-parameter gamma
> gdata <- data.frame(y = rgamma(n = 500, rate = exp(-1), shape = exp(2)))
> fit2 <- vglm(y ~ 1, gamma2, data = gdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 3.1846902, 2.6913897
VGLM    linear loop  2 :  coefficients = 3.00453120, 0.98973465
VGLM    linear loop  3 :  coefficients = 2.9862152, 1.5767509
VGLM    linear loop  4 :  coefficients = 2.9860454, 1.8675566
VGLM    linear loop  5 :  coefficients = 2.9860453, 1.9225730
VGLM    linear loop  6 :  coefficients = 2.9860453, 1.9242205
VGLM    linear loop  7 :  coefficients = 2.9860453, 1.9242219
VGLM    linear loop  8 :  coefficients = 2.9860453, 1.9242219
> coef(fit2, matrix = TRUE)
            loglink(mu) loglink(shape)
(Intercept)    2.986045       1.924222
> c(Coef(fit2), colMeans(gdata))
       mu     shape         y 
19.807197  6.849817 19.807197 
> summary(fit2)
Call:
vglm(formula = y ~ 1, family = gamma2, data = gdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  2.98605    0.01709  174.75   <2e-16 ***
(Intercept):2  1.92422    0.06177   31.15   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(mu), loglink(shape)

Log-likelihood: -1696.202 on 998 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("gammaR")
> ### * gammaR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gammaR
> ### Title: 2-parameter Gamma Regression Family Function
> ### Aliases: gammaR
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Essentially a 1-parameter gamma
> gdata <- data.frame(y1 = rgamma(n <- 100, shape =  exp(1)))
> fit1 <- vglm(y1 ~ 1, gamma1, data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -175.39341
VGLM    linear loop  2 :  loglikelihood = -168.10607
VGLM    linear loop  3 :  loglikelihood = -168.10431
VGLM    linear loop  4 :  loglikelihood = -168.10431
> fit2 <- vglm(y1 ~ 1, gammaR, data = gdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.51106682, 1.78798563
VGLM    linear loop  2 :  coefficients = -0.51468706,  0.49222431
VGLM    linear loop  3 :  coefficients = 0.020963347, 0.984146144
VGLM    linear loop  4 :  coefficients = 0.22573269, 1.18793106
VGLM    linear loop  5 :  coefficients = 0.25222246, 1.21442034
VGLM    linear loop  6 :  coefficients = 0.25261126, 1.21480914
VGLM    linear loop  7 :  coefficients = 0.25261134, 1.21480922
VGLM    linear loop  8 :  coefficients = 0.25261134, 1.21480922
> coef(fit2, matrix = TRUE)
            loglink(rate) loglink(shape)
(Intercept)     0.2526113       1.214809
> Coef(fit2)
    rate    shape 
1.287383 3.369651 
> 
> # Essentially a 2-parameter gamma
> gdata <- data.frame(y2 = rgamma(n = 500, rate = exp(1), shape = exp(2)))
> fit2 <- vglm(y2 ~ 1, gammaR, data = gdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.5066994, 2.6913897
VGLM    linear loop  2 :  coefficients = -0.014796552,  0.989734652
VGLM    linear loop  3 :  coefficients = 0.59053576, 1.57675091
VGLM    linear loop  4 :  coefficients = 0.88151123, 1.86755658
VGLM    linear loop  5 :  coefficients = 0.93652765, 1.92257298
VGLM    linear loop  6 :  coefficients = 0.93817514, 1.92422048
VGLM    linear loop  7 :  coefficients = 0.93817656, 1.92422190
VGLM    linear loop  8 :  coefficients = 0.93817656, 1.92422190
> coef(fit2, matrix = TRUE)
            loglink(rate) loglink(shape)
(Intercept)     0.9381766       1.924222
> Coef(fit2)
    rate    shape 
2.555318 6.849817 
> summary(fit2)
Call:
vglm(formula = y2 ~ 1, family = gammaR, data = gdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.93818    0.06409   14.64   <2e-16 ***
(Intercept):2  1.92422    0.06177   31.15   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(rate), loglink(shape)

Log-likelihood: -696.2016 on 998 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("gammaff.mm")
> ### * gammaff.mm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gammaff.mm
> ### Title: Multivariate Gamma Family Function: Mathai and Moschopoulos
> ###   (1992)
> ### Aliases: gammaff.mm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data("mbflood", package = "VGAMdata")
> ##D mbflood <- transform(mbflood, VdivD = V / D)
> ##D fit <- vglm(cbind(Q, y2 = Q + VdivD) ~ 1,
> ##D             gammaff.mm, trace = TRUE, data = mbflood)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D vcov(fit)
> ##D colMeans(depvar(fit))  # Check moments
> ##D head(fitted(fit), 1)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("gammahyperbola")
> ### * gammahyperbola
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gammahyperbola
> ### Title: Gamma Hyperbola Bivariate Distribution
> ### Aliases: gammahyperbola
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 1000))
> gdata <- transform(gdata, theta = exp(-2 + x2))
> gdata <- transform(gdata, y1 = rexp(nn, rate = exp(-theta)/theta),
+                           y2 = rexp(nn, rate = theta) + 1)
> fit <- vglm(cbind(y1, y2) ~ x2, gammahyperbola(expected = TRUE), data = gdata)
> coef(fit, matrix = TRUE)
            loglink(theta)
(Intercept)     -1.9876685
x2               0.9641934
> Coef(fit)
(Intercept)          x2 
 -1.9876685   0.9641934 
> head(fitted(fit))
         y1       y2
1 0.2112575 6.650063
2 0.2386594 6.098106
3 0.3020117 5.201019
4 0.4569992 4.040374
5 0.1965610 7.008698
6 0.4512927 4.069293
> summary(fit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = gammahyperbola(expected = TRUE), 
    data = gdata)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.98767    0.04086  -48.65   <2e-16 ***
x2           0.96419    0.06890   13.99   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(theta) 

Log-likelihood: -2305.268 on 998 degrees of freedom

Number of Fisher scoring iterations: 13 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("garma")
> ### * garma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: garma
> ### Title: GARMA (Generalized Autoregressive Moving-Average) Models
> ### Aliases: garma
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(interspike = c(68, 41, 82, 66, 101, 66, 57,  41,  27, 78,
+ 59, 73,  6, 44,  72, 66, 59,  60,  39, 52,
+ 50, 29, 30, 56,  76, 55, 73, 104, 104, 52,
+ 25, 33, 20, 60,  47,  6, 47,  22,  35, 30,
+ 29, 58, 24, 34,  36, 34,  6,  19,  28, 16,
+ 36, 33, 12, 26,  36, 39, 24,  14,  28, 13,
+  2, 30, 18, 17,  28,  9, 28,  20,  17, 12,
+ 19, 18, 14, 23,  18, 22, 18,  19,  26, 27,
+ 23, 24, 35, 22,  29, 28, 17,  30,  34, 17,
+ 20, 49, 29, 35,  49, 25, 55,  42,  29, 16))  # See Zeger and Qaqish (1988)
> gdata <- transform(gdata, spikenum = seq(interspike))
> bvalue <- 0.1  # .Machine$double.xmin # Boundary value
> fit <- vglm(interspike ~ 1, trace = TRUE, data = gdata,
+             garma(loglink(bvalue = bvalue),
+                   p = 2, coefstart = c(4, 0.3, 0.4)))
VGLM    linear loop  1 :  loglikelihood = -33517179
Taking a modified step.....
VGLM    linear loop  1 :  loglikelihood = 9063.765
VGLM    linear loop  2 :  loglikelihood = 9039.4617
Taking a modified step.
VGLM    linear loop  2 :  loglikelihood = 9294.1272
VGLM    linear loop  3 :  loglikelihood = 8776.2807
Taking a modified step..
VGLM    linear loop  3 :  loglikelihood = 9311.6036
VGLM    linear loop  4 :  loglikelihood = 9236.8891
Taking a modified step..
VGLM    linear loop  4 :  loglikelihood = 9325.2546
VGLM    linear loop  5 :  loglikelihood = 9303.4811
Taking a modified step..
VGLM    linear loop  5 :  loglikelihood = 9327.5058
VGLM    linear loop  6 :  loglikelihood = 9323.9082
Taking a modified step.
VGLM    linear loop  6 :  loglikelihood = 9327.635
VGLM    linear loop  7 :  loglikelihood = 9318.4243
Taking a modified step...
VGLM    linear loop  7 :  loglikelihood = 9328.5283
VGLM    linear loop  8 :  loglikelihood = 9329.2394
VGLM    linear loop  9 :  loglikelihood = 9316.0423
Taking a modified step..
VGLM    linear loop  9 :  loglikelihood = 9329.4185
VGLM    linear loop  10 :  loglikelihood = 9329.8789
VGLM    linear loop  11 :  loglikelihood = 9330.6516
VGLM    linear loop  12 :  loglikelihood = 9330.6545
VGLM    linear loop  13 :  loglikelihood = 9330.6617
VGLM    linear loop  14 :  loglikelihood = 9330.6617
> summary(fit)
Call:
vglm(formula = interspike ~ 1, family = garma(loglink(bvalue = bvalue), 
    p = 2, coefstart = c(4, 0.3, 0.4)), data = gdata, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.71331    0.04117  90.191  < 2e-16 ***
(lag1)       0.33723    0.03076  10.962  < 2e-16 ***
(lag2)       0.24180    0.02988   8.094 5.79e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(mu) 

Log-likelihood: 9330.662 on 95 degrees of freedom

Number of Fisher scoring iterations: 14 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)   3.7133065
(lag1)        0.3372322
(lag2)        0.2418008
> Coef(fit)  # A bug here
       mu      <NA>      <NA> 
40.989114  1.401064  1.273541 
> ## Not run: 
> ##D  with(gdata, plot(interspike, ylim = c(0, 120), las = 1,
> ##D      xlab = "Spike Number", ylab = "Inter-Spike Time (ms)", col = "blue"))
> ##D with(gdata, lines(spikenum[-(1:fit@misc$plag)], fitted(fit), col = "orange"))
> ##D abline(h = mean(with(gdata, interspike)), lty = "dashed", col = "gray") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genbetaII")
> ### * genbetaII
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genbetaII
> ### Title: Generalized Beta Distribution of the Second Kind
> ### Aliases: genbetaII
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D gdata <- data.frame(y = rsinmad(3000, shape1 = exp(1), scale = exp(2),
> ##D                                 shape3 = exp(1)))  # A special case!
> ##D fit <- vglm(y ~ 1, genbetaII(lss = FALSE), data = gdata, trace = TRUE)
> ##D fit <- vglm(y ~ 1, data = gdata, trace = TRUE,
> ##D             genbetaII(ishape1.a = 3, iscale = 7, ishape3.q = 2.3))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genbetaIIUC")
> ### * genbetaIIUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GenbetaII
> ### Title: The Generalized Beta II Distribution
> ### Aliases: GenbetaII dgenbetaII
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dgenbetaII(0, shape1.a = 1/4, shape2.p = 4, shape3.q = 3)
[1] 15
> dgenbetaII(0, shape1.a = 1/4, shape2.p = 2, shape3.q = 3)
[1] Inf
> dgenbetaII(0, shape1.a = 1/4, shape2.p = 8, shape3.q = 3)
[1] 0
> 
> 
> 
> cleanEx()
> nameEx("gengamma")
> ### * gengamma
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gengamma.stacy
> ### Title: Generalized Gamma distribution family function
> ### Aliases: gengamma.stacy
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D k <- exp(-1); Scale <- exp(1); dd <- exp(0.5); set.seed(1)
> ##D gdata <- data.frame(y = rgamma(2000, shape = k, scale = Scale))
> ##D gfit <- vglm(y ~ 1, gengamma.stacy, data = gdata, trace = TRUE)
> ##D coef(gfit, matrix = TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gengammaUC")
> ### * gengammaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gengammaUC
> ### Title: Generalized Gamma Distribution
> ### Aliases: gengammaUC dgengamma.stacy pgengamma.stacy qgengamma.stacy
> ###   rgengamma.stacy
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(0, 14, by = 0.01); d <- 1.5; Scale <- 2; k <- 6
> ##D plot(x, dgengamma.stacy(x, Scale, d = d, k = k), type = "l",
> ##D      col = "blue", ylim = 0:1,
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Purple are 5,10,...,95 percentiles", las = 1, ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qgengamma.stacy(seq(0.05, 0.95, by = 0.05), Scale, d = d, k = k),
> ##D       dgengamma.stacy(qgengamma.stacy(seq(0.05, 0.95, by = 0.05),
> ##D                                       Scale, d = d, k = k),
> ##D             Scale, d = d, k = k), col = "purple", lty = 3, type = "h")
> ##D lines(x, pgengamma.stacy(x, Scale, d = d, k = k), col = "orange")
> ##D abline(h = 0, lty = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genpois0UC")
> ### * genpois0UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Genpois0
> ### Title: Generalized Poisson Distribution (Original Parameterization)
> ### Aliases: Genpois0 dgenpois0 pgenpois0 qgenpois0 rgenpois0
> ### Keywords: distribution
> 
> ### ** Examples
> 
> sum(dgenpois0(0:1000, theta = 2, lambda = 0.5))
[1] 1
> ## Not run: 
> ##D theta <- 2; lambda <- 0.2; y <- 0:10
> ##D proby <- dgenpois0(y, theta = theta, lambda = lambda, log = FALSE)
> ##D plot(y, proby, type = "h", col = "blue", lwd = 2, ylab = "Pr(Y=y)",
> ##D      main = paste0("Y ~ GP-0(theta=", theta, ", lambda=",
> ##D                    lambda, ")"), las = 1, ylim = c(0, 0.3),
> ##D      sub = "Orange is the Poisson probability function")
> ##D lines(y + 0.1, dpois(y, theta), type = "h", lwd = 2, col = "orange") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genpois1UC")
> ### * genpois1UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Genpois1
> ### Title: Generalized Poisson Distribution (GP-1 and GP-2
> ###   Parameterizations of the Mean)
> ### Aliases: Genpois1 Genpois2 dgenpois1 pgenpois1 qgenpois1 rgenpois1
> ###   dgenpois2 pgenpois2 qgenpois2 rgenpois2
> ### Keywords: distribution
> 
> ### ** Examples
> 
> sum(dgenpois1(0:1000, meanpar = 5, dispind = 2))
[1] 1
> ## Not run: 
> ##D dispind <- 5; meanpar <- 5; y <- 0:15
> ##D proby <- dgenpois1(y, meanpar = meanpar, dispind)
> ##D plot(y, proby, type = "h", col = "blue", lwd = 2, ylab = "P[Y=y]",
> ##D      main = paste0("Y ~ GP-1(meanpar=", meanpar, ", dispind=",
> ##D                    dispind, ")"), las = 1, ylim = c(0, 0.3),
> ##D      sub = "Orange is the Poisson probability function")
> ##D lines(y + 0.1, dpois(y, meanpar), type = "h", lwd = 2, col = "orange") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genpoisson0")
> ### * genpoisson0
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genpoisson0
> ### Title: Generalized Poisson Regression (Original Parameterization)
> ### Aliases: genpoisson0
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 500))
> gdata <- transform(gdata, y1 = rgenpois0(nn, theta = exp(2 + x2),
+                                          logitlink(1, inverse = TRUE)))
> gfit0 <- vglm(y1 ~ x2, genpoisson0, data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2296.1724
VGLM    linear loop  2 :  loglikelihood = -2264.7653
VGLM    linear loop  3 :  loglikelihood = -2262.5441
VGLM    linear loop  4 :  loglikelihood = -2262.5345
VGLM    linear loop  5 :  loglikelihood = -2262.5345
> coef(gfit0, matrix = TRUE)
            loglink(theta) logitlink(lambda)
(Intercept)       1.922599           1.06193
x2                1.089403           0.00000
> summary(gfit0)
Call:
vglm(formula = y1 ~ x2, family = genpoisson0, data = gdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.92260    0.05354   35.91   <2e-16 ***
(Intercept):2  1.06193    0.05213   20.37   <2e-16 ***
x2             1.08940    0.07833   13.91   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(theta), logitlink(lambda)

Log-likelihood: -2262.535 on 997 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("genpoisson1")
> ### * genpoisson1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genpoisson1
> ### Title: Generalized Poisson Regression (GP-1 Parameterization)
> ### Aliases: genpoisson1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 500))
> gdata <- transform(gdata, y1 = rgenpois1(nn, exp(2 + x2),
+                                logloglink(-1, inverse = TRUE)))
> gfit1 <- vglm(y1 ~ x2, genpoisson1, gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1455.2538
VGLM    linear loop  2 :  loglikelihood = -1434.7075
VGLM    linear loop  3 :  loglikelihood = -1431.5721
VGLM    linear loop  4 :  loglikelihood = -1431.4597
VGLM    linear loop  5 :  loglikelihood = -1431.4595
VGLM    linear loop  6 :  loglikelihood = -1431.4595
> coef(gfit1, matrix = TRUE)
            loglink(meanpar) logloglink(dispind)
(Intercept)         1.972853          -0.8323405
x2                  1.062122           0.0000000
> summary(gfit1)
Call:
vglm(formula = y1 ~ x2, family = genpoisson1, data = gdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.97285    0.03570  55.255  < 2e-16 ***
(Intercept):2 -0.83234    0.14909  -5.583 2.37e-08 ***
x2             1.06212    0.05533  19.196  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(meanpar), logloglink(dispind)

Log-likelihood: -1431.459 on 997 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("genpoisson2")
> ### * genpoisson2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genpoisson2
> ### Title: Generalized Poisson Regression (GP-2 Parameterization)
> ### Aliases: genpoisson2
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 500))
> gdata <- transform(gdata, y1 = rgenpois2(nn, exp(2 + x2),
+                                loglink(-1, inverse = TRUE)))
> gfit2 <- vglm(y1 ~ x2, genpoisson2, gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1755.7539
VGLM    linear loop  2 :  loglikelihood = -1751.6171
VGLM    linear loop  3 :  loglikelihood = -1751.4703
VGLM    linear loop  4 :  loglikelihood = -1751.4702
VGLM    linear loop  5 :  loglikelihood = -1751.4702
> coef(gfit2, matrix = TRUE)
            loglink(meanpar) loglink(disppar)
(Intercept)         1.898071       -0.9816156
x2                  1.374751        0.0000000
> summary(gfit2)
Call:
vglm(formula = y1 ~ x2, family = genpoisson2, data = gdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   1.8981     0.1354   14.02  < 2e-16 ***
(Intercept):2  -0.9816     0.0506  -19.40  < 2e-16 ***
x2              1.3747     0.2680    5.13  2.9e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(meanpar), loglink(disppar)

Log-likelihood: -1751.47 on 997 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("genrayleigh")
> ### * genrayleigh
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genrayleigh
> ### Title: Generalized Rayleigh Distribution Family Function
> ### Aliases: genrayleigh
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D Scale <- exp(1); shape <- exp(1)
> ##D rdata <- data.frame(y = rgenray(n = 1000, scale = Scale, shape = shape))
> ##D fit <- vglm(y ~ 1, genrayleigh, data = rdata, trace = TRUE)
> ##D c(with(rdata, mean(y)), head(fitted(fit), 1))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("genrayleighUC")
> ### * genrayleighUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: genray
> ### Title: The Generalized Rayleigh Distribution
> ### Aliases: genray dgenray pgenray qgenray rgenray
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape <- 0.5; Scale <- 1; nn <- 501
> ##D x <- seq(-0.10, 3.0, len = nn)
> ##D plot(x, dgenray(x, shape, scale = Scale), type = "l", las = 1, ylim = c(0, 1.2),
> ##D      ylab = paste("[dp]genray(shape = ", shape, ", scale = ", Scale, ")"),
> ##D      col = "blue", cex.main = 0.8,
> ##D      main = "Blue is density, orange is cumulative distribution function",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D lines(x, pgenray(x, shape, scale = Scale), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qgenray(probs, shape, scale = Scale)
> ##D lines(Q, dgenray(Q, shape, scale = Scale), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pgenray(Q, shape, scale = Scale), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pgenray(Q, shape, scale = Scale) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gensh")
> ### * gensh
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gensh
> ### Title: Generalized Secant Hyperbolic Regression Family Function
> ### Aliases: gensh
> ### Keywords: models regression
> 
> ### ** Examples
> sh <- -pi / 2; loc <- 2
> hdata <- data.frame(x2 = rnorm(nn <- 200))
> hdata <- transform(hdata, y = rgensh(nn, sh, loc))
> fit <- vglm(y ~ x2, gensh(sh), hdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -285.58464
VGLM    linear loop  2 :  loglikelihood = -285.5438
VGLM    linear loop  3 :  loglikelihood = -285.54372
VGLM    linear loop  4 :  loglikelihood = -285.54372
> coef(fit, matrix = TRUE)
              location loglink(scale)
(Intercept) 2.06230508     0.03413661
x2          0.02962967     0.00000000
> 
> 
> 
> cleanEx()
> nameEx("genshUC")
> ### * genshUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gensh
> ### Title: Generalized Secant Hyperbolic Distribution
> ### Aliases: Gensh dgensh pgensh qgensh rgensh
> ### Keywords: distribution
> 
> ### ** Examples
> 
> x <- seq(-2, 4, by = 0.01)
> loc <- 1; shape <- -pi /2
> ## Not run: 
> ##D plot(x, dgensh(x, shape, loc), type = "l",
> ##D      main = "Blue is density, orange is the CDF",
> ##D      ylim = 0:1, las = 1, ylab = "", 
> ##D      sub = "Purple are 5, 10, ..., 95 percentiles",
> ##D      col = "blue")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qgensh((1:19) / 20, shape, loc), type = "h",
> ##D       dgensh(qgensh((1:19) / 20, shape, loc),
> ##D              shape, loc), col = "purple", lty = 3)
> ##D lines(x, pgensh(x, shape, loc), col = "orange")
> ##D abline(h = 0, lty = 2) 
> ## End(Not run)
> 
> pp <- (1:19) / 20  # Test two functions
> max(abs(pgensh(qgensh(pp, shape, loc),
+                shape,loc) - pp))  # Should be 0
[1] 1.110223e-16
> 
> 
> 
> cleanEx()
> nameEx("geometric")
> ### * geometric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: geometric
> ### Title: Geometric (Truncated and Untruncated) Distributions
> ### Aliases: geometric truncgeometric
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 1000) - 0.5)
> gdata <- transform(gdata, x3 = runif(nn) - 0.5,
+                           x4 = runif(nn) - 0.5)
> gdata <- transform(gdata, eta  = -1.0 - 1.0 * x2 + 2.0 * x3)
> gdata <- transform(gdata, prob = logitlink(eta, inverse = TRUE))
> gdata <- transform(gdata, y1 = rgeom(nn, prob))
> with(gdata, table(y1))
y1
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 
274 186 142  93  65  52  31  32  17  22  15   5  13  12   6   2   2   2   4   6 
 20  22  23  24  25  26  28  30  32  33  34  35  36  56 
  2   3   1   1   1   1   3   1   1   1   1   1   1   1 
> fit1 <- vglm(y1 ~ x2 + x3 + x4, geometric, data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2237.4573
VGLM    linear loop  2 :  loglikelihood = -2224.0736
VGLM    linear loop  3 :  loglikelihood = -2223.9405
VGLM    linear loop  4 :  loglikelihood = -2223.9404
VGLM    linear loop  5 :  loglikelihood = -2223.9404
> coef(fit1, matrix = TRUE)
            logitlink(prob)
(Intercept)      -1.0226325
x2               -0.8121932
x3                1.8649961
x4                0.1443508
> summary(fit1)
Call:
vglm(formula = y1 ~ x2 + x3 + x4, family = geometric, data = gdata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.0226     0.0379 -26.986  < 2e-16 ***
x2           -0.8122     0.1291  -6.290 3.18e-10 ***
x3            1.8650     0.1280  14.570  < 2e-16 ***
x4            0.1444     0.1276   1.131    0.258    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logitlink(prob) 

Log-likelihood: -2223.94 on 996 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> # Truncated geometric (between 0 and upper.limit)
> upper.limit <- 5
> tdata <- subset(gdata, y1 <= upper.limit)
> nrow(tdata)  # Less than nn
[1] 812
> fit2 <- vglm(y1 ~ x2 + x3 + x4, truncgeometric(upper.limit),
+              data = tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1308.6654
VGLM    linear loop  2 :  loglikelihood = -1308.3463
VGLM    linear loop  3 :  loglikelihood = -1308.3459
VGLM    linear loop  4 :  loglikelihood = -1308.3459
> coef(fit2, matrix = TRUE)
            logitlink(prob)
(Intercept)      -1.0097109
x2               -0.5231247
x3                1.7006471
x4                0.4822835
> 
> # Generalized truncated geometric (between lower.limit and upper.limit)
> lower.limit <- 1
> upper.limit <- 8
> gtdata <- subset(gdata, lower.limit <= y1 & y1 <= upper.limit)
> with(gtdata, table(y1))
y1
  1   2   3   4   5   6   7   8 
186 142  93  65  52  31  32  17 
> nrow(gtdata)  # Less than nn
[1] 618
> fit3 <- vglm(y1 - lower.limit ~ x2 + x3 + x4,
+              truncgeometric(upper.limit - lower.limit),
+              data = gtdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1111.836
VGLM    linear loop  2 :  loglikelihood = -1111.6881
VGLM    linear loop  3 :  loglikelihood = -1111.6865
VGLM    linear loop  4 :  loglikelihood = -1111.6865
VGLM    linear loop  5 :  loglikelihood = -1111.6865
> coef(fit3, matrix = TRUE)
            logitlink(prob)
(Intercept)      -0.9332431
x2               -0.8789578
x3                1.3760739
x4                0.5306081
> 
> 
> 
> cleanEx()
> nameEx("get.smart")
> ### * get.smart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: get.smart
> ### Title: Retrieve One Component of ".smart.prediction"
> ### Aliases: get.smart
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> print(sm.min1)
function (x) 
{
    x <- x
    minx <- min(x)
    if (smart.mode.is("read")) {
        smart <- get.smart()
        minx <- smart$minx
    }
    else if (smart.mode.is("write")) 
        put.smart(list(minx = minx))
    minx
}
<bytecode: 0x556f6bf06bc8>
<environment: namespace:VGAM>
attr(,"smart")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("get.smart.prediction")
> ### * get.smart.prediction
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: get.smart.prediction
> ### Title: Retrieves ".smart.prediction"
> ### Aliases: get.smart.prediction
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> ## Not run: 
> ##D fit$smart <- get.smart.prediction()  # Put at the end of lm()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gev")
> ### * gev
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gev
> ### Title: Generalized Extreme Value Regression Family Function
> ### Aliases: gev gevff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Multivariate example
> ##D fit1 <- vgam(cbind(r1, r2) ~ s(year, df = 3), gev(zero = 2:3),
> ##D              data = venice, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D head(fitted(fit1))
> ##D par(mfrow = c(1, 2), las = 1)
> ##D plot(fit1, se = TRUE, lcol = "blue", scol = "forestgreen",
> ##D      main = "Fitted mu(year) function (centered)", cex.main = 0.8)
> ##D with(venice, matplot(year, depvar(fit1)[, 1:2], ylab = "Sea level (cm)",
> ##D      col = 1:2, main = "Highest 2 annual sea levels", cex.main = 0.8))
> ##D with(venice, lines(year, fitted(fit1)[,1], lty = "dashed", col = "blue"))
> ##D legend("topleft", lty = "dashed", col = "blue", "Fitted 95 percentile")
> ##D 
> ##D # Univariate example
> ##D (fit <- vglm(maxtemp ~ 1, gevff, data = oxtemp, trace = TRUE))
> ##D head(fitted(fit))
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D vcov(fit)
> ##D vcov(fit, untransform = TRUE)
> ##D sqrt(diag(vcov(fit)))  # Approximate standard errors
> ##D rlplot(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gevUC")
> ### * gevUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gevUC
> ### Title: The Generalized Extreme Value Distribution
> ### Aliases: gevUC dgev pgev qgev rgev
> ### Keywords: distribution
> 
> ### ** Examples
>  loc <- 2; sigma <- 1; xi <- -0.4
> pgev(qgev(seq(0.05, 0.95, by = 0.05), loc, sigma, xi), loc, sigma, xi)
 [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
[16] 0.80 0.85 0.90 0.95
> ## Not run: 
> ##D  x <- seq(loc - 3, loc + 3, by = 0.01)
> ##D plot(x, dgev(x, loc, sigma, xi), type = "l", col = "blue", ylim = c(0, 1),
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Purple are 10,...,90 percentiles", ylab = "", las = 1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qgev(seq(0.1, 0.9, by = 0.1), loc, sigma, xi),
> ##D       dgev(qgev(seq(0.1, 0.9, by = 0.1), loc, sigma, xi), loc, sigma, xi),
> ##D       col = "purple", lty = 3, type = "h")
> ##D lines(x, pgev(x, loc, sigma, xi), type = "l", col = "orange")
> ##D abline(h = (0:10)/10, lty = 2, col = "gray50")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gew")
> ### * gew
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gew
> ### Title: General Electric and Westinghouse Data
> ### Aliases: gew
> ### Keywords: datasets
> 
> ### ** Examples
> 
> str(gew)
'data.frame':	20 obs. of  7 variables:
 $ year     : int  1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 ...
 $ invest.g : num  33.1 45 77.2 44.6 48.1 74.4 113 91.9 61.3 56.8 ...
 $ value.g  : num  1171 2016 2803 2040 2256 ...
 $ capital.g: num  97.8 104.4 118 156.2 172.6 ...
 $ invest.w : num  12.9 25.9 35 22.9 18.8 ...
 $ value.w  : num  192 516 729 560 520 ...
 $ capital.w: num  1.8 0.8 7.4 18.1 23.5 26.5 36.2 60.8 84.4 91.2 ...
> 
> 
> 
> cleanEx()
> nameEx("goffset")
> ### * goffset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: goffset
> ### Title: GAITD Offset for the GTE Method
> ### Aliases: goffset
> 
> ### ** Examples
> 
> i.mix <- c(5, 10, 15, 20); a.mlm <- 13; mymux <- 2
> goffset(mymux, 10, i.mix = i.mix, a.mlm = a.mlm)
           [,1] [,2]      [,3] [,4]
 [1,] 0.6931472    0 0.6931472    0
 [2,] 0.6931472    0 0.6931472    0
 [3,] 0.6931472    0 0.6931472    0
 [4,] 0.6931472    0 0.6931472    0
 [5,] 0.6931472    0 0.6931472    0
 [6,] 0.6931472    0 0.6931472    0
 [7,] 0.6931472    0 0.6931472    0
 [8,] 0.6931472    0 0.6931472    0
 [9,] 0.6931472    0 0.6931472    0
[10,] 0.6931472    0 0.6931472    0
> ## Not run: 
> ##D org1  <- with(gdata, range(y))  # Original range of the data
> ##D vglm(mymux * y ~ 1,
> ##D      offset = goffset(mymux, nrow(gdata), i.mix = i.mix, a.mlm = a.mlm),
> ##D      gaitdpoisson(a.mlm = mymux * a.mlm, i.mix = mymux * i.mix,
> ##D                   truncate = Trunc(org1, mymux)),
> ##D      data = gdata)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("gompertz")
> ### * gompertz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gompertz
> ### Title: Gompertz Regression Family Function
> ### Aliases: gompertz
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D gdata <- data.frame(x2 = runif(nn <- 1000))
> ##D gdata <- transform(gdata, eta1  = -1,
> ##D                           eta2  = -1 + 0.2 * x2,
> ##D                           ceta1 =  1,
> ##D                           ceta2 = -1 + 0.2 * x2)
> ##D gdata <- transform(gdata, shape1 = exp(eta1),
> ##D                           shape2 = exp(eta2),
> ##D                           scale1 = exp(ceta1),
> ##D                           scale2 = exp(ceta2))
> ##D gdata <- transform(gdata, y1 = rgompertz(nn, scale = scale1, shape = shape1),
> ##D                           y2 = rgompertz(nn, scale = scale2, shape = shape2))
> ##D 
> ##D fit1 <- vglm(y1 ~ 1,  gompertz, data = gdata, trace = TRUE)
> ##D fit2 <- vglm(y2 ~ x2, gompertz, data = gdata, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D Coef(fit1)
> ##D summary(fit1)
> ##D coef(fit2, matrix = TRUE)
> ##D summary(fit2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gompertzUC")
> ### * gompertzUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gompertz
> ### Title: Gompertz Distribution
> ### Aliases: Gompertz dgompertz pgompertz qgompertz rgompertz
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.01, 0.99, by = 0.01)
> Shape <- exp(1); Scale <- exp(1)
> max(abs(pgompertz(qgompertz(p = probs, Scale, shape = Shape),
+                   Scale, shape = Shape) - probs))  # Should be 0
[1] 1.665335e-16
> 
> ## Not run: 
> ##D  x <- seq(-0.1, 1.0, by = 0.001)
> ##D plot(x, dgompertz(x, Scale,shape = Shape), type = "l", las = 1,
> ##D      main = "Blue is density, orange is the CDF", col = "blue",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pgompertz(x, Scale, shape = Shape), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qgompertz(probs, Scale, shape = Shape)
> ##D lines(Q, dgompertz(Q, Scale, shape = Shape), col = "purple",
> ##D       lty = 3, type = "h")
> ##D pgompertz(Q, Scale, shape = Shape) - probs  # Should be all zero
> ##D abline(h = probs, col = "purple", lty = 3) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gpd")
> ### * gpd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpd
> ### Title: Generalized Pareto Distribution Regression Family Function
> ### Aliases: gpd
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Simulated data from an exponential distribution (xi = 0)
> Threshold <- 0.5
> gdata <- data.frame(y1 = Threshold + rexp(n = 3000, rate = 2))
> fit <- vglm(y1 ~ 1, gpd(threshold = Threshold), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1015.67888
VGLM    linear loop  2 :  loglikelihood = -1015.67888
> head(fitted(fit))
       90%      95%
1 1.688958 2.048585
2 1.688958 2.048585
3 1.688958 2.048585
4 1.688958 2.048585
5 1.688958 2.048585
6 1.688958 2.048585
> summary(depvar(fit))  # The original uncentred data
       V1        
 Min.   :0.5005  
 1st Qu.:0.6457  
 Median :0.8570  
 Mean   :1.0161  
 3rd Qu.:1.2093  
 Max.   :4.9796  
> coef(fit, matrix = TRUE)  # xi should be close to 0
            loglink(scale) logofflink(shape, offset = 0.5)
(Intercept)     -0.6646332                      -0.6867819
> Coef(fit)
      scale       shape 
0.514462212 0.003192798 
> summary(fit)
Call:
vglm(formula = y1 ~ 1, family = gpd(threshold = Threshold), data = gdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.66463    0.02586  -25.70   <2e-16 ***
(Intercept):2 -0.68678    0.03640  -18.87   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), logofflink(shape, offset = 0.5)

Log-likelihood: -1015.679 on 5998 degrees of freedom

Number of Fisher scoring iterations: 2 

No Hauck-Donner effect found in any of the estimates

> 
> head(fit@extra$threshold)  # Note the threshold is stored here
     [,1]
[1,]  0.5
[2,]  0.5
[3,]  0.5
[4,]  0.5
[5,]  0.5
[6,]  0.5
> 
> # Check the 90 percentile
> ii <- depvar(fit) < fitted(fit)[1, "90%"]
> 100 * table(ii) / sum(table(ii))  # Should be 90%
ii
    FALSE      TRUE 
 9.266667 90.733333 
> 
> # Check the 95 percentile
> ii <- depvar(fit) < fitted(fit)[1, "95%"]
> 100 * table(ii) / sum(table(ii))  # Should be 95%
ii
    FALSE      TRUE 
 4.933333 95.066667 
> 
> ## Not run: 
> ##D  plot(depvar(fit), col = "blue", las = 1,
> ##D                main = "Fitted 90% and 95% quantiles")
> ##D matlines(1:length(depvar(fit)), fitted(fit), lty = 2:3, lwd = 2) 
> ## End(Not run)
> 
> 
> # Another example
> gdata <- data.frame(x2 = runif(nn <- 2000))
> Threshold <- 0; xi <- exp(-0.8) - 0.5
> gdata <- transform(gdata, y2 = rgpd(nn, scale = exp(1 + 0.1*x2), shape = xi))
> fit <- vglm(y2 ~ x2, gpd(Threshold), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -4052.92543
VGLM    linear loop  2 :  loglikelihood = -4052.92432
VGLM    linear loop  3 :  loglikelihood = -4052.92431
> coef(fit, matrix = TRUE)
            loglink(scale) logofflink(shape, offset = 0.5)
(Intercept)     1.11494823                       -0.774308
x2             -0.09541516                        0.000000
> 
> 
> ## Not run: 
> ##D  # Nonparametric fits
> ##D # Not so recommended:
> ##D fit1 <- vgam(y2 ~ s(x2), gpd(Threshold), data = gdata, trace = TRUE)
> ##D par(mfrow = c(2, 1))
> ##D plot(fit1, se = TRUE, scol = "blue")
> ##D # More recommended:
> ##D fit2 <- vglm(y2 ~ sm.bs(x2), gpd(Threshold), data = gdata, trace = TRUE)
> ##D plot(as(fit2, "vgam"), se = TRUE, scol = "blue") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gpdUC")
> ### * gpdUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpdUC
> ### Title: The Generalized Pareto Distribution
> ### Aliases: gpdUC dgpd pgpd qgpd rgpd
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  loc <- 2; sigma <- 1; xi <- -0.4
> ##D x <- seq(loc - 0.2, loc + 3, by = 0.01)
> ##D plot(x, dgpd(x, loc, sigma, xi), type = "l", col = "blue",
> ##D      main = "Blue is density, red is the CDF", ylim = c(0, 1),
> ##D      sub = "Purple are 5,10,...,95 percentiles", ylab = "", las = 1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qgpd(seq(0.05, 0.95, by = 0.05), loc, sigma, xi),
> ##D   dgpd(qgpd(seq(0.05, 0.95, by = 0.05), loc, sigma, xi), loc, sigma, xi),
> ##D       col = "purple", lty = 3, type = "h")
> ##D lines(x, pgpd(x, loc, sigma, xi), type = "l", col = "red")
> ##D abline(h = 0, lty = 2)
> ##D 
> ##D pgpd(qgpd(seq(0.05, 0.95, by = 0.05), loc, sigma, xi), loc, sigma, xi)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("grain.us")
> ### * grain.us
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: grain.us
> ### Title: Grain Prices Data in USA
> ### Aliases: grain.us
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D cgrain <- scale(grain.us, scale = FALSE)  # Center the time series only
> ##D fit <- vglm(cgrain ~ 1, rrar(Rank = c(4, 1)),
> ##D             epsilon = 1e-3, stepsize = 0.5, trace = TRUE, maxit = 50)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("grc")
> ### * grc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: grc
> ### Title: Row-Column Interaction Models including Goodman's RC Association
> ###   Model and Unconstrained Quadratic Ordination
> ### Aliases: grc rcim uqo
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: Undergraduate enrolments at Auckland University in 1990
> fitted(grc1 <- grc(auuc))
      Commerce      Arts     SciEng       Law   Medicine
SES1 475.85997  878.2698  493.95585 161.85630 181.058072
SES2 878.46156 1863.6116 1001.20590 262.34445 203.376507
SES3 332.38295  806.8308  414.67427  87.52446  47.587562
SES4  56.29551  142.2879   72.16398  14.27479   6.977858
> summary(grc1)
Call:
rrvglm(formula = as.formula(str2), family = poissonff, data = .grc.df, 
    control = myrrcontrol, constraints = cms)

Coefficients: 
                 Estimate Std. Error  z value  Pr(>|z|)    
I(latvar.mat):1  0.671116   0.273160   2.4569 0.0070078 ** 
I(latvar.mat):2 -0.934155   0.703733  -1.3274 0.0921834 .  
I(latvar.mat):3 -3.567244   1.395441  -2.5564 0.0052888 ** 
(Intercept)      6.165124   0.041920 147.0674 < 2.2e-16 ***
Row.2            0.613049   0.039909  15.3610 < 2.2e-16 ***
Row.3           -0.358836   0.071240  -5.0370 2.365e-07 ***
Row.4           -2.134509   0.103431 -20.6369 < 2.2e-16 ***
Col.2            0.612830   0.049725  12.3243 < 2.2e-16 ***
Col.3            0.037322   0.056252   0.6635 0.2535088    
Col.4           -1.078415   0.085464 -12.6184 < 2.2e-16 ***
Col.5           -0.966306   0.086804 -11.1320 < 2.2e-16 ***
SES2             0.139269   0.042297   3.2927 0.0004962 ***
SES3             0.273996   0.085204   3.2158 0.0006505 ***
SES4             0.314407   0.125144   2.5124 0.0059962 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  5 

Names of linear predictors: loglink(E[Commerce]), loglink(E[Arts]), 
loglink(E[SciEng]), loglink(E[Law]), loglink(E[Medicine])

Residual deviance: 15.34689 on 3 degrees of freedom

Log-likelihood: -79.2795 on 3 degrees of freedom

Number of Fisher scoring iterations: 5 

> 
> grc2 <- grc(auuc, Rank = 2, Index.corner = c(2, 5))
> fitted(grc2)
      Commerce      Arts     SciEng       Law   Medicine
SES1 446.72562  898.1898  492.40616 169.30568 184.372752
SES2 935.08389 1825.5769 1003.49006 247.83345 197.015696
SES3 314.09988  818.6269  414.64701  92.03384  49.592407
SES4  47.09061  148.6064   71.45678  16.82703   8.019146
> summary(grc2)
Call:
rrvglm(formula = as.formula(str2), family = poissonff, data = .grc.df, 
    control = myrrcontrol, constraints = cms)

Coefficients: 
                   Estimate Std. Error  z value  Pr(>|z|)    
I(latvar.mat)1:1  0.7249577  0.2563932   2.8275  0.002345 ** 
I(latvar.mat)1:2  0.8416871  0.5015285   1.6782  0.046650 *  
I(latvar.mat)2:1  0.0080708  0.0984120   0.0820  0.467319    
I(latvar.mat)2:2  0.4950808  0.1670503   2.9637  0.001520 ** 
(Intercept)       6.1019446  0.0476880 127.9554 < 2.2e-16 ***
Row.2             0.7386917  0.0588087  12.5609 < 2.2e-16 ***
Row.3            -0.3522336  0.0744515  -4.7311 1.117e-06 ***
Row.4            -2.2498710  0.1576723 -14.2693 < 2.2e-16 ***
Col.2             0.6984368  0.0578205  12.0794 < 2.2e-16 ***
Col.3             0.0973593  0.0679655   1.4325  0.076003 .  
Col.4            -0.9702388  0.0918504 -10.5632 < 2.2e-16 ***
Col.5            -0.8849851  0.0874646 -10.1182 < 2.2e-16 ***
SES2:1           -0.0294217  0.0700273  -0.4201  0.337189    
SES2:2           -0.6723678  0.1167812  -5.7575 4.268e-09 ***
SES3:1            0.2594806  0.0896916   2.8930  0.001908 ** 
SES3:2           -0.9608882  0.1729631  -5.5555 1.384e-08 ***
SES4:1            0.4507911  0.1934699   2.3300  0.009902 ** 
SES4:2           -0.8852566  0.3726966  -2.3753  0.008768 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  5 

Names of linear predictors: loglink(E[Commerce]), loglink(E[Arts]), 
loglink(E[SciEng]), loglink(E[Law]), loglink(E[Medicine])

Residual deviance: 3.33405 on -2 degrees of freedom

Log-likelihood: -73.27308 on -2 degrees of freedom

Number of Fisher scoring iterations: 4 

> 
> model3 <- rcim(auuc, Rank = 1, fam = multinomial,
+                M = ncol(auuc)-1, cindex = 2:(ncol(auuc)-1), trace = TRUE)
RR-VGLM    linear loop  1 :  deviance = 20.88535
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  12.950138952 
   Alternating iteration 2 ,   Convergence criterion  =  0.01085112 
    ResSS  =  12.81112387 
   Alternating iteration 3 ,   Convergence criterion  =  0.0005406309 
    ResSS  =  12.804201523 
   Alternating iteration 4 ,   Convergence criterion  =  8.323552e-05 
    ResSS  =  12.803135847 
   Alternating iteration 5 ,   Convergence criterion  =  1.40008e-05 
    ResSS  =  12.802956596 
   Alternating iteration 6 ,   Convergence criterion  =  2.36086e-06 
    ResSS  =  12.80292637 
   Alternating iteration 7 ,   Convergence criterion  =  3.979584e-07 
    ResSS  =  12.802921275 
   Alternating iteration 8 ,   Convergence criterion  =  6.706887e-08 
    ResSS  =  12.802920416 
   Alternating iteration 9 ,   Convergence criterion  =  1.130236e-08 
    ResSS  =  12.802920271 
   Alternating iteration 10 ,   Convergence criterion  =  1.904593e-09 
    ResSS  =  12.802920247 
RR-VGLM    linear loop  2 :  deviance = 12.43317
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  12.416957469 
   Alternating iteration 2 ,   Convergence criterion  =  3.003519e-05 
    ResSS  =  12.416584534 
   Alternating iteration 3 ,   Convergence criterion  =  4.313829e-06 
    ResSS  =  12.416530972 
   Alternating iteration 4 ,   Convergence criterion  =  6.635795e-07 
    ResSS  =  12.416522732 
   Alternating iteration 5 ,   Convergence criterion  =  1.023868e-07 
    ResSS  =  12.416521461 
   Alternating iteration 6 ,   Convergence criterion  =  1.579596e-08 
    ResSS  =  12.416521265 
   Alternating iteration 7 ,   Convergence criterion  =  2.436684e-09 
    ResSS  =  12.416521235 
RR-VGLM    linear loop  3 :  deviance = 12.42705
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  12.42104318 
   Alternating iteration 2 ,   Convergence criterion  =  2.444145e-11 
    ResSS  =  12.42104318 
RR-VGLM    linear loop  4 :  deviance = 12.42705
> fitted(model3)
      Commerce      Arts    SciEng        Law   Medicine
SES1 0.2162753 0.4042486 0.2269779 0.06851827 0.08397992
SES2 0.2129750 0.4373898 0.2377631 0.06483014 0.04704205
SES3 0.1915616 0.4821370 0.2443905 0.05349167 0.02841918
SES4 0.1684297 0.5136590 0.2437366 0.04335273 0.03082192
> summary(model3)
Call:
vglm(formula = as.formula(str2), family = family, data = .rcim.df, 
    weights = if (length(weights)) weights else rep_len(1, nrow(y)), 
    control = mycontrol, offset = offset.matrix, constraints = Hlist)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -1.637e+00  1.427e+00  -1.148    0.251
Row.2       -4.939e-17  1.414e+00   0.000    1.000
Row.3       -3.310e-16  1.414e+00   0.000    1.000
Row.4       -2.137e-16  1.414e+00   0.000    1.000
Col.2        8.706e-01  1.350e+00   0.645    0.519
Col.3        1.777e-01  1.537e+00   0.116    0.908
Col.4       -1.161e+00  2.321e+00  -0.500    0.617
Col.5       -1.408e+00  2.558e+00  -0.551    0.582

Number of linear predictors:  5 

Names of linear predictors: loglink(E[Commerce]), loglink(E[Arts]), 
loglink(E[SciEng]), loglink(E[Law]), loglink(E[Medicine])

Residual deviance: 0.0776 on 12 degrees of freedom

Log-likelihood: -Inf on 12 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> # Median polish but not 100 percent reliable. Maybe call alaplace2()...
> ## Not run: 
> ##D rcim0 <- rcim(auuc, fam = alaplace1(tau = 0.5), trace=FALSE, maxit = 500)
> ##D round(fitted(rcim0), digits = 0)
> ##D round(100 * (fitted(rcim0) - auuc) / auuc, digits = 0)  # Discrepancy
> ##D depvar(rcim0)
> ##D round(coef(rcim0, matrix = TRUE), digits = 2)
> ##D Coef(rcim0, matrix = TRUE)
> ##D # constraints(rcim0)
> ##D names(constraints(rcim0))
> ##D 
> ##D # Compare with medpolish():
> ##D (med.a <- medpolish(auuc))
> ##D fv <- med.a$overall + outer(med.a$row, med.a$col, "+")
> ##D round(100 * (fitted(rcim0) - fv) / fv)  # Hopefully should be all 0s
> ## End(Not run)
> 
> 
> # Example 2: 2012 Summer Olympic Games in London
> ## Not run: 
> ##D  top10 <- head(olym12, 10)
> ##D grc1.oly12 <- with(top10, grc(cbind(gold, silver, bronze)))
> ##D round(fitted(grc1.oly12))
> ##D round(resid(grc1.oly12, type = "response"), digits = 1)  # Resp. resids
> ##D summary(grc1.oly12)
> ##D Coef(grc1.oly12)
> ## End(Not run)
> 
> 
> # Example 3: UQO; see Yee and Hadi (2014)
> ## Not run: 
> ##D n <- 100; p <- 5; S <- 10
> ##D pdata <- rcqo(n, p, S, es.opt = FALSE, eq.max = FALSE,
> ##D               eq.tol = TRUE, sd.latvar = 0.75)  # Poisson counts
> ##D true.nu <- attr(pdata, "latvar")  # The 'truth'; site scores
> ##D attr(pdata, "tolerances")  # The 'truth'; tolerances
> ##D 
> ##D Y <- Select(pdata, "y", sort = FALSE)  # Y matrix (n x S); the "y" vars
> ##D uqo.rcim1 <- rcim(Y, Rank = 1,
> ##D                   str0 = NULL,  # Delta covers entire n x M matrix
> ##D                   iindex = 1:nrow(Y),  # RRR covers the entire Y
> ##D                   has.intercept = FALSE)  # Suppress the intercept
> ##D 
> ##D # Plot 1
> ##D par(mfrow = c(2, 2))
> ##D plot(attr(pdata, "optimums"), Coef(uqo.rcim1)@A,
> ##D      col = "blue", type = "p", main = "(a) UQO optimums",
> ##D      xlab = "True optimums", ylab = "Estimated (UQO) optimums")
> ##D mylm <- lm(Coef(uqo.rcim1)@A ~ attr(pdata, "optimums"))
> ##D abline(coef = coef(mylm), col = "orange", lty = "dashed")
> ##D 
> ##D # Plot 2
> ##D fill.val <- NULL  # Choose this for the new parameterization
> ##D plot(attr(pdata, "latvar"), c(fill.val, concoef(uqo.rcim1)),
> ##D      las = 1, col = "blue", type = "p", main = "(b) UQO site scores",
> ##D      xlab = "True site scores", ylab = "Estimated (UQO) site scores" )
> ##D mylm <- lm(c(fill.val, concoef(uqo.rcim1)) ~ attr(pdata, "latvar"))
> ##D abline(coef = coef(mylm), col = "orange", lty = "dashed")
> ##D 
> ##D # Plots 3 and 4
> ##D myform <- attr(pdata, "formula")
> ##D p1ut <- cqo(myform, family = poissonff,
> ##D             eq.tol = FALSE, trace = FALSE, data = pdata)
> ##D c1ut <- cqo(Select(pdata, "y", sort = FALSE) ~ scale(latvar(uqo.rcim1)),
> ##D         family = poissonff, eq.tol = FALSE, trace = FALSE, data = pdata)
> ##D lvplot(p1ut, lcol = 1:S, y = TRUE, pcol = 1:S, pch = 1:S, pcex = 0.5,
> ##D        main = "(c) CQO fitted to the original data",
> ##D        xlab = "Estimated (CQO) site scores")
> ##D lvplot(c1ut, lcol = 1:S, y = TRUE, pcol = 1:S, pch = 1:S, pcex = 0.5,
> ##D        main = "(d) CQO fitted to the scaled UQO site scores",
> ##D        xlab = "Estimated (UQO) site scores")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gumbel")
> ### * gumbel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gumbel
> ### Title: Gumbel Regression Family Function
> ### Aliases: gumbel gumbelff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: Simulated data
> gdata <- data.frame(y1 = rgumbel(n = 1000, loc = 100, scale = exp(1)))
> fit1 <- vglm(y1 ~ 1, gumbelff(perc = NULL), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2591.3473
VGLM    linear loop  2 :  loglikelihood = -2579.2228
VGLM    linear loop  3 :  loglikelihood = -2579.0751
VGLM    linear loop  4 :  loglikelihood = -2579.0749
VGLM    linear loop  5 :  loglikelihood = -2579.0749
> coef(fit1, matrix = TRUE)
            location loglink(scale)
(Intercept) 100.0272      0.9929757
> Coef(fit1)
  location      scale 
100.027199   2.699255 
> head(fitted(fit1))
         [,1]
[1,] 101.5853
[2,] 101.5853
[3,] 101.5853
[4,] 101.5853
[5,] 101.5853
[6,] 101.5853
> with(gdata, mean(y1))
[1] 101.6092
> 
> # Example 2: Venice data
> (fit2 <- vglm(cbind(r1, r2, r3, r4, r5) ~ year, data = venice,
+               gumbel(R = 365, mpv = TRUE), trace = TRUE))
VGLM    linear loop  1 :  loglikelihood = -719.30727
VGLM    linear loop  2 :  loglikelihood = -706.68238
VGLM    linear loop  3 :  loglikelihood = -705.03014
VGLM    linear loop  4 :  loglikelihood = -705.02544
VGLM    linear loop  5 :  loglikelihood = -705.02542
VGLM    linear loop  6 :  loglikelihood = -705.02542

Call:
vglm(formula = cbind(r1, r2, r3, r4, r5) ~ year, family = gumbel(R = 365, 
    mpv = TRUE), data = venice, trace = TRUE)


Coefficients:
(Intercept):1 (Intercept):2        year:1        year:2 
-8.447828e+02 -1.495767e-01  4.912844e-01  1.378431e-03 

Degrees of Freedom: 102 Total; 98 Residual
Log-likelihood: -705.0254 
> head(fitted(fit2))
       95%      99%      MPV
1 68.07416 87.92128 108.4072
2 68.51604 88.39054 108.9047
3 68.95786 88.85977 109.4022
4 69.39961 89.32897 109.8998
5 69.84129 89.79814 110.3973
6 70.28290 90.26728 110.8949
> coef(fit2, matrix = TRUE)
                location loglink(scale)
(Intercept) -844.7827502   -0.149576688
year           0.4912844    0.001378431
> sqrt(diag(vcov(summary(fit2))))   # Standard errors
Warning in npred.vlm(object, ...) :
  contradiction in values after computing it two ways
(Intercept):1 (Intercept):2        year:1        year:2 
 1.975734e+02  7.653386e+00  1.010370e-01  3.912663e-03 
> 
> # Example 3: Try a nonparametric fit ---------------------
> # Use the entire data set, including missing values
> # Same as as.matrix(venice[, paste0("r", 1:10)]):
> Y <- Select(venice, "r", sort = FALSE)
> fit3 <- vgam(Y ~ s(year, df = 3), gumbel(R = 365, mpv = TRUE),
+              data = venice, trace = TRUE, na.action = na.pass)
VGAM  s.vam  loop  1 :  loglikelihood = -1137.5884
VGAM  s.vam  loop  2 :  loglikelihood = -1088.6181
VGAM  s.vam  loop  3 :  loglikelihood = -1079.7142
VGAM  s.vam  loop  4 :  loglikelihood = -1078.882
VGAM  s.vam  loop  5 :  loglikelihood = -1078.7252
VGAM  s.vam  loop  6 :  loglikelihood = -1078.713
VGAM  s.vam  loop  7 :  loglikelihood = -1078.7071
VGAM  s.vam  loop  8 :  loglikelihood = -1078.707
VGAM  s.vam  loop  9 :  loglikelihood = -1078.7066
VGAM  s.vam  loop  10 :  loglikelihood = -1078.7066
> depvar(fit3)[4:5, ]  # NAs used to pad the matrix
   r1  r2  r3  r4 r5 r6 r7 r8 r9 r10
4 116 113  91  91 91 89 88 88 86  81
5 115 107 105 101 93 91 NA NA NA  NA
> 
> ## Not run: 
> ##D   # Plot the component functions
> ##D par(mfrow = c(2, 3), mar = c(6, 4, 1, 2) + 0.3, xpd = TRUE)
> ##D plot(fit3, se = TRUE, lcol = "blue", scol = "limegreen", lty = 1,
> ##D      lwd = 2, slwd = 2, slty = "dashed")
> ##D 
> ##D # Quantile plot --- plots all the fitted values
> ##D qtplot(fit3, mpv = TRUE, lcol = c(1, 2, 5), tcol = c(1, 2, 5), lwd = 2,
> ##D        pcol = "blue", tadj = 0.1, ylab = "Sea level (cm)")
> ##D 
> ##D # Plot the 99 percentile only
> ##D year <- venice[["year"]]
> ##D matplot(year, Y, ylab = "Sea level (cm)", type = "n")
> ##D matpoints(year, Y, pch = "*", col = "blue")
> ##D lines(year, fitted(fit3)[, "99%"], lwd = 2, col = "orange")
> ##D 
> ##D # Check the 99 percentiles with a smoothing spline.
> ##D # Nb. (1-0.99) * 365 = 3.65 is approx. 4, meaning the 4th order
> ##D # statistic is approximately the 99 percentile.
> ##D plot(year, Y[, 4], ylab = "Sea level (cm)", type = "n",
> ##D      main = "Orange is 99 percentile, Green is a smoothing spline")
> ##D points(year, Y[, 4], pch = "4", col = "blue")
> ##D lines(year, fitted(fit3)[, "99%"], lty = 1, col = "orange")
> ##D lines(smooth.spline(year, Y[, 4], df = 4), col = "limegreen", lty = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gumbelII")
> ### * gumbelII
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gumbelII
> ### Title: Gumbel-II Regression Family Function
> ### Aliases: gumbelII
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 1000))
> gdata <- transform(gdata, heta1  = +1,
+                           heta2  = -1 + 0.1 * x2,
+                           ceta1 =  0,
+                           ceta2 =  1)
> gdata <- transform(gdata, shape1 = exp(heta1),
+                           shape2 = exp(heta2),
+                           scale1 = exp(ceta1),
+                           scale2 = exp(ceta2))
> gdata <- transform(gdata,
+                    y1 = rgumbelII(nn, scale = scale1, shape = shape1),
+                    y2 = rgumbelII(nn, scale = scale2, shape = shape2))
> 
> fit <- vglm(cbind(y1, y2) ~ x2,
+             gumbelII(zero = c(1, 2, 3)), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -5732.6701
VGLM    linear loop  2 :  loglikelihood = -5732.6544
VGLM    linear loop  3 :  loglikelihood = -5732.6544
> coef(fit, matrix = TRUE)
            loglink(scale1) loglink(shape1) loglink(scale2) loglink(shape2)
(Intercept)     -0.01664401       0.9798984       0.9225551    -0.938008236
x2               0.00000000       0.0000000       0.0000000     0.003801696
> vcov(fit)
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4
(Intercept):1  1.562103e-04 -9.647728e-05  0.000000e+00  0.0000000000
(Intercept):2 -9.647728e-05  6.079271e-04  0.000000e+00  0.0000000000
(Intercept):3  0.000000e+00  0.000000e+00  7.209476e-03 -0.0006541776
(Intercept):4  0.000000e+00  0.000000e+00 -6.541776e-04  0.0022556298
x2             0.000000e+00  0.000000e+00 -2.492828e-06 -0.0032976655
                         x2
(Intercept):1  0.000000e+00
(Intercept):2  0.000000e+00
(Intercept):3 -2.492828e-06
(Intercept):4 -3.297665e-03
x2             6.599854e-03
> summary(fit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = gumbelII(zero = c(1, 
    2, 3)), data = gdata, trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.016644   0.012498  -1.332    0.183    
(Intercept):2  0.979898   0.024656  39.743   <2e-16 ***
(Intercept):3  0.922555   0.084909  10.865   <2e-16 ***
(Intercept):4 -0.938008   0.047493 -19.750   <2e-16 ***
x2             0.003802   0.081239   0.047    0.963    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale1), loglink(shape1), loglink(scale2), 
loglink(shape2)

Log-likelihood: -5732.654 on 3995 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("gumbelIIUC")
> ### * gumbelIIUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gumbel-II
> ### Title: The Gumbel-II Distribution
> ### Aliases: Gumbel-II dgumbelII pgumbelII qgumbelII rgumbelII
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.01, 0.99, by = 0.01)
> Scale <- exp(1); Shape <- exp( 0.5);
> max(abs(pgumbelII(qgumbelII(p = probs, shape = Shape, Scale),
+                   shape = Shape, Scale) - probs))  # Should be 0
[1] 1.110223e-16
> 
> ## Not run: 
> ##D  x <- seq(-0.1, 10, by = 0.01);
> ##D plot(x, dgumbelII(x, shape = Shape, Scale), type = "l", col = "blue",
> ##D      main = "Blue is density, orange is the CDF", las = 1,
> ##D      sub = "Red lines are the 10,20,...,90 percentiles",
> ##D      ylab = "", ylim = 0:1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pgumbelII(x, shape = Shape, Scale), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qgumbelII(probs, shape = Shape, Scale)
> ##D lines(Q, dgumbelII(Q, Scale, Shape), col = "red", lty = 3, type = "h")
> ##D pgumbelII(Q, shape = Shape, Scale) - probs # Should be all zero
> ##D abline(h = probs, col = "red", lty = 3) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("gumbelUC")
> ### * gumbelUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gumbelUC
> ### Title: The Gumbel Distribution
> ### Aliases: dgumbel pgumbel qgumbel rgumbel
> ### Keywords: distribution
> 
> ### ** Examples
> 
> mu <- 1; sigma <- 2;
> y <- rgumbel(n = 100, loc = mu, scale = sigma)
> c(mean(y), mu - sigma * digamma(1))  # Sample and population means
[1] 2.166000 2.154431
> c(var(y), sigma^2 * pi^2 / 6)  # Sample and population variances
[1] 4.648932 6.579736
> 
> ## Not run: 
> ##D  x <- seq(-2.5, 3.5, by = 0.01)
> ##D loc <- 0; sigma <- 1
> ##D plot(x, dgumbel(x, loc, sigma), type = "l", col = "blue",
> ##D      main = "Blue is density, red is the CDF", ylim = c(0, 1),
> ##D      sub = "Purple are 5,10,...,95 percentiles", ylab = "", las = 1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qgumbel(seq(0.05, 0.95, by = 0.05), loc, sigma),
> ##D   dgumbel(qgumbel(seq(0.05, 0.95, by = 0.05), loc, sigma), loc, sigma),
> ##D       col = "purple", lty = 3, type = "h")
> ##D lines(x, pgumbel(x, loc, sigma), type = "l", col = "red")
> ##D abline(h = 0, lty = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("guplot")
> ### * guplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: guplot
> ### Title: Gumbel Plot
> ### Aliases: guplot guplot.default guplot.vlm
> ### Keywords: regression hplot
> 
> ### ** Examples
> ## Not run: 
> ##D guplot(rnorm(500), las = 1) -> ii
> ##D names(ii)
> ##D 
> ##D guplot(with(venice, r1), col = "blue")  # Venice sea levels data
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("has.intercept")
> ### * has.intercept
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: has.interceptvlm
> ### Title: Has a Fitted VGLM Got an Intercept Term?
> ### Aliases: has.intercept has.interceptvlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example: this is based on a glm example
> counts <- c(18,17,15,20,10,20,25,13,12)
> outcome <- gl(3, 1, 9); treatment <- gl(3, 3)
> pdata <- data.frame(counts, outcome, treatment)  # Better style
> vglm.D93 <- vglm(counts ~ outcome + treatment, poissonff, data = pdata)
> formula(vglm.D93)
counts ~ outcome + treatment
> term.names(vglm.D93)
[1] "(Intercept)" "outcome"     "treatment"  
> responseName(vglm.D93)
[1] "counts"
> has.intercept(vglm.D93)
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("hatvalues")
> ### * hatvalues
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hatvalues
> ### Title: Hat Values and Regression Deletion Diagnostics
> ### Aliases: hatvalues hatvaluesvlm hatplot hatplot.vlm dfbeta dfbetavlm
> ### Keywords: regression
> 
> ### ** Examples
> 
> # Proportional odds model, p.179, in McCullagh and Nelder (1989)
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, cumulative, data = pneumo)
> hatvalues(fit)  # n x M matrix, with positive values
  logitlink(P[Y<=1]) logitlink(P[Y<=2])
1          0.2569868          0.1700224
2          0.3424524          0.2952981
3          0.2386174          0.2398881
4          0.2154987          0.2228574
5          0.2480763          0.2342776
6          0.2668986          0.2601074
7          0.3034072          0.3414842
8          0.1613736          0.2027538
attr(,"predictors.names")
[1] "logitlink(P[Y<=1])" "logitlink(P[Y<=2])"
attr(,"ncol.X.vlm")
[1] 4
> all.equal(sum(hatvalues(fit)), fit@rank)  # Should be TRUE
[1] TRUE
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D hatplot(fit, ylim = c(0, 1), las = 1, col = "blue") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hdeff")
> ### * hdeff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hdeff
> ### Title: Hauck-Donner Effects: A Detection Test for Wald Tests
> ### Aliases: hdeff hdeff.vglm hdeff.matrix hdeff.numeric
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, data = pneumo,
+             trace = TRUE, crit = "c",  # Get some more accuracy
+             cumulative(reverse = TRUE,  parallel = TRUE))
VGLM    linear loop  1 :  coefficients = 
 -9.5689529, -10.4389146,   2.5663064
VGLM    linear loop  2 :  coefficients = 
 -9.6770689, -10.5823198,   2.5971219
VGLM    linear loop  3 :  coefficients = 
 -9.6761243, -10.5817555,   2.5968162
VGLM    linear loop  4 :  coefficients = 
 -9.6760928, -10.5817253,   2.5968065
VGLM    linear loop  5 :  coefficients = 
 -9.6760926, -10.5817251,   2.5968065
> cumulative()@infos()$hadof  # Analytical solution implemented
[1] TRUE
> hdeff(fit)
(Intercept):1 (Intercept):2           let 
         TRUE         FALSE         FALSE 
> hdeff(fit, deriv = 1)  # Analytical solution
(Intercept):1 (Intercept):2           let 
   -1.6226841     0.6352025     7.3861886 
> hdeff(fit, deriv = 2)  # It is a partial analytical solution
Warning in hdeff.vglm(object, ...) :
  2nd derivs available only when M1==1 and with trivial constraints; try setting 'fd.only = TRUE'; returning NAs
Warning in hdeff.vglm(object, ...) :
  NAs detected. Setting 'fd.only = TRUE' and making a full recursive call
                  deriv1     deriv2
(Intercept):1 -1.6226869  0.2690279
(Intercept):2  0.6351971  0.2034829
let            7.3861953 -1.6727407
> hdeff(fit, deriv = 2, se.arg = TRUE,
+       fd.only = TRUE)  # All derivatives solved numerically by FDs
                  deriv1     deriv2   SE.deriv1  SE.deriv2
(Intercept):1 -1.6226869  0.2690279 -0.43084724 0.24008144
(Intercept):2  0.6351971  0.2034829 -0.01848513 0.03182355
let            7.3861953 -1.6727407 -0.26634826 0.67098783
> 
> # 2 x 2 table of counts
> R0 <- 25; N0 <- 100  # Hauck Donner (1977) data set
> mymat <- c(N0-R0, R0, 8, 92)  # HDE present
> (mymat <- matrix(mymat, 2, 2, byrow = TRUE))
     [,1] [,2]
[1,]   75   25
[2,]    8   92
> hdeff(mymat)
[1] TRUE
> hdeff(c(mymat))  # Input is a vector
[1] TRUE
> hdeff(c(t(mymat)), byrow = TRUE)  # Reordering of the data
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("hdeffsev")
> ### * hdeffsev
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hdeffsev
> ### Title: Hauck-Donner Effect: Severity Measures
> ### Aliases: hdeffsev hdeffsev2
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> deg <- 4  # myfun is a function that approximates the HDE
> myfun <- function(x, deriv = 0) switch(as.character(deriv),
+   '0' = x^deg * exp(-x),
+   '1' = (deg * x^(deg-1) - x^deg) * exp(-x),
+   '2' = (deg*(deg-1)*x^(deg-2) - 2*deg*x^(deg-1) + x^deg)*exp(-x))
> 
> xgrid <- seq(0, 10, length = 101)
> ansm <- hdeffsev(xgrid, myfun(xgrid), myfun(xgrid, deriv = 1),
+                  myfun(xgrid, deriv = 2), allofit = TRUE)
Warning in min(which(x < COPS0 & dy >= 0)) :
  no non-missing arguments to min; returning Inf
Warning in min(which(x < COPS0 & ddy >= 0)) :
  no non-missing arguments to min; returning Inf
> digg <- 4
> cbind(severity = ansm$sev, 
+       fun      = round(myfun(xgrid), digg),
+       deriv1   = round(myfun(xgrid, deriv = 1), digg),
+       deriv2   = round(myfun(xgrid, deriv = 2), digg),
+       zderiv1  = round(1 + (myfun(xgrid, deriv = 1))^2 +
+                        myfun(xgrid, deriv = 2) * myfun(xgrid), digg))
       severity  fun      deriv1    deriv2    zderiv1  
  [1,] "None"    "0"      "0"       "0"       "1"      
  [2,] "None"    "1e-04"  "0.0035"  "0.1014"  "1"      
  [3,] "None"    "0.0013" "0.0249"  "0.3419"  "1.0011" 
  [4,] "None"    "0.006"  "0.074"   "0.6461"  "1.0094" 
  [5,] "None"    "0.0172" "0.1544"  "0.961"   "1.0403" 
  [6,] "None"    "0.0379" "0.2654"  "1.251"   "1.1178" 
  [7,] "None"    "0.0711" "0.403"   "1.4936"  "1.2687" 
  [8,] "None"    "0.1192" "0.5621"  "1.6765"  "1.5158" 
  [9,] "None"    "0.184"  "0.7362"  "1.7944"  "1.8722" 
 [10,] "None"    "0.2668" "0.9188"  "1.8475"  "2.337"  
 [11,] "None"    "0.3679" "1.1036"  "1.8394"  "2.8947" 
 [12,] "None"    "0.4874" "1.2848"  "1.7762"  "3.5165" 
 [13,] "None"    "0.6246" "1.4573"  "1.6655"  "4.1639" 
 [14,] "None"    "0.7784" "1.6166"  "1.5153"  "4.793"  
 [15,] "None"    "0.9473" "1.7593"  "1.334"   "5.3589" 
 [16,] "None"    "1.1296" "1.8827"  "1.1296"  "5.8204" 
 [17,] "None"    "1.3231" "1.9847"  "0.9097"  "6.1427" 
 [18,] "None"    "1.5258" "2.0643"  "0.6811"  "6.3005" 
 [19,] "None"    "1.7352" "2.1209"  "0.4499"  "6.2787" 
 [20,] "None"    "1.9492" "2.1544"  "0.2214"  "6.0728" 
 [21,] "None"    "2.1654" "2.1654"  "0"       "5.6888" 
 [22,] "None"    "2.3815" "2.1547"  "-0.2106" "5.1413" 
 [23,] "None"    "2.5956" "2.1237"  "-0.4076" "4.4522" 
 [24,] "None"    "2.8057" "2.0737"  "-0.5887" "3.6487" 
 [25,] "None"    "3.0098" "2.0065"  "-0.7525" "2.7615" 
 [26,] "None"    "3.2064" "1.9239"  "-0.8978" "1.8225" 
 [27,] "None"    "3.3941" "1.8276"  "-1.0243" "0.8637" 
 [28,] "None"    "3.5716" "1.7196"  "-1.1317" "-0.0849"
 [29,] "None"    "3.7377" "1.6019"  "-1.2205" "-0.9958"
 [30,] "None"    "3.8917" "1.4762"  "-1.2911" "-1.8454"
 [31,] "None"    "4.0328" "1.3443"  "-1.3443" "-2.614" 
 [32,] "None"    "4.1604" "1.2079"  "-1.381"  "-3.2867"
 [33,] "None"    "4.2742" "1.0686"  "-1.4025" "-3.8527"
 [34,] "None"    "4.3741" "0.9278"  "-1.4098" "-4.3058"
 [35,] "None"    "4.4598" "0.787"   "-1.4043" "-4.6435"
 [36,] "None"    "4.5315" "0.6474"  "-1.3872" "-4.867" 
 [37,] "None"    "4.5893" "0.5099"  "-1.3598" "-4.9806"
 [38,] "None"    "4.6336" "0.3757"  "-1.3234" "-4.9909"
 [39,] "None"    "4.6646" "0.2455"  "-1.2792" "-4.9068"
 [40,] "None"    "4.6828" "0.1201"  "-1.2284" "-4.7382"
 [41,] "Extreme" "4.6888" "0"       "-1.1722" "-4.4962"
 [42,] "Extreme" "4.683"  "-0.1142" "-1.1116" "-4.1924"
 [43,] "Extreme" "4.6662" "-0.2222" "-1.0475" "-3.8385"
 [44,] "Extreme" "4.6388" "-0.3236" "-0.981"  "-3.4457"
 [45,] "Extreme" "4.6017" "-0.4183" "-0.9127" "-3.0251"
 [46,] "Extreme" "4.5554" "-0.5062" "-0.8436" "-2.5867"
 [47,] "Extreme" "4.5007" "-0.587"  "-0.7742" "-2.1399"
 [48,] "Extreme" "4.4382" "-0.661"  "-0.7052" "-1.6929"
 [49,] "Extreme" "4.3687" "-0.7281" "-0.6371" "-1.2531"
 [50,] "Extreme" "4.2928" "-0.7885" "-0.5703" "-0.8267"
 [51,] "Extreme" "4.2112" "-0.8422" "-0.5053" "-0.4187"
 [52,] "Extreme" "4.1246" "-0.8896" "-0.4424" "-0.0334"
 [53,] "Extreme" "4.0335" "-0.9308" "-0.3819" "0.3261" 
 [54,] "Extreme" "3.9386" "-0.9661" "-0.3239" "0.6576" 
 [55,] "Extreme" "3.8405" "-0.9957" "-0.2687" "0.9595" 
 [56,] "Extreme" "3.7397" "-1.0199" "-0.2163" "1.2312" 
 [57,] "Extreme" "3.6367" "-1.039"  "-0.167"  "1.4723" 
 [58,] "Extreme" "3.532"  "-1.0534" "-0.1207" "1.6835" 
 [59,] "Extreme" "3.4261" "-1.0633" "-0.0774" "1.8654" 
 [60,] "Extreme" "3.3195" "-1.069"  "-0.0372" "2.0193" 
 [61,] "Extreme" "3.2125" "-1.0708" "0"       "2.1467" 
 [62,] "Extreme" "3.1054" "-1.0691" "0.0342"  "2.2492" 
 [63,] "Extreme" "2.9988" "-1.0641" "0.0655"  "2.3288" 
 [64,] "Extreme" "2.8927" "-1.0561" "0.094"   "2.3873" 
 [65,] "Extreme" "2.7876" "-1.0454" "0.1198"  "2.4267" 
 [66,] "Extreme" "2.6837" "-1.0322" "0.1429"  "2.449"  
 [67,] "Extreme" "2.5813" "-1.0169" "0.1636"  "2.4562" 
 [68,] "Extreme" "2.4804" "-0.9996" "0.1818"  "2.4501" 
 [69,] "Extreme" "2.3814" "-0.9806" "0.1978"  "2.4325" 
 [70,] "Extreme" "2.2844" "-0.9601" "0.2116"  "2.4051" 
 [71,] "Extreme" "2.1894" "-0.9383" "0.2234"  "2.3696" 
 [72,] "Extreme" "2.0967" "-0.9155" "0.2333"  "2.3273" 
 [73,] "Extreme" "2.0064" "-0.8917" "0.2415"  "2.2797" 
 [74,] "Extreme" "1.9184" "-0.8672" "0.248"   "2.2279" 
 [75,] "Extreme" "1.8329" "-0.8422" "0.253"   "2.1731" 
 [76,] "Extreme" "1.75"   "-0.8167" "0.2567"  "2.1161" 
 [77,] "Extreme" "1.6696" "-0.7909" "0.259"   "2.0579" 
 [78,] "Extreme" "1.5918" "-0.7649" "0.2602"  "1.9992" 
 [79,] "Extreme" "1.5166" "-0.7389" "0.2603"  "1.9406" 
 [80,] "Extreme" "1.444"  "-0.7129" "0.2594"  "1.8828" 
 [81,] "Extreme" "1.3741" "-0.687"  "0.2576"  "1.826"  
 [82,] "Extreme" "1.3066" "-0.6614" "0.2551"  "1.7708" 
 [83,] "Extreme" "1.2418" "-0.636"  "0.2519"  "1.7173" 
 [84,] "Extreme" "1.1794" "-0.611"  "0.2481"  "1.6659" 
 [85,] "Extreme" "1.1196" "-0.5864" "0.2437"  "1.6167" 
 [86,] "Extreme" "1.0621" "-0.5623" "0.2389"  "1.5699" 
 [87,] "Extreme" "1.0071" "-0.5387" "0.2337"  "1.5255" 
 [88,] "Extreme" "0.9544" "-0.5156" "0.2281"  "1.4835" 
 [89,] "Extreme" "0.9039" "-0.4931" "0.2222"  "1.444"  
 [90,] "Extreme" "0.8557" "-0.4711" "0.2162"  "1.407"  
 [91,] "Extreme" "0.8097" "-0.4498" "0.2099"  "1.3723" 
 [92,] "Extreme" "0.7657" "-0.4292" "0.2035"  "1.34"   
 [93,] "Extreme" "0.7238" "-0.4091" "0.197"   "1.31"   
 [94,] "Extreme" "0.6839" "-0.3897" "0.1905"  "1.2822" 
 [95,] "Extreme" "0.6459" "-0.371"  "0.1839"  "1.2564" 
 [96,] "Extreme" "0.6097" "-0.353"  "0.1773"  "1.2327" 
 [97,] "Extreme" "0.5753" "-0.3356" "0.1708"  "1.2108" 
 [98,] "Extreme" "0.5425" "-0.3188" "0.1643"  "1.1908" 
 [99,] "Extreme" "0.5115" "-0.3027" "0.1578"  "1.1724" 
[100,] "Extreme" "0.482"  "-0.2872" "0.1515"  "1.1555" 
[101,] "Extreme" "0.454"  "-0.2724" "0.1453"  "1.1402" 
> 
> 
> 
> cleanEx()
> nameEx("hormone")
> ### * hormone
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hormone
> ### Title: Hormone Assay Data
> ### Aliases: hormone
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(hormone)
> ##D summary(hormone)
> ##D 
> ##D modelI <-rrvglm(Y ~ 1 + X, data = hormone, trace = TRUE,
> ##D                 uninormal(zero = NULL, lsd = "identitylink", imethod = 2))
> ##D 
> ##D # Alternative way to fit modelI
> ##D modelI.other <- vglm(Y ~ 1 + X, data = hormone, trace = TRUE,
> ##D                      uninormal(zero = NULL, lsd = "identitylink"))
> ##D 
> ##D # Inferior to modelI
> ##D modelII <- vglm(Y ~ 1 + X, data = hormone, trace = TRUE,
> ##D                 family = uninormal(zero = NULL))
> ##D 
> ##D logLik(modelI)
> ##D logLik(modelII)  # Less than logLik(modelI)
> ##D 
> ##D 
> ##D # Reproduce the top 3 equations on p.65 of Carroll and Ruppert (1988).
> ##D # They are called Equations (1)--(3) here.
> ##D 
> ##D # Equation (1)
> ##D hormone <- transform(hormone, rX = 1 / X)
> ##D clist <- list("(Intercept)" = diag(2), X = diag(2), rX = rbind(0, 1))
> ##D fit1 <- vglm(Y ~ 1 + X + rX, family = uninormal(zero = NULL),
> ##D              constraints = clist, data = hormone, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D summary(fit1)  # Actually, the intercepts do not seem significant
> ##D plot(Y ~ X, hormone, col = "blue")
> ##D lines(fitted(fit1) ~ X, hormone, col = "orange")
> ##D 
> ##D # Equation (2)
> ##D fit2 <- rrvglm(Y ~ 1 + X, uninormal(zero = NULL), hormone, trace = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ##D plot(Y ~ X, hormone, col = "blue")
> ##D lines(fitted(fit2) ~ X, hormone, col = "red")
> ##D # Add +- 2 SEs
> ##D lines(fitted(fit2) + 2 * exp(predict(fit2)[, "loglink(sd)"]) ~ X,
> ##D       hormone, col = "orange")
> ##D lines(fitted(fit2) - 2 * exp(predict(fit2)[, "loglink(sd)"]) ~ X,
> ##D       hormone, col = "orange")
> ##D 
> ##D # Equation (3)
> ##D # Does not fit well because the loglink link for the mean is not good.
> ##D fit3 <- rrvglm(Y ~ 1 + X, maxit = 300, data = hormone, trace = TRUE,
> ##D                uninormal(lmean = "loglink", zero = NULL))
> ##D coef(fit3, matrix = TRUE)
> ##D plot(Y ~ X, hormone, col = "blue")  # Does not look okay.
> ##D lines(exp(predict(fit3)[, 1]) ~ X, hormone, col = "red")
> ##D # Add +- 2 SEs
> ##D lines(fitted(fit3) + 2 * exp(predict(fit3)[, "loglink(sd)"]) ~ X,
> ##D       hormone, col = "orange")
> ##D lines(fitted(fit3) - 2 * exp(predict(fit3)[, "loglink(sd)"]) ~ X,
> ##D       hormone, col = "orange")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hspider")
> ### * hspider
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hspider
> ### Title: Hunting Spider Data
> ### Aliases: hspider
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(hspider)
    WaterCon         BareSand        FallTwig        CoveMoss     
 Min.   :0.9555   Min.   :0.000   Min.   :0.000   Min.   :0.0000  
 1st Qu.:2.1040   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.6931  
 Median :2.6494   Median :0.000   Median :0.000   Median :1.7918  
 Mean   :2.4713   Mean   :1.129   Mean   :1.529   Mean   :2.1145  
 3rd Qu.:3.0922   3rd Qu.:2.560   3rd Qu.:4.296   3rd Qu.:3.7424  
 Max.   :3.5175   Max.   :4.511   Max.   :4.605   Max.   :4.3307  
    CoveHerb         ReflLux          Alopacce         Alopcune     
 Min.   :0.6931   Min.   :0.0000   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:3.0445   1st Qu.:0.9972   1st Qu.: 0.000   1st Qu.: 0.000  
 Median :3.4340   Median :2.6492   Median : 2.000   Median : 1.000  
 Mean   :3.2550   Mean   :2.3618   Mean   : 6.214   Mean   : 5.393  
 3rd Qu.:4.4684   3rd Qu.:3.6889   3rd Qu.:12.000   3rd Qu.: 6.250  
 Max.   :4.6151   Max.   :4.3820   Max.   :29.000   Max.   :43.000  
    Alopfabr         Arctlute          Arctperi         Auloalbi     
 Min.   : 0.000   Min.   : 0.0000   Min.   : 0.000   Min.   : 0.000  
 1st Qu.: 0.000   1st Qu.: 0.0000   1st Qu.: 0.000   1st Qu.: 0.000  
 Median : 0.000   Median : 0.0000   Median : 0.000   Median : 0.000  
 Mean   : 3.464   Mean   : 0.9286   Mean   : 1.393   Mean   : 4.643  
 3rd Qu.: 3.000   3rd Qu.: 0.2500   3rd Qu.: 0.000   3rd Qu.: 6.250  
 Max.   :20.000   Max.   :12.0000   Max.   :18.000   Max.   :30.000  
    Pardlugu         Pardmont        Pardnigr        Pardpull     
 Min.   : 0.000   Min.   : 0.00   Min.   :  0.0   Min.   :  0.00  
 1st Qu.: 0.000   1st Qu.: 0.75   1st Qu.:  0.0   1st Qu.:  0.00  
 Median : 1.000   Median : 4.50   Median :  1.0   Median :  0.50  
 Mean   : 4.536   Mean   :16.04   Mean   : 14.5   Mean   : 20.79  
 3rd Qu.: 3.500   3rd Qu.:22.50   3rd Qu.: 15.0   3rd Qu.: 39.00  
 Max.   :55.000   Max.   :96.00   Max.   :135.0   Max.   :105.00  
    Trocterr         Zoraspin     
 Min.   :  0.00   Min.   : 0.000  
 1st Qu.:  2.00   1st Qu.: 0.000  
 Median : 22.50   Median : 2.000  
 Mean   : 34.68   Mean   : 6.607  
 3rd Qu.: 63.50   3rd Qu.: 6.750  
 Max.   :118.00   Max.   :34.000  
> 
> ## Not run: 
> ##D # Standardize the environmental variables:
> ##D hspider[, 1:6] <- scale(subset(hspider, select = WaterCon:ReflLux))
> ##D 
> ##D # Fit a rank-1 binomial CAO
> ##D hsbin <- hspider  # Binary species data
> ##D hsbin[, -(1:6)] <- as.numeric(hsbin[, -(1:6)] > 0)
> ##D set.seed(123)
> ##D ahsb1 <- cao(cbind(Alopcune, Arctlute, Auloalbi, Zoraspin) ~
> ##D              WaterCon + ReflLux,
> ##D              family = binomialff(multiple.responses = TRUE),
> ##D              df1.nl = 2.2, Bestof = 3, data = hsbin)
> ##D par(mfrow = 2:1, las = 1)
> ##D lvplot(ahsb1, type = "predictors", llwd = 2,
> ##D        ylab = "logitlink(p)", lcol = 1:9)
> ##D persp(ahsb1, rug = TRUE, col = 1:10, lwd = 2)
> ##D coef(ahsb1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("huber")
> ### * huber
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: huber2
> ### Title: Huber's Least Favourable Distribution Family Function
> ### Aliases: huber2 huber1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(1231); NN <- 30; coef1 <- 1; coef2 <- 10
> hdata <- data.frame(x2 = sort(runif(NN)))
> hdata <- transform(hdata, y  = rhuber(NN, mu = coef1 + coef2 * x2))
> 
> hdata$x2[1] <- 0.0  # Add an outlier
> hdata$y[1] <- 10
> 
> fit.huber2 <- vglm(y ~ x2, huber2(imethod = 3), hdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -67.24962
VGLM    linear loop  2 :  loglikelihood = -61.145177
VGLM    linear loop  3 :  loglikelihood = -59.955971
VGLM    linear loop  4 :  loglikelihood = -59.836191
VGLM    linear loop  5 :  loglikelihood = -59.82747
VGLM    linear loop  6 :  loglikelihood = -59.826981
VGLM    linear loop  7 :  loglikelihood = -59.826955
VGLM    linear loop  8 :  loglikelihood = -59.826954
VGLM    linear loop  9 :  loglikelihood = -59.826954
> fit.huber1 <- vglm(y ~ x2, huber1(imethod = 3), hdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -71.478426
VGLM    linear loop  2 :  loglikelihood = -61.066324
VGLM    linear loop  3 :  loglikelihood = -60.041872
VGLM    linear loop  4 :  loglikelihood = -59.957341
VGLM    linear loop  5 :  loglikelihood = -59.955772
VGLM    linear loop  6 :  loglikelihood = -59.955742
VGLM    linear loop  7 :  loglikelihood = -59.955742
> 
> coef(fit.huber2, matrix = TRUE)
            location loglink(scale)
(Intercept) 1.465782     0.08756576
x2          8.624609     0.00000000
> summary(fit.huber2)
Call:
vglm(formula = y ~ x2, family = huber2(imethod = 3), data = hdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.46578    0.49572   2.957  0.00311 ** 
(Intercept):2  0.08757    0.16225   0.540  0.58941    
x2             8.62461    0.90399   9.541  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: location, loglink(scale)

Log-likelihood: -59.827 on 57 degrees of freedom

Number of Fisher scoring iterations: 9 

No Hauck-Donner effect found in any of the estimates

> 
> 
> ## Not run: 
> ##D  # Plot the results
> ##D plot(y ~ x2, data = hdata, col = "blue", las = 1)
> ##D lines(fitted(fit.huber2) ~ x2, data = hdata, col = "darkgreen", lwd = 2)
> ##D 
> ##D fit.lm <- lm(y ~ x2, hdata)  # Compare to a LM:
> ##D lines(fitted(fit.lm) ~ x2, data = hdata, col = "lavender", lwd = 3)
> ##D 
> ##D # Compare to truth:
> ##D lines(coef1 + coef2 * x2 ~ x2, data = hdata, col = "orange",
> ##D       lwd = 2, lty = "dashed")
> ##D 
> ##D legend("bottomright", legend = c("truth", "huber", "lm"),
> ##D        col = c("orange", "darkgreen", "lavender"),
> ##D        lty = c("dashed", "solid", "solid"), lwd = c(2, 2, 3)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("huberUC")
> ### * huberUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dhuber
> ### Title: Huber's Least Favourable Distribution
> ### Aliases: dhuber edhuber rhuber qhuber phuber
> ### Keywords: distribution
> 
> ### ** Examples
> 
> set.seed(123456)
> edhuber(1:5, k = 1.5)
$val
[1] 0.232871117 0.058879039 0.013137689 0.002931415 0.000654087

$eps
[1] 0.03760623

> rhuber(5)
[1]  0.6909149 -0.5883181 -1.4595939 -1.2289721 -1.3205611
> 
> ## Not run: 
> ##D  mu <- 3; xx <- seq(-2, 7, len = 100)  # Plot CDF and PDF
> ##D plot(xx, dhuber(xx, mu = mu), type = "l", col = "blue", las = 1,
> ##D      main = "blue is density, orange is the CDF", ylab = "",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      ylim = 0:1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(xx, phuber(xx, mu = mu), type = "l", col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qhuber(probs, mu = mu)
> ##D lines(Q, dhuber(Q, mu = mu), col = "purple", lty = 3, type = "h")
> ##D lines(Q, phuber(Q, mu = mu), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D phuber(Q, mu = mu) - probs  # Should be all 0s
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hunua")
> ### * hunua
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hunua
> ### Title: Hunua Ranges Data
> ### Aliases: hunua
> ### Keywords: datasets
> 
> ### ** Examples
> 
> # Fit a GAM using vgam() and compare it with the Waitakere Ranges one
> fit.h <- vgam(agaaus ~ s(altitude, df = 2), binomialff, data = hunua)
> ## Not run: 
> ##D plot(fit.h, se = TRUE, lcol = "orange", scol = "orange",
> ##D      llwd = 2, slwd = 2, main = "Orange is Hunua, Blue is Waitakere") 
> ## End(Not run)
> head(predict(fit.h, hunua, type = "response"))
[1] 0.2138857 0.2069122 0.1844355 0.1767492 0.1767492 0.1844355
> 
> fit.w <- vgam(agaaus ~ s(altitude, df = 2), binomialff, data = waitakere)
> ## Not run: 
> ##D plot(fit.w, se = TRUE, lcol = "blue", scol = "blue", add = TRUE) 
> ## End(Not run)
> head(predict(fit.w, hunua, type = "response"))   # Same as above?
[1] 0.2704341 0.2631798 0.2396424 0.2316180 0.2316180 0.2396424
> 
> 
> 
> cleanEx()
> nameEx("hurea")
> ### * hurea
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hurea
> ### Title: Husler-Reiss Angular Surface Distribution Family Function
> ### Aliases: hurea
> ### Keywords: models regression
> 
> ### ** Examples
> nn <- 100; set.seed(1)
> hdata <- data.frame(x2 = runif(nn))
> hdata <-
+   transform(hdata,  # Cannot generate proper random variates!
+     y1 = rbeta(nn, shape1 = 0.5, shape2 = 0.5),  # "U" shaped
+     y2 = rnorm(nn, 0.65, sd = exp(-3 - 4 * x2)))
> # Multiple responses:
> hfit <- vglm(cbind(y1, y2) ~ x2, hurea, hdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 73.813735
VGLM    linear loop  2 :  loglikelihood = 74.08741
VGLM    linear loop  3 :  loglikelihood = 74.087877
VGLM    linear loop  4 :  loglikelihood = 74.087877
> coef(hfit, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)       0.1754818      1.15637460
x2               -0.4697071      0.08442861
> summary(hfit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = hurea, data = hdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.17548    0.09127   1.923  0.05451 .  
(Intercept):2  1.15637    0.14183   8.153 3.54e-16 ***
x2:1          -0.46971    0.14677  -3.200  0.00137 ** 
x2:2           0.08443    0.24425   0.346  0.72959    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2)

Log-likelihood: 74.0879 on 196 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("hureaUC")
> ### * hureaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Hurea
> ### Title: The Husler-Reiss Angular Surface Distribution
> ### Aliases: Hurea dhurea
> ### Keywords: distribution
> 
> ### ** Examples
> 
> integrate(dhurea, 0, 1, shape = 0.20)  # Incorrect
1.426387e-05 with absolute error < 2.4e-05
> integrate(dhurea, 0, 1, shape = 0.35)  # struggling but okay
1.000074 with absolute error < 5.3e-05
> ## Not run: 
> ##D x <- seq(0, 1, length = 501)
> ##D par(mfrow = c(2, 2))
> ##D plot(x, dhurea(x, 0.7), col = "blue", type = "l")
> ##D plot(x, dhurea(x, 1.1), col = "blue", type = "l")
> ##D plot(x, dhurea(x, 1.4), col = "blue", type = "l")
> ##D plot(x, dhurea(x, 3.0), col = "blue", type = "l")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hyperg")
> ### * hyperg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hyperg
> ### Title: Hypergeometric Family Function
> ### Aliases: hyperg
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 100
> m <- 5  # Number of white balls in the population
> k <- rep(4, len = nn)  # Sample sizes
> n <- 4  # Number of black balls in the population
> y  <- rhyper(nn = nn, m = m, n = n, k = k)
> yprop <- y / k  # Sample proportions
> 
> # N is unknown, D is known. Both models are equivalent:
> fit <- vglm(cbind(y,k-y) ~ 1, hyperg(D = m), trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 0.28377803
VGLM    linear loop  2 :  coefficients = 0.22696738
VGLM    linear loop  3 :  coefficients = 0.22478284
VGLM    linear loop  4 :  coefficients = 0.22478008
VGLM    linear loop  5 :  coefficients = 0.22478008
> fit <- vglm(yprop ~ 1, hyperg(D = m), weight = k, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 0.28377803
VGLM    linear loop  2 :  coefficients = 0.22696738
VGLM    linear loop  3 :  coefficients = 0.22478284
VGLM    linear loop  4 :  coefficients = 0.22478008
VGLM    linear loop  5 :  coefficients = 0.22478008
> 
> # N is known, D is unknown. Both models are equivalent:
> fit <- vglm(cbind(y, k-y) ~ 1, hyperg(N = m+n), trace = TRUE, crit = "l")
VGLM    linear loop  1 :  loglikelihood = 536.94438
VGLM    linear loop  2 :  loglikelihood = 537.01903
VGLM    linear loop  3 :  loglikelihood = 537.01903
> fit <- vglm(yprop ~ 1, hyperg(N = m+n), weight = k, trace = TRUE, crit = "l")
VGLM    linear loop  1 :  loglikelihood = 536.94438
VGLM    linear loop  2 :  loglikelihood = 537.01903
VGLM    linear loop  3 :  loglikelihood = 537.01903
> 
> coef(fit, matrix = TRUE)
            logitlink(prob)
(Intercept)       0.1985816
> Coef(fit)  # Should be equal to the true population proportion
     prob 
0.5494829 
> unique(m / (m+n))  # The true population proportion
[1] 0.5555556
> fit@extra
$Nvector
[1] 9

$Nunknown
[1] FALSE

> head(fitted(fit))
       [,1]
1 0.5494829
2 0.5494829
3 0.5494829
4 0.5494829
5 0.5494829
6 0.5494829
> summary(fit)
Call:
vglm(formula = yprop ~ 1, family = hyperg(N = m + n), weights = k, 
    trace = TRUE, crit = "l")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.19858    0.04071   4.878 1.07e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logitlink(prob) 

Log-likelihood: 537.019 on 99 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("hypersecant")
> ### * hypersecant
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hypersecant
> ### Title: Hyperbolic Secant Regression Family Function
> ### Aliases: hypersecant hypersecant01 nef.hs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> hdata <- data.frame(x2 = rnorm(nn <- 200))
> hdata <- transform(hdata, y = rnorm(nn))  # Not very good data!
> fit1 <- vglm(y ~ x2, hypersecant, hdata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients =  0.052785489, -0.029383427
VGLM    linear loop  2 :  coefficients =  0.052725933, -0.029299492
VGLM    linear loop  3 :  coefficients =  0.052726087, -0.029299592
VGLM    linear loop  4 :  coefficients =  0.052726087, -0.029299591
> coef(fit1, matrix = TRUE)
            extlogitlink(theta, min = -1.5707963267949, max = 1.5707963267949)
(Intercept)                                                         0.05272609
x2                                                                 -0.02929959
> fit1@misc$earg
$theta
$theta$theta


$theta$min
-pi/2

$theta$max
pi/2

$theta$bminvalue
NULL

$theta$bmaxvalue
NULL

$theta$inverse
[1] FALSE

$theta$deriv
[1] 0

$theta$short
[1] TRUE

$theta$tag
[1] FALSE

attr(,"function.name")
[1] "extlogitlink"

> 
> # Not recommended:
> fit2 <- vglm(y ~ x2, hypersecant(link = "identitylink"), hdata)
> coef(fit2, matrix = TRUE)
                  theta
(Intercept)  0.04141608
x2          -0.02303395
> fit2@misc$earg
$theta
$theta$theta


$theta$inverse
[1] FALSE

$theta$deriv
[1] 0

$theta$short
[1] TRUE

$theta$tag
[1] FALSE

attr(,"function.name")
[1] "identitylink"

> 
> 
> 
> cleanEx()
> nameEx("hzeta")
> ### * hzeta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hzeta
> ### Title: Haight's Zeta Family Function
> ### Aliases: hzeta
> ### Keywords: models regression
> 
> ### ** Examples
> 
> shape <- exp(exp(-0.1))  # The parameter
> hdata <- data.frame(y = rhzeta(n = 1000, shape))
> fit <- vglm(y ~ 1, hzeta, data = hdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -0.020694033
VGLM    linear loop  2 :  coefficients = -0.078018908
VGLM    linear loop  3 :  coefficients = -0.084497667
VGLM    linear loop  4 :  coefficients = -0.084148629
VGLM    linear loop  5 :  coefficients = -0.084173285
VGLM    linear loop  6 :  coefficients = -0.084171565
VGLM    linear loop  7 :  coefficients = -0.084171685
VGLM    linear loop  8 :  coefficients = -0.084171677
> coef(fit, matrix = TRUE)
            logloglink(shape)
(Intercept)       -0.08417168
> Coef(fit)  # Useful for intercept-only models; should be same as shape
   shape 
2.507468 
> c(with(hdata, mean(y)), head(fitted(fit), 1))
[1] 1.094000 1.103197
> summary(fit)
Call:
vglm(formula = y ~ 1, family = hzeta, data = hdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)  
(Intercept) -0.08417    0.04699  -1.791   0.0733 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logloglink(shape) 

Log-likelihood: -299.0674 on 999 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("hzetaUC")
> ### * hzetaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Hzeta
> ### Title: Haight's Zeta Distribution
> ### Aliases: Hzeta dhzeta phzeta qhzeta rhzeta
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dhzeta(1:20, 2.1)
 [1] 9.004491e-01 6.549734e-02 1.725418e-02 6.889024e-03 3.407965e-03
 [6] 1.923970e-03 1.188388e-03 7.835704e-04 5.429356e-04 3.911642e-04
[11] 2.908317e-04 2.219182e-04 1.730589e-04 1.374781e-04 1.109676e-04
[16] 9.082036e-05 7.524208e-05 6.301177e-05 5.327917e-05 4.544002e-05
> rhzeta(20, 2.1)
 [1] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1
> 
> round(1000 * dhzeta(1:8, 2))
[1] 889  71  20   8   4   2   1   1
> table(rhzeta(1000, 2))

  1   2   3   4   5   6   7  11  12  14 
892  72  20   5   6   1   1   1   1   1 
> 
> ## Not run: 
> ##D  shape <- 1.1; x <- 1:10
> ##D plot(x, dhzeta(x, shape = shape), type = "h", ylim = 0:1,
> ##D      sub = paste("shape =", shape), las = 1, col = "blue",
> ##D      ylab = "Probability", lwd = 2,
> ##D      main = "Haight's zeta: blue = density; orange = CDF")
> ##D lines(x+0.1, phzeta(x, shape = shape), col = "orange", lty = 3, lwd = 2,
> ##D       type = "h")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("iam")
> ### * iam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: iam
> ### Title: Index from Array to Matrix
> ### Aliases: iam
> ### Keywords: manip programming
> 
> ### ** Examples
> 
> iam(1, 2, M = 3)  # The 4th coln represents elt (1,2) of a 3x3 matrix
[1] 4
> iam(NULL, NULL, M = 3, both = TRUE)  # Return the row & column indices
$row.index
[1] 1 2 3 1 2 1

$col.index
[1] 1 2 3 2 3 3

> 
> dirichlet()@weight
expression({
    index <- iam(NA, NA, M, both = TRUE, diag = TRUE)
    wz <- matrix(-trigamma(sumshape), nrow = n, ncol = dimm(M))
    wz[, 1:M] <- trigamma(shape) + wz[, 1:M]
    wz <- c(w) * wz * dsh.deta[, index$row] * dsh.deta[, index$col]
    wz
})
> 
> M <- 4
> temp1 <- iam(NA, NA, M = M, both = TRUE)
> mat1 <- matrix(NA, M, M)
> mat1[cbind(temp1$row, temp1$col)] = 1:length(temp1$row)
> mat1  # More commonly used
     [,1] [,2] [,3] [,4]
[1,]    1    5    8   10
[2,]   NA    2    6    9
[3,]   NA   NA    3    7
[4,]   NA   NA   NA    4
> 
> temp2 <- iam(NA, NA, M = M, both = TRUE, diag = FALSE)
> mat2 <- matrix(NA, M, M)
> mat2[cbind(temp2$row, temp2$col)] = 1:length(temp2$row)
> mat2  # Rarely used
     [,1] [,2] [,3] [,4]
[1,]   NA    1    4    6
[2,]   NA   NA    2    5
[3,]   NA   NA   NA    3
[4,]   NA   NA   NA   NA
> 
> 
> 
> cleanEx()
> nameEx("identitylink")
> ### * identitylink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: identitylink
> ### Title: Identity Link Function
> ### Aliases: identitylink negidentitylink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> identitylink((-5):5)
 [1] -5 -4 -3 -2 -1  0  1  2  3  4  5
> identitylink((-5):5, deriv = 1)
 [1] 1 1 1 1 1 1 1 1 1 1 1
> identitylink((-5):5, deriv = 2)
 [1] 0 0 0 0 0 0 0 0 0 0 0
> negidentitylink((-5):5)
 [1]  5  4  3  2  1  0 -1 -2 -3 -4 -5
> negidentitylink((-5):5, deriv = 1)
 [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
> negidentitylink((-5):5, deriv = 2)
 [1] 0 0 0 0 0 0 0 0 0 0 0
> 
> 
> 
> cleanEx()
> nameEx("inv.binomial")
> ### * inv.binomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inv.binomial
> ### Title: Inverse Binomial Distribution Family Function
> ### Aliases: inv.binomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> idata <- data.frame(y = rnbinom(n <- 1000, mu = exp(3), size = exp(1)))
> fit <- vglm(y ~ 1, inv.binomial, data = idata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -253916.86
VGLM    linear loop  2 :  loglikelihood = -252920.54
VGLM    linear loop  3 :  loglikelihood = -251933.89
VGLM    linear loop  4 :  loglikelihood = -250972.98
VGLM    linear loop  5 :  loglikelihood = -250078.2
VGLM    linear loop  6 :  loglikelihood = -249338.13
VGLM    linear loop  7 :  loglikelihood = -248884.55
VGLM    linear loop  8 :  loglikelihood = -248745.17
VGLM    linear loop  9 :  loglikelihood = -248734.58
VGLM    linear loop  10 :  loglikelihood = -248734.52
VGLM    linear loop  11 :  loglikelihood = -248734.52
> with(idata, c(mean(y), head(fitted(fit), 1)))
[1] 20.729 20.729
> summary(fit)
Call:
vglm(formula = y ~ 1, family = inv.binomial, data = idata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -5.67239    1.43564  -3.951 7.78e-05 ***
(Intercept):2 -1.94771    0.03156 -61.715  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: extlogitlink(rho, min = 0.5, max = 1), 
loglink(lambda)

Log-likelihood: -248734.5 on 1998 degrees of freedom

Number of Fisher scoring iterations: 11 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'

> coef(fit, matrix = TRUE)
            extlogitlink(rho, min = 0.5, max = 1) loglink(lambda)
(Intercept)                             -5.672388       -1.947707
> Coef(fit)
      rho    lambda 
0.5017139 0.1426006 
> sum(weights(fit))  # Sum of the prior weights
[1] 1000
> sum(weights(fit, type = "work"))  # Sum of the working weights
[1] 1004.484
> 
> 
> 
> cleanEx()
> nameEx("inv.gaussianff")
> ### * inv.gaussianff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inv.gaussianff
> ### Title: Inverse Gaussian Distribution Family Function
> ### Aliases: inv.gaussianff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> idata <- data.frame(x2 = runif(nn <- 1000))
> idata <- transform(idata, mymu   = exp(2 + 1 * x2),
+                           Lambda = exp(2 + 1 * x2))
> idata <- transform(idata, y = rinv.gaussian(nn, mu = mymu, Lambda))
> fit1 <-   vglm(y ~ x2, inv.gaussianff, data = idata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3405.1864
VGLM    linear loop  2 :  loglikelihood = -3401.4606
VGLM    linear loop  3 :  loglikelihood = -3401.445
VGLM    linear loop  4 :  loglikelihood = -3401.445
> rrig <- rrvglm(y ~ x2, inv.gaussianff, data = idata, trace = TRUE)
RR-VGLM    linear loop  1 :  loglikelihood = -3423.6158
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  1715.1412252 
   Alternating iteration 2 ,   Convergence criterion  =  0 
    ResSS  =  1715.1412252 
RR-VGLM    linear loop  2 :  loglikelihood = -3401.5654
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  1999.6378908 
   Alternating iteration 2 ,   Convergence criterion  =  1.137074e-16 
    ResSS  =  1999.6378908 
RR-VGLM    linear loop  3 :  loglikelihood = -3401.4454
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  2011.9008651 
   Alternating iteration 2 ,   Convergence criterion  =  0 
    ResSS  =  2011.9008651 
RR-VGLM    linear loop  4 :  loglikelihood = -3401.445
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  2012.486886 
   Alternating iteration 2 ,   Convergence criterion  =  0 
    ResSS  =  2012.486886 
RR-VGLM    linear loop  5 :  loglikelihood = -3401.445
> coef(fit1, matrix = TRUE)
            loglink(mu) loglink(lambda)
(Intercept)    2.010298       1.9518294
x2             1.024812       0.9423104
> coef(rrig, matrix = TRUE)
            loglink(mu) loglink(lambda)
(Intercept)    2.010276       1.9518483
x2             1.024855       0.9422726
> Coef(rrig)
A matrix:
                   latvar
loglink(mu)     1.0000000
loglink(lambda) 0.9194201

C matrix:
     latvar
x2 1.024862

B1 matrix:
            loglink(mu) loglink(lambda)
(Intercept)    2.010276        1.951848
> summary(fit1)
Call:
vglm(formula = y ~ x2, family = inv.gaussianff, data = idata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  2.01030    0.06587  30.521  < 2e-16 ***
(Intercept):2  1.95183    0.08950  21.808  < 2e-16 ***
x2:1           1.02481    0.11537   8.883  < 2e-16 ***
x2:2           0.94231    0.15515   6.073 1.25e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(mu), loglink(lambda)

Log-likelihood: -3401.445 on 1996 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'

> 
> 
> 
> cleanEx()
> nameEx("inv.lomax")
> ### * inv.lomax
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inv.lomax
> ### Title: Inverse Lomax Distribution Family Function
> ### Aliases: inv.lomax
> ### Keywords: models regression
> 
> ### ** Examples
> 
> idata <- data.frame(y = rinv.lomax(2000, sc = exp(2), exp(1)))
> fit <- vglm(y ~ 1, inv.lomax, data = idata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -10268.5867
VGLM    linear loop  2 :  loglikelihood = -10268.5865
VGLM    linear loop  3 :  loglikelihood = -10268.5865
> fit <- vglm(y ~ 1, inv.lomax(iscale = exp(3)), data = idata,
+             trace = TRUE, epsilon = 1e-8, crit = "coef")
VGLM    linear loop  1 :  coefficients = 2.694614000, 0.419632861
VGLM    linear loop  2 :  coefficients = 2.216127104, 0.787366063
VGLM    linear loop  3 :  coefficients = 2.025821065, 0.954514816
VGLM    linear loop  4 :  coefficients = 1.985774773, 0.989067277
VGLM    linear loop  5 :  coefficients = 1.983385732, 0.991012943
VGLM    linear loop  6 :  coefficients = 1.983311543, 0.991067667
VGLM    linear loop  7 :  coefficients = 1.983309340, 0.991069274
VGLM    linear loop  8 :  coefficients = 1.983309274, 0.991069322
VGLM    linear loop  9 :  coefficients = 1.983309273, 0.991069323
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(shape2.p)
(Intercept)       1.983309         0.9910693
> Coef(fit)
   scale shape2.p 
7.266751 2.694114 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = inv.lomax(iscale = exp(3)), data = idata, 
    trace = TRUE, epsilon = 1e-08, crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   1.9833     0.1090   18.19   <2e-16 ***
(Intercept):2   0.9911     0.0826   12.00   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), loglink(shape2.p)

Log-likelihood: -10268.59 on 3998 degrees of freedom

Number of Fisher scoring iterations: 9 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("inv.lomaxUC")
> ### * inv.lomaxUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Inv.lomax
> ### Title: The Inverse Lomax Distribution
> ### Aliases: Inv.lomax dinv.lomax pinv.lomax qinv.lomax rinv.lomax
> ### Keywords: distribution
> 
> ### ** Examples
> 
> idata <- data.frame(y = rinv.lomax(n = 1000, exp(2), exp(1)))
> fit <- vglm(y ~ 1, inv.lomax, idata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.8619799, 1.1137060
VGLM    linear loop  2 :  coefficients = 1.8464422, 1.1275162
VGLM    linear loop  3 :  coefficients = 1.8459260, 1.1279348
VGLM    linear loop  4 :  coefficients = 1.8459149, 1.1279432
VGLM    linear loop  5 :  coefficients = 1.8459147, 1.1279434
VGLM    linear loop  6 :  coefficients = 1.8459147, 1.1279434
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(shape2.p)
(Intercept)       1.845915          1.127943
> Coef(fit)
   scale shape2.p 
6.333890 3.089297 
> 
> 
> 
> cleanEx()
> nameEx("inv.paralogistic")
> ### * inv.paralogistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inv.paralogistic
> ### Title: Inverse Paralogistic Distribution Family Function
> ### Aliases: inv.paralogistic
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D idata <- data.frame(y = rinv.paralogistic(3000, exp(1), sc = exp(2)))
> ##D fit <- vglm(y ~ 1, inv.paralogistic(lss = FALSE), idata, trace = TRUE)
> ##D fit <- vglm(y ~ 1, inv.paralogistic(imethod = 2, ishape1.a = 4),
> ##D             data = idata, trace = TRUE, crit = "coef")
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("inv.paralogisticUC")
> ### * inv.paralogisticUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Inv.paralogistic
> ### Title: The Inverse Paralogistic Distribution
> ### Aliases: Inv.paralogistic dinv.paralogistic pinv.paralogistic
> ###   qinv.paralogistic rinv.paralogistic
> ### Keywords: distribution
> 
> ### ** Examples
> 
> idata <- data.frame(y = rinv.paralogistic(3000, exp(1), sc = exp(2)))
> fit <- vglm(y ~ 1, inv.paralogistic(lss = FALSE, ishape1.a = 2.1),
+             data = idata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.98109522, 2.11482390
VGLM    linear loop  2 :  coefficients = 0.95956057, 1.97891611
VGLM    linear loop  3 :  coefficients = 0.99687983, 2.00612872
VGLM    linear loop  4 :  coefficients = 0.99144686, 1.98659106
VGLM    linear loop  5 :  coefficients = 0.99323309, 1.98979811
VGLM    linear loop  6 :  coefficients = 0.99293251, 1.98886340
VGLM    linear loop  7 :  coefficients = 0.99299078, 1.98902414
VGLM    linear loop  8 :  coefficients = 0.99298002, 1.98899322
VGLM    linear loop  9 :  coefficients = 0.99298203, 1.98899894
VGLM    linear loop  10 :  coefficients = 0.99298165, 1.98899788
VGLM    linear loop  11 :  coefficients = 0.99298172, 1.98899808
> coef(fit, matrix = TRUE)
            loglink(shape1.a) loglink(scale)
(Intercept)         0.9929817       1.988998
> Coef(fit)
shape1.a    scale 
2.699271 7.308208 
> 
> 
> 
> cleanEx()
> nameEx("is.buggy")
> ### * is.buggy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.buggy
> ### Title: Does the Fitted Object Suffer from a Known Bug?
> ### Aliases: is.buggy is.buggy.vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fit1 <- vgam(cbind(agaaus, kniexc) ~ s(altitude, df = c(3, 4)),
+              binomialff(multiple.responses = TRUE), data = hunua)
> is.buggy(fit1)  # Okay
[1] FALSE
> is.buggy(fit1, each.term = TRUE)  # No terms are buggy
              (Intercept) s(altitude, df = c(3, 4)) 
                    FALSE                     FALSE 
> fit2 <-
+   vgam(cbind(agaaus, kniexc) ~ s(altitude, df = c(3, 4)),
+        binomialff(multiple.responses = TRUE), data = hunua,
+        constraints =
+        list("(Intercept)" = diag(2),
+             "s(altitude, df = c(3, 4))" = matrix(c(1, 1, 0, 1), 2, 2)))
Warning in vgam(cbind(agaaus, kniexc) ~ s(altitude, df = c(3, 4)), binomialff(multiple.responses = TRUE),  :
  some s() terms have constraint matrices that have columns which are not orthogonal; try using sm.os() or sm.ps() instead of s().
> is.buggy(fit2)  # TRUE
[1] TRUE
> is.buggy(fit2, each.term = TRUE)
              (Intercept) s(altitude, df = c(3, 4)) 
                    FALSE                      TRUE 
> constraints(fit2)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$`s(altitude, df = c(3, 4))`
     [,1] [,2]
[1,]    1    0
[2,]    1    1

> 
> # fit2b is an approximate alternative to fit2:
> fit2b <-
+   vglm(cbind(agaaus, kniexc) ~ bs(altitude, df=3) + bs(altitude, df=4),
+        binomialff(multiple.responses = TRUE), data = hunua,
+        constraints =
+          list("(Intercept)" = diag(2),
+               "bs(altitude, df = 3)" = rbind(1, 1),
+               "bs(altitude, df = 4)" = rbind(0, 1)))
> is.buggy(fit2b)  # Okay
[1] FALSE
> is.buggy(fit2b, each.term = TRUE)
          (Intercept) bs(altitude, df = 3)1 bs(altitude, df = 3)2 
                FALSE                 FALSE                 FALSE 
bs(altitude, df = 3)3 bs(altitude, df = 4)1 bs(altitude, df = 4)2 
                FALSE                 FALSE                 FALSE 
bs(altitude, df = 4)3 bs(altitude, df = 4)4 
                FALSE                 FALSE 
> constraints(fit2b)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$`bs(altitude, df = 3)1`
     [,1]
[1,]    1
[2,]    1

$`bs(altitude, df = 3)2`
     [,1]
[1,]    1
[2,]    1

$`bs(altitude, df = 3)3`
     [,1]
[1,]    1
[2,]    1

$`bs(altitude, df = 4)1`
     [,1]
[1,]    0
[2,]    1

$`bs(altitude, df = 4)2`
     [,1]
[1,]    0
[2,]    1

$`bs(altitude, df = 4)3`
     [,1]
[1,]    0
[2,]    1

$`bs(altitude, df = 4)4`
     [,1]
[1,]    0
[2,]    1

> 
> 
> 
> cleanEx()
> nameEx("is.crossing")
> ### * is.crossing
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.crossing
> ### Title: Quantile Crossing Detection
> ### Aliases: is.crossing is.crossing.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  ooo <- with(bmi.nz, order(age))
> ##D bmi.nz <- bmi.nz[ooo, ]  # Sort by age
> ##D with(bmi.nz, plot(age, BMI, col = "blue"))
> ##D mytau <- c(50, 93, 95, 97) / 100  # Some quantiles are quite close
> ##D fit1 <- vglm(BMI ~ ns(age, 7), extlogF1(mytau), bmi.nz, trace = TRUE)
> ##D plot(BMI ~ age, bmi.nz, col = "blue", las = 1,
> ##D      main = "Partially parallel (darkgreen) & nonparallel quantiles",
> ##D      sub = "Crossing quantiles are orange")
> ##D is.crossing(fit1)
> ##D matlines(with(bmi.nz, age), fitted(fit1), lty = 1, col = "orange") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("is.parallel")
> ### * is.parallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.parallel
> ### Title: Parallelism Constraint Matrices
> ### Aliases: is.parallel is.parallel.matrix is.parallel.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  require("VGAMdata")
> ##D fit <- vglm(educ ~ sm.bs(age) * sex + ethnicity,
> ##D             cumulative(parallel = TRUE), head(xs.nz, 200))
> ##D is.parallel(fit)
> ##D is.parallel(fit, type = "lm")  # For each column of the LM matrix
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("is.smart")
> ### * is.smart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.smart
> ### Title: Test For a Smart Object
> ### Aliases: is.smart
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> is.smart(sm.min1)  # TRUE
[1] TRUE
> is.smart(sm.poly)  # TRUE
[1] TRUE
> library(splines)
> is.smart(sm.bs)  # TRUE
[1] TRUE
> is.smart(sm.ns)  # TRUE
[1] TRUE
> is.smart(tan)  # FALSE
[1] FALSE
> ## Not run: 
> ##D udata <- data.frame(x2 = rnorm(9))
> ##D fit1 <- vglm(rnorm(9) ~ x2, uninormal, data = udata)
> ##D is.smart(fit1)  # TRUE
> ##D fit2 <- vglm(rnorm(9) ~ x2, uninormal, data = udata, smart = FALSE)
> ##D is.smart(fit2)  # FALSE
> ##D fit2@smart.prediction
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("is.zero")
> ### * is.zero
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.zero
> ### Title: Zero Constraint Matrices
> ### Aliases: is.zero is.zero.matrix is.zero.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> coalminers <- transform(coalminers, Age = (age - 42) / 5)
> fit <- vglm(cbind(nBnW,nBW,BnW,BW) ~ Age, binom2.or(zero = NULL),
+             data = coalminers)
> is.zero(fit)
 logitlink(mu1)  logitlink(mu2) loglink(oratio) 
          FALSE           FALSE           FALSE 
> is.zero(coef(fit, matrix = TRUE))
 logitlink(mu1)  logitlink(mu2) loglink(oratio) 
          FALSE           FALSE           FALSE 
> 
> 
> 
> cleanEx()
> nameEx("kendall.tau")
> ### * kendall.tau
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kendall.tau
> ### Title: Kendall's Tau Statistic
> ### Aliases: kendall.tau
> ### Keywords: math
> 
> ### ** Examples
> 
> N <- 5000; x <- 1:N; y <- runif(N)
> true.rho <- -0.8
> ymat <- rbinorm(N, cov12 =  true.rho)  # Bivariate normal, aka N_2
> x <- ymat[, 1]
> y <- ymat[, 2]
> 
> ## Not run: plot(x, y, col = "blue")
> 
> kendall.tau(x, y)  # A random sample is taken here
[1] -0.6166344
> kendall.tau(x, y)  # A random sample is taken here
[1] -0.605617
> 
> kendall.tau(x, y, exact = TRUE)  # Costly if length(x) is large
[1] -0.6035405
> kendall.tau(x, y, max.n = N)     # Same as exact = TRUE
[1] -0.6035405
> 
> (rhohat <- sin(kendall.tau(x, y) * pi / 2))  # Holds for N_2 actually
[1] -0.811667
> true.rho  # rhohat should be near this value
[1] -0.8
> 
> 
> 
> cleanEx()
> nameEx("kumar")
> ### * kumar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kumar
> ### Title: Kumaraswamy Regression Family Function
> ### Aliases: kumar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> shape1 <- exp(1); shape2 <- exp(2)
> kdata <- data.frame(y = rkumar(n = 1000, shape1, shape2))
> fit <- vglm(y ~ 1, kumar, data = kdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 463.72758
VGLM    linear loop  2 :  loglikelihood = 466.69992
VGLM    linear loop  3 :  loglikelihood = 466.72805
VGLM    linear loop  4 :  loglikelihood = 466.72806
VGLM    linear loop  5 :  loglikelihood = 466.72806
> c(with(kdata, mean(y)), head(fitted(fit), 1))
[1] 0.4114681 0.4109757
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)        1.004417        2.021374
> Coef(fit)
  shape1   shape2 
2.730314 7.548691 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = kumar, data = kdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.00442    0.02891   34.75   <2e-16 ***
(Intercept):2  2.02137    0.06381   31.68   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2)

Log-likelihood: 466.7281 on 1998 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("kumarUC")
> ### * kumarUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Kumar
> ### Title: The Kumaraswamy Distribution
> ### Aliases: Kumar dkumar pkumar qkumar rkumar
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D shape1 <- 2; shape2 <- 2; nn <- 201; # shape1 <- shape2 <- 0.5;
> ##D x <- seq(-0.05, 1.05, len = nn)
> ##D plot(x, dkumar(x, shape1, shape2), type = "l", las = 1, 
> ##D      ylab = paste("dkumar(shape1 = ", shape1,
> ##D                   ", shape2 = ", shape2, ")"),
> ##D      col = "blue", cex.main = 0.8, ylim = c(0,1.5),
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Red lines are the 10,20,...,90 percentiles")
> ##D lines(x, pkumar(x, shape1, shape2), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qkumar(probs, shape1, shape2)
> ##D lines(Q, dkumar(Q, shape1, shape2), col = "red", lty = 3, type = "h")
> ##D lines(Q, pkumar(Q, shape1, shape2), col = "red", lty = 3, type = "h")
> ##D abline(h = probs, col = "red", lty = 3)
> ##D max(abs(pkumar(Q, shape1, shape2) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lakeO")
> ### * lakeO
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lakeO
> ### Title: Annual catches on Lake Otamangakau from October 1974 to October
> ###   1989
> ### Aliases: lakeO
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(lakeO)
> lakeO
   year total.fish brown rainbow visits
1  1974         14     1      13      8
2  1975         23     0      23     12
3  1976         19     1      18      6
4  1977         47     1      46     15
5  1978         45     4      41     18
6  1979         65     0      65     22
7  1980         13     3      10      7
8  1981         22     2      20     10
9  1982         27     2      25     11
10 1983          3     1       2      2
11 1984         20     4      16      8
12 1985         17     5      12      9
13 1986         10     1       9      4
14 1987         20     8      12     11
15 1988         17     2      15      8
> summary(lakeO)
      year        total.fish        brown          rainbow         visits     
 Min.   :1974   Min.   : 3.00   Min.   :0.000   Min.   : 2.0   Min.   : 2.00  
 1st Qu.:1978   1st Qu.:15.50   1st Qu.:1.000   1st Qu.:12.0   1st Qu.: 7.50  
 Median :1981   Median :20.00   Median :2.000   Median :16.0   Median : 9.00  
 Mean   :1981   Mean   :24.13   Mean   :2.333   Mean   :21.8   Mean   :10.07  
 3rd Qu.:1984   3rd Qu.:25.00   3rd Qu.:3.500   3rd Qu.:24.0   3rd Qu.:11.50  
 Max.   :1988   Max.   :65.00   Max.   :8.000   Max.   :65.0   Max.   :22.00  
> 
> 
> 
> cleanEx()
> nameEx("lambertW")
> ### * lambertW
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lambertW
> ### Title: The Lambert W Function
> ### Aliases: lambertW
> ### Keywords: math
> 
> ### ** Examples
>  ## Not run: 
> ##D curve(lambertW, -exp(-1), 3, xlim = c(-1, 3), ylim = c(-2, 1),
> ##D       las = 1, col = "orange", n = 1001)
> ##D abline(v = -exp(-1), h = -1, lwd = 2, lty = "dotted", col = "gray")
> ##D abline(h = 0, v = 0, lty = "dashed", col = "blue") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("laplace")
> ### * laplace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: laplace
> ### Title: Laplace Regression Family Function
> ### Aliases: laplace
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y = rlaplace(nn <- 100, 2, scale = exp(1)))
> fit <- vglm(y  ~ 1, laplace, ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -249.59519
VGLM    linear loop  2 :  loglikelihood = -249.59517
VGLM    linear loop  3 :  loglikelihood = -249.59517
> coef(fit, matrix = TRUE)
            location loglink(scale)
(Intercept) 1.932723      0.8028046
> Coef(fit)
location    scale 
1.932723 2.231791 
> with(ldata, median(y))
[1] 1.932723
> 
> ldata <- data.frame(x = runif(nn <- 1001))
> ldata <- transform(ldata, y = rlaplace(nn, 2, scale = exp(-1 + 1*x)))
> coef(vglm(y ~ x, laplace(iloc = 0.2, imethod = 2, zero = 1), ldata,
+           trace = TRUE), matrix = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2320.94219
VGLM    linear loop  2 :  loglikelihood = -2262.19767
VGLM    linear loop  3 :  loglikelihood = -1636.22509
VGLM    linear loop  4 :  loglikelihood = -1329.15139
VGLM    linear loop  5 :  loglikelihood = -1253.09572
VGLM    linear loop  6 :  loglikelihood = -1247.12785
VGLM    linear loop  7 :  loglikelihood = -1247.08913
VGLM    linear loop  8 :  loglikelihood = -1247.08718
VGLM    linear loop  9 :  loglikelihood = -1247.08524
VGLM    linear loop  10 :  loglikelihood = -1247.0833
VGLM    linear loop  11 :  loglikelihood = -1247.0816
VGLM    linear loop  12 :  loglikelihood = -1247.08153
VGLM    linear loop  13 :  loglikelihood = -1247.08146
VGLM    linear loop  14 :  loglikelihood = -1247.08139
VGLM    linear loop  15 :  loglikelihood = -1247.08132
VGLM    linear loop  16 :  loglikelihood = -1247.08125
VGLM    linear loop  17 :  loglikelihood = -1247.08119
VGLM    linear loop  18 :  loglikelihood = -1247.08112
VGLM    linear loop  19 :  loglikelihood = -1247.08105
VGLM    linear loop  20 :  loglikelihood = -1247.08098
VGLM    linear loop  21 :  loglikelihood = -1247.08091
VGLM    linear loop  22 :  loglikelihood = -1247.08084
VGLM    linear loop  23 :  loglikelihood = -1247.08077
VGLM    linear loop  24 :  loglikelihood = -1247.08117
Taking a modified step..
VGLM    linear loop  24 :  loglikelihood = -1247.08077
VGLM    linear loop  25 :  loglikelihood = -1247.08129
Taking a modified step......
VGLM    linear loop  25 :  loglikelihood = -1247.08076
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
            location loglink(scale)
(Intercept) 1.986402     -0.9552243
x           0.000000      1.0209347
> 
> 
> 
> cleanEx()
> nameEx("laplaceUC")
> ### * laplaceUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: laplaceUC
> ### Title: The Laplace Distribution
> ### Aliases: dlaplace plaplace qlaplace rlaplace
> ### Keywords: distribution
> 
> ### ** Examples
> 
> loc <- 1; b <- 2
> y <- rlaplace(n = 100, loc = loc, scale = b)
> mean(y)  # sample mean
[1] 1.122646
> loc      # population mean
[1] 1
> var(y)   # sample variance
[1] 5.066177
> 2 * b^2  # population variance
[1] 8
> 
> ## Not run: 
> ##D  loc <- 0; b <- 1.5; x <- seq(-5, 5, by = 0.01)
> ##D plot(x, dlaplace(x, loc, b), type = "l", col = "blue",
> ##D      main = "Blue is density, orange is the CDF", ylim = c(0,1),
> ##D      sub = "Purple are 5,10,...,95 percentiles", las = 1, ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qlaplace(seq(0.05,0.95,by = 0.05), loc, b),
> ##D       dlaplace(qlaplace(seq(0.05, 0.95, by = 0.05), loc, b), loc, b),
> ##D       col = "purple", lty = 3, type = "h")
> ##D lines(x, plaplace(x, loc, b), type = "l", col = "orange")
> ##D abline(h = 0, lty = 2) 
> ## End(Not run)
> 
> plaplace(qlaplace(seq(0.05, 0.95, by = 0.05), loc, b), loc, b)
 [1] 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
[16] 0.80 0.85 0.90 0.95
> 
> 
> 
> cleanEx()
> nameEx("latvar")
> ### * latvar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latvar
> ### Title: Latent Variables
> ### Aliases: lv latvar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Standardized environmental vars
> ##D set.seed(123)
> ##D p1 <- cao(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           family = poissonff, data = hspider, Rank = 1, df1.nl =
> ##D           c(Zoraspin = 2.5, 3), Bestof = 3, Crow1positive = TRUE)
> ##D 
> ##D var(latvar(p1))  # Scaled to unit variance  # Scaled to unit variance
> ##D c(latvar(p1))    # Estimated site scores
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("leipnik")
> ### * leipnik
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: leipnik
> ### Title: Leipnik Regression Family Function
> ### Aliases: leipnik
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y = rnorm(2000, 0.5, 0.1))  # Improper data
> fit <- vglm(y ~ 1, leipnik(ilambda = 1), ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -45931.9034
VGLM    linear loop  2 :  loglikelihood = -13293.4536
VGLM    linear loop  3 :  loglikelihood = -1883.61962
VGLM    linear loop  4 :  loglikelihood = 1772.16935
VGLM    linear loop  5 :  loglikelihood = 2700.55798
VGLM    linear loop  6 :  loglikelihood = 2831.03795
VGLM    linear loop  7 :  loglikelihood = 2835.69914
VGLM    linear loop  8 :  loglikelihood = 2835.70772
VGLM    linear loop  9 :  loglikelihood = 2835.70772
> head(fitted(fit))
          [,1]
[1,] 0.4984732
[2,] 0.4984732
[3,] 0.4984732
[4,] 0.4984732
[5,] 0.4984732
[6,] 0.4984732
> with(ldata, mean(y))
[1] 0.4986045
> summary(fit)
Call:
vglm(formula = y ~ 1, family = leipnik(ilambda = 1), data = ldata, 
    trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.006107   0.010186    -0.6    0.549    
(Intercept):2  3.095209   0.030932   100.1   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(mu), logofflink(lambda, offset = 1)

Log-likelihood: 2835.708 on 3998 degrees of freedom

Number of Fisher scoring iterations: 9 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            logitlink(mu) logofflink(lambda, offset = 1)
(Intercept)  -0.006107388                       3.095209
> Coef(fit)
        mu     lambda 
 0.4984732 21.0918502 
> 
> sum(weights(fit))  # Sum of the prior weights
[1] 2000
> sum(weights(fit, type = "work"))  # Sum of the working weights
[1] 10683.83
> 
> 
> 
> cleanEx()
> nameEx("lerch")
> ### * lerch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lerch
> ### Title: Lerch Phi Function
> ### Aliases: lerch
> ### Keywords: math
> 
> ### ** Examples
> 
> ## Not run: 
> ##D s <- 2; v <- 1; x <- seq(-1.1, 1.1, length = 201)
> ##D plot(x, lerch(x, s = s, v = v), type = "l", col = "blue",
> ##D      las = 1, main = paste0("lerch(x, s = ", s,", v = ", v, ")"))
> ##D abline(v = 0, h = 1, lty = "dashed", col = "gray")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("levy")
> ### * levy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: levy
> ### Title: Levy Distribution Family Function
> ### Aliases: levy
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000; loc1 <- 0; loc2 <- 10
> myscale <- 1  # log link ==> 0 is the answer
> ldata <-
+   data.frame(y1 = loc1 + myscale/rnorm(nn)^2,  # Levy(myscale, a)
+              y2 = rlevy(nn, loc = loc2, scale = exp(+2)))
> # Cf. Table 1.1 of Nolan for Levy(1,0)
> with(ldata, sum(y1 > 1) / length(y1))  # Should be 0.6827
[1] 0.674
> with(ldata, sum(y1 > 2) / length(y1))  # Should be 0.5205
[1] 0.515
> 
> fit1 <- vglm(y1 ~ 1, levy(location = loc1), ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3322.9466
VGLM    linear loop  2 :  loglikelihood = -3282.6667
VGLM    linear loop  3 :  loglikelihood = -3280.9544
VGLM    linear loop  4 :  loglikelihood = -3280.9515
VGLM    linear loop  5 :  loglikelihood = -3280.9515
> coef(fit1, matrix = TRUE)
            loglink(scale)
(Intercept)    -0.06776652
> Coef(fit1)
    scale 
0.9344786 
> summary(fit1)
Call:
vglm(formula = y1 ~ 1, family = levy(location = loc1), data = ldata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)
(Intercept) -0.06777    0.04472  -1.515     0.13

Name of linear predictor: loglink(scale) 

Log-likelihood: -3280.952 on 999 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> head(weights(fit1, type = "work"))
     [,1]
[1,]  0.5
[2,]  0.5
[3,]  0.5
[4,]  0.5
[5,]  0.5
[6,]  0.5
> 
> fit2 <- vglm(y2 ~ 1, levy(location = loc2), ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -5289.5821
VGLM    linear loop  2 :  loglikelihood = -5264.9295
VGLM    linear loop  3 :  loglikelihood = -5264.3014
VGLM    linear loop  4 :  loglikelihood = -5264.301
VGLM    linear loop  5 :  loglikelihood = -5264.301
> coef(fit2, matrix = TRUE)
            loglink(scale)
(Intercept)       1.968837
> Coef(fit2)
   scale 
7.162341 
> c(median = with(ldata, median(y2)),
+   fitted.median = head(fitted(fit2), 1))
       median fitted.median 
     24.73342      25.74361 
> 
> 
> 
> cleanEx()
> nameEx("lgammaUC")
> ### * lgammaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lgammaUC
> ### Title: The Log-Gamma Distribution
> ### Aliases: lgammaUC dlgamma plgamma qlgamma rlgamma
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  loc <- 1; Scale <- 1.5; shape <- 1.4
> ##D x <- seq(-3.2, 5, by = 0.01)
> ##D plot(x, dlgamma(x, loc = loc, Scale, shape = shape), type = "l",
> ##D      col = "blue", ylim = 0:1,
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Red are 5,10,...,95 percentiles", las = 1, ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(qlgamma(seq(0.05, 0.95, by = 0.05), loc = loc, Scale, sh = shape),
> ##D       dlgamma(qlgamma(seq(0.05, 0.95, by = 0.05), loc = loc, sc = Scale,
> ##D                       shape = shape),
> ##D     loc = loc, Scale, shape = shape), col = "red", lty = 3, type = "h")
> ##D lines(x, plgamma(x, loc = loc, Scale, shape = shape), col = "orange")
> ##D abline(h = 0, lty = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lgammaff")
> ### * lgammaff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lgamma1
> ### Title: Log-gamma Distribution Family Function
> ### Aliases: lgamma1 lgamma3
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y = rlgamma(100, shape = exp(1)))
> fit <- vglm(y ~ 1, lgamma1, ldata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.97725753
VGLM    linear loop  2 :  coefficients = 1.001281
VGLM    linear loop  3 :  coefficients = 1.0013363
VGLM    linear loop  4 :  coefficients = 1.0013363
> summary(fit)
Call:
vglm(formula = y ~ 1, family = lgamma1, data = ldata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.0013     0.0552   18.14   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(shape) 

Log-likelihood: -87.4506 on 99 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            loglink(shape)
(Intercept)       1.001336
> Coef(fit)
   shape 
2.721917 
> 
> ldata <- data.frame(x2 = runif(nn <- 5000))  # Another example
> ldata <- transform(ldata, loc = -1 + 2 * x2, Scale = exp(1))
> ldata <- transform(ldata, y = rlgamma(nn, loc, sc = Scale, sh = exp(0)))
> fit2 <- vglm(y ~ x2, lgamma3, data = ldata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 
-1.33863888,  1.13617057,  0.17462887,  1.68145246
VGLM    linear loop  2 :  coefficients = 
-1.033026709,  1.019035968,  0.043261676,  1.970763408
VGLM    linear loop  3 :  coefficients = 
-0.9183604867,  0.9822283312, -0.0066527254,  1.9896295248
VGLM    linear loop  4 :  coefficients = 
-0.912077471,  0.979295705, -0.010443643,  1.991920320
VGLM    linear loop  5 :  coefficients = 
-0.91211826,  0.97927927, -0.01046297,  1.99206593
VGLM    linear loop  6 :  coefficients = 
-0.912113419,  0.979278391, -0.010464328,  1.992065420
VGLM    linear loop  7 :  coefficients = 
-0.912113509,  0.979278403, -0.010464309,  1.992065474
VGLM    linear loop  8 :  coefficients = 
-0.91211351,  0.97927840, -0.01046431,  1.99206547
> coef(fit2, matrix = TRUE)
              location loglink(scale) loglink(shape)
(Intercept) -0.9121135      0.9792784    -0.01046431
x2           1.9920655      0.0000000     0.00000000
> 
> 
> 
> cleanEx()
> nameEx("lindUC")
> ### * lindUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Lindley
> ### Title: The Lindley Distribution
> ### Aliases: Lindley dlind plind rlind
> ### Keywords: distribution
> 
> ### ** Examples
> 
> theta <- exp(-1); x <- seq(0.0, 17, length = 700)
> dlind(0:10, theta)
 [1] 0.09893802 0.13696992 0.14221600 0.13125600 0.11356936 0.09433534
 [7] 0.07618214 0.06026666 0.04693120 0.03609534 0.02748374
> ## Not run: 
> ##D plot(x, dlind(x, theta), type = "l", las = 1, col = "blue",
> ##D      main = "dlind(x, theta = exp(-1))")
> ##D abline(h = 1, col = "grey", lty = "dashed") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lindley")
> ### * lindley
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lindley
> ### Title: 1-parameter Lindley Distribution
> ### Aliases: lindley
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y = rlind(n = 1000, theta = exp(3)))
> fit <- vglm(y ~ 1, lindley, data = ldata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 0.66986868
VGLM    linear loop  2 :  coefficients = 1.4579401
VGLM    linear loop  3 :  coefficients = 2.1681347
VGLM    linear loop  4 :  coefficients = 2.6896068
VGLM    linear loop  5 :  coefficients = 2.917252
VGLM    linear loop  6 :  coefficients = 2.9507612
VGLM    linear loop  7 :  coefficients = 2.9513843
VGLM    linear loop  8 :  coefficients = 2.9513845
> coef(fit, matrix = TRUE)
            loglink(theta)
(Intercept)       2.951385
> Coef(fit)
   theta 
19.13242 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = lindley, data = ldata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  2.95138    0.03019   97.75   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(theta) 

Log-likelihood: 1902.971 on 999 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("linkfunvlm")
> ### * linkfunvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: linkfun
> ### Title: Link Functions for VGLMs
> ### Aliases: linkfun linkfunvlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo)
> coef(fit1, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> linkfun(fit1)
    P[Y>=2]     P[Y>=3] 
"logitlink" "logitlink" 
> linkfun(fit1, earg = TRUE)
$link
    P[Y>=2]     P[Y>=3] 
"logitlink" "logitlink" 

$earg
$earg$`P[Y>=2]`
$earg$`P[Y>=2]`$theta


$earg$`P[Y>=2]`$bvalue
NULL

$earg$`P[Y>=2]`$inverse
[1] FALSE

$earg$`P[Y>=2]`$deriv
[1] 0

$earg$`P[Y>=2]`$short
[1] TRUE

$earg$`P[Y>=2]`$tag
[1] FALSE

attr(,"function.name")
[1] "logitlink"

$earg$`P[Y>=3]`
$earg$`P[Y>=3]`$theta


$earg$`P[Y>=3]`$bvalue
NULL

$earg$`P[Y>=3]`$inverse
[1] FALSE

$earg$`P[Y>=3]`$deriv
[1] 0

$earg$`P[Y>=3]`$short
[1] TRUE

$earg$`P[Y>=3]`$tag
[1] FALSE

attr(,"function.name")
[1] "logitlink"


> 
> fit2 <- vglm(cbind(normal, mild, severe) ~ let, multinomial, data = pneumo)
> coef(fit2, matrix = TRUE)
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)          11.975092          3.0390622
let                  -3.067466         -0.9020936
> linkfun(fit2)
[1] "multilogitlink"
> linkfun(fit2, earg = TRUE)
$link
[1] "multilogitlink"

$earg
$earg$multilogitlink
$earg$multilogitlink$M
[1] 2

$earg$multilogitlink$refLevel
[1] 3



> 
> 
> 
> cleanEx()
> nameEx("lino")
> ### * lino
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lino
> ### Title: Generalized Beta Distribution Family Function
> ### Aliases: lino
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y1 = rbeta(n = 1000, exp(0.5), exp(1)))  # Std beta
> fit <- vglm(y1 ~ 1, lino, data = ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 192.96631
VGLM    linear loop  2 :  loglikelihood = 192.9668
VGLM    linear loop  3 :  loglikelihood = 192.9668
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2) loglink(lambda)
(Intercept)       0.4076734       0.9889892      -0.1025723
> Coef(fit)
   shape1    shape2    lambda 
1.5033161 2.6885156 0.9025129 
> head(fitted(fit))
          [,1]
[1,] 0.3576224
[2,] 0.3576224
[3,] 0.3576224
[4,] 0.3576224
[5,] 0.3576224
[6,] 0.3576224
> summary(fit)
Call:
vglm(formula = y1 ~ 1, family = lino, data = ldata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.40767    0.07066   5.770 7.95e-09 ***
(Intercept):2  0.98899    0.11819   8.368  < 2e-16 ***
(Intercept):3 -0.10257    0.21290  -0.482     0.63    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2), loglink(lambda)

Log-likelihood: 192.9668 on 2997 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> 
> # Nonstandard beta distribution
> ldata <- transform(ldata, y2 = rlino(1000, shape1 = exp(1),
+                                      shape2 = exp(2), lambda = exp(1)))
> fit2 <- vglm(y2 ~ 1,
+              lino(lshape1 = "identitylink", lshape2 = "identitylink",
+                   ilamb = 10), data = ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 1249.7292
VGLM    linear loop  2 :  loglikelihood = 1251.9077
VGLM    linear loop  3 :  loglikelihood = 1251.9884
VGLM    linear loop  4 :  loglikelihood = 1251.9889
VGLM    linear loop  5 :  loglikelihood = 1251.9889
> coef(fit2, matrix = TRUE)
              shape1  shape2 loglink(lambda)
(Intercept) 3.025268 4.84028        1.610565
> 
> 
> 
> cleanEx()
> nameEx("linoUC")
> ### * linoUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Lino
> ### Title: The Generalized Beta Distribution (Libby and Novick, 1982)
> ### Aliases: Lino dlino plino qlino rlino
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D   lambda <- 0.4; shape1 <- exp(1.3); shape2 <- exp(1.3)
> ##D x <- seq(0.0, 1.0, len = 101)
> ##D plot(x, dlino(x, shape1 = shape1, shape2 = shape2, lambda = lambda),
> ##D      type = "l", col = "blue", las = 1, ylab = "",
> ##D      main = "Blue is PDF, orange is the CDF",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, plino(x, shape1, shape2, lambda = lambda), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qlino(probs, shape1 = shape1, shape2 = shape2, lambda = lambda)
> ##D lines(Q, dlino(Q, shape1 = shape1, shape2 = shape2, lambda = lambda),
> ##D       col = "purple", lty = 3, type = "h")
> ##D plino(Q, shape1, shape2, lambda = lambda) - probs  # Should be all 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lirat")
> ### * lirat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lirat
> ### Title: Low-iron Rat Teratology Data
> ### Aliases: lirat
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # cf. Figure 3 of Moore and Tsiatis (1991)
> ##D plot(R / N ~ hb, data = lirat, pch = as.character(grp), col = grp,
> ##D      las = 1, xlab = "Hemoglobin level", ylab = "Proportion Dead") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lms.bcg")
> ### * lms.bcg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lms.bcg
> ### Title: LMS Quantile Regression with a Box-Cox transformation to a Gamma
> ###   Distribution
> ### Aliases: lms.bcg
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # This converges, but deplot(fit) and qtplot(fit) do not work
> fit0 <- vglm(BMI ~ sm.bs(age, df = 4), lms.bcg, bmi.nz, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2003.6436
VGLM    linear loop  2 :  loglikelihood = -2009.2245
Taking a modified step.
VGLM    linear loop  2 :  loglikelihood = -1996.7124
VGLM    linear loop  3 :  loglikelihood = -1996.3429
VGLM    linear loop  4 :  loglikelihood = -1996.5846
Taking a modified step.
VGLM    linear loop  4 :  loglikelihood = -1995.9549
VGLM    linear loop  5 :  loglikelihood = -1995.9509
VGLM    linear loop  6 :  loglikelihood = -1995.9569
Taking a modified step.
VGLM    linear loop  6 :  loglikelihood = -1995.9381
VGLM    linear loop  7 :  loglikelihood = -1995.9378
VGLM    linear loop  8 :  loglikelihood = -1995.9379
Taking a modified step.
VGLM    linear loop  8 :  loglikelihood = -1995.9375
VGLM    linear loop  9 :  loglikelihood = -1995.9375
VGLM    linear loop  10 :  loglikelihood = -1995.9375
Taking a modified step.
VGLM    linear loop  10 :  loglikelihood = -1995.9375
VGLM    linear loop  11 :  loglikelihood = -1995.9375
> coef(fit0, matrix = TRUE)
                       lambda         mu loglink(sigma)
(Intercept)         -1.836875 24.6068044      -1.851626
sm.bs(age, df = 4)1  0.000000 -0.3320701       0.000000
sm.bs(age, df = 4)2  0.000000  2.8342499       0.000000
sm.bs(age, df = 4)3  0.000000  2.8573231       0.000000
sm.bs(age, df = 4)4  0.000000 -3.7988440       0.000000
> ## Not run: 
> ##D par(mfrow = c(1, 1))
> ##D plotvgam(fit0, se = TRUE)  # Plot mu function (only)
> ## End(Not run)
> 
> # Use a trick: fit0 is used for initial values for fit1.
> fit1 <- vgam(BMI ~ s(age, df = c(4, 2)), etastart = predict(fit0),
+              lms.bcg(zero = 1), bmi.nz, trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = -1994.7413
VGAM  s.vam  loop  2 :  loglikelihood = -1994.7709
VGAM  s.vam  loop  3 :  loglikelihood = -1994.767
VGAM  s.vam  loop  4 :  loglikelihood = -1994.7675
VGAM  s.vam  loop  5 :  loglikelihood = -1994.7674
VGAM  s.vam  loop  6 :  loglikelihood = -1994.7678
VGAM  s.vam  loop  7 :  loglikelihood = -1994.7679
> 
> # Difficult to get a model that converges.  Here, we prematurely
> # stop iterations because it fails near the solution.
> fit2 <- vgam(BMI ~ s(age, df = c(4, 2)), maxit = 4,
+              lms.bcg(zero = 1, ilam = 3), bmi.nz, trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = -2064.7271
VGAM  s.vam  loop  2 :  loglikelihood = -2021.6408
VGAM  s.vam  loop  3 :  loglikelihood = -2002.1765
VGAM  s.vam  loop  4 :  loglikelihood = -1996.8621
Warning in vgam.fit(x = x, y = y, w = w, mf = mf, Xm2 = Xm2, Ym2 = Ym2,  :
  convergence not obtained in 4 IRLS iterations
> summary(fit1)

Call:
vgam(formula = BMI ~ s(age, df = c(4, 2)), family = lms.bcg(zero = 1), 
    data = bmi.nz, etastart = predict(fit0), trace = TRUE)

Names of additive predictors: lambda, mu, loglink(sigma)

Dispersion Parameter for lms.bcg family:   1

Log-likelihood: -1994.768 on 2091.033 degrees of freedom

Number of Fisher scoring iterations:  7 

DF for Terms and Approximate Chi-squares for Nonparametric Effects

                       Df Npar Df Npar Chisq  P(Chi)
(Intercept):1           1                           
(Intercept):2           1                           
(Intercept):3           1                           
s(age, df = c(4, 2)):1  1       3     35.986 0.00000
s(age, df = c(4, 2)):2  1       1      0.813 0.36733
> head(predict(fit1))
     lambda       mu loglink(sigma)
1 -1.887622 25.11243      -1.857957
2 -1.887622 25.80759      -1.860518
3 -1.887622 26.26919      -1.860749
4 -1.887622 25.37608      -1.859674
5 -1.887622 26.77079      -1.860267
6 -1.887622 25.78518      -1.860509
> head(fitted(fit1))
       25%      50%      75%
1 23.00604 25.50349 28.47362
2 23.64719 26.20739 29.25077
3 24.07054 26.67595 29.77294
4 23.25041 25.76987 28.76527
5 24.52932 27.18573 30.34360
6 23.62664 26.18465 29.22542
> head(bmi.nz)
       age      BMI
1 31.52966 22.77107
2 39.38045 27.70033
3 43.38940 28.18127
4 34.84894 25.08380
5 53.81990 26.46388
6 39.17002 36.19648
> # Person 1 is near the lower quartile of BMI amongst people his age
> head(cdf(fit1))
        1         2         3         4         5         6 
0.2280748 0.6353228 0.6341872 0.4312237 0.4313463 0.9693634 
> 
> ## Not run: 
> ##D # Quantile plot
> ##D par(bty = "l", mar=c(5, 4, 4, 3) + 0.1, xpd = TRUE)
> ##D qtplot(fit1, percentiles=c(5, 50, 90, 99), main = "Quantiles",
> ##D        xlim = c(15, 90), las = 1, ylab = "BMI", lwd = 2, lcol = 4)
> ##D 
> ##D # Density plot
> ##D ygrid <- seq(15, 43, len = 100)  # BMI ranges
> ##D par(mfrow = c(1, 1), lwd = 2)
> ##D (aa <- deplot(fit1, x0 = 20, y = ygrid, xlab = "BMI", col = "black",
> ##D   main = "PDFs at Age = 20 (black), 42 (red) and 55 (blue)"))
> ##D aa <- deplot(fit1, x0 = 42, y = ygrid, add=TRUE, llty=2, col="red")
> ##D aa <- deplot(fit1, x0 = 55, y = ygrid, add=TRUE, llty=4, col="blue",
> ##D              Attach = TRUE)
> ##D aa@post$deplot  # Contains density function values
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lms.bcn")
> ### * lms.bcn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lms.bcn
> ### Title: LMS Quantile Regression with a Box-Cox Transformation to
> ###   Normality
> ### Aliases: lms.bcn
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  require("VGAMdata")
> ##D mysub <- subset(xs.nz, sex == "M" & ethnicity == "Maori" & study1)
> ##D mysub <- transform(mysub, BMI = weight / height^2)
> ##D BMIdata <- na.omit(mysub)
> ##D BMIdata <- subset(BMIdata, BMI < 80 & age < 65,
> ##D                    select = c(age, BMI))  # Delete an outlier
> ##D summary(BMIdata)
> ##D 
> ##D fit <- vgam(BMI ~ s(age, df = c(4, 2)), lms.bcn(zero = 1), BMIdata)
> ##D 
> ##D par(mfrow = c(1, 2))
> ##D plot(fit, scol = "blue", se = TRUE)  # The two centered smooths
> ##D 
> ##D head(predict(fit))
> ##D head(fitted(fit))
> ##D head(BMIdata)
> ##D head(cdf(fit))  # Person 46 is probably overweight, given his age
> ##D 100 * colMeans(c(depvar(fit)) < fitted(fit))  # Empirical proportions
> ##D 
> ##D # Correct for "vgam" objects but not very elegant:
> ##D fit@family@linkinv(eta = predict(fit, data.frame(age = 60)),
> ##D    extra = list(percentiles = c(10, 50)))
> ##D 
> ##D if (FALSE) {
> ##D # These work for "vglm" objects:
> ##D fit2 <- vglm(BMI ~ bs(age, df = 4), lms.bcn(zero = 3), BMIdata)
> ##D predict(fit2, percentiles = c(10, 50),
> ##D         newdata = data.frame(age = 60), type = "response")
> ##D head(fitted(fit2, percentiles = c(10, 50)))  # Different percentiles
> ##D }
> ##D 
> ##D # Convergence problems? Use fit0 for initial values for fit1
> ##D fit0 <- vgam(BMI ~ s(age, df = 4), lms.bcn(zero = c(1, 3)), BMIdata)
> ##D fit1 <- vgam(BMI ~ s(age, df = c(4, 2)), lms.bcn(zero = 1), BMIdata,
> ##D             etastart = predict(fit0))
> ## End(Not run)
> 
> ## Not run: 
> ##D # Quantile plot
> ##D par(bty = "l", mar = c(5, 4, 4, 3) + 0.1, xpd = TRUE)
> ##D qtplot(fit, percentiles = c(5, 50, 90, 99), main = "Quantiles",
> ##D        xlim = c(15, 66), las = 1, ylab = "BMI", lwd = 2, lcol = 4)
> ##D 
> ##D # Density plot
> ##D ygrid <- seq(15, 43, len = 100)  # BMI ranges
> ##D par(mfrow = c(1, 1), lwd = 2)
> ##D (aa <- deplot(fit, x0 = 20, y = ygrid, xlab = "BMI", col = "black",
> ##D   main = "PDFs at Age = 20 (black), 42 (red) and 55 (blue)"))
> ##D aa <- deplot(fit, x0 = 42, y = ygrid, add = TRUE, llty = 2, col = "red")
> ##D aa <- deplot(fit, x0 = 55, y = ygrid, add = TRUE, llty = 4, col = "blue",
> ##D              Attach = TRUE)
> ##D aa@post$deplot  # Contains density function values
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lms.yjn")
> ### * lms.yjn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lms.yjn
> ### Title: LMS Quantile Regression with a Yeo-Johnson Transformation to
> ###   Normality
> ### Aliases: lms.yjn lms.yjn2
> ### Keywords: models regression
> 
> ### ** Examples
> 
> fit <- vgam(BMI ~ s(age, df = 4), lms.yjn, bmi.nz, trace = TRUE)
VGAM  s.vam  loop  1 :  loglikelihood = -1361.2243
VGAM  s.vam  loop  2 :  loglikelihood = -1357.5289
VGAM  s.vam  loop  3 :  loglikelihood = -1357.3829
VGAM  s.vam  loop  4 :  loglikelihood = -1357.3542
VGAM  s.vam  loop  5 :  loglikelihood = -1357.3539
VGAM  s.vam  loop  6 :  loglikelihood = -1357.3519
VGAM  s.vam  loop  7 :  loglikelihood = -1357.3521
VGAM  s.vam  loop  8 :  loglikelihood = -1357.3519
VGAM  s.vam  loop  9 :  loglikelihood = -1357.3519
> head(predict(fit))
     lambda          mu loglink(sigma)
1 0.8078551 -0.77249525       1.422234
2 0.8078551 -0.05619456       1.422234
3 0.8078551  0.42445223       1.422234
4 0.8078551 -0.50654859       1.422234
5 0.8078551  0.90505962       1.422234
6 0.8078551 -0.07976647       1.422234
> head(fitted(fit))
       25%      50%      75%
1 23.07743 25.37026 28.41762
2 23.63333 26.04344 29.34193
3 24.01604 26.53978 29.98527
4 23.28199 25.61305 28.75568
5 24.40811 27.07187 30.64528
6 23.61477 26.02016 29.31084
> head(bmi.nz)
       age      BMI
1 31.52966 22.77107
2 39.38045 27.70033
3 43.38940 28.18127
4 34.84894 25.08380
5 53.81990 26.46388
6 39.17002 36.19648
> # Person 1 is near the lower quartile of BMI amongst people his age
> head(cdf(fit))
        1         2         3         4         5         6 
0.2201431 0.6410319 0.6331574 0.4435149 0.4470689 0.9646218 
> 
> ## Not run: 
> ##D # Quantile plot
> ##D par(bty = "l", mar = c(5, 4, 4, 3) + 0.1, xpd = TRUE)
> ##D qtplot(fit, percentiles = c(5, 50, 90, 99), main = "Quantiles",
> ##D        xlim = c(15, 90), las = 1, ylab = "BMI", lwd = 2, lcol = 4)
> ##D 
> ##D # Density plot
> ##D ygrid <- seq(15, 43, len = 100)  # BMI ranges
> ##D par(mfrow = c(1, 1), lwd = 2)
> ##D (Z <- deplot(fit, x0 = 20, y = ygrid, xlab = "BMI", col = "black",
> ##D     main = "PDFs at Age = 20 (black), 42 (red) and 55 (blue)"))
> ##D Z <- deplot(fit, x0 = 42, y = ygrid, add = TRUE, llty = 2, col = "red")
> ##D Z <- deplot(fit, x0 = 55, y = ygrid, add = TRUE, llty = 4, col = "blue",
> ##D             Attach = TRUE)
> ##D with(Z@post, deplot)  # Contains PDF values; == a@post$deplot
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("log1mexp")
> ### * log1mexp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: log1mexp
> ### Title: Logarithms with an Unit Offset and Exponential Term
> ### Aliases: log1mexp log1pexp
> 
> ### ** Examples
> 
> x <-  c(10, 50, 100, 200, 400, 500, 800, 1000, 1e4, 1e5, 1e20, Inf, NA)
> log1pexp(x)
 [1] 1.000005e+01 5.000000e+01 1.000000e+02 2.000000e+02 4.000000e+02
 [6] 5.000000e+02 8.000000e+02 1.000000e+03 1.000000e+04 1.000000e+05
[11] 1.000000e+20          Inf           NA
> log(1 + exp(x))  # Naive; suffers from overflow
 [1]  10.00005  50.00000 100.00000 200.00000 400.00000 500.00000       Inf
 [8]       Inf       Inf       Inf       Inf       Inf        NA
> log1mexp(x)
 [1]  -4.540096e-05  -1.928750e-22  -3.720076e-44  -1.383897e-87 -1.915170e-174
 [6] -7.124576e-218   0.000000e+00   0.000000e+00   0.000000e+00   0.000000e+00
[11]   0.000000e+00   0.000000e+00             NA
> log(1 - exp(-x))
 [1] -4.540096e-05  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00
 [6]  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00
[11]  0.000000e+00  0.000000e+00            NA
> y <- -x
> log1pexp(y)
 [1]  4.539890e-05  1.928750e-22  3.720076e-44  1.383897e-87 1.915170e-174
 [6] 7.124576e-218  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00
[11]  0.000000e+00  0.000000e+00            NA
> log(1 + exp(y))  # Naive; suffers from inaccuracy
 [1] 4.53989e-05 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00
 [7] 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00
[13]          NA
> 
> 
> 
> cleanEx()
> nameEx("logF")
> ### * logF
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logF
> ### Title: Natural Exponential Family Generalized Hyperbolic Secant
> ###   Distribution Family Function
> ### Aliases: logF
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> ldata <- data.frame(y1 = rnorm(nn, +1, sd = exp(2)),  # Not proper data
+                     x2 = rnorm(nn, -1, sd = exp(2)),
+                     y2 = rnorm(nn, -1, sd = exp(2)))  # Not proper data
> fit1 <- vglm(y1 ~ 1 , logF, ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -5867.6777
VGLM    linear loop  2 :  loglikelihood = -5087.4955
VGLM    linear loop  3 :  loglikelihood = -4349.7242
VGLM    linear loop  4 :  loglikelihood = -3832.1051
VGLM    linear loop  5 :  loglikelihood = -3566.5114
VGLM    linear loop  6 :  loglikelihood = -3495.5544
VGLM    linear loop  7 :  loglikelihood = -3490.5587
VGLM    linear loop  8 :  loglikelihood = -3490.5328
VGLM    linear loop  9 :  loglikelihood = -3490.5328
> fit2 <- vglm(y2 ~ x2, logF, ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -4646.8705
VGLM    linear loop  2 :  loglikelihood = -4092.0968
VGLM    linear loop  3 :  loglikelihood = -3678.8916
VGLM    linear loop  4 :  loglikelihood = -3517.99
VGLM    linear loop  5 :  loglikelihood = -3490.1745
VGLM    linear loop  6 :  loglikelihood = -3489.1624
VGLM    linear loop  7 :  loglikelihood = -3489.1596
VGLM    linear loop  8 :  loglikelihood = -3489.1596
> coef(fit2, matrix = TRUE)
            loglink(shape1) loglink(shape2)
(Intercept)    -1.831619071   -1.6893645100
x2              0.003081468   -0.0004550506
> summary(fit2)
Call:
vglm(formula = y2 ~ x2, family = logF, data = ldata, trace = TRUE)

Coefficients: 
                Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.8316191  0.0387469 -47.271   <2e-16 ***
(Intercept):2 -1.6893645  0.0405418 -41.670   <2e-16 ***
x2:1           0.0030815  0.0049863   0.618    0.537    
x2:2          -0.0004551  0.0052302  -0.087    0.931    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2)

Log-likelihood: -3489.16 on 1996 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> vcov(fit2)
              (Intercept):1 (Intercept):2         x2:1         x2:2
(Intercept):1  1.501324e-03  5.845900e-04 2.945581e-05 1.099973e-05
(Intercept):2  5.845900e-04  1.643635e-03 1.100002e-05 2.865142e-05
x2:1           2.945581e-05  1.100002e-05 2.486284e-05 9.697001e-06
x2:2           1.099973e-05  2.865142e-05 9.697001e-06 2.735499e-05
> 
> head(fitted(fit1))
          [,1]
[1,] 0.9139312
[2,] 0.9139312
[3,] 0.9139312
[4,] 0.9139312
[5,] 0.9139312
[6,] 0.9139312
> with(ldata, mean(y1))
[1] 0.9139312
> max(abs(head(fitted(fit1)) - with(ldata, mean(y1))))
[1] 5.778295e-09
> 
> 
> 
> cleanEx()
> nameEx("logF.UC")
> ### * logF.UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dlogF
> ### Title: log F Distribution
> ### Aliases: dlogF
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  shape1 <- 1.5; shape2 <- 0.5; x <- seq(-5, 8, length = 1001)
> ##D plot(x, dlogF(x, shape1, shape2), type = "l",
> ##D      las = 1, col = "blue", ylab = "pdf",
> ##D      main = "log F density function")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("logLikvlm")
> ### * logLikvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logLik.vlm
> ### Title: Extract Log-likelihood for VGLMs/VGAMs/etc.
> ### Aliases: logLik.vlm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(x2 = runif(nn <- 50))
> zdata <- transform(zdata, Ps01    = logitlink(-0.5       , inverse = TRUE),
+                           Ps02    = logitlink( 0.5       , inverse = TRUE),
+                           lambda1 =  loglink(-0.5 + 2*x2, inverse = TRUE),
+                           lambda2 =  loglink( 0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata, y1 = rzipois(nn, lambda = lambda1, pstr0 = Ps01),
+                           y2 = rzipois(nn, lambda = lambda2, pstr0 = Ps02))
> 
> with(zdata, table(y1))  # Eyeball the data
y1
 0  1  2  3  4  5 
27  5  9  4  4  1 
> with(zdata, table(y2))
y2
 0  1  2  3  5  6  7  8 11 12 17 18 
35  2  1  3  2  1  1  1  1  1  1  1 
> fit2 <- vglm(cbind(y1, y2) ~ x2, zipoisson(zero = NULL), data = zdata)
> 
> logLik(fit2)  # Summed over the two responses
[1] -128.545
> sum(logLik(fit2, sum = FALSE))  # For checking purposes
[1] -128.545
> (ll.matrix <- logLik(fit2, sum = FALSE))  # nn x 2 matrix
           y1         y2
1  -0.4107583 -2.4703049
2  -2.1299906 -0.3154584
3  -0.6811986 -0.3917259
4  -0.9429246 -3.6545690
5  -0.3637334 -0.2302663
6  -0.9375150 -0.4810598
7  -0.9616704 -0.4942216
8  -1.8537914 -0.4167564
9  -1.8094684 -3.4984173
10 -0.2750019 -0.1627359
11 -2.5432769 -5.6744212
12 -0.3463442 -2.3665710
13 -1.9169591 -0.4238130
14 -0.5082621 -0.3209432
15 -1.8807877 -0.4458603
16 -0.6110571 -0.3670297
17 -2.0000103 -3.2521104
18 -2.2443523 -0.5079365
19 -0.5047231 -0.3190925
20 -2.3840775 -5.2540805
21 -0.9567525 -3.6025733
22 -1.5877395 -0.2356179
23 -0.7534455 -0.4142743
24 -0.3130128 -0.1922974
25 -0.4120749 -2.5957309
26 -2.1012915 -0.3218530
27 -1.7766032 -0.1421673
28 -0.5067682 -0.3201642
29 -2.1700012 -0.4730354
30 -3.2611515 -0.3004285
31 -0.5965748 -0.3613533
32 -1.8203591 -0.3996508
33 -0.6071950 -0.3655386
34 -0.3529548 -0.2223948
35 -2.2528717 -3.1274923
36 -0.7683589 -0.4188305
37 -1.9827307 -0.4524064
38 -0.3021121 -0.1838842
39 -0.8155609 -4.2801942
40 -0.5321961 -0.3329686
41 -0.8893004 -3.9286203
42 -0.7493105 -0.4130124
43 -1.8992510 -0.4493666
44 -2.4482117 -0.3855920
45 -1.8759872 -2.7975292
46 -2.3496788 -0.4510921
47 -0.2539927 -0.1462485
48 -0.5920933 -2.6941825
49 -2.0415140 -0.4358657
50 -1.8147530 -2.9634632
> colSums(ll.matrix)  # log-likelihood for each response
       y1        y2 
-64.08975 -64.45520 
> 
> 
> 
> cleanEx()
> nameEx("logUC")
> ### * logUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Log
> ### Title: Logarithmic Distribution
> ### Aliases: Log dlog plog qlog rlog
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dlog(1:20, 0.5)
 [1] 7.213475e-01 1.803369e-01 6.011229e-02 2.254211e-02 9.016844e-03
 [6] 3.757018e-03 1.610151e-03 7.044409e-04 3.130849e-04 1.408882e-04
[11] 6.404009e-05 2.935171e-05 1.354694e-05 6.289651e-06 2.935171e-06
[16] 1.375861e-06 6.474641e-07 3.057469e-07 1.448275e-07 6.879306e-08
> rlog(20, 0.5)
 [1] 1 1 1 3 1 2 3 1 1 1 1 1 1 1 2 1 1 5 1 2
> 
> ## Not run: 
> ##D  shape <- 0.8; x <- 1:10
> ##D plot(x, dlog(x, shape = shape), type = "h", ylim = 0:1,
> ##D      sub = "shape=0.8", las = 1, col = "blue", ylab = "shape",
> ##D      main = "Logarithmic distribution: blue=PDF; orange=CDF")
> ##D lines(x + 0.1, plog(x, shape), col = "orange", lty = 3, type = "h") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("logclink")
> ### * logclink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logclink
> ### Title: Complementary-log Link Function
> ### Aliases: logclink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D logclink(seq(-0.2, 1.1, by = 0.1))  # Has NAs
> ## End(Not run)
> logclink(seq(-0.2,1.1,by=0.1),bvalue=1-.Machine$double.eps) # Has no NAs
 [1]   0.18232156   0.09531018   0.00000000  -0.10536052  -0.22314355
 [6]  -0.35667494  -0.51082562  -0.69314718  -0.91629073  -1.20397280
[11]  -1.60943791  -2.30258509 -36.04365339 -36.04365339
> 
> 
> 
> cleanEx()
> nameEx("logff")
> ### * logff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logff
> ### Title: Logarithmic Distribution
> ### Aliases: logff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000
> ldata <- data.frame(y = rlog(nn, shape = logitlink(0.2, inv = TRUE)))
> fit <- vglm(y ~ 1, logff, data = ldata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 0.2858634
VGLM    linear loop  2 :  coefficients = 0.24149289
VGLM    linear loop  3 :  coefficients = 0.24064958
VGLM    linear loop  4 :  coefficients = 0.24064928
VGLM    linear loop  5 :  coefficients = 0.24064928
> coef(fit, matrix = TRUE)
            logitlink(shape)
(Intercept)        0.2406493
> Coef(fit)
    shape 
0.5598736 
> ## Not run: 
> ##D with(ldata, spikeplot(y, col = "blue", capped = TRUE))
> ##D x <- seq(1, with(ldata, max(y)), by = 1)
> ##D with(ldata, lines(x + 0.1, dlog(x, Coef(fit)[1]), col = "orange",
> ##D         type = "h", lwd = 2)) 
> ## End(Not run)
> 
> # Example: Corbet (1943) butterfly Malaya data
> corbet <- data.frame(nindiv = 1:24,
+                  ofreq = c(118, 74, 44, 24, 29, 22, 20, 19, 20, 15, 12,
+                            14, 6, 12, 6, 9, 9, 6, 10, 10, 11, 5, 3, 3))
> fit <- vglm(nindiv ~ 1, logff, data = corbet, weights = ofreq)
> coef(fit, matrix = TRUE)
            logitlink(shape)
(Intercept)         3.002278
> shapehat <- Coef(fit)["shape"]
> pdf2 <- dlog(x = with(corbet, nindiv), shape = shapehat)
> print(with(corbet, cbind(nindiv, ofreq, fitted = pdf2 * sum(ofreq))),
+       digits = 1)
      nindiv ofreq fitted
 [1,]      1   118    156
 [2,]      2    74     75
 [3,]      3    44     47
 [4,]      4    24     34
 [5,]      5    29     26
 [6,]      6    22     20
 [7,]      7    20     17
 [8,]      8    19     14
 [9,]      9    20     12
[10,]     10    15     10
[11,]     11    12      9
[12,]     12    14      8
[13,]     13     6      7
[14,]     14    12      6
[15,]     15     6      5
[16,]     16     9      5
[17,]     17     9      4
[18,]     18     6      4
[19,]     19    10      3
[20,]     20    10      3
[21,]     21    11      3
[22,]     22     5      3
[23,]     23     3      2
[24,]     24     3      2
> 
> 
> 
> cleanEx()
> nameEx("logistic")
> ### * logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logistic
> ### Title: Logistic Distribution Family Function
> ### Aliases: logistic logistic1 logistic
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Location unknown, scale known
> ldata <- data.frame(x2 = runif(nn <- 500))
> ldata <- transform(ldata, y1 = rlogis(nn, loc = 1+5*x2, sc = exp(2)))
> fit1 <- vglm(y1 ~ x2, logistic1(scale = exp(2)), ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2022.2269
VGLM    linear loop  2 :  loglikelihood = -2022.1506
VGLM    linear loop  3 :  loglikelihood = -2022.1506
VGLM    linear loop  4 :  loglikelihood = -2022.1506
> coef(fit1, matrix = TRUE)
              location
(Intercept) 0.08678239
x2          7.18352312
> 
> # Both location and scale unknown
> ldata <- transform(ldata, y2 = rlogis(nn, loc = 1 + 5*x2, exp(x2)))
> fit2 <- vglm(cbind(y1, y2) ~ x2, logistic, data = ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3996.9725
VGLM    linear loop  2 :  loglikelihood = -3662.6472
VGLM    linear loop  3 :  loglikelihood = -3422.3424
VGLM    linear loop  4 :  loglikelihood = -3315.2843
VGLM    linear loop  5 :  loglikelihood = -3299.9519
VGLM    linear loop  6 :  loglikelihood = -3299.664
VGLM    linear loop  7 :  loglikelihood = -3299.6636
VGLM    linear loop  8 :  loglikelihood = -3299.6636
> coef(fit2, matrix = TRUE)
             location1 loglink(scale1) location2 loglink(scale2)
(Intercept) 0.07864622        2.040647  1.466349       0.5483126
x2          7.19421302        0.000000  3.444389       0.0000000
> vcov(fit2)
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4      x2:1
(Intercept):1      1.445029   0.000000000    0.00000000   0.000000000 -2.198499
(Intercept):2      0.000000   0.001398644    0.00000000   0.000000000  0.000000
(Intercept):3      0.000000   0.000000000    0.07305773   0.000000000  0.000000
(Intercept):4      0.000000   0.000000000    0.00000000   0.001398644  0.000000
x2:1              -2.198499   0.000000000    0.00000000   0.000000000  4.435544
x2:2               0.000000   0.000000000   -0.11115163   0.000000000  0.000000
                    x2:2
(Intercept):1  0.0000000
(Intercept):2  0.0000000
(Intercept):3 -0.1111516
(Intercept):4  0.0000000
x2:1           0.0000000
x2:2           0.2242521
> summary(fit2)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = logistic, data = ldata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.07865    1.20209   0.065 0.947836    
(Intercept):2  2.04065    0.03740  54.565  < 2e-16 ***
(Intercept):3  1.46635    0.27029   5.425 5.79e-08 ***
(Intercept):4  0.54831    0.03740  14.661  < 2e-16 ***
x2:1           7.19421    2.10607   3.416 0.000636 ***
x2:2           3.44439    0.47355   7.274 3.50e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: location1, loglink(scale1), location2, 
loglink(scale2)

Log-likelihood: -3299.664 on 1994 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("logitlink")
> ### * logitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logitlink
> ### Title: Logit Link Function
> ### Aliases: logitlink extlogitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, by = 0.01)
> logitlink(p)
 [1] -4.59511985 -3.89182030 -3.47609869 -3.17805383 -2.94443898 -2.75153531
 [7] -2.58668934 -2.44234704 -2.31363493 -2.19722458 -2.09074110 -1.99243016
[13] -1.90095876 -1.81528997 -1.73460106 -1.65822808 -1.58562726 -1.51634749
[19] -1.45001018 -1.38629436 -1.32492541 -1.26566637 -1.20831121 -1.15267951
[25] -1.09861229 -1.04596856 -0.99462258 -0.94446161 -0.89538405 -0.84729786
[31] -0.80011930 -0.75377180 -0.70818506 -0.66329422 -0.61903921 -0.57536414
[37] -0.53221681 -0.48954823 -0.44731222 -0.40546511 -0.36396538 -0.32277339
[43] -0.28185115 -0.24116206 -0.20067070 -0.16034265 -0.12014431 -0.08004271
[49] -0.04000533  0.00000000  0.04000533  0.08004271  0.12014431  0.16034265
[55]  0.20067070  0.24116206  0.28185115  0.32277339  0.36396538  0.40546511
[61]  0.44731222  0.48954823  0.53221681  0.57536414  0.61903921  0.66329422
[67]  0.70818506  0.75377180  0.80011930  0.84729786  0.89538405  0.94446161
[73]  0.99462258  1.04596856  1.09861229  1.15267951  1.20831121  1.26566637
[79]  1.32492541  1.38629436  1.45001018  1.51634749  1.58562726  1.65822808
[85]  1.73460106  1.81528997  1.90095876  1.99243016  2.09074110  2.19722458
[91]  2.31363493  2.44234704  2.58668934  2.75153531  2.94443898  3.17805383
[97]  3.47609869  3.89182030  4.59511985
> max(abs(logitlink(logitlink(p), inverse = TRUE) - p))  # 0?
[1] 1.110223e-16
> 
> p <- c(seq(-0.02, 0.02, by = 0.01), seq(0.97, 1.02, by = 0.01))
> logitlink(p)  # Has NAs
Warning in qlogis(theta) : NaNs produced
 [1]       NaN       NaN      -Inf -4.595120 -3.891820  3.476099  3.891820
 [8]  4.595120       Inf       NaN       NaN
> logitlink(p, bvalue = .Machine$double.eps)  # Has no NAs
 [1] -36.043653 -36.043653 -36.043653  -4.595120  -3.891820   3.476099
 [7]   3.891820   4.595120  36.043653  36.043653  36.043653
> 
> p <- seq(0.9, 2.2, by = 0.1)
> extlogitlink(p, min = 1, max = 2,
+              bminvalue = 1 + .Machine$double.eps,
+              bmaxvalue = 2 - .Machine$double.eps)  # Has no NAs
 [1] -36.0436534 -36.0436534  -2.1972246  -1.3862944  -0.8472979  -0.4054651
 [7]   0.0000000   0.4054651   0.8472979   1.3862944   2.1972246  36.0436534
[13]  36.0436534  36.0436534
> 
> ## Not run: 
> ##D  par(mfrow = c(2,2), lwd = (mylwd <- 2))
> ##D y <- seq(-4, 4, length = 100)
> ##D p <- seq(0.01, 0.99, by = 0.01)
> ##D for (d in 0:1) {
> ##D   myinv <- (d > 0)
> ##D   matplot(p, cbind( logitlink(p, deriv = d, inv = myinv),
> ##D                    probitlink(p, deriv = d, inv = myinv)), las = 1,
> ##D           type = "n", col = "purple", ylab = "transformation",
> ##D           main = if (d ==  0) "Some probability link functions"
> ##D           else "1 / first derivative")
> ##D   lines(p,   logitlink(p, deriv = d, inverse = myinv), col = "limegreen")
> ##D   lines(p,  probitlink(p, deriv = d, inverse = myinv), col = "purple")
> ##D   lines(p, clogloglink(p, deriv = d, inverse = myinv), col = "chocolate")
> ##D   lines(p, cauchitlink(p, deriv = d, inverse = myinv), col = "tan")
> ##D   if (d ==  0) {
> ##D     abline(v = 0.5, h = 0, lty = "dashed")
> ##D     legend(0, 4.5, c("logitlink", "probitlink",
> ##D            "clogloglink", "cauchitlink"), col = c("limegreen", "purple",
> ##D            "chocolate", "tan"), lwd = mylwd)
> ##D   } else
> ##D     abline(v = 0.5, lty = "dashed")
> ##D }
> ##D 
> ##D for (d in 0) {
> ##D   matplot(y, cbind(logitlink(y, deriv = d, inverse = TRUE),
> ##D                    probitlink(y, deriv = d, inverse = TRUE)), las = 1,
> ##D           type = "n", col = "purple", xlab = "transformation", ylab = "p",
> ##D           main = if (d ==  0) "Some inverse probability link functions"
> ##D           else "First derivative")
> ##D   lines(y,   logitlink(y, deriv = d, inv = TRUE), col = "limegreen")
> ##D   lines(y,  probitlink(y, deriv = d, inv = TRUE), col = "purple")
> ##D   lines(y, clogloglink(y, deriv = d, inv = TRUE), col = "chocolate")
> ##D   lines(y, cauchitlink(y, deriv = d, inv = TRUE), col = "tan")
> ##D   if (d ==  0) {
> ##D     abline(h = 0.5, v = 0, lty = "dashed")
> ##D     legend(-4, 1, c("logitlink", "probitlink", "clogloglink",
> ##D            "cauchitlink"), col = c("limegreen", "purple",
> ##D            "chocolate", "tan"), lwd = mylwd)
> ##D   }
> ##D }
> ##D 
> ##D p <- seq(0.21, 0.59, by = 0.01)
> ##D plot(p, extlogitlink(p, min = 0.2, max = 0.6), xlim = c(0, 1),
> ##D      type = "l", col = "black", ylab = "transformation",
> ##D      las = 1, main = "extlogitlink(p, min = 0.2, max = 0.6)")
> ##D par(lwd = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("logitoffsetlink")
> ### * logitoffsetlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logitoffsetlink
> ### Title: Logit-with-an-Offset Link Function
> ### Aliases: logitoffsetlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.05, 0.99, by = 0.01); myoff <- 0.05
> logitoffsetlink(p, myoff)
 [1] -5.940171253 -4.280930518 -3.678184165 -3.298013145 -3.017955410
 [6] -2.795061578 -2.609171320 -2.449188567 -2.308348798 -2.182221411
[11] -2.067745502 -1.962717267 -1.865496796 -1.774830407 -1.689737855
[16] -1.609437912 -1.533297649 -1.460796888 -1.391502706 -1.325050736
[21] -1.261131218 -1.199478415 -1.139862457 -1.082082987 -1.025964134
[26] -0.971350509 -0.918103969 -0.866100987 -0.815230494 -0.765392087
[31] -0.716494545 -0.668454568 -0.621195727 -0.574647556 -0.528744780
[36] -0.483426650 -0.438636360 -0.394320544 -0.350428827 -0.306913434
[41] -0.263728831 -0.220831412 -0.178179205 -0.135731614 -0.093449167
[46] -0.051293294 -0.009226103  0.032789823  0.074791629  0.116816385
[51]  0.158901283  0.201083833  0.243402071  0.285894762  0.328601623
[56]  0.371563556  0.414822897  0.458423682  0.502411949  0.546836061
[61]  0.591747066  0.637199107  0.683249879  0.729961154  0.777399378
[66]  0.825636364  0.874750088  0.924825634  0.975956288  1.028244847
[71]  1.081805170  1.136764055  1.193263490  1.251463423  1.311545158
[76]  1.373715579  1.438212460  1.505311203  1.575333500  1.648658626
[81]  1.725738368  1.807117125  1.893459447  1.985588633  2.084542148
[86]  2.191653532  2.308677607  2.437989730  2.582918804  2.748338720
[91]  2.941803932  3.175968324  3.474551101  3.890799369  4.594614672
> max(abs(logitoffsetlink(logitoffsetlink(p, myoff),
+         myoff, inverse = TRUE) - p))  # Should be 0
[1] 2.220446e-16
> 
> 
> 
> cleanEx()
> nameEx("loglinb2")
> ### * loglinb2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: loglinb2
> ### Title: Loglinear Model for Two Binary Responses
> ### Aliases: loglinb2
> ### Keywords: models regression
> 
> ### ** Examples
> 
> coalminers <- transform(coalminers, Age = (age - 42) / 5)
> # Get the n x 4 matrix of counts
> fit0 <- vglm(cbind(nBnW,nBW,BnW,BW) ~ Age, binom2.or, coalminers)
> counts <- round(c(weights(fit0, type = "prior")) * depvar(fit0))
> # Create a n x 2 matrix response for loglinb2()
> # bwmat <- matrix(c(0,0, 0,1, 1,0, 1,1), 4, 2, byrow = TRUE)
> bwmat <- cbind(bln = c(0,0,1,1), wheeze = c(0,1,0,1))
> matof1 <- matrix(1, nrow(counts), 1)
> newminers <-
+   data.frame(bln    = kronecker(matof1, bwmat[, 1]),
+              wheeze = kronecker(matof1, bwmat[, 2]),
+              wt     = c(t(counts)),
+              Age    = with(coalminers, rep(age, rep(4, length(age)))))
> newminers <- newminers[with(newminers, wt) > 0,]
> 
> fit <- vglm(cbind(bln,wheeze) ~ Age, loglinb2(zero = NULL),
+             weight = wt, data = newminers)
> coef(fit, matrix = TRUE)  # Same! (at least for the log odds-ratio)
                    u1          u2        u12
(Intercept) -7.8071500 -3.69405946  4.4551596
Age          0.1030801  0.04012099 -0.0332305
> summary(fit)
Call:
vglm(formula = cbind(bln, wheeze) ~ Age, family = loglinb2(zero = NULL), 
    data = newminers, weights = wt)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -7.807150   0.229187 -34.065  < 2e-16 ***
(Intercept):2 -3.694059   0.099871 -36.988  < 2e-16 ***
(Intercept):3  4.455160   0.278700  15.986  < 2e-16 ***
Age:1          0.103080   0.004476  23.030  < 2e-16 ***
Age:2          0.040121   0.002232  17.974  < 2e-16 ***
Age:3         -0.033230   0.005492  -6.051 1.44e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: u1, u2, u12

Log-likelihood: -12863.55 on 102 degrees of freedom

Number of Fisher scoring iterations: 5 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1', '(Intercept):2'

> 
> # Try reconcile this with McCullagh and Nelder (1989), p.234
> (0.166-0.131) / 0.027458   # 1.275 is approximately 1.25
[1] 1.274674
> 
> 
> 
> cleanEx()
> nameEx("loglinb3")
> ### * loglinb3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: loglinb3
> ### Title: Loglinear Model for Three Binary Responses
> ### Aliases: loglinb3
> ### Keywords: models regression
> 
> ### ** Examples
> 
> lfit <- vglm(cbind(cyadea, beitaw, kniexc) ~ altitude, loglinb3,
+              data = hunua, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -747.96409
VGLM    linear loop  2 :  loglikelihood = -746.63769
VGLM    linear loop  3 :  loglikelihood = -746.63197
VGLM    linear loop  4 :  loglikelihood = -746.63169
VGLM    linear loop  5 :  loglikelihood = -746.63166
VGLM    linear loop  6 :  loglikelihood = -746.63166
VGLM    linear loop  7 :  loglikelihood = -746.63166
> coef(lfit, matrix = TRUE)
                      u1          u2          u3       u12       u13     u23
(Intercept) -0.977443113 -1.89016208 -0.37718273 0.6079861 0.1550313 1.11723
altitude    -0.000570124  0.00385029  0.00161104 0.0000000 0.0000000 0.00000
> head(fitted(lfit))
        000       001        010       011        100        101        110
1 0.2667112 0.2114476 0.05697031 0.1380439 0.09533643 0.08825711 0.03740342
2 0.2720142 0.2122054 0.05590845 0.1333059 0.09778795 0.08907985 0.03691613
3 0.2877835 0.2139148 0.05269713 0.1197206 0.10524166 0.09134649 0.03539596
4 0.2929812 0.2142980 0.05162252 0.1154050 0.10775504 0.09203332 0.03487242
5 0.2929812 0.2142980 0.05162252 0.1154050 0.10775504 0.09203332 0.03487242
6 0.2877835 0.2139148 0.05269713 0.1197206 0.10524166 0.09134649 0.03539596
         111
1 0.10583008
2 0.10278206
3 0.09389985
4 0.09103252
5 0.09103252
6 0.09389985
> summary(lfit)
Call:
vglm(formula = cbind(cyadea, beitaw, kniexc) ~ altitude, family = loglinb3, 
    data = hunua, trace = TRUE)

Coefficients: 
                Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.9774431  0.2222395  -4.398 1.09e-05 ***
(Intercept):2 -1.8901621  0.2647499  -7.139 9.37e-13 ***
(Intercept):3 -0.3771827  0.1969334  -1.915  0.05546 .  
(Intercept):4  0.6079861  0.2326655   2.613  0.00897 ** 
(Intercept):5  0.1550313  0.2467644   0.628  0.52984    
(Intercept):6  1.1172304  0.2456804   4.547 5.43e-06 ***
altitude:1    -0.0005701  0.0009213  -0.619  0.53602    
altitude:2     0.0038503  0.0009624   4.001 6.31e-05 ***
altitude:3     0.0016110  0.0009695   1.662  0.09657 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: u1, u2, u3, u12, u13, u23

Log-likelihood: -746.6317 on 2343 degrees of freedom

Number of Fisher scoring iterations: 7 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("loglink")
> ### * loglink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: loglink
> ### Title: Log Link Function, and Variants
> ### Aliases: loglink negloglink logneglink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  loglink(seq(-0.2, 0.5, by = 0.1))
> ##D  loglink(seq(-0.2, 0.5, by = 0.1), bvalue = .Machine$double.xmin)
> ##D negloglink(seq(-0.2, 0.5, by = 0.1))
> ##D negloglink(seq(-0.2, 0.5, by = 0.1), bvalue = .Machine$double.xmin) 
> ## End(Not run)
> logneglink(seq(-0.5, -0.2, by = 0.1))
[1] -0.6931472 -0.9162907 -1.2039728 -1.6094379
> 
> 
> 
> cleanEx()
> nameEx("logloglink")
> ### * logloglink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logloglink
> ### Title: Log-log and Log-log-log Link Functions
> ### Aliases: logloglink loglog loglogloglink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> x <- seq(0.8, 1.5, by = 0.1)
> logloglink(x)  # Has NAs
Warning in log(log(theta)) : NaNs produced
[1]        NaN        NaN       -Inf -2.3506187 -1.7019834 -1.3380214 -1.0892396
[8] -0.9027205
> logloglink(x, bvalue = 1.0 + .Machine$double.eps)  # Has no NAs
[1] -36.0436534 -36.0436534 -36.0436534  -2.3506187  -1.7019834  -1.3380214
[7]  -1.0892396  -0.9027205
> 
> x <- seq(1.01, 10, len = 100)
> logloglink(x)
  [1] -4.610149477 -2.342943351 -1.741196565 -1.391297954 -1.148325818
  [6] -0.964431072 -0.817853393 -0.696875256 -0.594476773 -0.506129099
 [11] -0.428746365 -0.360135528 -0.298685004 -0.243177735 -0.192673499
 [16] -0.146431869 -0.103860074 -0.064476739 -0.027886026  0.006241174
 [21]  0.038181203  0.068168147  0.096401880  0.123054303  0.148274228
 [26]  0.172191262  0.194918903  0.216557058  0.237194077  0.256908438
 [31]  0.275770131  0.293841816  0.311179785  0.327834784  0.343852697
 [36]  0.359275137  0.374139947  0.388481631  0.402331731  0.415719141
 [41]  0.428670396  0.441209912  0.453360203  0.465142065  0.476574746
 [46]  0.487676093  0.498462677  0.508949912  0.519152159  0.529082815
 [51]  0.538754394  0.548178608  0.557366423  0.566328126  0.575073377
 [56]  0.583611258  0.591950315  0.600098601  0.608063711  0.615852816
 [61]  0.623472691  0.630929747  0.638230052  0.645379356  0.652383114
 [66]  0.659246501  0.665974436  0.672571593  0.679042420  0.685391151
 [71]  0.691621822  0.697738278  0.703744191  0.709643063  0.715438242
 [76]  0.721132927  0.726730178  0.732232923  0.737643967  0.742965997
 [81]  0.748201587  0.753353207  0.758423229  0.763413928  0.768327490
 [86]  0.773166015  0.777931524  0.782625959  0.787251191  0.791809018
 [91]  0.796301176  0.800729334  0.805095103  0.809400036  0.813645631
 [96]  0.817833334  0.821964542  0.826040602  0.830062817  0.834032445
> max(abs(logloglink(logloglink(x), inverse = TRUE) - x))  # 0?
[1] 1.776357e-15
> 
> 
> 
> cleanEx()
> nameEx("lognormal")
> ### * lognormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lognormal
> ### Title: Lognormal Distribution
> ### Aliases: lognormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata2 <- data.frame(x2 = runif(nn <- 1000))
> ldata2 <- transform(ldata2, y1 = rlnorm(nn, 1 + 2 * x2, sd = exp(-1)),
+                             y2 = rlnorm(nn, 1, sd = exp(-1 + x2)))
> fit1 <- vglm(y1 ~ x2, lognormal(zero = 2), data = ldata2, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3148.6796
VGLM    linear loop  2 :  loglikelihood = -2770.2715
VGLM    linear loop  3 :  loglikelihood = -2529.442
VGLM    linear loop  4 :  loglikelihood = -2448.9762
VGLM    linear loop  5 :  loglikelihood = -2441.763
VGLM    linear loop  6 :  loglikelihood = -2441.7105
VGLM    linear loop  7 :  loglikelihood = -2441.7105
> fit2 <- vglm(y2 ~ x2, lognormal(zero = 1), data = ldata2, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1972.4928
VGLM    linear loop  2 :  loglikelihood = -1964.9823
VGLM    linear loop  3 :  loglikelihood = -1964.9248
VGLM    linear loop  4 :  loglikelihood = -1964.9248
> coef(fit1, matrix = TRUE)
              meanlog loglink(sdlog)
(Intercept) 0.9870984     -0.9676114
x2          2.0078082      0.0000000
> coef(fit2, matrix = TRUE)
              meanlog loglink(sdlog)
(Intercept) 0.9939172     -0.9622932
x2          0.0000000      1.0278995
> 
> 
> 
> cleanEx()
> nameEx("logofflink")
> ### * logofflink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logofflink
> ### Title: Log Link Function with an Offset
> ### Aliases: logofflink log1plink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D logofflink(seq(-0.2, 0.5, by = 0.1))
> ##D logofflink(seq(-0.2, 0.5, by = 0.1), offset = 0.5)
> ##D        log(seq(-0.2, 0.5, by = 0.1) + 0.5) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lomax")
> ### * lomax
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lomax
> ### Title: Lomax Distribution Family Function
> ### Aliases: lomax
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ldata <- data.frame(y = rlomax(n = 1000, scale =  exp(1), exp(2)))
> fit <- vglm(y ~ 1, lomax, data = ldata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -149.00703
VGLM    linear loop  2 :  loglikelihood = -148.41031
VGLM    linear loop  3 :  loglikelihood = -148.40192
VGLM    linear loop  4 :  loglikelihood = -148.40192
VGLM    linear loop  5 :  loglikelihood = -148.40192
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(shape3.q)
(Intercept)       0.763072          1.782831
> Coef(fit)
   scale shape3.q 
2.144855 5.946670 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = lomax, data = ldata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   0.7631     0.2539   3.005  0.00266 ** 
(Intercept):2   1.7828     0.2197   8.116 4.82e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), loglink(shape3.q)

Log-likelihood: -148.4019 on 1998 degrees of freedom

Number of Fisher scoring iterations: 5 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):2'

> 
> 
> 
> cleanEx()
> nameEx("lomaxUC")
> ### * lomaxUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Lomax
> ### Title: The Lomax Distribution
> ### Aliases: Lomax dlomax plomax qlomax rlomax
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.1, 0.9, by = 0.1)
> max(abs(plomax(qlomax(p = probs, shape3.q =  1),
+                shape3.q = 1) - probs))  # Should be 0
[1] 5.551115e-17
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D x <- seq(-0.01, 5, len = 401)
> ##D plot(x, dexp(x), type = "l", col = "black", ylab = "", ylim = c(0, 3),
> ##D      main = "Black is std exponential, others are dlomax(x, shape3.q)")
> ##D lines(x, dlomax(x, shape3.q = 1), col = "orange")
> ##D lines(x, dlomax(x, shape3.q = 2), col = "blue")
> ##D lines(x, dlomax(x, shape3.q = 5), col = "green")
> ##D legend("topright", col = c("orange","blue","green"), lty = rep(1, 3),
> ##D        legend = paste("shape3.q =", c(1, 2, 5)))
> ##D 
> ##D plot(x, pexp(x), type = "l", col = "black", ylab = "", las = 1,
> ##D      main = "Black is std exponential, others are plomax(x, shape3.q)")
> ##D lines(x, plomax(x, shape3.q = 1), col = "orange")
> ##D lines(x, plomax(x, shape3.q = 2), col = "blue")
> ##D lines(x, plomax(x, shape3.q = 5), col = "green")
> ##D legend("bottomright", col = c("orange","blue","green"), lty = rep(1, 3),
> ##D        legend = paste("shape3.q =", c(1, 2, 5)))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lpossums")
> ### * lpossums
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lpossums
> ### Title: Leadbeater's Possums
> ### Aliases: lpossums
> ### Keywords: datasets
> 
> ### ** Examples
> 
> lpossums
   number ofreq
1       0    95
2       1     9
3       2    10
4       3    12
5       4     8
6       5     9
7       7     4
8       8     1
9       9     1
10     10     2
> (samplemean <- with(lpossums, weighted.mean(number, ofreq)))
[1] 1.370861
> with(lpossums,  var(rep(number, times = ofreq)) / samplemean)
[1] 3.731143
> sum(with(lpossums, ofreq))
[1] 151
> ## Not run: 
> ##D  spikeplot(with(lpossums, rep(number, times = ofreq)),
> ##D   main = "Leadbeater's possums", col = "blue", xlab = "Number") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lqnorm")
> ### * lqnorm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lqnorm
> ### Title: Minimizing the L-q norm Family Function
> ### Aliases: lqnorm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(123)
> ldata <- data.frame(x = sort(runif(nn <- 10 )))
> realfun <- function(x) 4 + 5*x
> ldata <- transform(ldata, y = realfun(x) + rnorm(nn, sd = exp(-1)))
> # Make the first observation an outlier
> ldata <- transform(ldata, y = c(4*y[1], y[-1]), x = c(-1, x[-1]))
> fit <- vglm(y ~ x, lqnorm(qpower = 1.2), data = ldata)
VGLM    linear loop  1 :  coefficients =  9.8969381, -2.6531240
VGLM    linear loop  2 :  coefficients = 10.5148343, -4.1442827
VGLM    linear loop  3 :  coefficients = 10.4189669, -4.2858735
VGLM    linear loop  4 :  coefficients = 10.4226089, -4.2980094
VGLM    linear loop  5 :  coefficients = 10.4226034, -4.2980089
VGLM    linear loop  6 :  coefficients = 10.4226034, -4.2980089
> coef(fit, matrix = TRUE)
                   mu
(Intercept) 10.422603
x           -4.298009
> head(fitted(fit))
       [,1]
1 14.720612
2  9.186593
3  8.664817
4  8.460069
5  8.152801
6  8.052531
> fit@misc$qpower
[1] 1.2
> fit@misc$objectiveFunction
[1] 28.38752
> 
> ## Not run: 
> ##D # Graphical check
> ##D with(ldata, plot(x, y,
> ##D      main = paste0("LS = red, lqnorm = blue (qpower = ",
> ##D      fit@misc$qpower, "), truth = black"), col = "blue"))
> ##D lmfit <- lm(y ~ x, data = ldata)
> ##D with(ldata, lines(x,  fitted(fit), col = "blue"))
> ##D with(ldata, lines(x, lmfit$fitted, col = "red"))
> ##D with(ldata, lines(x,  realfun(x),  col = "black")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lrt.stat")
> ### * lrt.stat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lrt.stat
> ### Title: Likelihood Ratio Test Statistics Evaluated at the Null Values
> ### Aliases: lrt.stat lrt.stat.vlm
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> set.seed(1)
> pneumo <- transform(pneumo, let = log(exposure.time),
+                             x3 = rnorm(nrow(pneumo)))
> fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo)
> cbind(coef(summary(fit)),
+       "signed LRT stat" = lrt.stat(fit, omit1s = FALSE))
                Estimate Std. Error   z value     Pr(>|z|) signed LRT stat
(Intercept):1  -9.676093  1.3240764 -7.307805 2.715407e-13      -11.897518
(Intercept):2 -10.581725  1.3454379 -7.864893 3.694127e-15      -13.090540
let             2.596807  0.3811019  6.813943 9.495965e-12        9.829229
> summary(fit, lrt0 = TRUE)  # Easy way to get it
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)

Likelihood ratio test coefficients: 
    Estimate z value Pr(>|z|)    
let    2.597   9.829   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 5.0268 on 13 degrees of freedom

Log-likelihood: -25.0903 on 13 degrees of freedom

Number of Fisher scoring iterations: 4 


Exponentiated coefficients:
     let 
13.42081 
> 
> 
> 
> cleanEx()
> nameEx("lrtest")
> ### * lrtest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lrtest
> ### Title: Likelihood Ratio Test of Nested Models
> ### Aliases: lrtest lrtest_vglm
> ### Keywords: htest
> 
> ### ** Examples
> 
> set.seed(1)
> pneumo <- transform(pneumo, let = log(exposure.time),
+                             x3  = runif(nrow(pneumo)))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let     , propodds, pneumo)
> fit2 <- vglm(cbind(normal, mild, severe) ~ let + x3, propodds, pneumo)
> fit3 <- vglm(cbind(normal, mild, severe) ~ let     , cumulative, pneumo)
> # Various equivalent specifications of the LR test for testing x3
> (ans1 <- lrtest(fit2, fit1))
Likelihood ratio test

Model 1: cbind(normal, mild, severe) ~ let + x3
Model 2: cbind(normal, mild, severe) ~ let
  #Df  LogLik Df  Chisq Pr(>Chisq)
1  12 -25.087                     
2  13 -25.090  1 0.0076     0.9304
> ans2 <- lrtest(fit2, 2)
> ans3 <- lrtest(fit2, "x3")
> ans4 <- lrtest(fit2, . ~ . - x3)
> c(all.equal(ans1, ans2), all.equal(ans1, ans3), all.equal(ans1, ans4))
[1] TRUE TRUE TRUE
> 
> # Doing it manually
> (testStatistic <- 2 * (logLik(fit2) - logLik(fit1)))
[1] 0.007619612
> (pval <- pchisq(testStatistic, df = df.residual(fit1) - df.residual(fit2),
+                 lower.tail = FALSE))
[1] 0.9304407
> 
> (ans4 <- lrtest(fit3, fit1))  # Test PO (parallelism) assumption
Likelihood ratio test

Model 1: cbind(normal, mild, severe) ~ let
Model 2: cbind(normal, mild, severe) ~ let
  #Df  LogLik Df  Chisq Pr(>Chisq)
1  12 -25.019                     
2  13 -25.090  1 0.1424     0.7059
> 
> 
> 
> cleanEx()
> nameEx("lvplot")
> ### * lvplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lvplot
> ### Title: Latent Variable Plot
> ### Aliases: lvplot
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Stdz environmental vars
> ##D set.seed(123)
> ##D p1 <- cao(cbind(Pardlugu, Pardmont, Pardnigr, Pardpull, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig +
> ##D           CoveMoss + CoveHerb + ReflLux,
> ##D           family = poissonff, data = hspider, Bestof = 3,
> ##D           df1.nl = c(Zoraspin = 2.5, 3), Crow1positive = TRUE)
> ##D index <- 1:ncol(depvar(p1))
> ##D lvplot(p1, lcol = index, pcol = index, y = TRUE, las = 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lvplot.qrrvglm")
> ### * lvplot.qrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lvplot.qrrvglm
> ### Title: Latent Variable Plot for QO models
> ### Aliases: lvplot.qrrvglm
> ### Keywords: regression nonlinear hplot
> 
> ### ** Examples
> 
> set.seed(123); nn <- 200
> cdata <- data.frame(x2 = rnorm(nn),  # Mean 0 (needed when I.tol=TRUE)
+                     x3 = rnorm(nn),  # Mean 0 (needed when I.tol=TRUE)
+                     x4 = rnorm(nn))  # Mean 0 (needed when I.tol=TRUE)
> cdata <- transform(cdata, latvar1 =  x2 + x3 - 2*x4,
+                           latvar2 = -x2 + x3 + 0*x4)
> # Nb. latvar2 is weakly correlated with latvar1
> cdata <- transform(cdata,
+            lambda1 = exp(6 - 0.5 * (latvar1-0)^2 - 0.5 * (latvar2-0)^2),
+            lambda2 = exp(5 - 0.5 * (latvar1-1)^2 - 0.5 * (latvar2-1)^2),
+            lambda3 = exp(5 - 0.5 * (latvar1+2)^2 - 0.5 * (latvar2-0)^2))
> cdata <- transform(cdata,
+             spp1 = rpois(nn, lambda1),
+             spp2 = rpois(nn, lambda2),
+             spp3 = rpois(nn, lambda3))
> set.seed(111)
> ## Not run: 
> ##D p2 <- cqo(cbind(spp1, spp2, spp3) ~ x2 + x3 + x4, poissonff,
> ##D           data = cdata, Rank = 2, I.tolerances = TRUE,
> ##D           Crow1positive = c(TRUE, FALSE))  # deviance = 505.81
> ##D if (deviance(p2) > 506) stop("suboptimal fit obtained")
> ##D sort(deviance(p2, history = TRUE))  # A history of the iterations
> ##D Coef(p2)
> ## End(Not run)
> 
> ## Not run: 
> ##D lvplot(p2, sites = TRUE, spch = "*", scol = "darkgreen", scex = 1.5,
> ##D   chull = TRUE, label = TRUE, Absolute = TRUE, ellipse = 140,
> ##D   adj = -0.5, pcol = "blue", pcex = 1.3, las = 1, Ccol = "orange",
> ##D   C = TRUE, Cadj = c(-0.3, -0.3, 1), Clwd = 2, Ccex = 1.4,
> ##D   main = paste("Contours at Abundance = 140 with",
> ##D                "convex hull of the site scores")) 
> ## End(Not run)
> ## Not run: 
> ##D var(latvar(p2))  # A diagonal matrix, i.e., uncorrelated latent vars
> ##D var(latvar(p2, varI.latvar = TRUE))  # Identity matrix
> ##D Tol(p2)[, , 1:2]  # Identity matrix
> ##D Tol(p2, varI.latvar = TRUE)[, , 1:2]  # A diagonal matrix
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("lvplot.rrvglm")
> ### * lvplot.rrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lvplot.rrvglm
> ### Title: Latent Variable Plot for RR-VGLMs
> ### Aliases: lvplot.rrvglm biplot.rrvglm
> ### Keywords: regression hplot
> 
> ### ** Examples
>  set.seed(1)
> nn <- nrow(pneumo)  # x1--x3 are some unrelated covariates
> pneumo <-
+   transform(pneumo, slet = scale(log(exposure.time)),
+                     imag = severe + 3,  # Fictitional!
+                     x1 = rnorm(nn), x2 = rnorm(nn), x3 = rnorm(nn))
> fit <-
+   rrvglm(cbind(normal, mild, severe, imag) ~ slet + x1 + x2 + x3,
+ #             Corner = FALSE, Uncorrel = TRUE,   # orig.
+               multinomial, data = pneumo, Rank = 2)
> ## Not run: 
> ##D lvplot(fit, chull = TRUE, scores = TRUE, clty = 2, ccol = 4,
> ##D        scol = "red", Ccol = "green3", Clwd = 2, Ccex = 2,
> ##D        main = "Biplot of some fictitional data") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("machinists")
> ### * machinists
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: machinists
> ### Title: Machinists Accidents
> ### Aliases: machinists
> ### Keywords: datasets
> 
> ### ** Examples
> 
> machinists
  accidents ofreq
1         0   296
2         1    74
3         2    26
4         3     8
5         4     4
6         5     4
7         6     1
8         8     1
> mean(with(machinists, rep(accidents, times = ofreq)))
[1] 0.4830918
>  var(with(machinists, rep(accidents, times = ofreq)))
[1] 1.010609
> ## Not run: 
> ##D  barplot(with(machinists, ofreq),
> ##D           names.arg = as.character(with(machinists, accidents)),
> ##D           main = "Machinists accidents",
> ##D           col = "lightblue", las = 1,
> ##D           ylab = "Frequency", xlab = "accidents") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("makeham")
> ### * makeham
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeham
> ### Title: Makeham Regression Family Function
> ### Aliases: makeham
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  set.seed(123)
> ##D mdata <- data.frame(x2 = runif(nn <- 1000))
> ##D mdata <- transform(mdata, eta1  = -1,
> ##D                           ceta1 =  1,
> ##D                           eeta1 = -2)
> ##D mdata <- transform(mdata, shape1 = exp(eta1),
> ##D                           scale1 = exp(ceta1),
> ##D                           epsil1 = exp(eeta1))
> ##D mdata <- transform(mdata,
> ##D          y1 = rmakeham(nn, shape = shape1, scale = scale1, eps = epsil1))
> ##D 
> ##D # A trick is to fit a Gompertz distribution first
> ##D fit0 <- vglm(y1 ~ 1, gompertz, data = mdata, trace = TRUE)
> ##D fit1 <- vglm(y1 ~ 1, makeham, data = mdata,
> ##D              etastart = cbind(predict(fit0), log(0.1)), trace = TRUE)
> ##D 
> ##D coef(fit1, matrix = TRUE)
> ##D summary(fit1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("makehamUC")
> ### * makehamUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Makeham
> ### Title: The Makeham Distribution
> ### Aliases: Makeham dmakeham pmakeham qmakeham rmakeham
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.01, 0.99, by = 0.01)
> Shape <- exp(-1); Scale <- exp(1); eps = Epsilon <- exp(-1)
> max(abs(pmakeham(qmakeham(probs, sca = Scale, Shape, eps = Epsilon),
+     sca = Scale, Shape, eps = Epsilon) - probs))  # Should be 0
[1] 5.551115e-16
> 
> ## Not run: 
> ##D  x <- seq(-0.1, 2.0, by = 0.01);
> ##D plot(x, dmakeham(x, sca = Scale, Shape, eps = Epsilon), type = "l",
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      col = "blue", las = 1, ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pmakeham(x, sca = Scale, Shape, eps = Epsilon), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qmakeham(probs, sca = Scale, Shape, eps = Epsilon)
> ##D lines(Q, dmakeham(Q, sca = Scale, Shape, eps = Epsilon),
> ##D       col = "purple", lty = 3, type = "h")
> ##D pmakeham(Q, sca = Scale, Shape, eps = Epsilon) - probs # Should be all 0
> ##D abline(h = probs, col = "purple", lty = 3) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("margeff")
> ### * margeff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: margeff
> ### Title: Marginal Effects for Several Categorical Response Models
> ### Aliases: margeff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Not a good example for multinomial() since the response is ordinal!!
> ii <- 3; hh <- 1/100
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, multinomial, pneumo)
> fit <- vglm(cbind(normal, mild, severe) ~ let,
+             cumulative(reverse = TRUE,  parallel = TRUE),
+             data = pneumo)
> fitted(fit)[ii, ]
    normal       mild     severe 
0.84670042 0.08509397 0.06820561 
> 
> mynewdata <- with(pneumo, data.frame(let = let[ii] + hh))
> (newp <- predict(fit, newdata = mynewdata, type = "response"))
     normal       mild     severe
1 0.8432994 0.08682604 0.06987459
> 
> # Compare the difference. Should be the same as hh --> 0.
> round((newp-fitted(fit)[ii, ]) / hh, 3)  # Finite-diff approxn
  normal  mild severe
1  -0.34 0.173  0.167
> round(margeff(fit, subset = ii)["let",], 3)
normal   mild severe 
-0.337  0.172  0.165 
> 
> # Other examples
> round(margeff(fit), 3)
, , 1

            normal   mild severe
(Intercept)  0.058 -0.032 -0.026
let         -0.015  0.009  0.006

, , 2

            normal   mild severe
(Intercept)  0.600 -0.312 -0.287
let         -0.161  0.090  0.071

, , 3

            normal   mild severe
(Intercept)  1.256 -0.583 -0.673
let         -0.337  0.172  0.165

, , 4

            normal   mild severe
(Intercept)  1.840 -0.708 -1.132
let         -0.494  0.216  0.278

, , 5

            normal   mild severe
(Intercept)  2.241 -0.625 -1.616
let         -0.601  0.205  0.396

, , 6

            normal   mild severe
(Intercept)  2.409 -0.362 -2.047
let         -0.646  0.144  0.502

, , 7

            normal  mild severe
(Intercept)  2.377 0.016 -2.392
let         -0.638 0.051  0.587

, , 8

            normal   mild severe
(Intercept)  2.239  0.329 -2.568
let         -0.601 -0.029  0.630

> round(margeff(fit, subset = 2)["let",], 3)
normal   mild severe 
-0.161  0.090  0.071 
> round(margeff(fit, subset = c(FALSE, TRUE))["let",,], 3)  # Recycling
            2      4      6      8
normal -0.161 -0.494 -0.646 -0.601
mild    0.090  0.216  0.144 -0.029
severe  0.071  0.278  0.502  0.630
> round(margeff(fit, subset = c(2, 4, 6, 8))["let",,], 3)
            2      4      6      8
normal -0.161 -0.494 -0.646 -0.601
mild    0.090  0.216  0.144 -0.029
severe  0.071  0.278  0.502  0.630
> 
> # Example 3; margeffs at a new value
> mynewdata2a <- data.frame(let = 2)  # New value
> mynewdata2b <- data.frame(let = 2 + hh)  # For finite-diff approxn
> (neweta2 <- predict(fit, newdata = mynewdata2a))
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1           -4.48248          -5.388112
> fit@x[1, ] <- c(1, unlist(mynewdata2a))
> fit@predictors[1, ] <- neweta2  # Needed
> max(abs(margeff(fit, subset = 1)["let", ] - (
+         predict(fit, newdata = mynewdata2b, type = "response") -
+         predict(fit, newdata = mynewdata2a, type = "response")) / hh
+ ))  # Should be 0
[1] 0.0003674048
> 
> 
> 
> cleanEx()
> nameEx("marital.nz")
> ### * marital.nz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: marital.nz
> ### Title: New Zealand Marital Data
> ### Aliases: marital.nz
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(marital.nz)
      age             ethnicity                  mstatus    
 Min.   :16.00   European  :6053   Divorced/Separated: 349  
 1st Qu.:33.00   Maori     :   0   Married/Partnered :4778  
 Median :43.00   Other     :   0   Single            : 811  
 Mean   :43.75   Polynesian:   0   Widowed           : 115  
 3rd Qu.:52.00                                              
 Max.   :88.00                                              
> 
> 
> 
> cleanEx()
> nameEx("maxwell")
> ### * maxwell
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: maxwell
> ### Title: Maxwell Regression Family Function
> ### Aliases: maxwell
> ### Keywords: models regression
> 
> ### ** Examples
> 
> mdata <- data.frame(y = rmaxwell(1000, rate = exp(2)))
> fit <- vglm(y ~ 1, maxwell, mdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 2.2661862
VGLM    linear loop  2 :  coefficients = 1.9860613
VGLM    linear loop  3 :  coefficients = 2.0186846
VGLM    linear loop  4 :  coefficients = 2.0192284
VGLM    linear loop  5 :  coefficients = 2.0192286
> coef(fit, matrix = TRUE)
            loglink(rate)
(Intercept)      2.019229
> Coef(fit)
    rate 
7.532512 
> 
> 
> 
> cleanEx()
> nameEx("maxwellUC")
> ### * maxwellUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Maxwell
> ### Title: The Maxwell Distribution
> ### Aliases: Maxwell dmaxwell pmaxwell qmaxwell rmaxwell
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  rate <- 3; x <- seq(-0.5, 3, length = 100)
> ##D plot(x, dmaxwell(x, rate = rate), type = "l", col = "blue",
> ##D      main = "Blue is density, orange is CDF", ylab = "", las = 1,
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pmaxwell(x, rate = rate), type = "l", col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qmaxwell(probs, rate = rate)
> ##D lines(Q, dmaxwell(Q, rate), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pmaxwell(Q, rate), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pmaxwell(Q, rate) - probs))  # Should be zero
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mccullagh89")
> ### * mccullagh89
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mccullagh89
> ### Title: McCullagh (1989) Distribution Family Function
> ### Aliases: mccullagh89
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Limit as theta = 0, nu = Inf:
> mdata <- data.frame(y = rnorm(1000, sd = 0.2))
> fit <- vglm(y ~ 1, mccullagh89, data = mdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1632.3772
VGLM    linear loop  2 :  loglikelihood = -1029.5495
VGLM    linear loop  3 :  loglikelihood = -600.86464
VGLM    linear loop  4 :  loglikelihood = -334.12704
VGLM    linear loop  5 :  loglikelihood = -49.225991
VGLM    linear loop  6 :  loglikelihood = 116.02867
VGLM    linear loop  7 :  loglikelihood = 153.19494
VGLM    linear loop  8 :  loglikelihood = 154.71078
VGLM    linear loop  9 :  loglikelihood = 154.71317
VGLM    linear loop  10 :  loglikelihood = 154.71317
> head(fitted(fit))
             [,1]
[1,] -0.002328107
[2,] -0.002328107
[3,] -0.002328107
[4,] -0.002328107
[5,] -0.002328107
[6,] -0.002328107
> with(mdata, mean(y))
[1] -0.002329628
> summary(fit)
Call:
vglm(formula = y ~ 1, family = mccullagh89, data = mdata, trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.005095   0.014369  -0.355    0.723    
(Intercept):2  2.407053   0.043749  55.020   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: rhobitlink(theta), logofflink(nu, offset = 0.5)

Log-likelihood: 154.7132 on 1998 degrees of freedom

Number of Fisher scoring iterations: 10 

No Hauck-Donner effect found in any of the estimates

> coef(fit, matrix = TRUE)
            rhobitlink(theta) logofflink(nu, offset = 0.5)
(Intercept)       -0.00509544                     2.407053
> Coef(fit)
       theta           nu 
-0.002547715 10.601199927 
> 
> 
> 
> cleanEx()
> nameEx("meangaitd")
> ### * meangaitd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: meangaitd
> ### Title: Mean of the GAITD Combo Density
> ### Aliases: meangaitd
> ### Keywords: models regression
> 
> ### ** Examples
> 
> i.mix <- seq(0, 15, by = 5)
> lambda.p <- 10
> meangaitd(lambda.p, a.mix = i.mix + 1, i.mix = i.mix,
+           max.support = 17, pobs.mix = 0.1, pstr.mix = 0.1)
[1] 9.868472
> 
> 
> 
> cleanEx()
> nameEx("melbmaxtemp")
> ### * melbmaxtemp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: melbmaxtemp
> ### Title: Melbourne Daily Maximum Temperatures
> ### Aliases: melbmaxtemp
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(melbmaxtemp)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   7.00   15.40   18.90   20.01   23.00   43.30 
> ## Not run: 
> ##D melb <- data.frame(today     = melbmaxtemp[-1],
> ##D                    yesterday = melbmaxtemp[-length(melbmaxtemp)])
> ##D plot(today ~ yesterday, data = melb,
> ##D      xlab = "Yesterday's Max Temperature",
> ##D      ylab = "Today's Max Temperature", cex = 1.4, type = "n")
> ##D points(today ~ yesterday, melb, pch = 0, cex = 0.50, col = "blue")
> ##D abline(a = 0, b = 1, lty = 3)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("meplot")
> ### * meplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: meplot
> ### Title: Mean Excess Plot
> ### Aliases: meplot meplot.default meplot.vlm
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D meplot(with(venice90, sealevel), las = 1) -> ii
> ##D names(ii)
> ##D abline(h = ii$meanExcess[1], col = "orange", lty = "dashed")
> ##D 
> ##D par(mfrow = c(2, 2))
> ##D for (ii in 1:4)
> ##D   meplot(rgpd(1000), col = c("orange", "blue", "orange"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("micmen")
> ### * micmen
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: micmen
> ### Title: Michaelis-Menten Model
> ### Aliases: micmen
> ### Keywords: models regression
> 
> ### ** Examples
> 
> mfit <- vglm(velocity ~ 1, micmen, data = enzyme, trace = TRUE,
+              crit = "coef", form2 = ~ conc - 1)
[1] "head(temp20101111) in @deriv"
     dmu.dtheta1  dmu.dtheta2
[1,]   0.5236626 -0.014950566
[2,]   0.5236626 -0.014950566
[3,]   0.2682751 -0.011765750
[4,]   0.2682751 -0.011765750
[5,]   0.1802408 -0.008855861
[6,]   0.1802408 -0.008855861
[1] "head(y) in @deriv"
     1      2      3      4      5      6 
0.0615 0.0527 0.0334 0.0258 0.0138 0.0258 
[1] "head(mu) in @deriv"
[1] 0.05710000 0.05710000 0.02925264 0.02925264 0.01965340 0.01965340
VGLM    linear loop  1 :  coefficients = 0.10557802, 1.70041562
[1] "head(temp20101111) in @deriv"
     dmu.dtheta1  dmu.dtheta2
[1,]   0.5404798 -0.015420643
[2,]   0.5404798 -0.015420643
[3,]   0.2817418 -0.012564648
[4,]   0.2817418 -0.012564648
[5,]   0.1904385 -0.009572448
[6,]   0.1904385 -0.009572448
[1] "head(y) in @deriv"
     1      2      3      4      5      6 
0.0615 0.0527 0.0334 0.0258 0.0138 0.0258 
[1] "head(mu) in @deriv"
[1] 0.05706279 0.05706279 0.02974574 0.02974574 0.02010612 0.02010612
VGLM    linear loop  2 :  coefficients = 0.10564361, 1.70272145
[1] "head(temp20101111) in @deriv"
     dmu.dtheta1  dmu.dtheta2
[1,]   0.5401433 -0.015411012
[2,]   0.5401433 -0.015411012
[3,]   0.2814677 -0.012547999
[4,]   0.2814677 -0.012547999
[5,]   0.1902297 -0.009557399
[6,]   0.1902297 -0.009557399
[1] "head(y) in @deriv"
     1      2      3      4      5      6 
0.0615 0.0527 0.0334 0.0258 0.0138 0.0258 
[1] "head(mu) in @deriv"
[1] 0.05706268 0.05706268 0.02973526 0.02973526 0.02009655 0.02009655
VGLM    linear loop  3 :  coefficients = 0.10564271, 1.70269012
[1] "head(temp20101111) in @deriv"
     dmu.dtheta1  dmu.dtheta2
[1,]   0.5401478 -0.015411141
[2,]   0.5401478 -0.015411141
[3,]   0.2814714 -0.012548223
[4,]   0.2814714 -0.012548223
[5,]   0.1902325 -0.009557603
[6,]   0.1902325 -0.009557603
[1] "head(y) in @deriv"
     1      2      3      4      5      6 
0.0615 0.0527 0.0334 0.0258 0.0138 0.0258 
[1] "head(mu) in @deriv"
[1] 0.05706268 0.05706268 0.02973540 0.02973540 0.02009668 0.02009668
VGLM    linear loop  4 :  coefficients = 0.10564271, 1.70268999
> summary(mfit)
[1] "head(temp20101111) in @deriv"
       conc         conc
1 0.5401478 -0.015411142
2 0.5401478 -0.015411142
3 0.2814714 -0.012548225
4 0.2814714 -0.012548225
5 0.1902325 -0.009557604
6 0.1902325 -0.009557604
[1] "head(y) in @deriv"
    [,1]
1 0.0615
2 0.0527
3 0.0334
4 0.0258
5 0.0138
6 0.0258
[1] "head(mu) in @deriv"
           [,1]
[1,] 0.05706268
[2,] 0.05706268
[3,] 0.02973540
[4,] 0.02973540
[5,] 0.02009668
[6,] 0.02009668
[1] "head(temp20101111) in @deriv"
       conc        conc
1 0.5401478 -0.01614054
2 0.5401478 -0.01614054
3 0.2814714 -0.01314212
4 0.2814714 -0.01314212
5 0.1902325 -0.01000996
6 0.1902325 -0.01000996
[1] "head(y) in @deriv"
    [,1]
1 0.0615
2 0.0527
3 0.0334
4 0.0258
5 0.0138
6 0.0258
[1] "head(mu) in @deriv"
           [,1]
[1,] 0.05976342
[2,] 0.05976342
[3,] 0.03114276
[4,] 0.03114276
[5,] 0.02104784
[6,] 0.02104784
[1] "head(temp20101111) in @deriv"
       conc         conc
1 0.5401478 -0.014681742
2 0.5401478 -0.014681742
3 0.2814714 -0.011954325
4 0.2814714 -0.011954325
5 0.1902325 -0.009105249
6 0.1902325 -0.009105249
[1] "head(y) in @deriv"
    [,1]
1 0.0615
2 0.0527
3 0.0334
4 0.0258
5 0.0138
6 0.0258
[1] "head(mu) in @deriv"
           [,1]
[1,] 0.05436194
[2,] 0.05436194
[3,] 0.02832805
[4,] 0.02832805
[5,] 0.01914551
[6,] 0.01914551
[1] "head(temp20101111) in @deriv"
       conc         conc
1 0.5394194 -0.015369604
2 0.5394194 -0.015369604
3 0.2808788 -0.012495439
4 0.2808788 -0.012495439
5 0.1897812 -0.009512311
6 0.1897812 -0.009512311
[1] "head(y) in @deriv"
    [,1]
1 0.0615
2 0.0527
3 0.0334
4 0.0258
5 0.0138
6 0.0258
[1] "head(mu) in @deriv"
           [,1]
[1,] 0.05698573
[2,] 0.05698573
[3,] 0.02967279
[4,] 0.02967279
[5,] 0.02004900
[6,] 0.02004900
[1] "head(temp20101111) in @deriv"
       conc        conc
1 0.5408782 -0.01545285
2 0.5408782 -0.01545285
3 0.2820666 -0.01260135
4 0.2820666 -0.01260135
5 0.1906859 -0.00960322
6 0.1906859 -0.00960322
[1] "head(y) in @deriv"
    [,1]
1 0.0615
2 0.0527
3 0.0334
4 0.0258
5 0.0138
6 0.0258
[1] "head(mu) in @deriv"
           [,1]
[1,] 0.05713984
[2,] 0.05713984
[3,] 0.02979828
[4,] 0.02979828
[5,] 0.02014458
[6,] 0.02014458
Call:
vglm(formula = velocity ~ 1, family = micmen, data = enzyme, 
    form2 = ~conc - 1, trace = TRUE, crit = "coef")

Coefficients: 
              Estimate Std. Error z value
(Intercept):1  0.10564    0.01766   5.983
(Intercept):2  1.70269    0.47890   3.555

Names of linear predictors: theta1, theta2

(Estimated) Dispersion Parameter for micmen family:   2.011e-05

Residual deviance: 0.0002011 on 10 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> ## Not run: 
> ##D plot(velocity ~ conc, enzyme, xlab = "concentration", las = 1,
> ##D      col = "blue",
> ##D      main = "Michaelis-Menten equation for the enzyme data",
> ##D      ylim = c(0, max(velocity)), xlim = c(0, max(conc)))
> ##D points(fitted(mfit) ~ conc, enzyme, col = 2, pch = "+", cex = 2)
> ##D 
> ##D # This predicts the response at a finer grid:
> ##D newenzyme <- data.frame(conc = seq(0, max(with(enzyme, conc)),
> ##D       len = 200))
> ##D mfit@extra$Xm2 <- newenzyme$conc # This is needed for prediction
> ##D lines(predict(mfit, newenzyme, "response") ~ conc, newenzyme,
> ##D       col = "red") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mills.ratio")
> ### * mills.ratio
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mills.ratio
> ### Title: Mills Ratio
> ### Aliases: mills.ratio mills.ratio2
> ### Keywords: math
> 
> ### ** Examples
> 
> ## Not run: 
> ##D curve(mills.ratio, -5, 5, col = "orange", las = 1)
> ##D curve(mills.ratio, -5, 5, col = "orange", las = 1, log = "y")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mix2exp")
> ### * mix2exp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mix2exp
> ### Title: Mixture of Two Exponential Distributions
> ### Aliases: mix2exp
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  lambda1 <- exp(1); lambda2 <- exp(3)
> ##D (phi <- logitlink(-1, inverse = TRUE))
> ##D mdata <- data.frame(y1 = rexp(nn <- 1000, lambda1))
> ##D mdata <- transform(mdata, y2 = rexp(nn, lambda2))
> ##D mdata <- transform(mdata, Y  = ifelse(runif(nn) < phi, y1, y2))
> ##D fit <- vglm(Y ~ 1, mix2exp, data = mdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D 
> ##D # Compare the results with the truth
> ##D round(rbind('Estimated' = Coef(fit),
> ##D             'Truth' = c(phi, lambda1, lambda2)), digits = 2)
> ##D 
> ##D with(mdata, hist(Y, prob = TRUE, main = "Orange=estimate, blue=truth"))
> ##D abline(v = 1 / Coef(fit)[c(2, 3)],  lty = 2, col = "orange", lwd = 2)
> ##D abline(v = 1 / c(lambda1, lambda2), lty = 2, col = "blue", lwd = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mix2normal")
> ### * mix2normal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mix2normal
> ### Title: Mixture of Two Univariate Normal Distributions
> ### Aliases: mix2normal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  mu1 <-  99; mu2 <- 150; nn <- 1000
> ##D sd1 <- sd2 <- exp(3)
> ##D (phi <- logitlink(-1, inverse = TRUE))
> ##D rrn <- runif(nn)
> ##D mdata <- data.frame(y = ifelse(rrn < phi, rnorm(nn, mu1, sd1),
> ##D                                           rnorm(nn, mu2, sd2)))
> ##D fit <- vglm(y ~ 1, mix2normal(eq.sd = TRUE), data = mdata)
> ##D 
> ##D # Compare the results
> ##D cfit <- coef(fit)
> ##D round(rbind('Estimated' = c(logitlink(cfit[1], inverse = TRUE),
> ##D             cfit[2], exp(cfit[3]), cfit[4]),
> ##D             'Truth' = c(phi, mu1, sd1, mu2)), digits = 2)
> ##D 
> ##D # Plot the results
> ##D xx <- with(mdata, seq(min(y), max(y), len = 200))
> ##D plot(xx, (1-phi) * dnorm(xx, mu2, sd2), type = "l", xlab = "y",
> ##D      main = "red = estimate, blue = truth",
> ##D      col = "blue", ylab = "Density")
> ##D phi.est <- logitlink(coef(fit)[1], inverse = TRUE)
> ##D sd.est <- exp(coef(fit)[3])
> ##D lines(xx, phi*dnorm(xx, mu1, sd1), col = "blue")
> ##D lines(xx, phi.est * dnorm(xx, Coef(fit)[2], sd.est), col = "red")
> ##D lines(xx, (1-phi.est)*dnorm(xx, Coef(fit)[4], sd.est), col="red")
> ##D abline(v = Coef(fit)[c(2,4)], lty = 2, col = "red")
> ##D abline(v = c(mu1, mu2), lty = 2, col = "blue")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mix2poisson")
> ### * mix2poisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mix2poisson
> ### Title: Mixture of Two Poisson Distributions
> ### Aliases: mix2poisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  # Example 1: simulated data
> ##D nn <- 1000
> ##D mu1 <- exp(2.5)  # Also known as lambda1
> ##D mu2 <- exp(3)
> ##D (phi <- logitlink(-0.5, inverse = TRUE))
> ##D mdata <- data.frame(y = rpois(nn, ifelse(runif(nn) < phi, mu1, mu2)))
> ##D mfit <- vglm(y ~ 1, mix2poisson, data = mdata)
> ##D coef(mfit, matrix = TRUE)
> ##D 
> ##D # Compare the results with the truth
> ##D round(rbind('Estimated' = Coef(mfit), 'Truth' = c(phi, mu1, mu2)), 2)
> ##D 
> ##D ty <- with(mdata, table(y))
> ##D plot(names(ty), ty, type = "h", main = "Orange=estimate, blue=truth",
> ##D      ylab = "Frequency", xlab = "y")
> ##D abline(v = Coef(mfit)[-1], lty = 2, col = "orange", lwd = 2)
> ##D abline(v = c(mu1, mu2), lty = 2, col = "blue", lwd = 2)
> ##D 
> ##D # Example 2: London Times data (Lange, 1997, p.31)
> ##D ltdata1 <- data.frame(deaths = 0:9,
> ##D                       freq = c(162,267,271, 185,111,61,27,8,3,1))
> ##D ltdata2 <- data.frame(y = with(ltdata1, rep(deaths, freq)))
> ##D 
> ##D # Usually this does not work well unless nsimEIM is large
> ##D Mfit <- vglm(deaths ~ 1, weight = freq, data = ltdata1,
> ##D         mix2poisson(iphi=0.3, il1=1, il2=2.5, nsimEIM=5000))
> ##D 
> ##D # This works better in general
> ##D Mfit = vglm(y ~ 1, mix2poisson(iphi=0.3, il1=1, il2=2.5), ltdata2)
> ##D coef(Mfit, matrix = TRUE)
> ##D Coef(Mfit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("model.framevlm")
> ### * model.framevlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: model.framevlm
> ### Title: Construct the Model Frame of a VLM Object
> ### Aliases: model.framevlm
> ### Keywords: models
> 
> ### ** Examples
> 
> # Illustrates smart prediction
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal,mild, severe) ~ poly(c(scale(let)), 2),
+             multinomial, pneumo, trace = TRUE, x = FALSE)
VGLM    linear loop  1 :  deviance = 3.716285
VGLM    linear loop  2 :  deviance = 3.499289
VGLM    linear loop  3 :  deviance = 3.487318
VGLM    linear loop  4 :  deviance = 3.487262
VGLM    linear loop  5 :  deviance = 3.487262
> class(fit)
[1] "vglm"
attr(,"package")
[1] "VGAM"
> 
> check1 <- head(model.frame(fit))
> check1
  cbind(normal, mild, severe).normal cbind(normal, mild, severe).mild
1                                 98                                0
2                                 51                                2
3                                 34                                6
4                                 35                                5
5                                 32                               10
6                                 23                                7
  cbind(normal, mild, severe).severe poly(c(scale(let)), 2).1
1                                  0              -0.77195016
2                                  1              -0.27226249
3                                  3              -0.08294405
4                                  8               0.04649255
5                                  9               0.15028005
6                                  8               0.23692162
  poly(c(scale(let)), 2).2
1               0.50285988
2              -0.43378435
3              -0.42122091
4              -0.29641247
5              -0.12815229
6               0.05878623
> check2 <- model.frame(fit, data = head(pneumo))
> check2
  cbind(normal, mild, severe).normal cbind(normal, mild, severe).mild
1                                 98                                0
2                                 51                                2
3                                 34                                6
4                                 35                                5
5                                 32                               10
6                                 23                                7
  cbind(normal, mild, severe).severe poly(c(scale(let)), 2).1
1                                  0              -0.79735249
2                                  1              -0.19033917
3                                  3               0.03964213
4                                  8               0.19687984
5                                  9               0.32295938
6                                  8               0.42821031
  poly(c(scale(let)), 2).2
1               0.43043654
2              -0.57044185
3              -0.41509933
4              -0.13981459
5               0.18011485
6               0.51480438
> all.equal(unlist(check1), unlist(check2))  # Should be TRUE
[1] "Mean relative difference: 0.5527089"
> 
> q0 <- head(predict(fit))
> q1 <- head(predict(fit, newdata = pneumo))
> q2 <- predict(fit, newdata = head(pneumo))
> all.equal(q0, q1)  # Should be TRUE
[1] TRUE
> all.equal(q1, q2)  # Should be TRUE
[1] "Mean relative difference: 0.2835235"
> 
> 
> 
> cleanEx()
> nameEx("model.matrixqrrvglm")
> ### * model.matrixqrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: model.matrixqrrvglm
> ### Title: Construct the Model Matrix of a QRR-VGLM Object
> ### Aliases: model.matrixqrrvglm
> ### Keywords: models nonlinear utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1); n <- 40; p <- 3; S <- 4; myrank <- 1
> ##D mydata <- rcqo(n, p, S, Rank = myrank, es.opt = TRUE, eq.max = TRUE)
> ##D (myform <- attr(mydata, "formula"))
> ##D mycqo <- cqo(myform, poissonff, data = mydata,
> ##D              I.tol = TRUE, Rank = myrank, Bestof = 5)
> ##D model.matrix(mycqo, type = "latvar")
> ##D model.matrix(mycqo, type = "lm")
> ##D model.matrix(mycqo, type = "vlm")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("model.matrixvlm")
> ### * model.matrixvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: model.matrixvlm
> ### Title: Construct the Design Matrix of a VLM Object
> ### Aliases: model.matrixvlm
> ### Keywords: models
> 
> ### ** Examples
> 
> # (I) Illustrates smart prediction ,,,,,,,,,,,,,,,,,,,,,,,
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~
+             sm.poly(c(sm.scale(let)), 2),
+             multinomial, data = pneumo, trace = TRUE, x = FALSE)
VGLM    linear loop  1 :  deviance = 3.716285
VGLM    linear loop  2 :  deviance = 3.499289
VGLM    linear loop  3 :  deviance = 3.487318
VGLM    linear loop  4 :  deviance = 3.487262
VGLM    linear loop  5 :  deviance = 3.487262
> class(fit)
[1] "vglm"
attr(,"package")
[1] "VGAM"
> fit@smart.prediction  # Data-dependent parameters
[[1]]
[[1]]$center
[1] 3.225777

[[1]]$scale
[1] 0.7187268

[[1]]$match.call
sm.scale.default(x = let)


[[2]]
[[2]]$degree
[1] 2

[[2]]$coefs
[[2]]$coefs$alpha
[1]  2.706169e-16 -1.002119e+00

[[2]]$coefs$norm2
[1] 1.000000 8.000000 7.000000 6.175489


[[2]]$raw
[1] FALSE

[[2]]$match.call
sm.poly(x = c(sm.scale(let)), 2)


> fit@x # Not saved on the object
<0 x 0 matrix>
> model.matrix(fit)
    (Intercept):1 (Intercept):2 sm.poly(c(sm.scale(let)), 2)1:1
1:1             1             0                     -0.77195016
1:2             0             1                      0.00000000
2:1             1             0                     -0.27226249
2:2             0             1                      0.00000000
3:1             1             0                     -0.08294405
3:2             0             1                      0.00000000
4:1             1             0                      0.04649255
4:2             0             1                      0.00000000
5:1             1             0                      0.15028005
5:2             0             1                      0.00000000
6:1             1             0                      0.23692162
6:2             0             1                      0.00000000
7:1             1             0                      0.31703465
7:2             0             1                      0.00000000
8:1             1             0                      0.37642782
8:2             0             1                      0.00000000
    sm.poly(c(sm.scale(let)), 2)1:2 sm.poly(c(sm.scale(let)), 2)2:1
1:1                      0.00000000                      0.50285988
1:2                     -0.77195016                      0.00000000
2:1                      0.00000000                     -0.43378435
2:2                     -0.27226249                      0.00000000
3:1                      0.00000000                     -0.42122091
3:2                     -0.08294405                      0.00000000
4:1                      0.00000000                     -0.29641247
4:2                      0.04649255                      0.00000000
5:1                      0.00000000                     -0.12815229
5:2                      0.15028005                      0.00000000
6:1                      0.00000000                      0.05878623
6:2                      0.23692162                      0.00000000
7:1                      0.00000000                      0.26926949
7:2                      0.31703465                      0.00000000
8:1                      0.00000000                      0.44865441
8:2                      0.37642782                      0.00000000
    sm.poly(c(sm.scale(let)), 2)2:2
1:1                      0.00000000
1:2                      0.50285988
2:1                      0.00000000
2:2                     -0.43378435
3:1                      0.00000000
3:2                     -0.42122091
4:1                      0.00000000
4:2                     -0.29641247
5:1                      0.00000000
5:2                     -0.12815229
6:1                      0.00000000
6:2                      0.05878623
7:1                      0.00000000
7:2                      0.26926949
8:1                      0.00000000
8:2                      0.44865441
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1 2

attr(,"assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 3 4 5 6

attr(,"vassign")
attr(,"vassign")$`(Intercept):1`
[1] 1

attr(,"vassign")$`(Intercept):2`
[1] 2

attr(,"vassign")$`sm.poly(c(sm.scale(let)), 2):1`
[1] 3 5

attr(,"vassign")$`sm.poly(c(sm.scale(let)), 2):2`
[1] 4 6

attr(,"constraints")
attr(,"constraints")$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"constraints")$`sm.poly(c(sm.scale(let)), 2)1`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"constraints")$`sm.poly(c(sm.scale(let)), 2)2`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

> model.matrix(fit, linpred.index = 1, type = "lm")
    (Intercept):1 sm.poly(c(sm.scale(let)), 2)1:1
1:1             1                     -0.77195016
2:1             1                     -0.27226249
3:1             1                     -0.08294405
4:1             1                      0.04649255
5:1             1                      0.15028005
6:1             1                      0.23692162
7:1             1                      0.31703465
8:1             1                      0.37642782
    sm.poly(c(sm.scale(let)), 2)2:1
1:1                      0.50285988
2:1                     -0.43378435
3:1                     -0.42122091
4:1                     -0.29641247
5:1                     -0.12815229
6:1                      0.05878623
7:1                      0.26926949
8:1                      0.44865441
attr(,"vassign")
attr(,"vassign")$`(Intercept):1`
[1] 1

attr(,"vassign")$`sm.poly(c(sm.scale(let)), 2):1`
[1] 2 3

attr(,"rm.vassign")
attr(,"rm.vassign")$`(Intercept):1`
[1] 1

attr(,"rm.vassign")$`sm.poly(c(sm.scale(let)), 2):1`
[1] 3 5

attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 2 3

attr(,"rm.assign")
attr(,"rm.assign")$`(Intercept)`
[1] 1 2

attr(,"rm.assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 3 4 5 6

> model.matrix(fit, linpred.index = 2, type = "lm")
    (Intercept):2 sm.poly(c(sm.scale(let)), 2)1:2
1:2             1                     -0.77195016
2:2             1                     -0.27226249
3:2             1                     -0.08294405
4:2             1                      0.04649255
5:2             1                      0.15028005
6:2             1                      0.23692162
7:2             1                      0.31703465
8:2             1                      0.37642782
    sm.poly(c(sm.scale(let)), 2)2:2
1:2                      0.50285988
2:2                     -0.43378435
3:2                     -0.42122091
4:2                     -0.29641247
5:2                     -0.12815229
6:2                      0.05878623
7:2                      0.26926949
8:2                      0.44865441
attr(,"vassign")
attr(,"vassign")$`(Intercept):2`
[1] 1

attr(,"vassign")$`sm.poly(c(sm.scale(let)), 2):2`
[1] 2 3

attr(,"rm.vassign")
attr(,"rm.vassign")$`(Intercept):2`
[1] 2

attr(,"rm.vassign")$`sm.poly(c(sm.scale(let)), 2):2`
[1] 4 6

attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 2 3

attr(,"rm.assign")
attr(,"rm.assign")$`(Intercept)`
[1] 1 2

attr(,"rm.assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 3 4 5 6

> 
> (Check1 <- head(model.matrix(fit, type = "lm")))
  (Intercept) sm.poly(c(sm.scale(let)), 2)1 sm.poly(c(sm.scale(let)), 2)2
1           1                   -0.77195016                    0.50285988
2           1                   -0.27226249                   -0.43378435
3           1                   -0.08294405                   -0.42122091
4           1                    0.04649255                   -0.29641247
5           1                    0.15028005                   -0.12815229
6           1                    0.23692162                    0.05878623
> (Check2 <- model.matrix(fit, data = head(pneumo), type = "lm"))
  (Intercept) sm.poly(c(sm.scale(let)), 2)1 sm.poly(c(sm.scale(let)), 2)2
1           1                   -0.77195016                    0.50285988
2           1                   -0.27226249                   -0.43378435
3           1                   -0.08294405                   -0.42122091
4           1                    0.04649255                   -0.29641247
5           1                    0.15028005                   -0.12815229
6           1                    0.23692162                    0.05878623
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$`sm.poly(c(sm.scale(let)), 2)`
[1] 2 3

> all.equal(c(Check1), c(Check2))  # Should be TRUE
[1] TRUE
> 
> q0 <- head(predict(fit))
> q1 <- head(predict(fit, newdata = pneumo))
> q2 <- predict(fit, newdata = head(pneumo))
> all.equal(q0, q1)  # Should be TRUE
[1] TRUE
> all.equal(q1, q2)  # Should be TRUE
[1] TRUE
> 
> # (II) Attributes ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
> fit2 <- vglm(cbind(normal, mild, severe) ~ let,  # x = TRUE
+              multinomial, data = pneumo, trace = TRUE)
VGLM    linear loop  1 :  deviance = 5.407271
VGLM    linear loop  2 :  deviance = 5.34745
VGLM    linear loop  3 :  deviance = 5.347382
VGLM    linear loop  4 :  deviance = 5.347382
> fit2@x  # "lm"-type; saved on the object; note the attributes
  (Intercept)      let
1           1 1.757858
2           1 2.708050
3           1 3.068053
4           1 3.314186
5           1 3.511545
6           1 3.676301
7           1 3.828641
8           1 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$let
[1] 2

attr(,"orig.assign.lm")
[1] 0 1
> model.matrix(fit2, type = "lm")  # Note the attributes
  (Intercept)      let
1           1 1.757858
2           1 2.708050
3           1 3.068053
4           1 3.314186
5           1 3.511545
6           1 3.676301
7           1 3.828641
8           1 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$let
[1] 2

attr(,"orig.assign.lm")
[1] 0 1
> model.matrix(fit2, type = "vlm")  # Note the attributes
    (Intercept):1 (Intercept):2    let:1    let:2
1:1             1             0 1.757858 0.000000
1:2             0             1 0.000000 1.757858
2:1             1             0 2.708050 0.000000
2:2             0             1 0.000000 2.708050
3:1             1             0 3.068053 0.000000
3:2             0             1 0.000000 3.068053
4:1             1             0 3.314186 0.000000
4:2             0             1 0.000000 3.314186
5:1             1             0 3.511545 0.000000
5:2             0             1 0.000000 3.511545
6:1             1             0 3.676301 0.000000
6:2             0             1 0.000000 3.676301
7:1             1             0 3.828641 0.000000
7:2             0             1 0.000000 3.828641
8:1             1             0 3.941582 0.000000
8:2             0             1 0.000000 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1 2

attr(,"assign")$let
[1] 3 4

attr(,"vassign")
attr(,"vassign")$`(Intercept):1`
[1] 1

attr(,"vassign")$`(Intercept):2`
[1] 2

attr(,"vassign")$`let:1`
[1] 3

attr(,"vassign")$`let:2`
[1] 4

attr(,"constraints")
attr(,"constraints")$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"constraints")$let
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"orig.assign.lm")
[1] 0 1
> 
> 
> 
> cleanEx()
> nameEx("moffset")
> ### * moffset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: moffset
> ### Title: Matrix Offset
> ### Aliases: moffset
> 
> ### ** Examples
> 
> # Some day's data is moved to previous day:
> moffset(alcoff, 3, 2, "*")
   Wed* Thu* Fri* Sat* Sun* Mon* Tue*
3    75  238  401  693  718   55   60
4    48  145  223  346  410   25   38
5    20   56  139  188  287   19   10
6     9   55   70  155  213   13    9
7     9   42   40  160  200   20    6
8    12   29   40   79   96    5    8
9    14   28   38   44   58    7    8
10   20   36   38   52   69    7   13
11   20   32   27   44   39    8   10
12    8   39   37   73   59   10   13
13   32   37   27   41   45    9   26
14   31   46   42   58   53   22   41
15   62   69   59   75   70   39   48
16   71   85   55   96   95   28   48
17   98  141  136  154  130   46   59
18  117  185  223  236  121   53  100
19  155  289  335  337  146   74  119
20  283  508  591  490  166   74  135
21  326  610  866  754  131   84  154
22  345  765  976 1026  114   90  143
23  363  899 1265 1179  159  110  169
0*  324  827 1379 1332  121   98  165
1*  278  619 1327 1356   97   92  157
2*  229  410  979 1011   60   69  107
> Rcim(alcoff, 3 + 1, 2 + 1)  # Data does not move as much.
   Wed Thu  Fri  Sat  Sun Mon Tue
3   75 238  401  693  718  55  60
4   48 145  223  346  410  25  38
5   20  56  139  188  287  19  10
6    9  55   70  155  213  13   9
7    9  42   40  160  200  20   6
8   12  29   40   79   96   5   8
9   14  28   38   44   58   7   8
10  20  36   38   52   69   7  13
11  20  32   27   44   39   8  10
12   8  39   37   73   59  10  13
13  32  37   27   41   45   9  26
14  31  46   42   58   53  22  41
15  62  69   59   75   70  39  48
16  71  85   55   96   95  28  48
17  98 141  136  154  130  46  59
18 117 185  223  236  121  53 100
19 155 289  335  337  146  74 119
20 283 508  591  490  166  74 135
21 326 610  866  754  131  84 154
22 345 765  976 1026  114  90 143
23 363 899 1265 1179  159 110 169
0  165 324  827 1379 1332 121  98
1  157 278  619 1327 1356  97  92
2  107 229  410  979 1011  60  69
> alcoff  # Original data
   Mon Tue Wed Thu  Fri  Sat  Sun
0  121  98 165 324  827 1379 1332
1   97  92 157 278  619 1327 1356
2   60  69 107 229  410  979 1011
3   55  60  75 238  401  693  718
4   25  38  48 145  223  346  410
5   19  10  20  56  139  188  287
6   13   9   9  55   70  155  213
7   20   6   9  42   40  160  200
8    5   8  12  29   40   79   96
9    7   8  14  28   38   44   58
10   7  13  20  36   38   52   69
11   8  10  20  32   27   44   39
12  10  13   8  39   37   73   59
13   9  26  32  37   27   41   45
14  22  41  31  46   42   58   53
15  39  48  62  69   59   75   70
16  28  48  71  85   55   96   95
17  46  59  98 141  136  154  130
18  53 100 117 185  223  236  121
19  74 119 155 289  335  337  146
20  74 135 283 508  591  490  166
21  84 154 326 610  866  754  131
22  90 143 345 765  976 1026  114
23 110 169 363 899 1265 1179  159
> moffset(alcoff, 3, 2, "*") -
+ Rcim(alcoff, 3+1, 2+1)  # Note the differences
   Wed* Thu* Fri* Sat*  Sun* Mon* Tue*
3     0    0    0    0     0    0    0
4     0    0    0    0     0    0    0
5     0    0    0    0     0    0    0
6     0    0    0    0     0    0    0
7     0    0    0    0     0    0    0
8     0    0    0    0     0    0    0
9     0    0    0    0     0    0    0
10    0    0    0    0     0    0    0
11    0    0    0    0     0    0    0
12    0    0    0    0     0    0    0
13    0    0    0    0     0    0    0
14    0    0    0    0     0    0    0
15    0    0    0    0     0    0    0
16    0    0    0    0     0    0    0
17    0    0    0    0     0    0    0
18    0    0    0    0     0    0    0
19    0    0    0    0     0    0    0
20    0    0    0    0     0    0    0
21    0    0    0    0     0    0    0
22    0    0    0    0     0    0    0
23    0    0    0    0     0    0    0
0*  159  503  552  -47 -1211  -23   67
1*  121  341  708   29 -1259   -5   65
2*  122  181  569   32  -951    9   38
> 
> # An 'effective day' data set:
> alcoff.e <- moffset(alcoff, roffset = "6", postfix = "*")
> fit.o <- rcim(alcoff)    # default baselines are 1st row and col
> fit.e <- rcim(alcoff.e)  # default baselines are 1st row and col
> 
> ## Not run: 
> ##D  par(mfrow = c(2, 2), mar = c(9, 4, 2, 1))
> ##D plot(fit.o, rsub = "Not very interpretable",
> ##D             csub = "Not very interpretable")
> ##D plot(fit.e, rsub = "More interpretable",
> ##D             csub = "More interpretable")
> ## End(Not run)
> 
> # Some checking
> all.equal(moffset(alcoff), alcoff)  # Should be no change
[1] TRUE
> moffset(alcoff, 1, 1, "*")
   Tue* Wed* Thu* Fri* Sat* Sun* Mon*
1    92  157  278  619 1327 1356   97
2    69  107  229  410  979 1011   60
3    60   75  238  401  693  718   55
4    38   48  145  223  346  410   25
5    10   20   56  139  188  287   19
6     9    9   55   70  155  213   13
7     6    9   42   40  160  200   20
8     8   12   29   40   79   96    5
9     8   14   28   38   44   58    7
10   13   20   36   38   52   69    7
11   10   20   32   27   44   39    8
12   13    8   39   37   73   59   10
13   26   32   37   27   41   45    9
14   41   31   46   42   58   53   22
15   48   62   69   59   75   70   39
16   48   71   85   55   96   95   28
17   59   98  141  136  154  130   46
18  100  117  185  223  236  121   53
19  119  155  289  335  337  146   74
20  135  283  508  591  490  166   74
21  154  326  610  866  754  131   84
22  143  345  765  976 1026  114   90
23  169  363  899 1265 1179  159  110
0*  165  324  827 1379 1332  121   98
> moffset(alcoff, 2, 3, "*")
   Thu* Fri* Sat* Sun* Mon* Tue* Wed*
2   229  410  979 1011   60   69  107
3   238  401  693  718   55   60   75
4   145  223  346  410   25   38   48
5    56  139  188  287   19   10   20
6    55   70  155  213   13    9    9
7    42   40  160  200   20    6    9
8    29   40   79   96    5    8   12
9    28   38   44   58    7    8   14
10   36   38   52   69    7   13   20
11   32   27   44   39    8   10   20
12   39   37   73   59   10   13    8
13   37   27   41   45    9   26   32
14   46   42   58   53   22   41   31
15   69   59   75   70   39   48   62
16   85   55   96   95   28   48   71
17  141  136  154  130   46   59   98
18  185  223  236  121   53  100  117
19  289  335  337  146   74  119  155
20  508  591  490  166   74  135  283
21  610  866  754  131   84  154  326
22  765  976 1026  114   90  143  345
23  899 1265 1179  159  110  169  363
0*  827 1379 1332  121   98  165  324
1*  619 1327 1356   97   92  157  278
> moffset(alcoff, 1, 0, "*")
   Mon* Tue* Wed* Thu* Fri* Sat* Sun*
1    97   92  157  278  619 1327 1356
2    60   69  107  229  410  979 1011
3    55   60   75  238  401  693  718
4    25   38   48  145  223  346  410
5    19   10   20   56  139  188  287
6    13    9    9   55   70  155  213
7    20    6    9   42   40  160  200
8     5    8   12   29   40   79   96
9     7    8   14   28   38   44   58
10    7   13   20   36   38   52   69
11    8   10   20   32   27   44   39
12   10   13    8   39   37   73   59
13    9   26   32   37   27   41   45
14   22   41   31   46   42   58   53
15   39   48   62   69   59   75   70
16   28   48   71   85   55   96   95
17   46   59   98  141  136  154  130
18   53  100  117  185  223  236  121
19   74  119  155  289  335  337  146
20   74  135  283  508  591  490  166
21   84  154  326  610  866  754  131
22   90  143  345  765  976 1026  114
23  110  169  363  899 1265 1179  159
0*   98  165  324  827 1379 1332  121
> moffset(alcoff, 0, 1, "*")
   Tue Wed Thu  Fri  Sat  Sun Mon
0   98 165 324  827 1379 1332 121
1   92 157 278  619 1327 1356  97
2   69 107 229  410  979 1011  60
3   60  75 238  401  693  718  55
4   38  48 145  223  346  410  25
5   10  20  56  139  188  287  19
6    9   9  55   70  155  213  13
7    6   9  42   40  160  200  20
8    8  12  29   40   79   96   5
9    8  14  28   38   44   58   7
10  13  20  36   38   52   69   7
11  10  20  32   27   44   39   8
12  13   8  39   37   73   59  10
13  26  32  37   27   41   45   9
14  41  31  46   42   58   53  22
15  48  62  69   59   75   70  39
16  48  71  85   55   96   95  28
17  59  98 141  136  154  130  46
18 100 117 185  223  236  121  53
19 119 155 289  335  337  146  74
20 135 283 508  591  490  166  74
21 154 326 610  866  754  131  84
22 143 345 765  976 1026  114  90
23 169 363 899 1265 1179  159 110
> moffset(alcoff, "6", "Mon", "*")  # This one is good
   Mon* Tue* Wed* Thu* Fri* Sat* Sun*
6    13    9    9   55   70  155  213
7    20    6    9   42   40  160  200
8     5    8   12   29   40   79   96
9     7    8   14   28   38   44   58
10    7   13   20   36   38   52   69
11    8   10   20   32   27   44   39
12   10   13    8   39   37   73   59
13    9   26   32   37   27   41   45
14   22   41   31   46   42   58   53
15   39   48   62   69   59   75   70
16   28   48   71   85   55   96   95
17   46   59   98  141  136  154  130
18   53  100  117  185  223  236  121
19   74  119  155  289  335  337  146
20   74  135  283  508  591  490  166
21   84  154  326  610  866  754  131
22   90  143  345  765  976 1026  114
23  110  169  363  899 1265 1179  159
0*   98  165  324  827 1379 1332  121
1*   92  157  278  619 1327 1356   97
2*   69  107  229  410  979 1011   60
3*   60   75  238  401  693  718   55
4*   38   48  145  223  346  410   25
5*   10   20   56  139  188  287   19
> 
> # Customise row and column baselines
> fit2 <- rcim(Rcim(alcoff.e, rbaseline = "11", cbaseline = "Mon*"))
> 
> 
> 
> cleanEx()
> nameEx("multilogitlink")
> ### * multilogitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multilogitlink
> ### Title: Multi-logit Link Function
> ### Aliases: multilogitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let,  # For illustration only!
+             multinomial, trace = TRUE, data = pneumo)
VGLM    linear loop  1 :  deviance = 5.407271
VGLM    linear loop  2 :  deviance = 5.34745
VGLM    linear loop  3 :  deviance = 5.347382
VGLM    linear loop  4 :  deviance = 5.347382
> fitted(fit)
     normal        mild      severe
1 0.9927503 0.005875947 0.001373768
2 0.9329702 0.043219077 0.023810688
3 0.8488899 0.085745054 0.065365011
4 0.7485338 0.128835331 0.122630879
5 0.6393787 0.168725388 0.191895881
6 0.5334715 0.201127232 0.265401245
7 0.4313692 0.226188995 0.342441766
8 0.3581471 0.239824757 0.402028109
> predict(fit)
  log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
1          6.5829217         1.45330985
2          3.6682387         0.59614746
3          2.5639424         0.27139129
4          1.8089375         0.04935622
5          1.2035440        -0.12868047
6          0.6981629        -0.27730511
7          0.2308628        -0.41473071
8         -0.1155781        -0.51661353
> 
> multilogitlink(fitted(fit))
        [,1]        [,2]
1  6.5829217  1.45330985
2  3.6682387  0.59614746
3  2.5639424  0.27139129
4  1.8089375  0.04935622
5  1.2035440 -0.12868047
6  0.6981629 -0.27730511
7  0.2308628 -0.41473071
8 -0.1155781 -0.51661353
> multilogitlink(fitted(fit)) - predict(fit)  # Should be all 0s
           [,1]          [,2]
1  0.000000e+00  2.220446e-16
2  0.000000e+00  2.220446e-16
3  0.000000e+00 -3.885781e-16
4  0.000000e+00 -1.179612e-16
5  0.000000e+00 -1.110223e-16
6  2.220446e-16  0.000000e+00
7 -1.387779e-16 -1.110223e-16
8  2.775558e-17  0.000000e+00
> 
> multilogitlink(predict(fit), inverse = TRUE)  # rowSums() add to unity
       [,1]        [,2]        [,3]
1 0.9927503 0.005875947 0.001373768
2 0.9329702 0.043219077 0.023810688
3 0.8488899 0.085745054 0.065365011
4 0.7485338 0.128835331 0.122630879
5 0.6393787 0.168725388 0.191895881
6 0.5334715 0.201127232 0.265401245
7 0.4313692 0.226188995 0.342441766
8 0.3581471 0.239824757 0.402028109
> multilogitlink(predict(fit), inverse = TRUE, refLevel = 1)
         [,1]      [,2]        [,3]
1 0.001373768 0.9927503 0.005875947
2 0.023810688 0.9329702 0.043219077
3 0.065365011 0.8488899 0.085745054
4 0.122630879 0.7485338 0.128835331
5 0.191895881 0.6393787 0.168725388
6 0.265401245 0.5334715 0.201127232
7 0.342441766 0.4313692 0.226188995
8 0.402028109 0.3581471 0.239824757
> multilogitlink(predict(fit), inverse = TRUE) -
+ fitted(fit)  # Should be all 0s
  [,1] [,2] [,3]
1    0    0    0
2    0    0    0
3    0    0    0
4    0    0    0
5    0    0    0
6    0    0    0
7    0    0    0
8    0    0    0
> 
> multilogitlink(fitted(fit), deriv = 1)
      normal       mild     severe
1 138.943754 171.191239 728.926256
2  15.990591  24.183102  43.022338
3   7.795702  12.756267  16.368641
4   5.312622   8.909735   9.294324
5   4.337011   7.129762   6.448624
6   4.018006   6.223741   5.129167
7   4.076810   5.713387   4.440982
8   4.350138   5.485197   4.159708
> multilogitlink(fitted(fit), deriv = 2)
        normal         mild        severe
1 19025.449830 -28962.03409 -5.298736e+05
2   221.420110   -534.27143 -1.762778e+03
3    42.406154   -134.81708 -2.329056e+02
4    14.029214    -58.92861 -6.519766e+01
5     5.243333    -33.67970 -2.562486e+01
6     1.080754    -23.15364 -1.234382e+01
7    -2.281339    -17.87591 -6.214829e+00
8    -5.368762    -15.65599 -3.390448e+00
> 
> 
> 
> cleanEx()
> nameEx("multinomial")
> ### * multinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multinomial
> ### Title: Multinomial Logit Model
> ### Aliases: multinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: Regn spline VGAM: marital status versus age
> data(marital.nz)
> ooo <- with(marital.nz, order(age))
> om.nz <- marital.nz[ooo, ]
> fit1 <- vglm(mstatus ~ sm.bs(age), multinomial, om.nz)
> coef(fit1, matrix = TRUE)  # Mostly meaningless
            log(mu[,1]/mu[,4]) log(mu[,2]/mu[,4]) log(mu[,3]/mu[,4])
(Intercept)           5.844418           9.502096           13.10287
sm.bs(age)1          -3.988086          -5.655932          -17.19862
sm.bs(age)2          -4.085133          -5.650005          -10.69507
sm.bs(age)3          -9.277378          -9.292940          -16.56821
> ## Not run: 
> ##D  with(om.nz,
> ##D matplot(age, fitted(fit1), type = "l", las = 1, lwd = 2))
> ##D legend("topright", leg = colnames(fitted(fit1)),
> ##D        lty = 1:4, col = 1:4, lwd = 2) 
> ## End(Not run)
> 
> # Example 2a: a simple example
> ycounts <- t(rmultinom(10, size = 20, prob = c(0.1, 0.2, 0.8)))
> fit <- vglm(ycounts ~ 1, multinomial)
> head(fitted(fit))   # Proportions
  [,1]  [,2]  [,3]
1 0.15 0.205 0.645
2 0.15 0.205 0.645
3 0.15 0.205 0.645
4 0.15 0.205 0.645
5 0.15 0.205 0.645
6 0.15 0.205 0.645
> fit@prior.weights   # NOT recommended for the prior weights
   [,1]
1    20
2    20
3    20
4    20
5    20
6    20
7    20
8    20
9    20
10   20
> weights(fit, type = "prior", matrix = FALSE)  # The better method
 [1] 20 20 20 20 20 20 20 20 20 20
> depvar(fit)         # Sample proportions; same as fit@y
   [,1] [,2] [,3]
1  0.15 0.25 0.60
2  0.30 0.10 0.60
3  0.10 0.35 0.55
4  0.15 0.30 0.55
5  0.10 0.05 0.85
6  0.05 0.35 0.60
7  0.10 0.20 0.70
8  0.15 0.10 0.75
9  0.25 0.15 0.60
10 0.15 0.20 0.65
> constraints(fit)    # Constraint matrices
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

> 
> # Example 2b: Different reference level used as the baseline
> fit2 <- vglm(ycounts ~ 1, multinomial(refLevel = 2))
> coef(fit2, matrix = TRUE)
            log(mu[,1]/mu[,2]) log(mu[,3]/mu[,2])
(Intercept)         -0.3123747            1.14624
> coef(fit , matrix = TRUE)  # Easy to reconcile this output with fit2
            log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
(Intercept)          -1.458615           -1.14624
> 
> # Example 3: The response is a factor.
> nn <- 10
> dframe3 <- data.frame(yfac = gl(3, nn, labels = c("Ctrl",
+                                 "Trt1", "Trt2")),
+                       x2   = runif(3 * nn))
> myrefLevel <- with(dframe3, yfac[12])
> fit3a <- vglm(yfac ~ x2, multinomial(refLevel = myrefLevel), dframe3)
> fit3b <- vglm(yfac ~ x2, multinomial(refLevel = 2), dframe3)
> coef(fit3a, matrix = TRUE)  # "Trt1" is the reference level
            log(mu[,1]/mu[,2]) log(mu[,3]/mu[,2])
(Intercept)           1.019257          -0.343633
x2                   -2.310184           0.621166
> coef(fit3b, matrix = TRUE)  # "Trt1" is the reference level
            log(mu[,1]/mu[,2]) log(mu[,3]/mu[,2])
(Intercept)           1.019257          -0.343633
x2                   -2.310184           0.621166
> margeff(fit3b)
, , 1

                  Ctrl        Trt1       Trt2
(Intercept)  0.1921453 -0.02263570 -0.1695096
x2          -0.4222329  0.07645457  0.3457783

, , 2

                  Ctrl       Trt1       Trt2
(Intercept)  0.2887710 -0.1264379 -0.1623331
x2          -0.6376898  0.2943581  0.3433317

, , 3

                  Ctrl        Trt1       Trt2
(Intercept)  0.1209635  0.02237170 -0.1433352
x2          -0.2652835 -0.02016881  0.2854523

, , 4

                  Ctrl       Trt1       Trt2
(Intercept)  0.2920775 -0.1143168 -0.1777606
x2          -0.6442741  0.2703558  0.3739184

, , 5

                  Ctrl       Trt1       Trt2
(Intercept)  0.2865094 -0.1274249 -0.1590845
x2          -0.6328123  0.2960894  0.3367229

, , 6

                 Ctrl        Trt1       Trt2
(Intercept)  0.251120 -0.06749442 -0.1836256
x2          -0.552752  0.17211595  0.3806361

, , 7

                  Ctrl       Trt1       Trt2
(Intercept)  0.2908329 -0.1248959 -0.1659370
x2          -0.6421034  0.2914866  0.3506168

, , 8

                  Ctrl        Trt1       Trt2
(Intercept)  0.2568443 -0.07249836 -0.1843460
x2          -0.5654598  0.18273087  0.3827289

, , 9

                  Ctrl       Trt1       Trt2
(Intercept)  0.2912663 -0.1244191 -0.1668473
x2          -0.6430234  0.2905758  0.3524476

, , 10

                  Ctrl       Trt1       Trt2
(Intercept)  0.2929544 -0.1202410 -0.1727134
x2          -0.6464868  0.2823601  0.3641267

, , 11

                  Ctrl        Trt1       Trt2
(Intercept)  0.2232603 -0.04511209 -0.1781482
x2          -0.4910210  0.12448031  0.3665407

, , 12

                  Ctrl       Trt1       Trt2
(Intercept)  0.2918265 -0.1136095 -0.1782170
x2          -0.6436914  0.2689071  0.3747843

, , 13

                  Ctrl         Trt1       Trt2
(Intercept)  0.1716377 -0.008845259 -0.1627925
x2          -0.3769615  0.046913781  0.3300477

, , 14

                  Ctrl       Trt1       Trt2
(Intercept)  0.2904385 -0.1252685 -0.1651700
x2          -0.6412633  0.2921922  0.3490711

, , 15

                  Ctrl        Trt1       Trt2
(Intercept)  0.1243010  0.02043675 -0.1447377
x2          -0.2726309 -0.01600047  0.2886314

, , 16

                  Ctrl        Trt1       Trt2
(Intercept)  0.2830272 -0.09925957 -0.1837677
x2          -0.6238011  0.23914200  0.3846591

, , 17

                  Ctrl       Trt1       Trt2
(Intercept)  0.2609774 -0.0762341 -0.1847433
x2          -0.5746421  0.1906450  0.3839971

, , 18

                  Ctrl        Trt1       Trt2
(Intercept)  0.2210796 -0.04346428 -0.1776153
x2          -0.4861953  0.12096509  0.3652302

, , 19

                  Ctrl        Trt1       Trt2
(Intercept)  0.2433086 -0.06092792 -0.1823807
x2          -0.5354262  0.15816480  0.3772614

, , 20

                  Ctrl       Trt1       Trt2
(Intercept)  0.2311319 -0.0511693 -0.1799626
x2          -0.5084466  0.1373933  0.3710533

, , 21

                  Ctrl        Trt1       Trt2
(Intercept)  0.1762443 -0.01188099 -0.1643634
x2          -0.3871267  0.05342127  0.3337054

, , 22

                  Ctrl        Trt1       Trt2
(Intercept)  0.1468462 0.006917267 -0.1537635
x2          -0.3222929 0.013081892  0.3092110

, , 23

                  Ctrl        Trt1       Trt2
(Intercept)  0.1396987 0.011287036 -0.1509857
x2          -0.3065431 0.003689179  0.3028539

, , 24

                  Ctrl        Trt1       Trt2
(Intercept)  0.1388465 0.011802862 -0.1506494
x2          -0.3046657 0.002579992  0.3020857

, , 25

                  Ctrl        Trt1       Trt2
(Intercept)  0.2808226 -0.09655622 -0.1842664
x2          -0.6188644  0.23348181  0.3853826

, , 26

                  Ctrl       Trt1       Trt2
(Intercept)  0.2832853 -0.1282103 -0.1550750
x2          -0.6258261  0.2973044  0.3285217

, , 27

                  Ctrl        Trt1       Trt2
(Intercept)  0.2066755 -0.03287136 -0.1738041
x2          -0.4543391  0.09834517  0.3559939

, , 28

                  Ctrl        Trt1       Trt2
(Intercept)  0.2442393 -0.06169632 -0.1825430
x2          -0.5374897  0.15979850  0.3776912

, , 29

                  Ctrl       Trt1       Trt2
(Intercept)  0.2929717 -0.1184922 -0.1744796
x2          -0.6464352  0.2788459  0.3675893

, , 30

                  Ctrl        Trt1       Trt2
(Intercept)  0.2565574 -0.07224308 -0.1843143
x2          -0.5648225  0.18218973  0.3826328

> 
> # Example 4: Fit a rank-1 stereotype model
> fit4 <- rrvglm(Country ~ Width + Height + HP, multinomial, car.all)
> coef(fit4)  # Contains the C matrix
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5 
   93.1972695    35.6049988    26.3461652    35.9166425    45.9366677 
(Intercept):6 (Intercept):7 (Intercept):8 (Intercept):9         Width 
   50.0560914    74.6327053    62.8788427    27.2281309    -1.6550861 
       Height            HP 
    0.2068449     0.0287241 
> constraints(fit4)$HP       # The A matrix
           [,1]
 [1,] 1.0000000
 [2,] 0.3941479
 [3,] 0.2926137
 [4,] 0.3737670
 [5,] 0.4650090
 [6,] 0.5202978
 [7,] 0.7834294
 [8,] 0.6651159
 [9,] 0.2945319
> coef(fit4, matrix = TRUE)  # The B matrix
            log(mu[,1]/mu[,10]) log(mu[,2]/mu[,10]) log(mu[,3]/mu[,10])
(Intercept)          93.1972695         35.60499878        26.346165214
Width                -1.6550861         -0.65234865        -0.484300882
Height                0.2068449          0.08152746         0.060525645
HP                    0.0287241          0.01132154         0.008405067
            log(mu[,4]/mu[,10]) log(mu[,5]/mu[,10]) log(mu[,6]/mu[,10])
(Intercept)         35.91664252         45.93666768         50.05609138
Width               -0.61861655         -0.76962992         -0.86113765
Height               0.07731179          0.09618473          0.10762093
HP                   0.01073612          0.01335697          0.01494509
            log(mu[,7]/mu[,10]) log(mu[,8]/mu[,10]) log(mu[,9]/mu[,10])
(Intercept)         74.63270531         62.87884265        27.228130873
Width               -1.29664308         -1.10082403        -0.487475584
Height               0.16204835          0.13757581         0.060922405
HP                   0.02250331          0.01910486         0.008460164
> Coef(fit4)@C               # The C matrix
           latvar
Width  -1.6550860
Height  0.2068449
HP      0.0287241
> concoef(fit4)              # Better to get the C matrix this way
           latvar
Width  -1.6550860
Height  0.2068449
HP      0.0287241
> Coef(fit4)@A               # The A matrix
                       latvar
log(mu[,1]/mu[,10]) 1.0000000
log(mu[,2]/mu[,10]) 0.3941479
log(mu[,3]/mu[,10]) 0.2926137
log(mu[,4]/mu[,10]) 0.3737670
log(mu[,5]/mu[,10]) 0.4650090
log(mu[,6]/mu[,10]) 0.5202978
log(mu[,7]/mu[,10]) 0.7834294
log(mu[,8]/mu[,10]) 0.6651159
log(mu[,9]/mu[,10]) 0.2945319
> svd(coef(fit4, matrix = TRUE)[-1, ])$d  # Has rank 1; = C %*% t(A)
[1] 2.894479e+00 7.198723e-17 2.854179e-18
> # Classification (but watch out for NAs in some of the variables):
> apply(fitted(fit4), 1, which.max)  # Classification
          Acura Integra            Acura Legend                Audi 100 
                      5                      10                      10 
                Audi 80                BMW 325i                BMW 535i 
                      5                       5                       5 
          Buick Century           Buick Electra          Buick Le Sabre 
                     10                      10                      10 
          Buick Riviera       Cadillac Brougham       Cadillac De Ville 
                     10                      10                      10 
      Cadillac Eldorado         Chevrolet Astro       Chevrolet Beretta 
                     10                      10                      10 
       Chevrolet Camaro       Chevrolet Caprice      Chevrolet Cavalier 
                     10                      10                       5 
     Chevrolet Corvette        Chevrolet Lumina    Chevrolet Lumina APV 
                     10                      10                      10 
      Chrysler Imperial       Chrysler Le Baron Chrysler Le Baron Coupe 
                     10                       5                      10 
          Dodge Caravan              Dodge Colt           Dodge Daytona 
                     10                       5                      10 
          Dodge Dynasty     Dodge Grand Caravan              Dodge Omni 
                     10                      10                       5 
           Dodge Shadow           Eagle Premier            Eagle Summit 
                      5                      10                       5 
          Ford Aerostar             Ford Escort            Ford Festiva 
                     10                       5                       5 
Ford LTD Crown Victoria            Ford Mustang              Ford Probe 
                     10                      10                      10 
            Ford Taurus              Ford Tempo        Ford Thunderbird 
                     10                      10                      10 
              GEO Metro               GEO Prizm               GEO Storm 
                      5                       5                       5 
           Honda Accord             Honda Civic         Honda Civic CRX 
                      5                       5                       5 
          Honda Prelude           Hyundai Excel          Hyundai Sonata 
                      5                       5                      10 
           Infiniti Q45             Lexus LS400     Lincoln Continental 
                     10                      10                      10 
       Lincoln Mark VII        Lincoln Town Car               Mazda 626 
                     10                      10                       5 
              Mazda 929               Mazda MPV        Mazda MX-5 Miata 
                      5                      10                       5 
             Mazda MX-6           Mazda Protege               Mazda RX7 
                      5                       5                       5 
      Mercedes-Benz 190      Mercedes-Benz 300E          Mercury Tracer 
                      5                       5                       5 
      Mitsubishi Galant       Mitsubishi Precis        Mitsubishi Sigma 
                      5                       5                       5 
       Mitsubishi Wagon            Nissan 240SX            Nissan 300ZX 
                      5                       5                      10 
          Nissan Axxess           Nissan Maxima        Nissan Pulsar NX 
                      5                      10                       5 
          Nissan Sentra           Nissan Stanza              Nissan Van 
                      5                       5                       5 
            Peugeot 405             Peugeot 505          Plymouth Laser 
                      5                       5                       5 
       Pontiac Grand Am          Pontiac LeMans             Porsche 944 
                      5                       5                       5 
               Saab 900               Saab 9000            Sterling 827 
                      5                      10                       5 
           Subaru Justy           Subaru Legacy           Subaru Loyale 
                      7                       5                       5 
              Subaru XT            Toyota Camry           Toyota Celica 
                      5                       5                      10 
         Toyota Corolla         Toyota Cressida            Toyota Supra 
                      5                       5                      10 
          Toyota Tercel      Volkswagen Corrado          Volkswagen Fox 
                      5                       5                       5 
         Volkswagen GTI         Volkswagen Golf        Volkswagen Jetta 
                      5                       5                       5 
     Volkswagen Vanagon               Volvo 240               Volvo 740 
                     10                       5                      10 
> # Classification:
> colnames(fitted(fit4))[apply(fitted(fit4), 1, which.max)]
  [1] "Japan" "USA"   "USA"   "Japan" "Japan" "Japan" "USA"   "USA"   "USA"  
 [10] "USA"   "USA"   "USA"   "USA"   "USA"   "USA"   "USA"   "USA"   "Japan"
 [19] "USA"   "USA"   "USA"   "USA"   "Japan" "USA"   "USA"   "Japan" "USA"  
 [28] "USA"   "USA"   "Japan" "Japan" "USA"   "Japan" "USA"   "Japan" "Japan"
 [37] "USA"   "USA"   "USA"   "USA"   "USA"   "USA"   "Japan" "Japan" "Japan"
 [46] "Japan" "Japan" "Japan" "Japan" "Japan" "USA"   "USA"   "USA"   "USA"  
 [55] "USA"   "USA"   "Japan" "Japan" "USA"   "Japan" "Japan" "Japan" "Japan"
 [64] "Japan" "Japan" "Japan" "Japan" "Japan" "Japan" "Japan" "Japan" "USA"  
 [73] "Japan" "USA"   "Japan" "Japan" "Japan" "Japan" "Japan" "Japan" "Japan"
 [82] "Japan" "Japan" "Japan" "Japan" "USA"   "Japan" "Korea" "Japan" "Japan"
 [91] "Japan" "Japan" "USA"   "Japan" "Japan" "USA"   "Japan" "Japan" "Japan"
[100] "Japan" "Japan" "Japan" "USA"   "Japan" "USA"  
> apply(predict(fit4, car.all, type = "response"),
+       1, which.max)  # Ditto
          Acura Integra            Acura Legend                Audi 100 
                      5                      10                      10 
                Audi 80                BMW 325i                BMW 535i 
                      5                       5                       5 
          Buick Century           Buick Electra          Buick Le Sabre 
                     10                      10                      10 
            Buick Regal           Buick Riviera       Cadillac Brougham 
                     10                      10                      10 
      Cadillac De Ville       Cadillac Eldorado         Chevrolet Astro 
                     10                      10                      10 
      Chevrolet Beretta        Chevrolet Camaro       Chevrolet Caprice 
                     10                      10                      10 
     Chevrolet Cavalier      Chevrolet Corvette        Chevrolet Lumina 
                      5                      10                      10 
   Chevrolet Lumina APV       Chrysler Imperial       Chrysler Le Baron 
                     10                      10                       5 
Chrysler Le Baron Coupe           Dodge Caravan              Dodge Colt 
                     10                      10                       5 
          Dodge Daytona           Dodge Dynasty     Dodge Grand Caravan 
                     10                      10                      10 
             Dodge Omni            Dodge Shadow            Dodge Spirit 
                      5                       5                       5 
          Eagle Premier            Eagle Summit           Ford Aerostar 
                     10                       5                      10 
            Ford Escort            Ford Festiva Ford LTD Crown Victoria 
                      5                       5                      10 
           Ford Mustang              Ford Probe             Ford Taurus 
                     10                      10                      10 
             Ford Tempo        Ford Thunderbird               GEO Metro 
                     10                      10                       5 
              GEO Prizm               GEO Storm            Honda Accord 
                      5                       5                       5 
            Honda Civic         Honda Civic CRX           Honda Prelude 
                      5                       5                       5 
          Hyundai Excel          Hyundai Sonata            Infiniti Q45 
                      5                      10                      10 
            Lexus LS400     Lincoln Continental        Lincoln Mark VII 
                     10                      10                      10 
       Lincoln Town Car               Mazda 626               Mazda 929 
                     10                       5                       5 
              Mazda MPV        Mazda MX-5 Miata              Mazda MX-6 
                     10                       5                       5 
          Mazda Protege               Mazda RX7       Mercedes-Benz 190 
                      5                       5                       5 
     Mercedes-Benz 300E          Mercury Tracer       Mitsubishi Galant 
                      5                       5                       5 
      Mitsubishi Precis        Mitsubishi Sigma        Mitsubishi Wagon 
                      5                       5                       5 
           Nissan 240SX            Nissan 300ZX           Nissan Axxess 
                      5                      10                       5 
          Nissan Maxima        Nissan Pulsar NX           Nissan Sentra 
                     10                       5                       5 
          Nissan Stanza              Nissan Van             Peugeot 405 
                      5                       5                       5 
            Peugeot 505          Plymouth Laser      Pontiac Bonneville 
                      5                       5                      10 
       Pontiac Grand Am          Pontiac LeMans             Porsche 944 
                      5                       5                       5 
               Saab 900               Saab 9000            Sterling 827 
                      5                      10                       5 
           Subaru Justy           Subaru Legacy           Subaru Loyale 
                      7                       5                       5 
              Subaru XT            Toyota Camry           Toyota Celica 
                      5                       5                      10 
         Toyota Corolla         Toyota Cressida            Toyota Supra 
                      5                       5                      10 
          Toyota Tercel      Volkswagen Corrado          Volkswagen Fox 
                      5                       5                       5 
         Volkswagen GTI         Volkswagen Golf        Volkswagen Jetta 
                      5                       5                       5 
     Volkswagen Vanagon               Volvo 240               Volvo 740 
                     10                       5                      10 
> 
> 
> # Example 5: Using the xij argument (aka conditional logit model)
> set.seed(111)
> nn <- 100  # Number of people who travel to work
> M <- 3  # There are M+1 models of transport to go to work
> ycounts <- matrix(0, nn, M+1)
> ycounts[cbind(1:nn, sample(x = M+1, size = nn, replace = TRUE))] = 1
> dimnames(ycounts) <- list(NULL, c("bus","train","car","walk"))
> gotowork <- data.frame(cost.bus  = runif(nn), time.bus  = runif(nn),
+                        cost.train= runif(nn), time.train= runif(nn),
+                        cost.car  = runif(nn), time.car  = runif(nn),
+                        cost.walk = runif(nn), time.walk = runif(nn))
> gotowork <- round(gotowork, digits = 2)  # For convenience
> gotowork <- transform(gotowork,
+               Cost.bus   = cost.bus   - cost.walk,
+               Cost.car   = cost.car   - cost.walk,
+               Cost.train = cost.train - cost.walk,
+               Cost       = cost.train - cost.walk,  # for labelling
+               Time.bus   = time.bus   - time.walk,
+               Time.car   = time.car   - time.walk,
+               Time.train = time.train - time.walk,
+               Time       = time.train - time.walk)  # for labelling
> fit <- vglm(ycounts ~ Cost + Time,
+             multinomial(parall = TRUE ~ Cost + Time - 1),
+             xij = list(Cost ~ Cost.bus + Cost.train + Cost.car,
+                        Time ~ Time.bus + Time.train + Time.car),
+             form2 =  ~ Cost + Cost.bus + Cost.train + Cost.car +
+                        Time + Time.bus + Time.train + Time.car,
+             data = gotowork, trace = TRUE)
VGLM    linear loop  1 :  deviance = 273.05051
VGLM    linear loop  2 :  deviance = 273.04187
VGLM    linear loop  3 :  deviance = 273.04187
> head(model.matrix(fit, type = "lm"))   # LM model matrix
  (Intercept)  Cost  Time
1           1  0.31  0.03
2           1 -0.15 -0.76
3           1 -0.60  0.20
4           1  0.41 -0.51
5           1  0.23  0.29
6           1 -0.07 -0.38
> head(model.matrix(fit, type = "vlm"))  # Big VLM model matrix
    (Intercept):1 (Intercept):2 (Intercept):3  Cost  Time
1:1             1             0             0  0.43  0.38
1:2             0             1             0  0.31  0.03
1:3             0             0             1  0.02  0.21
2:1             1             0             0 -0.49 -0.02
2:2             0             1             0 -0.15 -0.76
2:3             0             0             1  0.17 -0.03
> coef(fit)
(Intercept):1 (Intercept):2 (Intercept):3          Cost          Time 
    0.2091905    -0.1847232    -0.1600374    -0.1811351    -0.5111686 
> coef(fit, matrix = TRUE)
            log(mu[,1]/mu[,4]) log(mu[,2]/mu[,4]) log(mu[,3]/mu[,4])
(Intercept)          0.2091905         -0.1847232         -0.1600374
Cost                -0.1811351         -0.1811351         -0.1811351
Time                -0.5111686         -0.5111686         -0.5111686
> constraints(fit)
$`(Intercept)`
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

$Cost
     [,1]
[1,]    1
[2,]    1
[3,]    1

$Time
     [,1]
[1,]    1
[2,]    1
[3,]    1

> summary(fit)
Call:
vglm(formula = ycounts ~ Cost + Time, family = multinomial(parall = TRUE ~ 
    Cost + Time - 1), data = gotowork, form2 = ~Cost + Cost.bus + 
    Cost.train + Cost.car + Time + Time.bus + Time.train + Time.car, 
    xij = list(Cost ~ Cost.bus + Cost.train + Cost.car, Time ~ 
        Time.bus + Time.train + Time.car), trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)
(Intercept):1   0.2092     0.2683   0.780    0.436
(Intercept):2  -0.1847     0.2960  -0.624    0.533
(Intercept):3  -0.1600     0.2914  -0.549    0.583
Cost           -0.1811     0.4208  -0.430    0.667
Time           -0.5112     0.4012  -1.274    0.203

Names of linear predictors: log(mu[,1]/mu[,4]), log(mu[,2]/mu[,4]), 
log(mu[,3]/mu[,4])

Residual deviance: 273.0419 on 295 degrees of freedom

Log-likelihood: -136.5209 on 295 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates


Reference group is level  4  of the response
> max(abs(predict(fit) - predict(fit, new = gotowork)))  # Should be 0
[1] 3.053113e-15
> 
> 
> 
> cleanEx()
> nameEx("nakagami")
> ### * nakagami
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nakagami
> ### Title: Nakagami Regression Family Function
> ### Aliases: nakagami
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000; shape <- exp(0); Scale <- exp(1)
> ndata <- data.frame(y1 = sqrt(rgamma(nn, shape = shape, scale = Scale/shape)))
> nfit <- vglm(y1 ~ 1, nakagami, data = ndata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -0.32303694, -2.62719509
VGLM    linear loop  2 :  coefficients =  2.3811927, -1.8811881
VGLM    linear loop  3 :  coefficients =  1.6290866, -1.2279408
VGLM    linear loop  4 :  coefficients =  1.15498450, -0.65575047
VGLM    linear loop  5 :  coefficients =  0.99987679, -0.24173415
VGLM    linear loop  6 :  coefficients =  0.986528268, -0.083889627
VGLM    linear loop  7 :  coefficients =  0.986438379, -0.067028682
VGLM    linear loop  8 :  coefficients =  0.986438375, -0.066858582
VGLM    linear loop  9 :  coefficients =  0.986438375, -0.066858565
VGLM    linear loop  10 :  coefficients =  0.986438375, -0.066858565
> ndata <- transform(ndata, y2 = rnaka(nn, scale = Scale, shape = shape))
> nfit <- vglm(y2 ~ 1, nakagami(iscale = 3), data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -8618.6645
VGLM    linear loop  2 :  loglikelihood = -7621.226
VGLM    linear loop  3 :  loglikelihood = -6627.319
VGLM    linear loop  4 :  loglikelihood = -5641.567
VGLM    linear loop  5 :  loglikelihood = -4673.9105
VGLM    linear loop  6 :  loglikelihood = -3744.1864
VGLM    linear loop  7 :  loglikelihood = -2887.2949
VGLM    linear loop  8 :  loglikelihood = -2153.3419
VGLM    linear loop  9 :  loglikelihood = -1595.5001
VGLM    linear loop  10 :  loglikelihood = -1250.5412
VGLM    linear loop  11 :  loglikelihood = -1112.4511
VGLM    linear loop  12 :  loglikelihood = -1091.3026
VGLM    linear loop  13 :  loglikelihood = -1090.83
VGLM    linear loop  14 :  loglikelihood = -1090.8298
VGLM    linear loop  15 :  loglikelihood = -1090.8298
> head(fitted(nfit))
         [,1]
[1,] 1.466248
[2,] 1.466248
[3,] 1.466248
[4,] 1.466248
[5,] 1.466248
[6,] 1.466248
> with(ndata, mean(y2))
[1] 1.465036
> coef(nfit, matrix = TRUE)
            loglink(scale) loglink(shape)
(Intercept)       1.002792     0.01855011
> (Cfit <- Coef(nfit))
   scale    shape 
2.725883 1.018723 
> ## Not run: 
> ##D  sy <- with(ndata, sort(y2))
> ##D hist(with(ndata, y2), prob = TRUE, main = "", xlab = "y", ylim = c(0, 0.6),
> ##D      col = "lightblue")
> ##D lines(dnaka(sy, scale = Cfit["scale"], shape = Cfit["shape"]) ~ sy,
> ##D       data = ndata, col = "orange") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("nakagamiUC")
> ### * nakagamiUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Nakagami
> ### Title: Nakagami Distribution
> ### Aliases: Nakagami dnaka pnaka qnaka rnaka
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(0, 3.2, len = 200)
> ##D plot(x, dgamma(x, shape = 1), type = "n", col = "black", ylab = "",
> ##D      ylim = c(0,1.5), main = "dnaka(x, shape = shape)")
> ##D lines(x, dnaka(x, shape = 1), col = "orange")
> ##D lines(x, dnaka(x, shape = 2), col = "blue")
> ##D lines(x, dnaka(x, shape = 3), col = "green")
> ##D legend(2, 1.0, col = c("orange","blue","green"), lty = rep(1, len = 3),
> ##D        legend = paste("shape =", c(1, 2, 3)))
> ##D 
> ##D plot(x, pnorm(x), type = "n", col = "black", ylab = "",
> ##D      ylim = 0:1, main = "pnaka(x, shape = shape)")
> ##D lines(x, pnaka(x, shape = 1), col = "orange")
> ##D lines(x, pnaka(x, shape = 2), col = "blue")
> ##D lines(x, pnaka(x, shape = 3), col = "green")
> ##D legend(2, 0.6, col = c("orange","blue","green"), lty = rep(1, len = 3),
> ##D        legend = paste("shape =", c(1, 2, 3))) 
> ## End(Not run)
> 
> probs <- seq(0.1, 0.9, by = 0.1)
> pnaka(qnaka(p = probs, shape = 2), shape = 2) - probs  # Should be all 0
[1] -3.427853e-06 -1.984841e-05  2.767343e-05  9.541734e-06  1.030903e-06
[6]  4.864279e-06 -5.028646e-07 -1.703812e-05 -1.239174e-08
> 
> 
> 
> cleanEx()
> nameEx("nbcanlink")
> ### * nbcanlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nbcanlink
> ### Title: Negative Binomial Canonical Link Function
> ### Aliases: nbcanlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> nbcanlink("mu", short = FALSE)
[1] "log(mu / (mu + size))"
> 
> mymu <- 1:10  # Test some basic operations:
> kmatrix <- cbind(runif(length(mymu)))
> eta1 <- nbcanlink(mymu, size = kmatrix)
> ans2 <- nbcanlink(eta1, size = kmatrix, inverse = TRUE)
> max(abs(ans2 - mymu))  # Should be 0
[1] 3.375078e-14
> 
> ## Not run: 
> ##D  mymu <- seq(0.5, 10, length = 101)
> ##D kmatrix <- matrix(10, length(mymu), 1)
> ##D plot(nbcanlink(mymu, size = kmatrix) ~ mymu, las = 1,
> ##D      type = "l", col = "blue", xlab = expression({mu}))
> ## End(Not run)
> 
> # Estimate the parameters from some simulated data
> ndata <- data.frame(x2 = runif(nn <- 500))
> ndata <- transform(ndata, eta1 = -1 - 1 * x2,  # eta1 < 0
+                           size1 = exp(1),
+                           size2 = exp(2))
> ndata <- transform(ndata,
+             mu1 = nbcanlink(eta1, size = size1, inverse = TRUE),
+             mu2 = nbcanlink(eta1, size = size2, inverse = TRUE))
> ndata <- transform(ndata, y1 = rnbinom(nn, mu = mu1, size1),
+                           y2 = rnbinom(nn, mu = mu2, size2))
> summary(ndata)
       x2                eta1            size1           size2      
 Min.   :0.001837   Min.   :-1.996   Min.   :2.718   Min.   :7.389  
 1st Qu.:0.262088   1st Qu.:-1.734   1st Qu.:2.718   1st Qu.:7.389  
 Median :0.477272   Median :-1.477   Median :2.718   Median :7.389  
 Mean   :0.497718   Mean   :-1.498   Mean   :2.718   Mean   :7.389  
 3rd Qu.:0.734146   3rd Qu.:-1.262   3rd Qu.:2.718   3rd Qu.:7.389  
 Max.   :0.996077   Max.   :-1.002   Max.   :2.718   Max.   :7.389  
      mu1              mu2              y1             y2        
 Min.   :0.4274   Min.   :1.162   Min.   :0.00   Min.   : 0.000  
 1st Qu.:0.5828   1st Qu.:1.584   1st Qu.:0.00   1st Qu.: 1.000  
 Median :0.8040   Median :2.185   Median :1.00   Median : 2.000  
 Mean   :0.8492   Mean   :2.308   Mean   :0.84   Mean   : 2.368  
 3rd Qu.:1.0732   3rd Qu.:2.917   3rd Qu.:1.00   3rd Qu.: 3.000  
 Max.   :1.5774   Max.   :4.288   Max.   :6.00   Max.   :10.000  
> 
> nbcfit <-
+   vglm(cbind(y1, y2) ~ x2,  #  crit = "c",
+        negbinomial(lmu = "nbcanlink"),
+        data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1555.6659
VGLM    linear loop  2 :  loglikelihood = -1543.51
VGLM    linear loop  3 :  loglikelihood = -1540.2688
VGLM    linear loop  4 :  loglikelihood = -1539.9706
VGLM    linear loop  5 :  loglikelihood = -1539.9651
VGLM    linear loop  6 :  loglikelihood = -1539.965
VGLM    linear loop  7 :  loglikelihood = -1539.965
> coef(nbcfit, matrix = TRUE)
            nbcanlink(mu1, mu1(size1)) loglink(size1)
(Intercept)                 -1.1699740       1.110201
x2                          -0.7982719       0.000000
            nbcanlink(mu2, mu2(size2)) loglink(size2)
(Intercept)                 -0.8240734       1.846994
x2                          -1.1321010       0.000000
> summary(nbcfit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = negbinomial(lmu = "nbcanlink"), 
    data = ndata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -1.1700     0.2382  -4.912 9.00e-07 ***
(Intercept):2   1.1102     0.3219   3.449 0.000562 ***
(Intercept):3  -0.8241     0.1460  -5.646 1.65e-08 ***
(Intercept):4   1.8470     0.2396   7.710 1.26e-14 ***
x2:1           -0.7983     0.1683  -4.744 2.10e-06 ***
x2:2           -1.1321     0.1196  -9.464  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: nbcanlink(mu1, mu1(size1)), loglink(size1), 
nbcanlink(mu2, mu2(size2)), loglink(size2)

Log-likelihood: -1539.965 on 1994 degrees of freedom

Number of Fisher scoring iterations: 7 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1', '(Intercept):3'

> 
> 
> 
> cleanEx()
> nameEx("negbinomial")
> ### * negbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: negbinomial
> ### Title: Negative Binomial Distribution Family Function
> ### Aliases: negbinomial polya polyaR
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1: apple tree data (Bliss and Fisher, 1953)
> ##D appletree <- data.frame(y = 0:7, w = c(70, 38, 17, 10, 9, 3, 2, 1))
> ##D fit <- vglm(y ~ 1, negbinomial(deviance = TRUE), data = appletree,
> ##D             weights = w, crit = "coef")  # Obtain the deviance
> ##D fit <- vglm(y ~ 1, negbinomial(deviance = TRUE), data = appletree,
> ##D             weights = w, half.step = FALSE)  # Alternative method
> ##D summary(fit)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)  # For intercept-only models
> ##D deviance(fit)  # NB2 only; needs 'crit="coef"' & 'deviance=T' above
> ##D 
> ##D # Example 2: simulated data with multiple responses
> ##D ndata <- data.frame(x2 = runif(nn <- 200))
> ##D ndata <- transform(ndata, y1 = rnbinom(nn, exp(1), mu = exp(3+x2)),
> ##D                           y2 = rnbinom(nn, exp(0), mu = exp(2-x2)))
> ##D fit1 <- vglm(cbind(y1, y2) ~ x2, negbinomial, ndata, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D 
> ##D # Example 3: large counts implies SFS is used
> ##D ndata <- transform(ndata, y3 = rnbinom(nn, exp(1), mu = exp(10+x2)))
> ##D with(ndata, range(y3))  # Large counts
> ##D fit2 <- vglm(y3 ~ x2, negbinomial, data = ndata, trace = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ##D head(weights(fit2, type = "working"))  # Non-empty; SFS was used
> ##D 
> ##D # Example 4: a NB-1 to estimate a NB with Var(Y)=phi0*mu
> ##D nn <- 200  # Number of observations
> ##D phi0 <- 10  # Specify this; should be greater than unity
> ##D delta0 <- 1 / (phi0 - 1)
> ##D mydata <- data.frame(x2 = runif(nn), x3 = runif(nn))
> ##D mydata <- transform(mydata, mu = exp(2 + 3 * x2 + 0 * x3))
> ##D mydata <- transform(mydata, y3 = rnbinom(nn, delta0 * mu, mu = mu))
> ##D plot(y3 ~ x2, data = mydata, pch = "+", col = "blue",
> ##D      main = paste("Var(Y) = ", phi0, " * mu", sep = ""), las = 1)
> ##D nb1 <- vglm(y3 ~ x2 + x3, negbinomial(parallel = TRUE, zero = NULL),
> ##D             data = mydata, trace = TRUE)
> ##D # Extracting out some quantities:
> ##D cnb1 <- coef(nb1, matrix = TRUE)
> ##D mydiff <- (cnb1["(Intercept)", "loglink(size)"] -
> ##D            cnb1["(Intercept)", "loglink(mu)"])
> ##D delta0.hat <- exp(mydiff)
> ##D (phi.hat <- 1 + 1 / delta0.hat)  # MLE of phi
> ##D summary(nb1)
> ##D # Obtain a 95 percent confidence interval for phi0:
> ##D myvec <- rbind(-1, 1, 0, 0)
> ##D (se.mydiff <- sqrt(t(myvec) %*%  vcov(nb1) %*%  myvec))
> ##D ci.mydiff <- mydiff + c(-1.96, 1.96) * c(se.mydiff)
> ##D ci.delta0 <- ci.exp.mydiff <- exp(ci.mydiff)
> ##D (ci.phi0 <- 1 + 1 / rev(ci.delta0))  # The 95##D 
> ##D Confint.nb1(nb1)  # Quick way to get it
> ##D # cf. moment estimator:
> ##D summary(glm(y3 ~ x2 + x3, quasipoisson, mydata))$disper
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("negbinomial.size")
> ### * negbinomial.size
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: negbinomial.size
> ### Title: Negative Binomial Distribution Family Function With Known Size
> ### Aliases: negbinomial.size
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Simulated data with various multiple responses
> size1 <- exp(1); size2 <- exp(2); size3 <- exp(0); size4 <- Inf
> ndata <- data.frame(x2 = runif(nn <- 1000))
> ndata <- transform(ndata, eta1  = -1 - 2 * x2,  # eta1 must be negative
+                           size1 = size1)
> ndata <- transform(ndata,
+                    mu1  = nbcanlink(eta1, size = size1, inv = TRUE))
> ndata <- transform(ndata,
+               y1 = rnbinom(nn, mu = mu1,         size = size1),  # NB-C
+               y2 = rnbinom(nn, mu = exp(2 - x2), size = size2),
+               y3 = rnbinom(nn, mu = exp(3 + x2), size = size3),  # NB-G
+               y4 =   rpois(nn, lambda = exp(1 + x2)))
> 
> # Also known as NB-C with size known (Hilbe, 2011)
> fit1 <- vglm(y1 ~ x2, negbinomial.size(size = size1, lmu = "nbcanlink"),
+              data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -929.74593
VGLM    linear loop  2 :  loglikelihood = -918.37237
VGLM    linear loop  3 :  loglikelihood = -918.13775
VGLM    linear loop  4 :  loglikelihood = -918.13758
VGLM    linear loop  5 :  loglikelihood = -918.13758
> coef(fit1, matrix = TRUE)
            nbcanlink(mu, mu(size))
(Intercept)               -0.989449
x2                        -2.029216
> head(fit1@misc$size)  # size saved here
         [,1]
[1,] 2.718282
[2,] 2.718282
[3,] 2.718282
[4,] 2.718282
[5,] 2.718282
[6,] 2.718282
> 
> fit2 <- vglm(cbind(y2, y3, y4) ~ x2, data = ndata, trace = TRUE,
+              negbinomial.size(size = c(size2, size3, size4)))
VGLM    linear loop  1 :  loglikelihood = -27143.425
VGLM    linear loop  2 :  loglikelihood = -27124.293
VGLM    linear loop  3 :  loglikelihood = -27124.284
VGLM    linear loop  4 :  loglikelihood = -27124.284
> coef(fit2, matrix = TRUE)
            loglink(mu1) loglink(mu2) loglink(mu3)
(Intercept)     2.050305     2.990093    0.9950234
x2             -1.088608     1.069040    1.0091051
> head(fit2@misc$size)  # size saved here
         [,1] [,2] [,3]
[1,] 7.389056    1  Inf
[2,] 7.389056    1  Inf
[3,] 7.389056    1  Inf
[4,] 7.389056    1  Inf
[5,] 7.389056    1  Inf
[6,] 7.389056    1  Inf
> 
> 
> 
> cleanEx()
> nameEx("normal.vcm")
> ### * normal.vcm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: normal.vcm
> ### Title: Univariate Normal Distribution as a Varying-Coefficient Model
> ### Aliases: normal.vcm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ndata <- data.frame(x2 = runif(nn <- 2000))
> # Note that coeff1 + coeff2 + coeff5 == 1. So try "multilogitlink".
> myoffset <- 10
> ndata <- transform(ndata,
+            coeff1 = 0.25,  # "multilogitlink"
+            coeff2 = 0.25,  # "multilogitlink"
+            coeff3 = exp(-0.5),  # "loglink"
+ # "logofflink" link:
+            coeff4 = logofflink(+0.5, offset = myoffset, inverse = TRUE),
+            coeff5 = 0.50,  # "multilogitlink"
+            coeff6 = 1.00,  # "identitylink"
+            v2 = runif(nn),
+            v3 = runif(nn),
+            v4 = runif(nn),
+            v5 = rnorm(nn),
+            v6 = rnorm(nn))
> ndata <- transform(ndata,
+            Coeff1 =              0.25 - 0 * x2,
+            Coeff2 =              0.25 - 0 * x2,
+            Coeff3 =   logitlink(-0.5  - 1 * x2, inverse = TRUE),
+            Coeff4 =  logloglink( 0.5  - 1 * x2, inverse = TRUE),
+            Coeff5 =              0.50 - 0 * x2,
+            Coeff6 =              1.00 + 1 * x2)
> ndata <- transform(ndata,
+                    y1 = coeff1 * 1 +
+                         coeff2 * v2 +
+                         coeff3 * v3 +
+                         coeff4 * v4 +
+                         coeff5 * v5 +
+                         coeff6 * v6 + rnorm(nn, sd = exp(0)),
+                    y2 = Coeff1 * 1 +
+                         Coeff2 * v2 +
+                         Coeff3 * v3 +
+                         Coeff4 * v4 +
+                         Coeff5 * v5 +
+                         Coeff6 * v6 + rnorm(nn, sd = exp(0)))
> 
> # An intercept-only model
> fit1 <- vglm(y1 ~ 1,
+              form2 = ~ 1 + v2 + v3 + v4 + v5 + v6,
+              normal.vcm(link.list = list("(Intercept)" = "multilogitlink",
+                                          "v2"          = "multilogitlink",
+                                          "v3"          = "loglink",
+                                          "v4"          = "logofflink",
+                                          "(Default)"   = "identitylink",
+                                          "v5"          = "multilogitlink"),
+                         earg.list = list("(Intercept)" = list(),
+                                          "v2"          = list(),
+                                          "v4"          = list(offset = myoffset),
+                                          "v3"          = list(),
+                                          "(Default)"   = list(),
+                                          "v5"          = list()),
+                         zero = c(1:2, 6)),
+              data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3169.92912
VGLM    linear loop  2 :  loglikelihood = -2865.38835
VGLM    linear loop  3 :  loglikelihood = -2808.43256
VGLM    linear loop  4 :  loglikelihood = -2806.74765
VGLM    linear loop  5 :  loglikelihood = -2806.74623
VGLM    linear loop  6 :  loglikelihood = -2806.74623
> coef(fit1, matrix = TRUE)
            multilogitlink(coeff:(Intercept)) multilogitlink(coeff:v2)
(Intercept)                        -0.8150088                -0.442383
            loglink(coeff:v3) logofflink(coeff:v4, offset = 10) coeff:v6
(Intercept)        -0.5381064                         0.5199879 1.021546
            loglink(sd)
(Intercept) -0.01556542
> summary(fit1)
Call:
vglm(formula = y1 ~ 1, family = normal.vcm(link.list = list(`(Intercept)` = "multilogitlink", 
    v2 = "multilogitlink", v3 = "loglink", v4 = "logofflink", 
    `(Default)` = "identitylink", v5 = "multilogitlink"), earg.list = list(`(Intercept)` = list(), 
    v2 = list(), v4 = list(offset = myoffset), v3 = list(), `(Default)` = list(), 
    v5 = list()), zero = c(1:2, 6)), data = ndata, form2 = ~1 + 
    v2 + v3 + v4 + v5 + v6, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.81501    0.30485  -2.673  0.00751 ** 
(Intercept):2 -0.44238    0.21745  -2.034  0.04191 *  
(Intercept):3 -0.53811    0.11090  -4.852 1.22e-06 ***
(Intercept):4  0.51999    0.03882  13.396  < 2e-16 ***
(Intercept):5  1.02155    0.02161  47.278  < 2e-16 ***
(Intercept):6 -0.01557    0.01581  -0.984  0.32490    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: multilogitlink(coeff:(Intercept)), 
multilogitlink(coeff:v2), loglink(coeff:v3), logofflink(coeff:v4, offset = 10), 
coeff:v6, loglink(sd)

Log-likelihood: -2806.746 on 11994 degrees of freedom

Number of Fisher scoring iterations: 6 

> # This works only for intercept-only models:
> multilogitlink(rbind(coef(fit1, matrix = TRUE)[1, c(1, 2)]), inverse = TRUE)
         [,1]      [,2]      [,3]
[1,] 0.212281 0.3081346 0.4795843
> 
> # A model with covariate x2 for the regression coefficients
> fit2 <- vglm(y2 ~ 1 + x2,
+              form2 = ~ 1 + v2 + v3 + v4 + v5 + v6,
+              normal.vcm(link.list = list("(Intercept)" = "multilogitlink",
+                                          "v2"          = "multilogitlink",
+                                          "v3"          = "logitlink",
+                                          "v4"          = "logloglink",
+                                          "(Default)"   = "identitylink",
+                                          "v5"          = "multilogitlink"),
+                         earg.list = list("(Intercept)" = list(),
+                                          "v2"          = list(),
+                                          "v3"          = list(),
+                                          "v4"          = list(),
+                                          "(Default)"   = list(),
+                                          "v5"          = list()),
+                         zero = c(1:2, 6)),
+              data = ndata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3425.87529
VGLM    linear loop  2 :  loglikelihood = -2957.99312
VGLM    linear loop  3 :  loglikelihood = -2806.93702
VGLM    linear loop  4 :  loglikelihood = -2794.29246
VGLM    linear loop  5 :  loglikelihood = -2794.21034
VGLM    linear loop  6 :  loglikelihood = -2794.21022
VGLM    linear loop  7 :  loglikelihood = -2794.21021
VGLM    linear loop  8 :  loglikelihood = -2794.21021
> 
> coef(fit2, matrix = TRUE)
            multilogitlink(coeff:(Intercept)) multilogitlink(coeff:v2)
(Intercept)                        -0.6391359               -0.6700587
x2                                  0.0000000                0.0000000
            logitlink(coeff:v3) logloglink(coeff:v4)  coeff:v6 loglink(sd)
(Intercept)          -1.4621614            0.5081673 0.9734343 -0.02183343
x2                    0.2282044           -1.0314119 1.1130777  0.00000000
> summary(fit2)
Call:
vglm(formula = y2 ~ 1 + x2, family = normal.vcm(link.list = list(`(Intercept)` = "multilogitlink", 
    v2 = "multilogitlink", v3 = "logitlink", v4 = "logloglink", 
    `(Default)` = "identitylink", v5 = "multilogitlink"), earg.list = list(`(Intercept)` = list(), 
    v2 = list(), v3 = list(), v4 = list(), `(Default)` = list(), 
    v5 = list()), zero = c(1:2, 6)), data = ndata, form2 = ~1 + 
    v2 + v3 + v4 + v5 + v6, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -0.63914    0.24985  -2.558   0.0105 *  
(Intercept):2 -0.67006    0.26104  -2.567   0.0103 *  
(Intercept):3 -1.46216    0.68029  -2.149   0.0316 *  
(Intercept):4  0.50817    0.01844  27.560   <2e-16 ***
(Intercept):5  0.97343    0.04222  23.058   <2e-16 ***
(Intercept):6 -0.02183    0.01581  -1.381   0.1673    
x2:1           0.22820    1.02940   0.222   0.8246    
x2:2          -1.03141    0.06506 -15.854   <2e-16 ***
x2:3           1.11308    0.07343  15.159   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: multilogitlink(coeff:(Intercept)), 
multilogitlink(coeff:v2), logitlink(coeff:v3), logloglink(coeff:v4), coeff:v6, 
loglink(sd)

Log-likelihood: -2794.21 on 11991 degrees of freedom

Number of Fisher scoring iterations: 8 

> 
> 
> 
> cleanEx()
> nameEx("nparamvglm")
> ### * nparamvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nparam.vlm
> ### Title: Number of Parameters
> ### Aliases: nparam.vlm nparam nparam.vgam nparam.rrvglm nparam.drrvglm
> ###   nparam.qrrvglm nparam.rrvgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit1 <- vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> coef(fit1)
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 
> coef(fit1, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> nparam(fit1)
[1] 3
> (fit2 <- vglm(hits ~ 1, poissonff, weights = ofreq, data = V1))

Call:
vglm(formula = hits ~ 1, family = poissonff, data = V1, weights = ofreq)


Coefficients:
(Intercept) 
-0.07010957 

Degrees of Freedom: 6 Total; 5 Residual
Residual deviance: 668.7322 
Log-likelihood: -732.5946 
> coef(fit2)
(Intercept) 
-0.07010957 
> coef(fit2, matrix = TRUE)
            loglink(lambda)
(Intercept)     -0.07010957
> nparam(fit2)
[1] 1
> nparam(fit2, dpar = FALSE)
[1] 1
> 
> 
> 
> cleanEx()
> nameEx("olym")
> ### * olym
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: olympics
> ### Title: 2008 and 2012 Summer Olympic Final Medal Count Data
> ### Aliases: olym08 olym12
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(olym08)
    ranking             country        gold            silver      
 Min.   : 1.00   Afghanistan: 1   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:22.00   Algeria    : 1   1st Qu.: 0.000   1st Qu.: 1.000  
 Median :44.00   Argentina  : 1   Median : 1.000   Median : 1.000  
 Mean   :43.03   Armenia    : 1   Mean   : 3.471   Mean   : 3.483  
 3rd Qu.:65.00   Australia  : 1   3rd Qu.: 3.000   3rd Qu.: 4.000  
 Max.   :81.00   Austria    : 1   Max.   :51.000   Max.   :38.000  
                 (Other)    :81                                    
     bronze         totalmedal    
 Min.   : 0.000   Min.   :  1.00  
 1st Qu.: 1.000   1st Qu.:  2.00  
 Median : 2.000   Median :  4.00  
 Mean   : 4.057   Mean   : 11.01  
 3rd Qu.: 4.000   3rd Qu.: 10.00  
 Max.   :36.000   Max.   :110.00  
                                  
> summary(olym12)
    ranking             country        gold            silver      
 Min.   : 1.00   Afghanistan: 1   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:22.00   Algeria    : 1   1st Qu.: 0.000   1st Qu.: 1.000  
 Median :42.00   Argentina  : 1   Median : 1.000   Median : 1.000  
 Mean   :42.11   Armenia    : 1   Mean   : 3.553   Mean   : 3.576  
 3rd Qu.:63.00   Australia  : 1   3rd Qu.: 3.000   3rd Qu.: 4.000  
 Max.   :79.00   Azerbaijan : 1   Max.   :46.000   Max.   :29.000  
                 (Other)    :79                                    
     bronze         totalmedal    
 Min.   : 0.000   Min.   :  1.00  
 1st Qu.: 1.000   1st Qu.:  2.00  
 Median : 2.000   Median :  4.00  
 Mean   : 4.188   Mean   : 11.32  
 3rd Qu.: 5.000   3rd Qu.: 12.00  
 Max.   :32.000   Max.   :104.00  
                                  
> ## maybe str(olym08) ; plot(olym08) ...
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D myylim <- c(0, 55)
> ##D with(head(olym08, n = 8),
> ##D barplot(rbind(gold, silver, bronze),
> ##D    col = c("gold", "grey", "brown"),  # No "silver" or "bronze"!
> ##D #          "gold", "grey71", "chocolate4",
> ##D    names.arg = country, cex.names = 0.5, ylim = myylim,
> ##D    beside = TRUE, main = "2008 Summer Olympic Final Medal Count",
> ##D    ylab = "Medal count", las = 1,
> ##D    sub = "Top 8 countries; 'gold'=gold, 'grey'=silver, 'brown'=bronze"))
> ##D with(head(olym12, n = 8),
> ##D barplot(rbind(gold, silver, bronze),
> ##D    col = c("gold", "grey", "brown"),  # No "silver" or "bronze"!
> ##D    names.arg = country, cex.names = 0.5, ylim = myylim,
> ##D    beside = TRUE, main = "2012 Summer Olympic Final Medal Count",
> ##D    ylab = "Medal count", las = 1,
> ##D    sub = "Top 8 countries; 'gold'=gold, 'grey'=silver, 'brown'=bronze")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ordpoisson")
> ### * ordpoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ordpoisson
> ### Title: Ordinal Poisson Family Function
> ### Aliases: ordpoisson
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> set.seed(123)  # Example 1
> x2 <- runif(n <- 1000); x3 <- runif(n)
> mymu <- exp(3 - 1 * x2 + 2 * x3)
> y1 <- rpois(n, lambda = mymu)
> cutpts <- c(-Inf, 20, 30, Inf)
> fcutpts <- cutpts[is.finite(cutpts)]  # finite cutpoints
> ystar <- cut(y1, breaks = cutpts, labels = FALSE)
> ## Not run: 
> ##D plot(x2, x3, col = ystar, pch = as.character(ystar))
> ## End(Not run)
> table(ystar) / sum(table(ystar))
ystar
    1     2     3 
0.260 0.194 0.546 
> fit <- vglm(ystar ~ x2 + x3, fam = ordpoisson(cutpoi = fcutpts))
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  2 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  27 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  34 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  34 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
> head(depvar(fit))  # This can be input if countdata = TRUE
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    1    0    0
[4,]    0    0    1
[5,]    0    0    1
[6,]    0    0    1
> head(fitted(fit))
        mu
1 26.37788
2 29.70400
3 18.82698
4 44.47479
5 41.56519
6 49.54660
> head(predict(fit))
     loglink(mu)
[1,]    3.272526
[2,]    3.391282
[3,]    2.935291
[4,]    3.794923
[5,]    3.727263
[6,]    3.902914
> coef(fit, matrix = TRUE)
            loglink(mu)
(Intercept)   3.0324949
x2           -0.9879523
x3            1.9155716
> fit@extra
$NOS
[1] 1

$Levels
[1] 3

$y.integer
[1] TRUE

$ncoly
[1] 3

$countdata
[1] FALSE

$cutpoints
[1]  20  30 Inf

$n
[1] 1000

> 
> # Example 2: multivariate and there are no obsns between some cutpoints
> cutpts2 <- c(-Inf, 0, 9, 10, 20, 70, 200, 201, Inf)
> fcutpts2 <- cutpts2[is.finite(cutpts2)]  # finite cutpoints
> y2 <- rpois(n, lambda = mymu)   # Same model as y1
> ystar2 <- cut(y2, breaks = cutpts2, labels = FALSE)
> table(ystar2) / sum(table(ystar2))
ystar2
    2     3     4     5     6 
0.037 0.029 0.214 0.571 0.149 
> fit <- vglm(cbind(ystar,ystar2) ~ x2 + x3, fam =
+             ordpoisson(cutpoi = c(fcutpts,Inf,fcutpts2,Inf),
+                        Levels = c(length(fcutpts)+1,length(fcutpts2)+1),
+                        parallel = TRUE), trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3421.6299
VGLM    linear loop  2 :  loglikelihood = -975.56347
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  124 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  3 :  loglikelihood = -763.12349
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  36 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  4 :  loglikelihood = -759.96202
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  49 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  5 :  loglikelihood = -759.9608
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  49 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  6 :  loglikelihood = -759.9608
> coef(fit, matrix = TRUE)
            loglink(mu1) loglink(mu2)
(Intercept)     2.993586     2.993586
x2             -1.017975    -1.017975
x3              2.032580     2.032580
> fit@extra
$NOS
[1] 2

$Levels
[1] 3 8

$y.integer
[1] TRUE

$ncoly
[1] 11

$countdata
[1] FALSE

$cutpoints
 [1]  20  30 Inf   0   9  10  20  70 200 201 Inf

$n
[1] 1000

> constraints(fit)
$`(Intercept)`
     [,1]
[1,]    1
[2,]    1

$x2
     [,1]
[1,]    1
[2,]    1

$x3
     [,1]
[1,]    1
[2,]    1

> summary(depvar(fit))  # Some columns have all zeros
       V1             V2              V3              V4          V5       
 Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0   Min.   :0.000  
 1st Qu.:0.00   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0   1st Qu.:0.000  
 Median :0.00   Median :0.000   Median :1.000   Median :0   Median :0.000  
 Mean   :0.26   Mean   :0.194   Mean   :0.546   Mean   :0   Mean   :0.037  
 3rd Qu.:1.00   3rd Qu.:0.000   3rd Qu.:1.000   3rd Qu.:0   3rd Qu.:0.000  
 Max.   :1.00   Max.   :1.000   Max.   :1.000   Max.   :0   Max.   :1.000  
       V6              V7              V8              V9             V10   
 Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0  
 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0  
 Median :0.000   Median :0.000   Median :1.000   Median :0.000   Median :0  
 Mean   :0.029   Mean   :0.214   Mean   :0.571   Mean   :0.149   Mean   :0  
 3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:1.000   3rd Qu.:0.000   3rd Qu.:0  
 Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :0  
      V11   
 Min.   :0  
 1st Qu.:0  
 Median :0  
 Mean   :0  
 3rd Qu.:0  
 Max.   :0  
> 
> 
> 
> cleanEx()
> nameEx("ordsup")
> ### * ordsup
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ordsup
> ### Title: Ordinal Superiority Measures
> ### Aliases: ordsup ordsup.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D Mental <- read.table("http://www.stat.ufl.edu/~aa/glm/data/Mental.dat",
> ##D                      header = TRUE)  # Make take a while to load in
> ##D Mental$impair <- ordered(Mental$impair)
> ##D pfit3 <- vglm(impair ~ ses + life, data = Mental,
> ##D          cumulative(link = "probitlink", reverse = FALSE, parallel = TRUE))
> ##D coef(pfit3, matrix = TRUE)
> ##D ordsup(pfit3)  # The 'ses' variable is binary
> ##D 
> ##D # Fit a crude LM
> ##D fit7 <- vglm(as.numeric(impair) ~ ses + life, uninormal, data = Mental)
> ##D coef(fit7, matrix = TRUE)  # 'sd' is estimated by MLE
> ##D ordsup(fit7)
> ##D ordsup(fit7, all.vars = TRUE)  # Some output may not be meaningful
> ##D ordsup(fit7, confint = TRUE, method = "profile")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("oxtemp")
> ### * oxtemp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: oxtemp
> ### Title: Oxford Temperature Data
> ### Aliases: oxtemp
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run:  fit <- vglm(maxtemp ~ 1, gevff, data = oxtemp, trace = TRUE) 
> 
> 
> 
> cleanEx()
> nameEx("paralogistic")
> ### * paralogistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: paralogistic
> ### Title: Paralogistic Distribution Family Function
> ### Aliases: paralogistic
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D pdata <- data.frame(y = rparalogistic(n = 3000, exp(1), scale = exp(1)))
> ##D fit <- vglm(y ~ 1, paralogistic(lss = FALSE), data = pdata, trace = TRUE)
> ##D fit <- vglm(y ~ 1, paralogistic(ishape1.a = 2.3, iscale = 5),
> ##D             data = pdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("paralogisticUC")
> ### * paralogisticUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Paralogistic
> ### Title: The Paralogistic Distribution
> ### Aliases: Paralogistic dparalogistic pparalogistic qparalogistic
> ###   rparalogistic
> ### Keywords: distribution
> 
> ### ** Examples
> 
> pdata <- data.frame(y = rparalogistic(n = 3000, scale = exp(1), exp(2)))
> fit <- vglm(y ~ 1, paralogistic(lss = FALSE, ishape1.a = 4.1),
+             data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1313.0752
VGLM    linear loop  2 :  loglikelihood = -986.102091
VGLM    linear loop  3 :  loglikelihood = -974.168732
VGLM    linear loop  4 :  loglikelihood = -956.730929
VGLM    linear loop  5 :  loglikelihood = -956.554516
VGLM    linear loop  6 :  loglikelihood = -956.477044
VGLM    linear loop  7 :  loglikelihood = -956.476639
VGLM    linear loop  8 :  loglikelihood = -956.476625
VGLM    linear loop  9 :  loglikelihood = -956.476625
Taking a modified step..............
VGLM    linear loop  9 :  loglikelihood = -956.476625
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step
> coef(fit, matrix = TRUE)
            loglink(shape1.a) loglink(scale)
(Intercept)          1.980109      0.9997595
> Coef(fit)
shape1.a    scale 
7.243535 2.717628 
> 
> 
> 
> cleanEx()
> nameEx("paretoIV")
> ### * paretoIV
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: paretoIV
> ### Title: Pareto(IV/III/II) Distribution Family Functions
> ### Aliases: paretoIV paretoIII paretoII
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pdata <- data.frame(y = rparetoIV(2000, scale = exp(1),
+                                   ineq = exp(-0.3), shape = exp(1)))
> ## Not run: 
> ##D par(mfrow = c(2, 1))
> ##D with(pdata, hist(y)); with(pdata, hist(log(y))) 
> ## End(Not run)
> fit <- vglm(y ~ 1, paretoIV, data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -4241.99109
VGLM    linear loop  2 :  loglikelihood = -3855.38906
VGLM    linear loop  3 :  loglikelihood = -3485.15403
VGLM    linear loop  4 :  loglikelihood = -3778.13119
Taking a modified step.
VGLM    linear loop  4 :  loglikelihood = -3233.43145
VGLM    linear loop  5 :  loglikelihood = -2984.0832
VGLM    linear loop  6 :  loglikelihood = -2914.83721
VGLM    linear loop  7 :  loglikelihood = -2903.87482
VGLM    linear loop  8 :  loglikelihood = -2902.46451
VGLM    linear loop  9 :  loglikelihood = -2902.44321
VGLM    linear loop  10 :  loglikelihood = -2902.4432
> head(fitted(fit))
         [,1]
[1,] 1.109938
[2,] 1.109938
[3,] 1.109938
[4,] 1.109938
[5,] 1.109938
[6,] 1.109938
> summary(pdata)
       y            
 Min.   : 0.001076  
 1st Qu.: 0.527372  
 Median : 1.135666  
 Mean   : 1.605567  
 3rd Qu.: 2.060864  
 Max.   :19.477180  
> coef(fit, matrix = TRUE)
            loglink(scale) loglink(inequality) loglink(shape)
(Intercept)       1.159682          -0.2656847       1.124702
> Coef(fit)
     scale inequality      shape 
 3.1889189  0.7666808  3.0792985 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = paretoIV, data = pdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   1.1597     0.1630   7.113 1.13e-12 ***
(Intercept):2  -0.2657     0.0299  -8.886  < 2e-16 ***
(Intercept):3   1.1247     0.1523   7.385 1.53e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale), loglink(inequality), 
loglink(shape)

Log-likelihood: -2902.443 on 5997 degrees of freedom

Number of Fisher scoring iterations: 10 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("paretoIVUC")
> ### * paretoIVUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ParetoIV
> ### Title: The Pareto(IV/III/II) Distributions
> ### Aliases: ParetoIV dparetoIV pparetoIV qparetoIV rparetoIV ParetoIII
> ###   dparetoIII pparetoIII qparetoIII rparetoIII ParetoII dparetoII
> ###   pparetoII qparetoII rparetoII ParetoI dparetoI pparetoI qparetoI
> ###   rparetoI
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- seq(-0.2, 4, by = 0.01)
> ##D loc <- 0; Scale <- 1; ineq <- 1; shape <- 1.0
> ##D plot(x, dparetoIV(x, loc, Scale, ineq, shape), type = "l",
> ##D      main = "Blue is density, orange is the CDF", col = "blue",
> ##D      sub = "Purple are 5,10,...,95 percentiles", ylim = 0:1,
> ##D      las = 1, ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D Q <- qparetoIV(seq(0.05, 0.95,by = 0.05), loc, Scale, ineq, shape)
> ##D lines(Q, dparetoIV(Q, loc, Scale, ineq, shape), col = "purple",
> ##D       lty = 3, type = "h")
> ##D lines(x, pparetoIV(x, loc, Scale, ineq, shape), col = "orange")
> ##D abline(h = 0, lty = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("paretoff")
> ### * paretoff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: paretoff
> ### Title: Pareto and Truncated Pareto Distribution Family Functions
> ### Aliases: paretoff truncpareto
> ### Keywords: models regression
> 
> ### ** Examples
> 
> alpha <- 2; kay <- exp(3)
> pdata <- data.frame(y = rpareto(n = 1000, scale = alpha, shape = kay))
> fit <- vglm(y ~ 1, paretoff, data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 1269.1498
VGLM    linear loop  2 :  loglikelihood = 1270.6604
VGLM    linear loop  3 :  loglikelihood = 1270.6615
VGLM    linear loop  4 :  loglikelihood = 1270.6615
> fit@extra  # The estimate of alpha is here
$scale
[1] 2.000007

$scaleEstimated
[1] TRUE

> head(fitted(fit))
     [,1]
1 2.10338
2 2.10338
3 2.10338
4 2.10338
5 2.10338
6 2.10338
> with(pdata, mean(y))
[1] 2.103213
> coef(fit, matrix = TRUE)
            loglink(shape)
(Intercept)       3.012958
> summary(fit)  # Standard errors are incorrect!!
Call:
vglm(formula = y ~ 1, family = paretoff, data = pdata, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.01296    0.03162   95.28   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(shape) 

Log-likelihood: 1270.661 on 999 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> # Here, alpha is assumed known
> fit2 <- vglm(y ~ 1, paretoff(scale = alpha), data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 1269.08
VGLM    linear loop  2 :  loglikelihood = 1270.5901
VGLM    linear loop  3 :  loglikelihood = 1270.5912
VGLM    linear loop  4 :  loglikelihood = 1270.5912
> fit2@extra  # alpha stored here
$scale
[1] 2

$scaleEstimated
[1] FALSE

> head(fitted(fit2))
     [,1]
1 2.10338
2 2.10338
3 2.10338
4 2.10338
5 2.10338
6 2.10338
> coef(fit2, matrix = TRUE)
            loglink(shape)
(Intercept)       3.012888
> summary(fit2)  # Standard errors are okay
Call:
vglm(formula = y ~ 1, family = paretoff(scale = alpha), data = pdata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.01289    0.03162   95.28   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(shape) 

Log-likelihood: 1270.591 on 999 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> # Upper truncated Pareto distribution
> lower <- 2; upper <- 8; kay <- exp(2)
> pdata3 <- data.frame(y = rtruncpareto(n = 100, lower = lower,
+                                       upper = upper, shape = kay))
> fit3 <- vglm(y ~ 1, truncpareto(lower, upper), data = pdata3, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 20.317515
VGLM    linear loop  2 :  loglikelihood = 20.317604
VGLM    linear loop  3 :  loglikelihood = 20.317604
> coef(fit3, matrix = TRUE)
            loglink(shape)
(Intercept)       2.027626
> c(fit3@misc$lower, fit3@misc$upper)
[1] 2 8
> 
> 
> 
> cleanEx()
> nameEx("perks")
> ### * perks
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: perks
> ### Title: Perks Distribution Family Function
> ### Aliases: perks
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  set.seed(123)
> ##D pdata <- data.frame(x2 = runif(nn <- 1000))  # x2 unused
> ##D pdata <- transform(pdata, eta1  = -1,
> ##D                           ceta1 =  1)
> ##D pdata <- transform(pdata, shape1 = exp(eta1),
> ##D                           scale1 = exp(ceta1))
> ##D pdata <- transform(pdata, y1 = rperks(nn, sh = shape1, sc = scale1))
> ##D fit1 <- vglm(y1 ~ 1, perks, data = pdata, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D summary(fit1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("perksUC")
> ### * perksUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Perks
> ### Title: The Perks Distribution
> ### Aliases: Perks dperks pperks qperks rperks
> ### Keywords: distribution
> 
> ### ** Examples
> 
> probs <- seq(0.01, 0.99, by = 0.01)
> Shape <- exp(-1.0); Scale <- exp(1);
> max(abs(pperks(qperks(p = probs, Shape, Scale),
+                   Shape, Scale) - probs))  # Should be 0
[1] 5.689893e-16
> 
> ## Not run: 
> ##D  x <- seq(-0.1, 07, by = 0.01);
> ##D plot(x, dperks(x, Shape, Scale), type = "l", col = "blue", las = 1,
> ##D      main = "Blue is density, orange is cumulative distribution function",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      ylab = "", ylim = 0:1)
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, pperks(x, Shape, Scale), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qperks(probs, Shape, Scale)
> ##D lines(Q, dperks(Q, Shape, Scale), col = "purple", lty = 3, type = "h")
> ##D pperks(Q, Shape, Scale) - probs  # Should be all zero
> ##D abline(h = probs, col = "purple", lty = 3) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("persp.qrrvglm")
> ### * persp.qrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: perspqrrvglm
> ### Title: Perspective plot for QRR-VGLMs
> ### Aliases: perspqrrvglm
> ### Keywords: models regression nonlinear
> 
> ### ** Examples
> ## Not run: 
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Good idea when I.tolerances = TRUE
> ##D set.seed(111)
> ##D r1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardmont, Pardnigr, Pardpull, Trocterr) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, trace = FALSE, I.tolerances = TRUE)
> ##D set.seed(111)  # r2 below is an ill-conditioned model
> ##D r2 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardmont, Pardnigr, Pardpull, Trocterr) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           isd.lv = c(2.4, 1.0), Muxfactor = 3.0, trace = FALSE,
> ##D           poissonff, data = hspider, Rank = 2, eq.tolerances = TRUE)
> ##D 
> ##D sort(deviance(r1, history = TRUE))  # A history of all the fits
> ##D sort(deviance(r2, history = TRUE))  # A history of all the fits
> ##D if (deviance(r2) > 857) stop("suboptimal fit obtained")
> ##D 
> ##D persp(r1, xlim = c(-6, 5), col = 1:4, label = TRUE)
> ##D 
> ##D # Involves all species
> ##D persp(r2, xlim = c(-6, 5), ylim = c(-4, 5), theta = 10, phi = 20, zlim = c(0, 220))
> ##D # Omit the two dominant species to see what is behind them
> ##D persp(r2, xlim = c(-6, 5), ylim = c(-4, 5), theta = 10, phi = 20, zlim = c(0, 220),
> ##D       which = (1:10)[-c(8, 10)])  # Use zlim to retain the original z-scale
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("pgamma.deriv")
> ### * pgamma.deriv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pgamma.deriv
> ### Title: Derivatives of the Incomplete Gamma Integral
> ### Aliases: pgamma.deriv
> ### Keywords: math
> 
> ### ** Examples
> 
> x <- seq(2, 10, length = 501)
> head(ans <- pgamma.deriv(x, 2))
             q        q^2      shape       shape^2    q.shape pgamma(q, shape)
[1,] 0.2706706 -0.1353353 -0.2958549  0.0004420282 0.07317926        0.5939942
[2,] 0.2685053 -0.1353181 -0.2946052 -0.0018133884 0.07473335        0.5983076
[3,] 0.2663406 -0.1352675 -0.2933336 -0.0040379197 0.07623631        0.6025863
[4,] 0.2641769 -0.1351843 -0.2920409 -0.0062311686 0.07768898        0.6068305
[5,] 0.2620148 -0.1350697 -0.2907277 -0.0083927774 0.07909219        0.6110400
[6,] 0.2598548 -0.1349246 -0.2893947 -0.0105224259 0.08044679        0.6152149
> ## Not run: 
> ##D  par(mfrow = c(2, 3))
> ##D for (jay in 1:6)
> ##D   plot(x, ans[, jay], type = "l", col = "blue", cex.lab = 1.5,
> ##D        cex.axis = 1.5, las = 1, log = "x",
> ##D        main = colnames(ans)[jay], xlab = "q", ylab = "") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("pgamma.deriv.unscaled")
> ### * pgamma.deriv.unscaled
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pgamma.deriv.unscaled
> ### Title: Derivatives of the Incomplete Gamma Integral (Unscaled Version)
> ### Aliases: pgamma.deriv.unscaled
> ### Keywords: math
> 
> ### ** Examples
> 
> x <- 3; aa <- seq(0.3, 04, by = 0.01)
> ans.u <- pgamma.deriv.unscaled(x, aa)
> head(ans.u)
            0          1        2
[1,] 2.972153 -10.503878 73.29731
[2,] 2.870660  -9.806261 66.37775
[3,] 2.775811  -9.173526 60.29808
[4,] 2.686999  -8.597913 54.93484
[5,] 2.603685  -8.072785 50.18570
[6,] 2.525394  -7.592438 45.96540
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 3))
> ##D for (jay in 1:3) {
> ##D   plot(aa, ans.u[, jay], type = "l", col = "blue", cex.lab = 1.5,
> ##D        cex.axis = 1.5, las = 1, main = colnames(ans.u)[jay],
> ##D        log = "", xlab = "shape", ylab = "")
> ##D   abline(h = 0, v = 1:2, lty = "dashed", col = "gray")  # Inaccurate at 1 and 2
> ##D }
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotdeplot.lmscreg")
> ### * plotdeplot.lmscreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotdeplot.lmscreg
> ### Title: Density Plot for LMS Quantile Regression
> ### Aliases: plotdeplot.lmscreg
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> fit <- vgam(BMI ~ s(age, df = c(4,2)), lms.bcn(zero = 1), bmi.nz)
VGAM  s.vam  loop  1 :  loglikelihood = -6429.7568
VGAM  s.vam  loop  2 :  loglikelihood = -6327.3502
VGAM  s.vam  loop  3 :  loglikelihood = -6313.2224
VGAM  s.vam  loop  4 :  loglikelihood = -6312.8069
VGAM  s.vam  loop  5 :  loglikelihood = -6312.8166
VGAM  s.vam  loop  6 :  loglikelihood = -6312.8032
VGAM  s.vam  loop  7 :  loglikelihood = -6312.8088
VGAM  s.vam  loop  8 :  loglikelihood = -6312.8062
VGAM  s.vam  loop  9 :  loglikelihood = -6312.8074
VGAM  s.vam  loop  10 :  loglikelihood = -6312.8068
VGAM  s.vam  loop  11 :  loglikelihood = -6312.8071
VGAM  s.vam  loop  12 :  loglikelihood = -6312.807
> ## Not run: 
> ##D  y = seq(15, 43, by = 0.25)
> ##D deplot(fit, x0 = 20, y = y, xlab = "BMI", col = "green", llwd = 2,
> ##D        main = "BMI distribution at ages 20 (green), 40 (blue), 60 (orange)")
> ##D deplot(fit, x0 = 40, y = y, add = TRUE, col = "blue", llwd = 2)
> ##D deplot(fit, x0 = 60, y = y, add = TRUE, col = "orange", llwd = 2) -> aa
> ##D 
> ##D names(aa@post$deplot)
> ##D aa@post$deplot$newdata
> ##D head(aa@post$deplot$y)
> ##D head(aa@post$deplot$density) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotdgaitd")
> ### * plotdgaitd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotdgaitd.vglm
> ### Title: Plotting the GAITD Combo Density from a GAITD Regression Object
> ### Aliases: plotdgaitd plotdgaitd.vglm
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D example(gaitdpoisson)
> ##D gaitpfit2 <-
> ##D   vglm(y1 ~ 1, crit = "coef", trace = TRUE, data = gdata,
> ##D        gaitdpoisson(a.mix = a.mix, i.mix = i.mix,
> ##D                     i.mlm = i.mlm, eq.ap = TRUE, eq.ip = TRUE,
> ##D                     truncate = tvec, max.support = max.support))
> ##D plotdgaitd(gaitpfit2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("plotqrrvglm")
> ### * plotqrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotqrrvglm
> ### Title: Model Diagnostic Plots for QRR-VGLMs
> ### Aliases: plotqrrvglm
> ### Keywords: regression nonlinear hplot
> 
> ### ** Examples
> ## Not run: 
> ##D # QRR-VGLM on the hunting spiders data
> ##D # This is computationally expensive
> ##D set.seed(111)  # This leads to the global solution
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Standardize environ vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute, Arctperi,
> ##D                 Auloalbi, Pardlugu, Pardmont, Pardnigr, Pardpull,
> ##D                 Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, Crow1positive = FALSE)
> ##D par(mfrow = c(3, 4))
> ##D plot(p1, rtype = "response", col = "blue", pch = 4, las = 1, main = "")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotqtplot.lmscreg")
> ### * plotqtplot.lmscreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotqtplot.lmscreg
> ### Title: Quantile Plot for LMS Quantile Regression
> ### Aliases: plotqtplot.lmscreg
> ### Keywords: regression hplot
> 
> ### ** Examples
> ## Not run: 
> ##D fit <- vgam(BMI ~ s(age, df = c(4,2)), lms.bcn(zero = 1), data = bmi.nz)
> ##D qtplot(fit)
> ##D qtplot(fit, perc = c(25,50,75,95), lcol = "blue", tcol = "blue", llwd = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotrcim0")
> ### * plotrcim0
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotrcim0
> ### Title: Main Effects Plot for a Row-Column Interaction Model (RCIM)
> ### Aliases: plotrcim0
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> alcoff.e <- moffset(alcoff, "6", "Mon", postfix = "*")  # Effective day
> fit0 <- rcim(alcoff.e, family = poissonff)
> ## Not run: 
> ##D par(oma = c(0, 0, 4, 0), mfrow = 1:2)  # For all plots below too
> ##D ii <- plot(fit0, rcol = "blue", ccol = "orange",
> ##D            lwd = 4, ylim = c(-2, 2),  # A common ylim
> ##D            cylab = "Effective daily effects", rylab = "Hourly effects",
> ##D            rxlab = "Hour", cxlab = "Effective day")
> ##D ii@post  # Endowed with additional information
> ## End(Not run)
> 
> # Negative binomial example
> ## Not run: 
> ##D fit1 <- rcim(alcoff.e, negbinomial, trace = TRUE)
> ##D plot(fit1, ylim = c(-2, 2)) 
> ## End(Not run)
> 
> # Univariate normal example
> fit2 <- rcim(alcoff.e, uninormal, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1232.2968
VGLM    linear loop  2 :  loglikelihood = -1168.6442
VGLM    linear loop  3 :  loglikelihood = -1128.0555
VGLM    linear loop  4 :  loglikelihood = -1114.4362
VGLM    linear loop  5 :  loglikelihood = -1113.2052
VGLM    linear loop  6 :  loglikelihood = -1113.1961
VGLM    linear loop  7 :  loglikelihood = -1113.1961
> ## Not run:  plot(fit2, ylim = c(-200, 400)) 
> 
> # Median-polish example
> ## Not run: 
> ##D fit3 <- rcim(alcoff.e, alaplace1(tau = 0.5), maxit = 1000, trace = FALSE)
> ##D plot(fit3, ylim = c(-200, 250)) 
> ## End(Not run)
> 
> # Zero-inflated Poisson example on "crashp" (no 0s in alcoff)
> ## Not run: 
> ##D cbind(rowSums(crashp))  # Easy to see the data
> ##D cbind(colSums(crashp))  # Easy to see the data
> ##D fit4 <- rcim(Rcim(crashp, rbaseline = "5", cbaseline = "Sun"),
> ##D              zipoissonff, trace = TRUE)
> ##D plot(fit4, ylim = c(-3, 3)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotvgam")
> ### * plotvgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotvgam
> ### Title: Default VGAM Plotting
> ### Aliases: plotvgam plot.vgam
> ### Keywords: models regression smooth hplot
> 
> ### ** Examples
> 
> coalminers <- transform(coalminers, Age = (age - 42) / 5)
> fit <- vgam(cbind(nBnW, nBW, BnW, BW) ~ s(Age),
+             binom2.or(zero = NULL), data = coalminers)
> ## Not run: 
> ##D  par(mfrow = c(1,3))
> ##D plot(fit, se = TRUE, ylim = c(-3, 2), las = 1)
> ##D plot(fit, se = TRUE, which.cf = 1:2, lcol = "blue", scol = "orange",
> ##D      ylim = c(-3, 2))
> ##D plot(fit, se = TRUE, which.cf = 1:2, lcol = "blue", scol = "orange",
> ##D      overlay = TRUE) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotvgam.control")
> ### * plotvgam.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotvgam.control
> ### Title: Control Function for plotvgam()
> ### Aliases: plotvgam.control
> ### Keywords: regression dplot
> 
> ### ** Examples
> 
> plotvgam.control(lcol = c("red", "blue"), scol = "darkgreen", se = TRUE)
$se
[1] TRUE

$which.cf
NULL

$xlim
NULL

$ylim
NULL

$llty
[1] "solid"

$slty
[1] "dashed"

$pcex
[1] 1

$pch
[1] 1

$pcol
[1] "black"

$lcol
[1] "red"  "blue"

$rcol
[1] "black"

$scol
[1] "darkgreen"

$llwd
[1] 1

$slwd
[1] 1

$add.arg
[1] FALSE

$noxmean
[1] FALSE

$one.at.a.time
[1] FALSE

$main
[1] ""

$shade
[1] FALSE

$shcol
[1] "gray80"

> 
> 
> 
> cleanEx()
> nameEx("plotvglm")
> ### * plotvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotvglm
> ### Title: Plots for VGLMs
> ### Aliases: plotvglm
> ### Keywords: models regression hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ndata <- data.frame(x2 = runif(nn <- 200))
> ##D ndata <- transform(ndata, y1 = rnbinom(nn, mu = exp(3+x2), size = exp(1)))
> ##D fit1 <- vglm(y1 ~ x2, negbinomial, data = ndata, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D par(mfrow = c(2, 2))
> ##D plot(fit1)
> ##D 
> ##D # Manually produce the four plots
> ##D plot(fit1, which = 1, col = "blue", las = 1, main = "main1")
> ##D abline(h = 0, lty = "dashed", col = "gray50")
> ##D plot(fit1, which = 2, col = "blue", las = 1, main = "main2")
> ##D abline(h = 0, lty = "dashed", col = "gray50")
> ##D plot(fit1, which = 3, col = "blue", las = 1, main = "main3")
> ##D plot(fit1, which = 4, col = "blue", las = 1, main = "main4")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("pneumo")
> ### * pneumo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pneumo
> ### Title: Pneumoconiosis in Coalminers Data
> ### Aliases: pneumo
> ### Keywords: datasets
> 
> ### ** Examples
> 
> # Fit the proportional odds model, p.179, in McCullagh and Nelder (1989)
> pneumo <- transform(pneumo, let = log(exposure.time))
> vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo)

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> 
> 
> 
> cleanEx()
> nameEx("poisson.points")
> ### * poisson.points
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: poisson.points
> ### Title: Poisson-points-on-a-plane/volume Distances Distribution
> ### Aliases: poisson.points
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pdata <- data.frame(y = rgamma(10, shape = exp(-1)))  # Not proper data!
> ostat <- 2
> fit <- vglm(y ~ 1, poisson.points(ostat, 2), data = pdata,
+             trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -20.038447
VGLM    linear loop  2 :  coefficients = -19.038447
VGLM    linear loop  3 :  coefficients = -18.038447
VGLM    linear loop  4 :  coefficients = -17.038447
VGLM    linear loop  5 :  coefficients = -16.038447
VGLM    linear loop  6 :  coefficients = -15.038447
VGLM    linear loop  7 :  coefficients = -14.038448
VGLM    linear loop  8 :  coefficients = -13.03845
VGLM    linear loop  9 :  coefficients = -12.038454
VGLM    linear loop  10 :  coefficients = -11.038467
VGLM    linear loop  11 :  coefficients = -10.038503
VGLM    linear loop  12 :  coefficients = -9.0385996
VGLM    linear loop  13 :  coefficients = -8.038862
VGLM    linear loop  14 :  coefficients = -7.0395753
VGLM    linear loop  15 :  coefficients = -6.0415128
VGLM    linear loop  16 :  coefficients = -5.0467692
VGLM    linear loop  17 :  coefficients = -4.0609827
VGLM    linear loop  18 :  coefficients = -3.0990736
VGLM    linear loop  19 :  coefficients = -2.1987458
VGLM    linear loop  20 :  coefficients = -1.4439802
VGLM    linear loop  21 :  coefficients = -0.96562139
VGLM    linear loop  22 :  coefficients = -0.80724997
VGLM    linear loop  23 :  coefficients = -0.79330309
VGLM    linear loop  24 :  coefficients = -0.79320492
VGLM    linear loop  25 :  coefficients = -0.79320491
> fit <- vglm(y ~ 1, poisson.points(ostat, 3), data = pdata,
+             trace = TRUE, crit = "coef")  # Slow convergence?
VGLM    linear loop  1 :  coefficients = -342.81534
VGLM    linear loop  2 :  coefficients = -341.81534
VGLM    linear loop  3 :  coefficients = -340.81534
VGLM    linear loop  4 :  coefficients = -339.81534
VGLM    linear loop  5 :  coefficients = -338.81534
VGLM    linear loop  6 :  coefficients = -337.81534
VGLM    linear loop  7 :  coefficients = -336.81534
VGLM    linear loop  8 :  coefficients = -335.81534
VGLM    linear loop  9 :  coefficients = -334.81534
VGLM    linear loop  10 :  coefficients = -333.81534
VGLM    linear loop  11 :  coefficients = -332.81534
VGLM    linear loop  12 :  coefficients = -331.81534
VGLM    linear loop  13 :  coefficients = -330.81534
VGLM    linear loop  14 :  coefficients = -329.81534
VGLM    linear loop  15 :  coefficients = -328.81534
VGLM    linear loop  16 :  coefficients = -327.81534
VGLM    linear loop  17 :  coefficients = -326.81534
VGLM    linear loop  18 :  coefficients = -325.81534
VGLM    linear loop  19 :  coefficients = -324.81534
VGLM    linear loop  20 :  coefficients = -323.81534
VGLM    linear loop  21 :  coefficients = -322.81534
VGLM    linear loop  22 :  coefficients = -321.81534
VGLM    linear loop  23 :  coefficients = -320.81534
VGLM    linear loop  24 :  coefficients = -319.81534
VGLM    linear loop  25 :  coefficients = -318.81534
VGLM    linear loop  26 :  coefficients = -317.81534
VGLM    linear loop  27 :  coefficients = -316.81534
VGLM    linear loop  28 :  coefficients = -315.81534
VGLM    linear loop  29 :  coefficients = -314.81534
VGLM    linear loop  30 :  coefficients = -313.81534
Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2,  :
  convergence not obtained in 30 IRLS iterations
> fit <- vglm(y ~ 1, poisson.points(ostat, 3, idensi = 1), data = pdata,
+             trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = -9.5586658
VGLM    linear loop  2 :  coefficients = -8.5594111
VGLM    linear loop  3 :  coefficients = -7.5614356
VGLM    linear loop  4 :  coefficients = -6.5669274
VGLM    linear loop  5 :  coefficients = -5.5817741
VGLM    linear loop  6 :  coefficients = -4.6215367
VGLM    linear loop  7 :  coefficients = -3.7254094
VGLM    linear loop  8 :  coefficients = -2.9799073
VGLM    linear loop  9 :  coefficients = -2.5162616
VGLM    linear loop  10 :  coefficients = -2.3689896
VGLM    linear loop  11 :  coefficients = -2.3570192
VGLM    linear loop  12 :  coefficients = -2.3569469
VGLM    linear loop  13 :  coefficients = -2.3569469
> head(fitted(fit))
      [,1]
1 1.620393
2 1.620393
3 1.620393
4 1.620393
5 1.620393
6 1.620393
> with(pdata, mean(y))
[1] 0.5119203
> coef(fit, matrix = TRUE)
            loglink(density)
(Intercept)        -2.356947
> Coef(fit)
   density 
0.09470894 
> 
> 
> 
> cleanEx()
> nameEx("poisson.pointsUC")
> ### * poisson.pointsUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: PoissonPoints
> ### Title: Poisson Points Distribution
> ### Aliases: PoissonPoints dpois.points rpois.points
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  lambda <- 1; xvec <- seq(0, 2, length = 400)
> ##D plot(xvec, dpois.points(xvec, lambda, ostat = 1, dimension = 2),
> ##D      type = "l", las = 1, col = "blue",
> ##D      sub = "First order statistic",
> ##D      main = paste("PDF of PoissonPoints distribution with lambda = ",
> ##D                   lambda, " and on the plane", sep = "")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("poissonff")
> ### * poissonff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: poissonff
> ### Title: Poisson Regression
> ### Aliases: poissonff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> poissonff()
Family:  poissonff 
Informal classes: poissonff, VGAMglm, VGAMcategorical 

Poisson distribution

Link:     loglink(lambda)
Variance: lambda
> 
> set.seed(123)
> pdata <- data.frame(x2 = rnorm(nn <- 100))
> pdata <- transform(pdata, y1 = rpois(nn, exp(1 + x2)),
+                           y2 = rpois(nn, exp(1 + x2)))
> (fit1 <- vglm(cbind(y1, y2) ~ x2, poissonff, data = pdata))

Call:
vglm(formula = cbind(y1, y2) ~ x2, family = poissonff, data = pdata)


Coefficients:
(Intercept):1 (Intercept):2          x2:1          x2:2 
    0.9175762     1.0204439     1.0471179     1.0084276 

Degrees of Freedom: 200 Total; 196 Residual
Residual deviance: 235.6729 
Log-likelihood: -387.3006 
> (fit2 <- vglm(y1 ~ x2, poissonff(bred = TRUE), data = pdata))

Call:
vglm(formula = y1 ~ x2, family = poissonff(bred = TRUE), data = pdata)


Coefficients:
(Intercept)          x2 
  0.9203612   1.0466136 

Degrees of Freedom: 100 Total; 98 Residual
Residual deviance: 116.6895 
Log-likelihood: -191.1954 
> coef(fit1, matrix = TRUE)
            loglink(E[y1]) loglink(E[y2])
(Intercept)      0.9175762       1.020444
x2               1.0471179       1.008428
> coef(fit2, matrix = TRUE)
            loglink(lambda)
(Intercept)       0.9203612
x2                1.0466136
> 
> nn <- 200
> cdata <- data.frame(x2 = rnorm(nn), x3 = rnorm(nn), x4 = rnorm(nn))
> cdata <- transform(cdata, lv1 = 0 + x3 - 2*x4)
> cdata <- transform(cdata, lambda1 = exp(3 - 0.5 *  (lv1-0)^2),
+                           lambda2 = exp(2 - 0.5 *  (lv1-1)^2),
+                           lambda3 = exp(2 - 0.5 * ((lv1+4)/2)^2))
> cdata <- transform(cdata, y1 = rpois(nn, lambda1),
+                           y2 = rpois(nn, lambda2),
+                           y3 = rpois(nn, lambda3))
> ## Not run:  lvplot(p1, y = TRUE, lcol = 2:4, pch = 2:4, pcol = 2:4, rug = FALSE) 
> 
> 
> 
> cleanEx()
> nameEx("polonoUC")
> ### * polonoUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Polono
> ### Title: The Poisson Lognormal Distribution
> ### Aliases: Polono dpolono ppolono rpolono
> ### Keywords: distribution
> 
> ### ** Examples
> 
> meanlog <- 0.5; sdlog <- 0.5; yy <- 0:19
> sum(proby <- dpolono(yy, m = meanlog, sd = sdlog))  # Should be 1
[1] 0.9999955
> max(abs(cumsum(proby) - ppolono(yy, m = meanlog, sd = sdlog)))  # 0?
[1] 2.220446e-16
> 
> ## Not run: 
> ##D  opar = par(no.readonly = TRUE)
> ##D par(mfrow = c(2, 2))
> ##D plot(yy, proby, type = "h", col = "blue", ylab = "P[Y=y]", log = "",
> ##D      main = paste0("Poisson lognormal(m = ", meanlog,
> ##D                   ", sdl = ", sdlog, ")"))
> ##D 
> ##D y <- 0:190  # More extreme values; use the approxn & plot on a log scale
> ##D (sum(proby <- dpolono(y, m = meanlog, sd = sdlog, bigx = 100)))  # 1?
> ##D plot(y, proby, type = "h", col = "blue", ylab = "P[Y=y] (log)", log = "y",
> ##D      main = paste0("Poisson lognormal(m = ", meanlog,
> ##D                   ", sdl = ", sdlog, ")"))  # Note the kink at bigx
> ##D 
> ##D # Random number generation
> ##D table(y <- rpolono(n = 1000, m = meanlog, sd = sdlog))
> ##D hist(y, breaks = ((-1):max(y))+0.5, prob = TRUE, border = "blue")
> ##D par(opar) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("posbernUC")
> ### * posbernUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posbernUC
> ### Title: Positive Bernoulli Sequence Model
> ### Aliases: posbernUC dposbern rposbern
> ### Keywords: distribution datagen
> 
> ### ** Examples
> 
> rposbern(n = 10)
   y1 y2 y3 y4 y5 x1         x2        x3 ch1 ch2 ch3 ch4 ch5
1   0  0  0  1  1  1 0.37212390 0.7155661   0   0   0   0   1
2   0  0  0  0  1  1 0.57285336 0.1031842   0   0   0   0   0
3   1  1  1  0  1  1 0.90820779 0.4462843   0   1   1   1   1
4   0  1  1  0  1  1 0.20168193 0.6401010   0   0   1   1   1
5   1  1  0  1  1  1 0.89838968 0.9918386   0   1   1   1   1
6   1  1  1  0  0  1 0.94467527 0.4955936   0   1   1   1   1
7   0  1  1  1  1  1 0.66079779 0.4843495   0   0   1   1   1
8   0  0  0  0  1  1 0.62911404 0.1734423   0   0   0   0   0
9   0  1  0  1  1  1 0.06178627 0.7548209   0   0   1   1   1
10  0  0  0  0  1  1 0.20597457 0.4538955   0   0   0   0   0
> attributes(pdata <- rposbern(n = 100))
$names
 [1] "y1"  "y2"  "y3"  "y4"  "y5"  "x1"  "x2"  "x3"  "ch1" "ch2" "ch3" "ch4"
[13] "ch5"

$row.names
  [1] "1"   "2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12" 
 [13] "13"  "14"  "15"  "16"  "17"  "18"  "19"  "20"  "21"  "22"  "23"  "24" 
 [25] "25"  "26"  "27"  "28"  "29"  "30"  "31"  "32"  "33"  "34"  "35"  "36" 
 [37] "37"  "38"  "39"  "40"  "41"  "42"  "43"  "44"  "45"  "46"  "47"  "48" 
 [49] "49"  "50"  "51"  "52"  "53"  "54"  "55"  "56"  "57"  "58"  "59"  "60" 
 [61] "61"  "62"  "63"  "64"  "65"  "66"  "67"  "68"  "69"  "70"  "71"  "72" 
 [73] "73"  "74"  "75"  "76"  "77"  "78"  "79"  "80"  "81"  "82"  "83"  "84" 
 [85] "85"  "86"  "87"  "88"  "89"  "90"  "91"  "92"  "93"  "94"  "95"  "96" 
 [97] "97"  "98"  "99"  "100"

$class
[1] "data.frame"

$pvars
[1] 3

$nTimePts
[1] 5

$cap.effect
[1] 1

$is.popn
[1] FALSE

$n
[1] 100

> M.bh <- vglm(cbind(y1, y2, y3, y4, y5) ~ x2 + x3,
+              posbernoulli.b(I2 = FALSE), pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -320.1369
VGLM    linear loop  2 :  loglikelihood = -319.01372
VGLM    linear loop  3 :  loglikelihood = -318.99522
VGLM    linear loop  4 :  loglikelihood = -318.99506
VGLM    linear loop  5 :  loglikelihood = -318.99506
> constraints(M.bh)
$`(Intercept)`
     [,1] [,2]
[1,]    0    1
[2,]    1    1

$x2
     [,1]
[1,]    1
[2,]    1

$x3
     [,1]
[1,]    1
[2,]    1

> summary(M.bh)
Call:
vglm(formula = cbind(y1, y2, y3, y4, y5) ~ x2 + x3, family = posbernoulli.b(I2 = FALSE), 
    data = pdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   1.2212     0.2849   4.287 1.81e-05 ***
(Intercept):2  -2.1321     0.4596  -4.639 3.50e-06 ***
x2              0.4785     0.3834   1.248    0.212    
x3              2.0702     0.4461   4.641 3.47e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pcapture), logitlink(precapture)

Log-likelihood: -318.9951 on 196 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates


Estimate of N:  128.131 

Std. Error of N:  14.292 

Approximate 95 percent confidence interval for N:
100.12 156.14 
> 
> 
> 
> cleanEx()
> nameEx("posbernoulli.b")
> ### * posbernoulli.b
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posbernoulli.b
> ### Title: Positive Bernoulli Family Function with Behavioural Effects
> ### Aliases: posbernoulli.b
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # deermice data ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
> ##D 
> ##D # Fit a M_b model
> ##D M.b <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ 1,
> ##D             posbernoulli.b, data = deermice, trace = TRUE)
> ##D coef(M.b)["(Intercept):1"]  # Behavioural effect on logit scale
> ##D coef(M.b, matrix = TRUE)
> ##D constraints(M.b, matrix = TRUE)
> ##D summary(M.b, presid = FALSE)
> ##D 
> ##D # Fit a M_bh model
> ##D M.bh <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight,
> ##D              posbernoulli.b, data = deermice, trace = TRUE)
> ##D coef(M.bh, matrix = TRUE)
> ##D coef(M.bh)["(Intercept):1"]  # Behavioural effect on logit scale
> ##D # (2,1) elt is for the behavioural effect:
> ##D constraints(M.bh)[["(Intercept)"]]
> ##D summary(M.bh, presid = FALSE)  # Significant trap-happy effect
> ##D # Approx. 95 percent confidence for the behavioural effect:
> ##D SE.M.bh <- coef(summary(M.bh))["(Intercept):1", "Std. Error"]
> ##D coef(M.bh)["(Intercept):1"] + c(-1, 1) * 1.96 * SE.M.bh
> ##D 
> ##D # Fit a M_h model
> ##D M.h <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight,
> ##D             posbernoulli.b(drop.b = TRUE ~ sex + weight),
> ##D             data = deermice, trace = TRUE)
> ##D coef(M.h, matrix = TRUE)
> ##D constraints(M.h, matrix = TRUE)
> ##D summary(M.h, presid = FALSE)
> ##D 
> ##D # Fit a M_0 model
> ##D M.0 <- vglm(cbind(    y1 + y2 + y3 + y4 + y5 + y6,
> ##D                   6 - y1 - y2 - y3 - y4 - y5 - y6) ~ 1,
> ##D             posbinomial, data = deermice, trace = TRUE)
> ##D coef(M.0, matrix = TRUE)
> ##D summary(M.0, presid = FALSE)
> ##D 
> ##D 
> ##D # Simulated data set ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
> ##D set.seed(123); nTimePts <- 5; N <- 1000  # N is the popn size
> ##D pdata <- rposbern(N, nTimePts=nTimePts, pvars=2, is.popn=TRUE)
> ##D nrow(pdata)  # < N (because some animals were never captured)
> ##D # The truth: xcoeffs are c(-2, 1, 2) and cap.effect = +1
> ##D 
> ##D M.bh.2 <- vglm(cbind(y1, y2, y3, y4, y5) ~ x2,
> ##D                posbernoulli.b, data = pdata, trace = TRUE)
> ##D coef(M.bh.2)
> ##D coef(M.bh.2, matrix = TRUE)
> ##D constraints(M.bh.2, matrix = TRUE)
> ##D summary(M.bh.2, presid = FALSE)
> ##D head(depvar(M.bh.2))    # Capture history response matrix
> ##D head(M.bh.2@extra$cap.hist1)  # Info on its capture history
> ##D head(M.bh.2@extra$cap1)  # When it was first captured
> ##D head(fitted(M.bh.2))     # Depends on capture history
> ##D (trap.effect <- coef(M.bh.2)["(Intercept):1"])  # Should be +1
> ##D head(model.matrix(M.bh.2, type = "vlm"), 21)
> ##D head(pdata)
> ##D summary(pdata)
> ##D dim(depvar(M.bh.2))
> ##D vcov(M.bh.2)
> ##D 
> ##D M.bh.2@extra$N.hat  # Population size estimate; should be about N
> ##D M.bh.2@extra$SE.N.hat  # SE of the estimate of the population size
> ##D # An approximate 95 percent confidence interval:
> ##D round(M.bh.2@extra$N.hat + c(-1, 1)*1.96* M.bh.2@extra$SE.N.hat, 1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("posbernoulli.t")
> ### * posbernoulli.t
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posbernoulli.t
> ### Title: Positive Bernoulli Family Function with Time Effects
> ### Aliases: posbernoulli.t
> ### Keywords: models regression
> 
> ### ** Examples
> 
> M.t <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ 1, posbernoulli.t,
+             data = deermice, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -152.42047
VGLM    linear loop  2 :  loglikelihood = -152.42047
> coef(M.t, matrix = TRUE)
            logitlink(E[y1]) logitlink(E[y2]) logitlink(E[y3]) logitlink(E[y4])
(Intercept)        -0.444797       0.08324016       -0.3365884      -0.02096825
            logitlink(E[y5]) logitlink(E[y6])
(Intercept)        0.6234269        0.6234269
> constraints(M.t, matrix = TRUE)
                 (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4
logitlink(E[y1])             1             0             0             0
logitlink(E[y2])             0             1             0             0
logitlink(E[y3])             0             0             1             0
logitlink(E[y4])             0             0             0             1
logitlink(E[y5])             0             0             0             0
logitlink(E[y6])             0             0             0             0
                 (Intercept):5 (Intercept):6
logitlink(E[y1])             0             0
logitlink(E[y2])             0             0
logitlink(E[y3])             0             0
logitlink(E[y4])             0             0
logitlink(E[y5])             1             0
logitlink(E[y6])             0             1
> summary(M.t, presid = FALSE)
Call:
vglm(formula = cbind(y1, y2, y3, y4, y5, y6) ~ 1, family = posbernoulli.t, 
    data = deermice, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)  
(Intercept):1 -0.44480    0.33196  -1.340   0.1803  
(Intercept):2  0.08324    0.32502   0.256   0.7979  
(Intercept):3 -0.33659    0.32865  -1.024   0.3058  
(Intercept):4 -0.02097    0.32456  -0.065   0.9485  
(Intercept):5  0.62343    0.34214   1.822   0.0684 .
(Intercept):6  0.62343    0.34214   1.822   0.0684 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: logitlink(E[y1]), logitlink(E[y2]), 
logitlink(E[y3]), logitlink(E[y4]), logitlink(E[y5]), logitlink(E[y6])

Log-likelihood: -152.4205 on 222 degrees of freedom

Number of Fisher scoring iterations: 2 

No Hauck-Donner effect found in any of the estimates


Estimate of N:  38.403 

Std. Error of N:  0.663 

Approximate 95 percent confidence interval for N:
37.1 39.7 
> 
> M.h.1 <- vglm(Select(deermice, "y") ~ sex + weight, trace = TRUE,
+               posbernoulli.t(parallel.t = FALSE ~ -1), deermice)
VGLM    linear loop  1 :  loglikelihood = -151.0045
VGLM    linear loop  2 :  loglikelihood = -151.00335
VGLM    linear loop  3 :  loglikelihood = -151.00335
> coef(M.h.1, matrix = TRUE)
            logitlink(E[y1]) logitlink(E[y2]) logitlink(E[y3]) logitlink(E[y4])
(Intercept)      0.551986329      0.551986329      0.551986329      0.551986329
sex             -1.004446946     -1.004446946     -1.004446946     -1.004446946
weight          -0.002816076     -0.002816076     -0.002816076     -0.002816076
            logitlink(E[y5]) logitlink(E[y6])
(Intercept)      0.551986329      0.551986329
sex             -1.004446946     -1.004446946
weight          -0.002816076     -0.002816076
> constraints(M.h.1)
$`(Intercept)`
     [,1]
[1,]    1
[2,]    1
[3,]    1
[4,]    1
[5,]    1
[6,]    1

$sex
     [,1]
[1,]    1
[2,]    1
[3,]    1
[4,]    1
[5,]    1
[6,]    1

$weight
     [,1]
[1,]    1
[2,]    1
[3,]    1
[4,]    1
[5,]    1
[6,]    1

> summary(M.h.1, presid = FALSE)
Call:
vglm(formula = Select(deermice, "y") ~ sex + weight, family = posbernoulli.t(parallel.t = FALSE ~ 
    -1), data = deermice, trace = TRUE)

Coefficients: 
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.551986   0.475349   1.161 0.245551    
sex         -1.004447   0.292591  -3.433 0.000597 ***
weight      -0.002816   0.030698  -0.092 0.926908    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: logitlink(E[y1]), logitlink(E[y2]), 
logitlink(E[y3]), logitlink(E[y4]), logitlink(E[y5]), logitlink(E[y6])

Log-likelihood: -151.0034 on 225 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates


Estimate of N:  39.095 

Std. Error of N:  1.215 

Approximate 95 percent confidence interval for N:
36.71 41.48 
> head(depvar(M.h.1))  # Response capture history matrix
  y1 y2 y3 y4 y5 y6
1  1  1  1  1  1  1
2  1  0  0  1  1  1
3  1  1  0  0  1  1
4  1  1  0  1  1  1
5  1  1  1  1  1  1
6  1  1  0  1  1  1
> dim(depvar(M.h.1))
[1] 38  6
> 
> M.th.2 <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight,
+                posbernoulli.t(parallel.t = FALSE), deermice)
> # Test the parallelism assumption wrt sex and weight:
> lrtest(M.h.1, M.th.2)
Likelihood ratio test

Model 1: Select(deermice, "y") ~ sex + weight
Model 2: cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight
  #Df  LogLik  Df  Chisq Pr(>Chisq)
1 225 -151.00                      
2 210 -141.34 -15 19.338     0.1988
> coef(M.th.2)
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5 
  0.327989870   1.063231839   0.233749135   0.615009321  -0.086926178 
(Intercept):6         sex:1         sex:2         sex:3         sex:4 
  1.273518863  -1.880402750  -1.847528280   0.314062011  -0.708518755 
        sex:5         sex:6      weight:1      weight:2      weight:3 
 -1.222817026  -1.132734885  -0.003129889  -0.010746940  -0.051033965 
     weight:4      weight:5      weight:6 
 -0.023257910   0.089139329  -0.008273639 
> coef(M.th.2, matrix = TRUE)
            logitlink(E[y1]) logitlink(E[y2]) logitlink(E[y3]) logitlink(E[y4])
(Intercept)      0.327989870       1.06323184       0.23374914       0.61500932
sex             -1.880402750      -1.84752828       0.31406201      -0.70851876
weight          -0.003129889      -0.01074694      -0.05103396      -0.02325791
            logitlink(E[y5]) logitlink(E[y6])
(Intercept)      -0.08692618      1.273518863
sex              -1.22281703     -1.132734885
weight            0.08913933     -0.008273639
> constraints(M.th.2)
$`(Intercept)`
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    0    0    0    0    0
[2,]    0    1    0    0    0    0
[3,]    0    0    1    0    0    0
[4,]    0    0    0    1    0    0
[5,]    0    0    0    0    1    0
[6,]    0    0    0    0    0    1

$sex
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    0    0    0    0    0
[2,]    0    1    0    0    0    0
[3,]    0    0    1    0    0    0
[4,]    0    0    0    1    0    0
[5,]    0    0    0    0    1    0
[6,]    0    0    0    0    0    1

$weight
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    0    0    0    0    0
[2,]    0    1    0    0    0    0
[3,]    0    0    1    0    0    0
[4,]    0    0    0    1    0    0
[5,]    0    0    0    0    1    0
[6,]    0    0    0    0    0    1

> summary(M.th.2, presid = FALSE)
Call:
vglm(formula = cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight, 
    family = posbernoulli.t(parallel.t = FALSE), data = deermice)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)  
(Intercept):1  0.327990   1.234733   0.266   0.7905  
(Intercept):2  1.063232   1.173779   0.906   0.3650  
(Intercept):3  0.233749   1.090390   0.214   0.8303  
(Intercept):4  0.615009   1.082779   0.568   0.5700  
(Intercept):5 -0.086926   1.165830  -0.075   0.9406  
(Intercept):6  1.273519   1.143690   1.114   0.2655  
sex:1         -1.880403   0.776102  -2.423   0.0154 *
sex:2         -1.847528   0.720662  -2.564   0.0104 *
sex:3          0.314062   0.671089   0.468   0.6398  
sex:4         -0.708519   0.664583  -1.066   0.2864  
sex:5         -1.222817   0.728478  -1.679   0.0932 .
sex:6         -1.132735   0.709319  -1.597   0.1103  
weight:1      -0.003130   0.080967  -0.039   0.9692  
weight:2      -0.010747   0.074814  -0.144   0.8858  
weight:3      -0.051034   0.070598  -0.723   0.4698  
weight:4      -0.023258   0.069258  -0.336   0.7370  
weight:5       0.089139   0.076602   1.164   0.2446  
weight:6      -0.008274   0.071522  -0.116   0.9079  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  6 

Names of linear predictors: logitlink(E[y1]), logitlink(E[y2]), 
logitlink(E[y3]), logitlink(E[y4]), logitlink(E[y5]), logitlink(E[y6])

Log-likelihood: -141.3346 on 210 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates


Estimate of N:  38.881 

Std. Error of N:  1.076 

Approximate 95 percent confidence interval for N:
36.77 40.99 
> head(model.matrix(M.th.2, type = "vlm"), 21)
    (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5
1:1             1             0             0             0             0
1:2             0             1             0             0             0
1:3             0             0             1             0             0
1:4             0             0             0             1             0
1:5             0             0             0             0             1
1:6             0             0             0             0             0
2:1             1             0             0             0             0
2:2             0             1             0             0             0
2:3             0             0             1             0             0
2:4             0             0             0             1             0
2:5             0             0             0             0             1
2:6             0             0             0             0             0
3:1             1             0             0             0             0
3:2             0             1             0             0             0
3:3             0             0             1             0             0
3:4             0             0             0             1             0
3:5             0             0             0             0             1
3:6             0             0             0             0             0
4:1             1             0             0             0             0
4:2             0             1             0             0             0
4:3             0             0             1             0             0
    (Intercept):6 sex:1 sex:2 sex:3 sex:4 sex:5 sex:6 weight:1 weight:2
1:1             0     0     0     0     0     0     0       12        0
1:2             0     0     0     0     0     0     0        0       12
1:3             0     0     0     0     0     0     0        0        0
1:4             0     0     0     0     0     0     0        0        0
1:5             0     0     0     0     0     0     0        0        0
1:6             1     0     0     0     0     0     0        0        0
2:1             0     1     0     0     0     0     0       15        0
2:2             0     0     1     0     0     0     0        0       15
2:3             0     0     0     1     0     0     0        0        0
2:4             0     0     0     0     1     0     0        0        0
2:5             0     0     0     0     0     1     0        0        0
2:6             1     0     0     0     0     0     1        0        0
3:1             0     0     0     0     0     0     0       15        0
3:2             0     0     0     0     0     0     0        0       15
3:3             0     0     0     0     0     0     0        0        0
3:4             0     0     0     0     0     0     0        0        0
3:5             0     0     0     0     0     0     0        0        0
3:6             1     0     0     0     0     0     0        0        0
4:1             0     0     0     0     0     0     0       15        0
4:2             0     0     0     0     0     0     0        0       15
4:3             0     0     0     0     0     0     0        0        0
    weight:3 weight:4 weight:5 weight:6
1:1        0        0        0        0
1:2        0        0        0        0
1:3       12        0        0        0
1:4        0       12        0        0
1:5        0        0       12        0
1:6        0        0        0       12
2:1        0        0        0        0
2:2        0        0        0        0
2:3       15        0        0        0
2:4        0       15        0        0
2:5        0        0       15        0
2:6        0        0        0       15
3:1        0        0        0        0
3:2        0        0        0        0
3:3       15        0        0        0
3:4        0       15        0        0
3:5        0        0       15        0
3:6        0        0        0       15
4:1        0        0        0        0
4:2        0        0        0        0
4:3       15        0        0        0
> 
> M.th.2@extra$N.hat  # Population size estimate; should be about N
[1] 38.88065
> M.th.2@extra$SE.N.hat  # SE of the estimate of the population size
[1] 1.076247
> # An approximate 95 percent confidence interval:
> round(M.th.2@extra$N.hat + c(-1, 1)*1.96* M.th.2@extra$SE.N.hat, 1)
[1] 36.8 41.0
> 
> # Fit a M_h model, effectively the parallel M_t model:
> deermice <- transform(deermice, ysum = y1 + y2 + y3 + y4 + y5 + y6,
+                                 tau  = 6)
> M.h.3 <- vglm(cbind(ysum, tau - ysum) ~ sex + weight,
+               posbinomial(omit.constant = TRUE), data = deermice)
> max(abs(coef(M.h.1) - coef(M.h.3)))  # Should be zero
[1] 1.139693e-10
> # Difference is due to the binomial constants:
> logLik(M.h.3) - logLik(M.h.1)
[1] 0
> 
> 
> 
> cleanEx()
> nameEx("posbernoulli.tb")
> ### * posbernoulli.tb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posbernoulli.tb
> ### Title: Positive Bernoulli Family Function with Time and Behavioural
> ###   Effects
> ### Aliases: posbernoulli.tb
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1: simulated data
> ##D nTimePts <- 5  # (aka tau == # of sampling occasions)
> ##D nnn <- 1000   # Number of animals
> ##D pdata <- rposbern(n = nnn, nTimePts = nTimePts, pvars = 2)
> ##D dim(pdata); head(pdata)
> ##D 
> ##D M_tbh.1 <- vglm(cbind(y1, y2, y3, y4, y5) ~ x2,
> ##D                 posbernoulli.tb, data = pdata, trace = TRUE)
> ##D coef(M_tbh.1)  # First element is the behavioural effect
> ##D coef(M_tbh.1, matrix = TRUE)
> ##D constraints(M_tbh.1, matrix = TRUE)
> ##D summary(M_tbh.1, presid = FALSE)  # Std errors are approximate
> ##D head(fitted(M_tbh.1))
> ##D head(model.matrix(M_tbh.1, type = "vlm"), 21)
> ##D dim(depvar(M_tbh.1))
> ##D 
> ##D M_tbh.2 <- vglm(cbind(y1, y2, y3, y4, y5) ~ x2,
> ##D                 posbernoulli.tb(parallel.t = FALSE ~  0),
> ##D                 data = pdata, trace = TRUE)
> ##D coef(M_tbh.2)  # First element is the behavioural effect
> ##D coef(M_tbh.2, matrix = TRUE)
> ##D constraints(M_tbh.2, matrix = TRUE)
> ##D summary(M_tbh.2, presid = FALSE)  # Std errors are approximate
> ##D head(fitted(M_tbh.2))
> ##D head(model.matrix(M_tbh.2, type = "vlm"), 21)
> ##D dim(depvar(M_tbh.2))
> ##D 
> ##D # Example 2: deermice subset data
> ##D fit1 <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight,
> ##D              posbernoulli.t, data = deermice, trace = TRUE)
> ##D coef(fit1)
> ##D coef(fit1, matrix = TRUE)
> ##D constraints(fit1, matrix = TRUE)
> ##D summary(fit1, presid = FALSE)  # Standard errors are approximate
> ##D 
> ##D # fit1 is the same as Fit1 (a M_{th} model):
> ##D Fit1 <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ sex + weight,
> ##D              posbernoulli.tb(drop.b = TRUE ~ sex + weight,
> ##D                 parallel.t = TRUE),  # But not for the intercept
> ##D              data = deermice, trace = TRUE)
> ##D constraints(Fit1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("posbinomial")
> ### * posbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posbinomial
> ### Title: Positive Binomial Distribution Family Function
> ### Aliases: posbinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Albinotic children in families with 5 kids (from Patil, 1962) ,,,,
> albinos <- data.frame(y = c(rep(1, 25), rep(2, 23), rep(3, 10), 4, 5),
+                       n = rep(5, 60))
> fit1 <- vglm(cbind(y, n-y) ~ 1, posbinomial, albinos, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -71.320528
VGLM    linear loop  2 :  loglikelihood = -71.296042
VGLM    linear loop  3 :  loglikelihood = -71.296039
VGLM    linear loop  4 :  loglikelihood = -71.296039
> summary(fit1)
Call:
vglm(formula = cbind(y, n - y) ~ 1, family = posbinomial, data = albinos, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -0.8056     0.1504  -5.357 8.46e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: logitlink(prob) 

Log-likelihood: -71.296 on 59 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> Coef(fit1)  # = MLE of p = 0.3088
     prob 
0.3088317 
> head(fitted(fit1))
       [,1]
1 0.3666667
2 0.3666667
3 0.3666667
4 0.3666667
5 0.3666667
6 0.3666667
> sqrt(vcov(fit1, untransform = TRUE))  # SE = 0.0322
           prob
prob 0.03209962
> 
> # Fit a M_0 model (Otis et al. 1978) to the deermice data ,,,,,,,,,,
> M.0 <- vglm(cbind(    y1 + y2 + y3 + y4 + y5 + y6,
+                   6 - y1 - y2 - y3 - y4 - y5 - y6) ~ 1, trace = TRUE,
+             posbinomial(omit.constant = TRUE), data = deermice)
VGLM    linear loop  1 :  loglikelihood = -157.34227
VGLM    linear loop  2 :  loglikelihood = -157.27221
VGLM    linear loop  3 :  loglikelihood = -157.27221
> coef(M.0, matrix = TRUE)
            logitlink(prob)
(Intercept)       0.0795139
> Coef(M.0)
    prob 
0.519868 
> constraints(M.0, matrix = TRUE)
                (Intercept)
logitlink(prob)           1
> summary(M.0)
Call:
vglm(formula = cbind(y1 + y2 + y3 + y4 + y5 + y6, 6 - y1 - y2 - 
    y3 - y4 - y5 - y6) ~ 1, family = posbinomial(omit.constant = TRUE), 
    data = deermice, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.07951    0.13739   0.579    0.563

Name of linear predictor: logitlink(prob) 

Log-likelihood: -157.2722 on 37 degrees of freedom

Number of Fisher scoring iterations: 3 

No Hauck-Donner effect found in any of the estimates

> c(   N.hat = M.0@extra$N.hat,     # As tau = 6, i.e., 6 Bernoulli trials
+   SE.N.hat = M.0@extra$SE.N.hat)  # per obsn is the same for each obsn
     N.hat   SE.N.hat 
38.4713036  0.7203915 
> 
> # Compare it to the M_b using AIC and BIC
> M.b <- vglm(cbind(y1, y2, y3, y4, y5, y6) ~ 1, trace = TRUE,
+             posbernoulli.b, data = deermice)
VGLM    linear loop  1 :  loglikelihood = -151.3106
VGLM    linear loop  2 :  loglikelihood = -150.4366
VGLM    linear loop  3 :  loglikelihood = -150.43416
VGLM    linear loop  4 :  loglikelihood = -150.43416
> sort(c(M.0 = AIC(M.0), M.b = AIC(M.b)))  # Ok since omit.constant=TRUE
     M.b      M.0 
304.8683 316.5444 
> sort(c(M.0 = BIC(M.0), M.b = BIC(M.b)))  # Ok since omit.constant=TRUE
     M.b      M.0 
308.1435 318.1820 
> 
> 
> 
> cleanEx()
> nameEx("posgeomUC")
> ### * posgeomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Posgeom
> ### Title: Positive-Geometric Distribution
> ### Aliases: Posgeom dposgeom pposgeom qposgeom rposgeom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> prob <- 0.75; y <- rposgeom(n = 1000, prob)
> table(y)
y
  1   2   3   4   5   6   7 
752 177  51  17   1   1   1 
> mean(y)  # Sample mean
[1] 1.345
> 1 / prob  # Population mean
[1] 1.333333
> 
> (ii <- dposgeom(0:7, prob))
[1] 0.0000000000 0.7500000000 0.1875000000 0.0468750000 0.0117187500
[6] 0.0029296875 0.0007324219 0.0001831055
> cumsum(ii) - pposgeom(0:7, prob)  # Should be 0s
[1] 0 0 0 0 0 0 0 0
> table(rposgeom(100, prob))

 1  2  3  4 
76 18  5  1 
> 
> table(qposgeom(runif(1000), prob))

  1   2   3   4   5 
737 200  48  11   4 
> round(dposgeom(1:10, prob) * 1000)  # Should be similar
 [1] 750 188  47  12   3   1   0   0   0   0
> 
> ## Not run: 
> ##D x <- 0:5
> ##D barplot(rbind(dposgeom(x, prob), dgeom(x, prob)),
> ##D         beside = TRUE, col = c("blue", "orange"),
> ##D         main = paste("Positive geometric(", prob, ") (blue) vs",
> ##D         " geometric(", prob, ") (orange)", sep = ""),
> ##D         names.arg = as.character(x), las = 1, lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("posnegbinomial")
> ### * posnegbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posnegbinomial
> ### Title: Positive Negative Binomial Distribution Family Function
> ### Aliases: posnegbinomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D pdata <- data.frame(x2 = runif(nn <- 1000))
> ##D pdata <- transform(pdata,
> ##D   y1 = rgaitdnbinom(nn, exp(1), munb.p = exp(0+2*x2), truncate = 0),
> ##D   y2 = rgaitdnbinom(nn, exp(3), munb.p = exp(1+2*x2), truncate = 0))
> ##D fit <- vglm(cbind(y1, y2) ~ x2, posnegbinomial, pdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D dim(depvar(fit))  # Using dim(fit@y) is not recommended
> ##D 
> ##D 
> ##D # Another artificial data example
> ##D pdata2 <- data.frame(munb = exp(2), size = exp(3)); nn <- 1000
> ##D pdata2 <- transform(pdata2,
> ##D                     y3 = rgaitdnbinom(nn, size, munb.p = munb,
> ##D                                       truncate = 0))
> ##D with(pdata2, table(y3))
> ##D fit <- vglm(y3 ~ 1, posnegbinomial, data = pdata2, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D with(pdata2, mean(y3))  # Sample mean
> ##D head(with(pdata2, munb/(1-(size/(size+munb))^size)), 1)  # Popn mean
> ##D head(fitted(fit), 3)
> ##D head(predict(fit), 3)
> ##D 
> ##D 
> ##D # Example: Corbet (1943) butterfly Malaya data
> ##D fit <- vglm(ofreq ~ 1, posnegbinomial, weights = species, corbet)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D (khat <- Coef(fit)["size"])
> ##D pdf2 <- dgaitdnbinom(with(corbet, ofreq), khat,
> ##D                      munb.p = fitted(fit), truncate = 0)
> ##D print(with(corbet,
> ##D            cbind(ofreq, species, fitted = pdf2*sum(species))), dig = 1)
> ##D with(corbet,
> ##D matplot(ofreq, cbind(species, fitted = pdf2*sum(species)), las = 1,
> ##D    xlab = "Observed frequency (of individual butterflies)",
> ##D    type = "b", ylab = "Number of species", col = c("blue", "orange"),
> ##D    main = "blue 1s = observe; orange 2s = fitted"))
> ##D 
> ##D # Data courtesy of Maxim Gerashchenko causes posbinomial() to fail
> ##D pnbd.fail <- data.frame(
> ##D  y1 = c(1:16, 18:21, 23:28, 33:38, 42, 44, 49:51, 55, 56, 58,
> ##D  59, 61:63, 66, 73, 76, 94, 107, 112, 124, 190, 191, 244),
> ##D  ofreq = c(130, 80, 38, 23, 22, 11, 21, 14, 6, 7, 9, 9, 9, 4, 4, 5, 1,
> ##D            4, 6, 1, 3, 2, 4, 3, 4, 5, 3, 1, 2, 1, 1, 4, 1, 2, 2, 1, 3,
> ##D            1, 1, 2, 2, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1))
> ##D fit.fail <- vglm(y1 ~ 1, weights = ofreq, posnegbinomial,
> ##D                trace = TRUE, data = pnbd.fail)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("posnormUC")
> ### * posnormUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Posnorm
> ### Title: The Positive-Normal Distribution
> ### Aliases: Posnorm dposnorm pposnorm qposnorm rposnorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  m <-  0.8; x <- seq(-1, 4, len = 501)
> ##D plot(x, dposnorm(x, m = m), type = "l", las = 1, ylim = 0:1,
> ##D      ylab = paste("posnorm(m = ", m, ", sd = 1)"), col = "blue",
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0, col = "grey")
> ##D lines(x, pposnorm(x, m = m), col = "orange", type = "l")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qposnorm(probs, m = m)
> ##D lines(Q, dposnorm(Q, m = m), col = "purple", lty = 3, type = "h")
> ##D lines(Q, pposnorm(Q, m = m), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(pposnorm(Q, m = m) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("posnormal")
> ### * posnormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: posnormal
> ### Title: Positive Normal Distribution Family Function
> ### Aliases: posnormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pdata <- data.frame(Mean = 1.0, SD = exp(1.0))
> pdata <- transform(pdata, y = rposnorm(n <- 1000, m = Mean, sd = SD))
> 
> ## Not run: 
> ##D with(pdata, hist(y, prob = TRUE, border = "blue",
> ##D   main = paste("posnorm(m =", Mean[1], ", sd =", round(SD[1], 2),")")))
> ## End(Not run)
> fit <- vglm(y ~ 1, posnormal, data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1881.8364
VGLM    linear loop  2 :  loglikelihood = -1881.7636
VGLM    linear loop  3 :  loglikelihood = -1881.7636
VGLM    linear loop  4 :  loglikelihood = -1881.7636
> coef(fit, matrix = TRUE)
                 mean loglink(sd)
(Intercept) 0.8419481     1.03747
> (Cfit <- Coef(fit))
     mean        sd 
0.8419481 2.8220679 
> mygrid <- with(pdata, seq(min(y), max(y), len = 200))
> ## Not run: lines(mygrid, dposnorm(mygrid, Cfit[1], Cfit[2]), col = "red")
> 
> 
> 
> cleanEx()
> nameEx("pospoisson")
> ### * pospoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pospoisson
> ### Title: Positive Poisson Distribution Family Function
> ### Aliases: pospoisson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Data from Coleman and James (1961)
> cjdata <- data.frame(y = 1:6, freq = c(1486, 694, 195, 37, 10, 1))
> fit <- vglm(y ~ 1, pospoisson, data = cjdata, weights = freq)
> Coef(fit)
   lambda 
0.8924961 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = pospoisson, data = cjdata, weights = freq)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.11373    0.02678  -4.248 2.16e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(lambda) 

Log-likelihood: -2304.659 on 5 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> fitted(fit)
      [,1]
1 1.511762
2 1.511762
3 1.511762
4 1.511762
5 1.511762
6 1.511762
> 
> pdata <- data.frame(x2 = runif(nn <- 1000))  # Artificial data
> pdata <- transform(pdata, lambda = exp(1 - 2 * x2))
> pdata <- transform(pdata, y1 = rgaitdpois(nn, lambda, truncate = 0))
> with(pdata, table(y1))
y1
  1   2   3   4   5   6 
563 252 112  54  16   3 
> fit <- vglm(y1 ~ x2, pospoisson, data = pdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients =  0.80144094, -1.08030061
VGLM    linear loop  2 :  coefficients =  0.93051923, -1.73687838
VGLM    linear loop  3 :  coefficients =  0.95857234, -1.90858840
VGLM    linear loop  4 :  coefficients =  0.96004096, -1.91834790
VGLM    linear loop  5 :  coefficients =  0.96004541, -1.91837808
VGLM    linear loop  6 :  coefficients =  0.96004541, -1.91837808
> coef(fit, matrix = TRUE)
            loglink(lambda)
(Intercept)       0.9600454
x2               -1.9183781
> 
> 
> 
> cleanEx()
> nameEx("powerlink")
> ### * powerlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: powerlink
> ### Title: Power Link Function
> ### Aliases: powerlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> powerlink("a", power = 2, short = FALSE, tag = TRUE)
[1] "Power link: a^(2)"
> powerlink(x <- 1:5)
[1] 1 2 3 4 5
> powerlink(x, power = 2)
[1]  1  4  9 16 25
> max(abs(powerlink(powerlink(x, power = 2),
+                   power = 2, inverse = TRUE) - x))  # Should be 0
[1] 0
> powerlink(x <- (-5):5, power = 0.5)  # Has NAs
 [1]      NaN      NaN      NaN      NaN      NaN 0.000000 1.000000 1.414214
 [9] 1.732051 2.000000 2.236068
> 
> # 1/2 = 0.5
> pdata <- data.frame(y = rbeta(n = 1000, shape1 = 2^2, shape2 = 3^2))
> fit <- vglm(y ~ 1, betaR(lshape1 = powerlink(power = 0.5), i1 = 3,
+                          lshape2 = powerlink(power = 0.5), i2 = 7), data = pdata)
> t(coef(fit, matrix = TRUE))
                               (Intercept)
powerlink(shape1, power = 0.5)    1.990107
powerlink(shape2, power = 0.5)    3.002109
> Coef(fit)  # Useful for intercept-only models
  shape1   shape2 
3.960526 9.012661 
> vcov(fit, untransform = TRUE)
           shape1     shape2
shape1 0.02912954 0.06273092
shape2 0.06273092 0.16196758
> 
> 
> 
> cleanEx()
> nameEx("prats")
> ### * prats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prats
> ### Title: Pregnant Rats Toxological Experiment Data
> ### Aliases: prats
> ### Keywords: datasets
> 
> ### ** Examples
> 
> prats
   treatment alive litter.size
1          0    13          13
2          0    12          12
3          0     9           9
4          0     9           9
5          0     8           8
6          0     8           8
7          0    12          13
8          0    11          12
9          0     9          10
10         0     9          10
11         0     8           9
12         0    11          13
13         0     4           5
14         0     5           7
15         0     7          10
16         0     7          10
17         1    12          12
18         1    11          11
19         1    10          10
20         1     9           9
21         1    10          11
22         1     9          10
23         1     9          10
24         1     8           9
25         1     8           9
26         1     4           5
27         1     7           9
28         1     4           7
29         1     5          10
30         1     3           6
31         1     3          10
32         1     0           7
> colSums(subset(prats, treatment == 0))
  treatment       alive litter.size 
          0         142         158 
> colSums(subset(prats, treatment == 1))
  treatment       alive litter.size 
         16         112         145 
> summary(prats)
   treatment       alive         litter.size    
 Min.   :0.0   Min.   : 0.000   Min.   : 5.000  
 1st Qu.:0.0   1st Qu.: 6.500   1st Qu.: 8.750  
 Median :0.5   Median : 8.500   Median :10.000  
 Mean   :0.5   Mean   : 7.938   Mean   : 9.469  
 3rd Qu.:1.0   3rd Qu.:10.000   3rd Qu.:10.250  
 Max.   :1.0   Max.   :13.000   Max.   :13.000  
> 
> 
> 
> cleanEx()
> nameEx("predictqrrvglm")
> ### * predictqrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predictqrrvglm
> ### Title: Predict Method for a CQO fit
> ### Aliases: predictqrrvglm
> ### Keywords: models nonlinear regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  set.seed(1234)
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Standardize the X vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute,
> ##D                 Arctperi, Auloalbi, Pardlugu, Pardmont,
> ##D                 Pardnigr, Pardpull, Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, Crow1positive = FALSE, I.toler = TRUE)
> ##D sort(deviance(p1, history = TRUE))  # A history of all the iterations
> ##D head(predict(p1))
> ##D 
> ##D # The following should be all 0s:
> ##D max(abs(predict(p1, newdata = head(hspider)) - head(predict(p1))))
> ##D max(abs(predict(p1, newdata = head(hspider), type = "res")-head(fitted(p1))))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("predictvglm")
> ### * predictvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predictvglm
> ### Title: Predict Method for a VGLM fit
> ### Aliases: predictvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Illustrates smart prediction
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ poly(c(scale(let)), 2),
+             propodds, pneumo, trace = TRUE, x.arg = FALSE)
VGLM    linear loop  1 :  deviance = 4.162527
VGLM    linear loop  2 :  deviance = 3.951629
VGLM    linear loop  3 :  deviance = 3.946558
VGLM    linear loop  4 :  deviance = 3.94655
VGLM    linear loop  5 :  deviance = 3.94655
> class(fit)
[1] "vglm"
attr(,"package")
[1] "VGAM"
> 
> (q0 <- head(predict(fit)))
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1         -6.6420717          -7.540193
2         -2.7470610          -3.645182
3         -1.6175447          -2.515666
4         -0.9547998          -1.852921
5         -0.4876278          -1.385749
6         -0.1414232          -1.039544
> (q1 <- predict(fit, newdata = head(pneumo)))
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1       -5.773141107         -6.6712621
2       -2.059406372         -2.9575274
3       -1.017291252         -1.9154122
4       -0.420223629         -1.3183446
5       -0.009188218         -0.9073092
6        0.287785773         -0.6103352
> (q2 <- predict(fit, newdata = head(pneumo)))
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1       -5.773141107         -6.6712621
2       -2.059406372         -2.9575274
3       -1.017291252         -1.9154122
4       -0.420223629         -1.3183446
5       -0.009188218         -0.9073092
6        0.287785773         -0.6103352
> all.equal(q0, q1)  # Should be TRUE
[1] "Mean relative difference: 0.2354654"
> all.equal(q1, q2)  # Should be TRUE
[1] TRUE
> 
> head(predict(fit))
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1         -6.6420717          -7.540193
2         -2.7470610          -3.645182
3         -1.6175447          -2.515666
4         -0.9547998          -1.852921
5         -0.4876278          -1.385749
6         -0.1414232          -1.039544
> head(predict(fit, untransform = TRUE))
      P[Y>=2]      P[Y>=3]
1 0.001302623 0.0005310131
2 0.060252851 0.0254519377
3 0.165543769 0.0747672299
4 0.277920572 0.1355303291
5 0.380452564 0.2000873101
6 0.464703016 0.2612379561
> 
> p0 <- head(predict(fit, type = "response"))
> p1 <- head(predict(fit, type = "response", newdata = pneumo))
> p2 <- head(predict(fit, type = "response", newdata = pneumo))
> p3 <- head(fitted(fit))
> all.equal(p0, p1)  # Should be TRUE
[1] TRUE
> all.equal(p1, p2)  # Should be TRUE
[1] TRUE
> all.equal(p2, p3)  # Should be TRUE
[1] TRUE
> 
> predict(fit, type = "terms", se = TRUE)
$fitted.values
  poly(c(scale(let)), 2):1 poly(c(scale(let)), 2):2
1               -5.1276963               -5.1276963
2               -1.2326855               -1.2326855
3               -0.1031692               -0.1031692
4                0.5595757                0.5595757
5                1.0267477                1.0267477
6                1.3729523                1.3729523
7                1.6576137                1.6576137
8                1.8466615                1.8466615
attr(,"vterm.assign")
attr(,"vterm.assign")$`poly(c(scale(let)), 2)`
[1] 1 2


$se.fit
  poly(c(scale(let)), 2):1 poly(c(scale(let)), 2):2
1                1.7242629                1.7242629
2                0.2327739                0.2327739
3                0.3311413                0.3311413
4                0.3768132                0.3768132
5                0.3653147                0.3653147
6                0.3317412                0.3317412
7                0.3044709                0.3044709
8                0.3113166                0.3113166
attr(,"vterm.assign")
attr(,"vterm.assign")$`poly(c(scale(let)), 2)`
[1] 1 2


$df
[1] 12

$sigma
[1] 1

> 
> 
> 
> cleanEx()
> nameEx("prentice74")
> ### * prentice74
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prentice74
> ### Title: Prentice (1974) Log-gamma Distribution
> ### Aliases: prentice74
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pdata <- data.frame(x2 = runif(nn <- 1000))
> pdata <- transform(pdata, loc = -1 + 2*x2, Scale = exp(1))
> pdata <- transform(pdata, y = rlgamma(nn, loc = loc, scale = Scale, shape = 1))
> fit <- vglm(y ~ x2, prentice74(zero = 2:3), data = pdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2605.0031
VGLM    linear loop  2 :  loglikelihood = -2601.4394
VGLM    linear loop  3 :  loglikelihood = -2601.1607
VGLM    linear loop  4 :  loglikelihood = -2601.1429
VGLM    linear loop  5 :  loglikelihood = -2601.1418
VGLM    linear loop  6 :  loglikelihood = -2601.1417
VGLM    linear loop  7 :  loglikelihood = -2601.1417
> coef(fit, matrix = TRUE)  # Note the coefficients for location
             location loglink(scale)    shape
(Intercept) -2.880051      0.9931207 1.103421
x2           2.377545      0.0000000 0.000000
> 
> 
> 
> cleanEx()
> nameEx("prinia")
> ### * prinia
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prinia
> ### Title: Yellow-bellied Prinia
> ### Aliases: prinia
> ### Keywords: datasets
> 
> ### ** Examples
> 
> head(prinia)
       length fat cap noncap y01 y02 y03 y04 y05 y06 y07 y08 y09 y10 y11 y12
1  1.00650390   1   5     14   0   0   0   0   0   0   0   0   1   1   1   1
2  1.26462566   1   3     16   0   0   0   0   0   0   0   0   1   0   1   1
3 -0.02598312   1   6     13   0   0   0   0   0   0   1   0   1   0   0   0
4  3.07147795   0   1     18   0   0   0   0   0   0   0   0   1   0   0   0
5  0.43863604   1   5     14   0   1   0   0   0   0   0   0   0   0   1   1
6  0.74838215   0   1     18   0   0   0   0   0   0   0   0   0   0   0   0
  y13 y14 y15 y16 y17 y18 y19
1   1   0   0   0   0   0   0
2   0   0   0   0   0   0   0
3   0   1   0   0   1   1   1
4   0   0   0   0   0   0   0
5   0   1   0   0   1   0   0
6   0   0   1   0   0   0   0
> summary(prinia)
     length              fat              cap            noncap     
 Min.   :-2.34908   Min.   :0.0000   Min.   :1.000   Min.   :13.00  
 1st Qu.:-0.80035   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:18.00  
 Median :-0.02598   Median :1.0000   Median :1.000   Median :18.00  
 Mean   : 0.00000   Mean   :0.5762   Mean   :1.477   Mean   :17.52  
 3rd Qu.: 0.74838   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:18.00  
 Max.   : 3.07148   Max.   :1.0000   Max.   :6.000   Max.   :18.00  
      y01                y02              y03               y04         
 Min.   :0.000000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.000000   Median :0.0000   Median :0.00000   Median :0.00000  
 Mean   :0.006622   Mean   :0.1325   Mean   :0.02649   Mean   :0.01324  
 3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :1.000000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000  
      y05               y06              y07              y08         
 Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  
 Median :0.00000   Median :0.0000   Median :0.0000   Median :0.00000  
 Mean   :0.04636   Mean   :0.0596   Mean   :0.1854   Mean   :0.06623  
 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  
 Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  
      y09               y10              y11             y12       
 Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :0.000  
 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.000  
 Median :0.00000   Median :0.0000   Median :0.000   Median :0.000  
 Mean   :0.09272   Mean   :0.1457   Mean   :0.106   Mean   :0.106  
 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:0.000  
 Max.   :1.00000   Max.   :1.0000   Max.   :1.000   Max.   :1.000  
      y13               y14              y15              y16         
 Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  
 Median :0.00000   Median :0.0000   Median :0.0000   Median :0.00000  
 Mean   :0.09272   Mean   :0.0596   Mean   :0.1722   Mean   :0.03311  
 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  
 Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  
      y17               y18               y19         
 Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  
 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  
 Median :0.00000   Median :0.00000   Median :0.00000  
 Mean   :0.03974   Mean   :0.03974   Mean   :0.05298  
 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  
 Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  
> rowSums(prinia[, c("cap", "noncap")])  # 19s
  [1] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
 [26] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
 [51] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
 [76] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
[101] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
[126] 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19
[151] 19
> 
> #  Fit a positive-binomial distribution (M.h) to the data:
> fit1 <- vglm(cbind(cap, noncap) ~ length + fat, posbinomial, prinia)
> 
> #  Fit another positive-binomial distribution (M.h) to the data:
> #  The response input is suitable for posbernoulli.*-type functions.
> fit2 <- vglm(cbind(y01, y02, y03, y04, y05, y06, y07, y08, y09, y10,
+                    y11, y12, y13, y14, y15, y16, y17, y18, y19) ~
+              length + fat, posbernoulli.b(drop.b = FALSE ~ 0), prinia)
> 
> 
> 
> cleanEx()
> nameEx("probitlink")
> ### * probitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: probitlink
> ### Title: Probit Link Function
> ### Aliases: probitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> p <- seq(0.01, 0.99, by = 0.01)
> probitlink(p)
 [1] -2.32634787 -2.05374891 -1.88079361 -1.75068607 -1.64485363 -1.55477359
 [7] -1.47579103 -1.40507156 -1.34075503 -1.28155157 -1.22652812 -1.17498679
[13] -1.12639113 -1.08031934 -1.03643339 -0.99445788 -0.95416525 -0.91536509
[19] -0.87789630 -0.84162123 -0.80642125 -0.77219321 -0.73884685 -0.70630256
[25] -0.67448975 -0.64334541 -0.61281299 -0.58284151 -0.55338472 -0.52440051
[31] -0.49585035 -0.46769880 -0.43991317 -0.41246313 -0.38532047 -0.35845879
[37] -0.33185335 -0.30548079 -0.27931903 -0.25334710 -0.22754498 -0.20189348
[43] -0.17637416 -0.15096922 -0.12566135 -0.10043372 -0.07526986 -0.05015358
[49] -0.02506891  0.00000000  0.02506891  0.05015358  0.07526986  0.10043372
[55]  0.12566135  0.15096922  0.17637416  0.20189348  0.22754498  0.25334710
[61]  0.27931903  0.30548079  0.33185335  0.35845879  0.38532047  0.41246313
[67]  0.43991317  0.46769880  0.49585035  0.52440051  0.55338472  0.58284151
[73]  0.61281299  0.64334541  0.67448975  0.70630256  0.73884685  0.77219321
[79]  0.80642125  0.84162123  0.87789630  0.91536509  0.95416525  0.99445788
[85]  1.03643339  1.08031934  1.12639113  1.17498679  1.22652812  1.28155157
[91]  1.34075503  1.40507156  1.47579103  1.55477359  1.64485363  1.75068607
[97]  1.88079361  2.05374891  2.32634787
> max(abs(probitlink(probitlink(p), inverse = TRUE) - p))  # Should be 0
[1] 1.110223e-16
> 
> p <- c(seq(-0.02, 0.02, by = 0.01), seq(0.97, 1.02, by = 0.01))
> probitlink(p)  # Has NAs
Warning in qnorm(theta) : NaNs produced
 [1]       NaN       NaN      -Inf -2.326348 -2.053749  1.880794  2.053749
 [8]  2.326348       Inf       NaN       NaN
> probitlink(p, bvalue = .Machine$double.eps)  # Has no NAs
 [1] -8.125891 -8.125891 -8.125891 -2.326348 -2.053749  1.880794  2.053749
 [8]  2.326348  8.125891  8.125891  8.125891
> 
> ## Not run: 
> ##D p <- seq(0.01, 0.99, by = 0.01); par(lwd = (mylwd <- 2))
> ##D plot(p, logitlink(p), type = "l", col = "limegreen", ylab = "transformation",
> ##D      las = 1, main = "Some probability link functions")
> ##D lines(p,  probitlink(p), col = "purple")
> ##D lines(p, clogloglink(p), col = "chocolate")
> ##D lines(p, cauchitlink(p), col = "tan")
> ##D abline(v = 0.5, h = 0, lty = "dashed")
> ##D legend(0.1, 4, c("logitlink", "probitlink", "clogloglink", "cauchitlink"),
> ##D        col = c("limegreen", "purple", "chocolate", "tan"), lwd = mylwd)
> ##D par(lwd = 1) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("profilevglm")
> ### * profilevglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: profilevglm
> ### Title: Method for Profiling vglm Objects
> ### Aliases: profilevglm
> ### Keywords: regression models
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let, propodds,
+              trace = TRUE, data = pneumo)
VGLM    linear loop  1 :  deviance = 5.10322
VGLM    linear loop  2 :  deviance = 5.026838
VGLM    linear loop  3 :  deviance = 5.026826
VGLM    linear loop  4 :  deviance = 5.026826
> pfit1 <- profile(fit1, trace = FALSE)
> confint(fit1, method = "profile", trace = FALSE)
                   2.5 %    97.5 %
(Intercept):1 -12.491514 -7.300884
(Intercept):2 -13.436780 -8.165407
let             1.907272  3.401708
> 
> 
> 
> cleanEx()
> nameEx("propodds")
> ### * propodds
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: propodds
> ### Title: Proportional Odds Model for Ordinal Regression
> ### Aliases: propodds
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Fit the proportional odds model, McCullagh and Nelder (1989,p.179)
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> depvar(fit)  # Sample proportions
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> weights(fit, type = "prior")  # Number of observations
  [,1]
1   98
2   54
3   43
4   48
5   51
6   38
7   28
8   11
> coef(fit, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> constraints(fit)  # Constraint matrices
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> summary(fit)
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = propodds, 
    data = pneumo)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -9.6761     1.3241  -7.308 2.72e-13 ***
(Intercept):2 -10.5817     1.3454  -7.865 3.69e-15 ***
let             2.5968     0.3811   6.814 9.50e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 5.0268 on 13 degrees of freedom

Log-likelihood: -25.0903 on 13 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'


Exponentiated coefficients:
     let 
13.42081 
> 
> # Check that the model is linear in let ----------------------
> fit2 <- vgam(cbind(normal, mild, severe) ~ s(let, df = 2), propodds,
+              pneumo)
> ## Not run:  plot(fit2, se = TRUE, lcol = 2, scol = 2) 
> 
> # Check the proportional odds assumption with a LRT ----------
> (fit3 <- vglm(cbind(normal, mild, severe) ~ let,
+               cumulative(parallel = FALSE, reverse = TRUE), pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = FALSE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
    -9.593308    -11.104791      2.571300      2.743550 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.884404 
Log-likelihood: -25.01905 
> pchisq(deviance(fit) - deviance(fit3),
+        df = df.residual(fit) - df.residual(fit3), lower.tail = FALSE)
[1] 0.7058849
> lrtest(fit3, fit)  # Easier
Likelihood ratio test

Model 1: cbind(normal, mild, severe) ~ let
Model 2: cbind(normal, mild, severe) ~ let
  #Df  LogLik Df  Chisq Pr(>Chisq)
1  12 -25.019                     
2  13 -25.090  1 0.1424     0.7059
> 
> 
> 
> cleanEx()
> nameEx("prplot")
> ### * prplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prplot
> ### Title: Probability Plots for Categorical Data Analysis
> ### Aliases: prplot prplot.control
> ### Keywords: regression dplot hplot
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, data = pneumo)
> M <- npred(fit)  # Or fit@misc$M
> ## Not run: 
> ##D  prplot(fit)
> ##D prplot(fit, lty = 1:M, col = (1:M)+2, rug = TRUE, las = 1,
> ##D        ylim = c(0, 1), rlwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("put.smart")
> ### * put.smart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: put.smart
> ### Title: Adds a List to the End of the List ".smart.prediction"
> ### Aliases: put.smart
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> print(sm.min1)
function (x) 
{
    x <- x
    minx <- min(x)
    if (smart.mode.is("read")) {
        smart <- get.smart()
        minx <- smart$minx
    }
    else if (smart.mode.is("write")) 
        put.smart(list(minx = minx))
    minx
}
<bytecode: 0x556f6bf06bc8>
<environment: namespace:VGAM>
attr(,"smart")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("qrrvglm.control")
> ### * qrrvglm.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qrrvglm.control
> ### Title: Control Function for QRR-VGLMs (CQO)
> ### Aliases: qrrvglm.control
> ### Keywords: optimize models regression nonlinear
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  # Poisson CQO with equal tolerances
> ##D set.seed(111)  # This leads to the global solution
> ##D hspider[,1:6] <- scale(hspider[,1:6])  # Good when I.tolerances = TRUE
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr,
> ##D                 Arctlute, Arctperi, Auloalbi,
> ##D                 Pardlugu, Pardmont, Pardnigr,
> ##D                 Pardpull, Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig +
> ##D           CoveMoss + CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, eq.tolerances = TRUE)
> ##D sort(deviance(p1, history = TRUE))  # Iteration history
> ##D 
> ##D (isd.latvar <- apply(latvar(p1), 2, sd))  # Approx isd.latvar
> ##D 
> ##D # Refit the model with better initial values
> ##D set.seed(111)  # This leads to the global solution
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr,
> ##D                 Arctlute, Arctperi, Auloalbi,
> ##D                 Pardlugu, Pardmont, Pardnigr,
> ##D                 Pardpull, Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig +
> ##D           CoveMoss + CoveHerb + ReflLux,
> ##D           I.tolerances = TRUE, poissonff, data = hspider,
> ##D           isd.latvar = isd.latvar)  # Note this
> ##D sort(deviance(p1, history = TRUE))  # Iteration history
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("qtplot.gumbel")
> ### * qtplot.gumbel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qtplot.gumbel
> ### Title: Quantile Plot for Gumbel Regression
> ### Aliases: qtplot.gumbel qtplot.gumbelff
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> ymat <- as.matrix(venice[, paste("r", 1:10, sep = "")])
> fit1 <- vgam(ymat ~ s(year, df = 3), gumbel(R = 365, mpv = TRUE),
+              data = venice, trace = TRUE, na.action = na.pass)
VGAM  s.vam  loop  1 :  loglikelihood = -1137.5884
VGAM  s.vam  loop  2 :  loglikelihood = -1088.6181
VGAM  s.vam  loop  3 :  loglikelihood = -1079.7142
VGAM  s.vam  loop  4 :  loglikelihood = -1078.882
VGAM  s.vam  loop  5 :  loglikelihood = -1078.7252
VGAM  s.vam  loop  6 :  loglikelihood = -1078.713
VGAM  s.vam  loop  7 :  loglikelihood = -1078.7071
VGAM  s.vam  loop  8 :  loglikelihood = -1078.707
VGAM  s.vam  loop  9 :  loglikelihood = -1078.7066
VGAM  s.vam  loop  10 :  loglikelihood = -1078.7066
> head(fitted(fit1))
       95%      99%      MPV
1 68.17273 90.04047 112.6121
2 68.46769 90.29102 112.8168
3 68.76404 90.54248 113.0219
4 69.05527 90.78985 113.2240
5 69.33842 91.03085 113.4215
6 69.61724 91.26808 113.6158
> 
> ## Not run: 
> ##D  par(mfrow = c(1, 1), bty = "l", xpd = TRUE, las = 1)
> ##D qtplot(fit1, mpv = TRUE, lcol = c(1, 2, 5), tcol = c(1, 2, 5),
> ##D        lwd = 2, pcol = "blue", tadj = 0.4, ylab = "Sea level (cm)")
> ##D 
> ##D qtplot(fit1, perc = 97, mpv = FALSE, lcol = 3, tcol = 3,
> ##D        lwd = 2, tadj = 0.4, add = TRUE) -> saved
> ##D head(saved@post$qtplot$fitted)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("qtplot.lmscreg")
> ### * qtplot.lmscreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qtplot.lmscreg
> ### Title: Quantile Plot for LMS Quantile Regression
> ### Aliases: qtplot.lmscreg
> ### Keywords: regression hplot
> 
> ### ** Examples
> ## Not run: 
> ##D fit <- vgam(BMI ~ s(age, df = c(4, 2)), lms.bcn(zero=1), bmi.nz)
> ##D qtplot(fit)
> ##D qtplot(fit, perc = c(25, 50, 75, 95), lcol = 4, tcol = 4, llwd = 2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("qvar")
> ### * qvar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qvar
> ### Title: Quasi-variances Extraction Function
> ### Aliases: qvar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> data("ships", package = "MASS")
> Shipmodel <- vglm(incidents ~ type + year + period,
+                   poissonff, offset = log(service),
+                   data = ships, subset = (service > 0))
> 
> # Easiest form of input
> fit1 = rcim(Qvar(Shipmodel, "type"), uninormal("explink"), maxit=99)
> qvar(fit1)             # Quasi-variances
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> qvar(fit1, se = TRUE)  # Quasi-standard errors
[1] 0.15574074 0.08631623 0.28812676 0.24622326 0.17557954
> 
> # Manually compute them:
> (quasiVar <- exp(diag(fitted(fit1))) / 2)                # Version 1
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (quasiVar <- diag(predict(fit1)[, c(TRUE, FALSE)]) / 2)  # Version 2
[1] 0.024255179 0.007450492 0.083017027 0.060625895 0.030828174
> (quasiSE  <- sqrt(quasiVar))
[1] 0.15574074 0.08631623 0.28812676 0.24622326 0.17557954
> 
> ## Not run: 
> ##D  qvplot(fit1, col = "green", lwd = 3, scol = "blue",
> ##D      slwd = 2, las = 1) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rayleigh")
> ### * rayleigh
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rayleigh
> ### Title: Rayleigh Regression Family Function
> ### Aliases: rayleigh cens.rayleigh
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 1000; Scale <- exp(2)
> rdata <- data.frame(ystar = rrayleigh(nn, scale = Scale))
> fit <- vglm(ystar ~ 1, rayleigh, data = rdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2936.5403
VGLM    linear loop  2 :  loglikelihood = -2936.5397
VGLM    linear loop  3 :  loglikelihood = -2936.5397
> head(fitted(fit))
      [,1]
1 9.201324
2 9.201324
3 9.201324
4 9.201324
5 9.201324
6 9.201324
> with(rdata, mean(ystar))
[1] 9.222639
> coef(fit, matrix = TRUE)
            loglink(scale)
(Intercept)       1.993556
> Coef(fit)
   scale 
7.341594 
> 
> # Censored data
> rdata <- transform(rdata, U = runif(nn, 5, 15))
> rdata <- transform(rdata, y = pmin(U, ystar))
> ## Not run: 
> ##D  par(mfrow = c(1, 2))
> ##D hist(with(rdata, ystar)); hist(with(rdata, y)) 
> ## End(Not run)
> extra <- with(rdata, list(rightcensored = ystar > U))
> fit <- vglm(y ~ 1, cens.rayleigh, data = rdata, trace = TRUE,
+             extra = extra, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.7442216
VGLM    linear loop  2 :  coefficients = 1.9202851
VGLM    linear loop  3 :  coefficients = 1.9761977
VGLM    linear loop  4 :  coefficients = 1.9953321
VGLM    linear loop  5 :  coefficients = 2.0021043
VGLM    linear loop  6 :  coefficients = 2.0045311
VGLM    linear loop  7 :  coefficients = 2.0054047
VGLM    linear loop  8 :  coefficients = 2.0057196
VGLM    linear loop  9 :  coefficients = 2.0058332
VGLM    linear loop  10 :  coefficients = 2.0058742
VGLM    linear loop  11 :  coefficients = 2.005889
VGLM    linear loop  12 :  coefficients = 2.0058944
VGLM    linear loop  13 :  coefficients = 2.0058963
VGLM    linear loop  14 :  coefficients = 2.005897
VGLM    linear loop  15 :  coefficients = 2.0058972
VGLM    linear loop  16 :  coefficients = 2.0058973
> table(fit@extra$rightcen)

FALSE  TRUE 
  566   434 
> coef(fit, matrix = TRUE)
            loglink(scale)
(Intercept)       2.005897
> head(fitted(fit))
      [,1]
1 9.315584
2 9.315584
3 9.315584
4 9.315584
5 9.315584
6 9.315584
> 
> 
> 
> cleanEx()
> nameEx("rayleighUC")
> ### * rayleighUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rayleigh
> ### Title: Rayleigh Distribution
> ### Aliases: Rayleigh drayleigh prayleigh qrayleigh rrayleigh
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  Scale <- 2; x <- seq(-1, 8, by = 0.1)
> ##D plot(x, drayleigh(x, scale = Scale), type = "l", ylim = c(0,1),
> ##D   las = 1, ylab = "",
> ##D   main = "Rayleigh density divided into 10 equal areas; red = CDF")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D qq <- qrayleigh(seq(0.1, 0.9, by = 0.1), scale = Scale)
> ##D lines(qq, drayleigh(qq, scale = Scale), col = 2, lty = 3, type = "h")
> ##D lines(x, prayleigh(x, scale = Scale), col = "red") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rcqo")
> ### * rcqo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rcqo
> ### Title: Constrained Quadratic Ordination
> ### Aliases: rcqo
> ### Keywords: distribution datagen
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1: Species packing model:
> ##D n <- 100; p <- 5; S <- 5
> ##D mydata <- rcqo(n, p, S, es.opt = TRUE, eq.max = TRUE)
> ##D names(mydata)
> ##D (myform <- attr(mydata, "formula"))
> ##D fit <- cqo(myform, poissonff, mydata, Bestof = 3)  # eq.tol = TRUE
> ##D matplot(attr(mydata, "latvar"), mydata[,-(1:(p-1))], col = 1:S)
> ##D persp(fit, col = 1:S, add = TRUE)
> ##D lvplot(fit, lcol = 1:S, y = TRUE, pcol = 1:S)  # Same plot as above
> ##D 
> ##D # Compare the fitted model with the 'truth'
> ##D concoef(fit)  # The fitted model
> ##D attr(mydata, "concoefficients")  # The 'truth'
> ##D 
> ##D c(apply(attr(mydata, "latvar"), 2, sd),
> ##D   apply(latvar(fit), 2, sd))  # Both values should be approx equal
> ##D 
> ##D 
> ##D # Example 2: negative binomial data fitted using a Poisson model:
> ##D n <- 200; p <- 5; S <- 5
> ##D mydata <- rcqo(n, p, S, fam = "negbin", sqrt = TRUE)
> ##D myform <- attr(mydata, "formula")
> ##D fit <- cqo(myform, fam = poissonff, dat = mydata)  # I.tol = TRUE,
> ##D lvplot(fit, lcol = 1:S, y = TRUE, pcol = 1:S)
> ##D # Compare the fitted model with the 'truth'
> ##D concoef(fit)  # The fitted model
> ##D attr(mydata, "concoefficients")  # The 'truth'
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rdiric")
> ### * rdiric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rdiric
> ### Title: The Dirichlet distribution
> ### Aliases: rdiric
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ddata <- data.frame(rdiric(n = 1000, shape = c(y1 = 3, y2 = 1, y3 = 4)))
> fit <- vglm(cbind(y1, y2, y3) ~ 1, dirichlet, data = ddata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 858.01191
VGLM    linear loop  2 :  loglikelihood = 5383.3056
VGLM    linear loop  3 :  loglikelihood = 13637.842
VGLM    linear loop  4 :  loglikelihood = 19119.802
VGLM    linear loop  5 :  loglikelihood = 19942.6
VGLM    linear loop  6 :  loglikelihood = 19954.666
VGLM    linear loop  7 :  loglikelihood = 19954.668
VGLM    linear loop  8 :  loglikelihood = 19954.668
> Coef(fit)
  shape1   shape2   shape3 
3.088759 1.125154 4.121847 
> coef(fit, matrix = TRUE)
            loglink(shape1) loglink(shape2) loglink(shape3)
(Intercept)        1.127769       0.1179199        1.416301
> 
> 
> 
> cleanEx()
> nameEx("rec.exp1")
> ### * rec.exp1
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rec.exp1
> ### Title: Upper Record Values from a 1-parameter Exponential Distribution
> ### Aliases: rec.exp1
> ### Keywords: models regression
> 
> ### ** Examples
> 
> rawy <- rexp(n <- 10000, rate = exp(1))
> y <- unique(cummax(rawy))  # Keep only the records
> 
> length(y) / y[length(y)]   # MLE of rate
[1] 2.959656
> 
> fit <- vglm(y ~ 1, rec.exp1, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 0.85073099
> coef(fit, matrix = TRUE)
            loglink(rate)
(Intercept)      1.085073
> Coef(fit)
    rate 
2.959656 
> 
> 
> 
> cleanEx()
> nameEx("rec.normal")
> ### * rec.normal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rec.normal
> ### Title: Upper Record Values from a Univariate Normal Distribution
> ### Aliases: rec.normal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> nn <- 10000; mymean <- 100
> # First value is reference value or trivial record
> Rdata <- data.frame(rawy = c(mymean, rnorm(nn, mymean, exp(3))))
> # Keep only observations that are records:
> rdata <- data.frame(y = unique(cummax(with(Rdata, rawy))))
> 
> fit <- vglm(y ~ 1, rec.normal, rdata, trace = TRUE, maxit = 200)
VGLM    linear loop  1 :  loglikelihood = -20.82555
7 weight matrices not updated out of 8 
VGLM    linear loop  2 :  loglikelihood = -20.33481
7 weight matrices not updated out of 8 
VGLM    linear loop  3 :  loglikelihood = -20.32615
7 weight matrices not updated out of 8 
VGLM    linear loop  4 :  loglikelihood = -20.32483
7 weight matrices not updated out of 8 
VGLM    linear loop  5 :  loglikelihood = -20.32471
7 weight matrices not updated out of 8 
VGLM    linear loop  6 :  loglikelihood = -20.32471
7 weight matrices not updated out of 8 
VGLM    linear loop  7 :  loglikelihood = -20.3247
7 weight matrices not updated out of 8 
VGLM    linear loop  8 :  loglikelihood = -20.3247
> coef(fit, matrix = TRUE)
                mean loglink(sd)
(Intercept) 99.99769    3.102593
> Coef(fit)
      mu       sd 
99.99769 22.25558 
> summary(fit)
Call:
vglm(formula = y ~ 1, family = rec.normal, data = rdata, trace = TRUE, 
    maxit = 200)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  99.9977     0.3541  282.43   <2e-16 ***
(Intercept):2   3.1026     0.1796   17.28   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: mean, loglink(sd)

Log-likelihood: -20.3247 on 14 degrees of freedom

Number of Fisher scoring iterations: 8 

> 
> 
> 
> cleanEx()
> nameEx("reciprocallink")
> ### * reciprocallink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: reciprocallink
> ### Title: Reciprocal Link Function
> ### Aliases: reciprocallink negreciprocallink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
>    reciprocallink(1:5)
[1] 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000
>    reciprocallink(1:5, inverse = TRUE, deriv = 2)
[1]   2  16  54 128 250
> negreciprocallink(1:5)
[1] -1.0000000 -0.5000000 -0.3333333 -0.2500000 -0.2000000
> negreciprocallink(1:5, inverse = TRUE, deriv = 2)
[1]   2  16  54 128 250
> 
> x <- (-3):3
> reciprocallink(x)  # Has Inf
[1] -0.3333333 -0.5000000 -1.0000000        Inf  1.0000000  0.5000000  0.3333333
> reciprocallink(x, bvalue = .Machine$double.eps)  # Has no Inf
[1] -3.333333e-01 -5.000000e-01 -1.000000e+00  4.503600e+15  1.000000e+00
[6]  5.000000e-01  3.333333e-01
> 
> 
> 
> cleanEx()
> nameEx("residualsvglm")
> ### * residualsvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residualsvglm
> ### Title: Residuals for a VGLM fit
> ### Aliases: residualsvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo)
> resid(fit)  # Same as having type = "working" (the default)
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1      -1.0060427611        -1.00245033
2      -0.1745467344        -0.34684069
3       0.4314586013         0.02457508
4       0.0809229590         0.41936185
5       0.0361646292        -0.07565599
6      -0.2930059007        -0.26666152
7       0.0215024659         0.05210247
8       0.0001832234         0.16583611
> resid(fit, type = "response")
         normal         mild       severe
1  5.992263e-03 -0.003560995 -0.002431267
2  1.081598e-02 -0.001396793 -0.009419182
3 -5.600275e-02  0.054440914  0.001561834
4 -1.539087e-02 -0.029468460  0.044859333
5 -8.373983e-03  0.019924341 -0.011550358
6  7.294547e-02 -0.021372077 -0.051573393
7 -5.281530e-03 -0.006498208  0.011779738
8 -4.240084e-05 -0.040198822  0.040241222
> resid(fit, type = "pearson")
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1         -0.7006808         -0.3159829
2         -0.1779045         -0.3864521
3          1.2478945         -0.3182214
4         -0.1713918          1.0444171
5          0.2553741         -0.3025368
6         -0.7714393         -0.5047617
7          0.0103928          0.1352679
8         -0.1168063          0.3307779
> resid(fit, type = "stdres")  # Test for independence
      normal        mild    severe
1  6.1471905 -3.89841752 -4.233223
2  3.1702107 -1.71442237 -2.460790
3  0.1970160  0.85353945 -1.053302
4 -0.8913183  0.04262976  1.103923
5 -2.8080105  2.37507146  1.376388
6 -2.7239929  1.75504571  1.850026
7 -4.6473611  2.03026670  4.060336
8 -3.3701809  0.88159772  3.498457
> 
> 
> 
> cleanEx()
> nameEx("rhobitlink")
> ### * rhobitlink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rhobitlink
> ### Title: Rhobit Link Function
> ### Aliases: rhobitlink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> theta <- seq(-0.99, 0.99, by = 0.01)
> y <- rhobitlink(theta)
> ## Not run: 
> ##D plot(theta, y, type = "l", ylab = "", main = "rhobitlink(theta)")
> ##D abline(v = 0, h = 0, lty = 2)
> ## End(Not run)
> 
> x <- c(seq(-1.02, -0.98, by = 0.01), seq(0.97, 1.02, by = 0.01))
> rhobitlink(x)  # Has NAs
Warning in log1p(theta) : NaNs produced
Warning in log1p(-theta) : NaNs produced
 [1]       NaN       NaN      -Inf -5.293305 -4.595120  4.184591  4.595120
 [8]  5.293305       Inf       NaN       NaN
> rhobitlink(x, bminvalue = -1 + .Machine$double.eps,
+               bmaxvalue =  1 - .Machine$double.eps)  # Has no NAs
 [1] -36.736801 -36.736801 -36.736801  -5.293305  -4.595120   4.184591
 [7]   4.595120   5.293305  36.736801  36.736801  36.736801
> 
> 
> 
> cleanEx()
> nameEx("riceUC")
> ### * riceUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rice
> ### Title: The Rice Distribution
> ### Aliases: Rice drice price qrice rrice
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(0.01, 7, len = 201)
> ##D plot(x, drice(x, vee = 0, sigma = 1), type = "n", las = 1,
> ##D      ylab = "",
> ##D      main = "Density of Rice distribution for various v values")
> ##D sigma <- 1; vee <- c(0, 0.5, 1, 2, 4)
> ##D for (ii in 1:length(vee))
> ##D   lines(x, drice(x, vee = vee[ii], sigma), col = ii)
> ##D legend(x = 5, y = 0.6, legend = as.character(vee),
> ##D        col = 1:length(vee), lty = 1)
> ##D 
> ##D x <- seq(0, 4, by = 0.01); vee <- 1; sigma <- 1
> ##D probs <- seq(0.05, 0.95, by = 0.05)
> ##D plot(x, drice(x, vee = vee, sigma = sigma), type = "l",
> ##D      main = "Blue is density, orange is CDF", col = "blue",
> ##D      ylim = c(0, 1), sub = "Red are 5, 10, ..., 95 percentiles",
> ##D      las = 1, ylab = "", cex.main = 0.9)
> ##D abline(h = 0:1, col = "black", lty = 2)
> ##D Q <- qrice(probs, sigma, vee = vee)
> ##D lines(Q, drice(qrice(probs, sigma, vee = vee),
> ##D                sigma, vee = vee), col = "red", lty = 3, type = "h")
> ##D lines(x, price(x, sigma, vee = vee), type = "l", col = "orange")
> ##D lines(Q, drice(Q, sigma, vee = vee), col = "red", lty = 3, type = "h")
> ##D lines(Q, price(Q, sigma, vee = vee), col = "red", lty = 3, type = "h")
> ##D abline(h = probs, col = "red", lty = 3)
> ##D max(abs(price(Q, sigma, vee = vee) - probs))  # Should be 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("riceff")
> ### * riceff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: riceff
> ### Title: Rice Distribution Family Function
> ### Aliases: riceff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  sigma <- exp(1); vee <- exp(2)
> ##D rdata <- data.frame(y = rrice(n <- 1000, sigma, vee = vee))
> ##D fit <- vglm(y ~ 1, riceff, data = rdata, trace = TRUE, crit = "c")
> ##D c(with(rdata, mean(y)), fitted(fit)[1])
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rigff")
> ### * rigff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rigff
> ### Title: Reciprocal Inverse Gaussian distribution
> ### Aliases: rigff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> rdata <- data.frame(y = rchisq(100, df = 14))  # Not 'proper' data!!
> fit <- vglm(y ~ 1, rigff, rdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -200.77733
VGLM    linear loop  2 :  loglikelihood = -199.89966
VGLM    linear loop  3 :  loglikelihood = -199.88877
VGLM    linear loop  4 :  loglikelihood = -199.88877
VGLM    linear loop  5 :  loglikelihood = -199.88877
> fit <- vglm(y ~ 1, rigff, rdata, trace = TRUE, crit = "c")
VGLM    linear loop  1 :  coefficients = 12.79976877, -0.27667731
VGLM    linear loop  2 :  coefficients = 12.48846937, -0.40033825
VGLM    linear loop  3 :  coefficients = 12.44519368, -0.41518153
VGLM    linear loop  4 :  coefficients = 12.44454179, -0.41539511
VGLM    linear loop  5 :  coefficients = 12.44454165, -0.41539516
VGLM    linear loop  6 :  coefficients = 12.44454165, -0.41539516
> summary(fit)
Call:
vglm(formula = y ~ 1, family = rigff, data = rdata, trace = TRUE, 
    crit = "c")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  12.4445     0.4342  28.661  < 2e-16 ***
(Intercept):2  -0.4154     0.1414  -2.937  0.00331 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: mu, loglink(lambda)

Log-likelihood: -199.8888 on 198 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("rlplot.gevff")
> ### * rlplot.gevff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rlplot.gevff
> ### Title: Return Level Plot for GEV Fits
> ### Aliases: rlplot.gevff rlplot.gev
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> gdata <- data.frame(y = rgev(n <- 100, scale = 2, shape = -0.1))
> fit <- vglm(y ~ 1, gevff, data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -207.36025
VGLM    linear loop  2 :  loglikelihood = -207.35453
VGLM    linear loop  3 :  loglikelihood = -207.35453
> 
> # Identity link for all parameters:
> fit2 <- vglm(y ~ 1, gevff(lshape = identitylink, lscale = identitylink,
+                           iscale = 10), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -266.88502
VGLM    linear loop  2 :  loglikelihood = -230.60199
VGLM    linear loop  3 :  loglikelihood = -210.42564
VGLM    linear loop  4 :  loglikelihood = -207.41874
VGLM    linear loop  5 :  loglikelihood = -207.35458
VGLM    linear loop  6 :  loglikelihood = -207.35453
VGLM    linear loop  7 :  loglikelihood = -207.35453
> coef(fit2, matrix = TRUE)
             location    scale      shape
(Intercept) 0.2217877 1.786841 -0.1441379
> ## Not run: 
> ##D par(mfrow = c(1, 2))
> ##D rlplot(fit) -> i1
> ##D rlplot(fit2, pcol = "darkorange", lcol = "blue", log.arg = FALSE,
> ##D        scol = "darkgreen", slty = "dashed", las = 1) -> i2
> ##D range(i2@post$rlplot$upper - i1@post$rlplot$upper)  # Should be near 0
> ##D range(i2@post$rlplot$lower - i1@post$rlplot$lower)  # Should be near 0
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rootogram4vglm")
> ### * rootogram4vglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rootogram4
> ### Title: Rootograms (S4 generic) for Assessing Goodness of Fit of
> ###   Probability Models
> ### Aliases: rootogram4 rootogram4vglm
> ### Keywords: models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data("hspider", package = "VGAM")  # Count responses
> ##D hs.p   <- vglm(Pardlugu ~ CoveHerb,   poissonff, data = hspider)
> ##D hs.nb  <- vglm(Pardlugu ~ CoveHerb, negbinomial, data = hspider)
> ##D hs.zip <- vglm(Pardlugu ~ CoveHerb,   zipoisson, data = hspider)
> ##D hs.zap <- vglm(Pardlugu ~ CoveHerb,   zapoisson, data = hspider)
> ##D 
> ##D opar <- par(mfrow = c(2, 2))  # Plot the rootograms
> ##D rootogram4(hs.p,   max = 15, main = "poissonff")
> ##D rootogram4(hs.nb,  max = 15, main = "negbinomial")
> ##D rootogram4(hs.zip, max = 15, main = "zipoisson")
> ##D rootogram4(hs.zap, max = 15, main = "zapoisson")
> ##D par(opar)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("round2")
> ### * round2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: round2
> ### Title: Rounding of Numbers to Base 2
> ### Aliases: round2
> ### Keywords: math
> 
> ### ** Examples
> 
> set.seed(1); x <- sort(rcauchy(10))
> x3 <- round2(x, 3)
> x3 == round2(x, 3)  # Supposed to be reliable (all TRUE)
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> rbind(x, x3)  # Comparison
        [,1]      [,2]      [,3]       [,4]       [,5]       [,6]      [,7]
x  -4.292626 -2.328624 -1.808243 -0.3305220 -0.2966426 -0.1755794 0.1965824
x3 -4.292480 -2.328613 -1.808105 -0.3305664 -0.2968750 -0.1757812 0.1967773
        [,8]     [,9]    [,10]
x  0.7346469 1.102520 2.353831
x3 0.7348633 1.102539 2.354004
> (x3[1]  * 2^(0:9)) / 2^(0:9)
 [1] -4.29248 -4.29248 -4.29248 -4.29248 -4.29248 -4.29248 -4.29248 -4.29248
 [9] -4.29248 -4.29248
> print((x3[1]  * 2^(0:11)), digits = 14)
 [1]    -4.29248046875    -8.58496093750   -17.16992187500   -34.33984375000
 [5]   -68.67968750000  -137.35937500000  -274.71875000000  -549.43750000000
 [9] -1098.87500000000 -2197.75000000000 -4395.50000000000 -8791.00000000000
> 
> # Round to approx 1 d.p.
> x1 <- round2(x, 1)
> x1 == round2(x, 1)  # Supposed to be reliable (all TRUE)
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> rbind(x, x1)
        [,1]      [,2]      [,3]      [,4]       [,5]       [,6]      [,7]
x  -4.292626 -2.328624 -1.808243 -0.330522 -0.2966426 -0.1755794 0.1965824
x1 -4.281250 -2.343750 -1.812500 -0.343750 -0.2812500 -0.1875000 0.1875000
        [,8]    [,9]    [,10]
x  0.7346469 1.10252 2.353831
x1 0.7500000 1.09375 2.343750
> x1[8] == 0.75  # 3/4
[1] TRUE
> print((x1[1]  * 2^(0:11)), digits = 9)
 [1]    -4.28125    -8.56250   -17.12500   -34.25000   -68.50000  -137.00000
 [7]  -274.00000  -548.00000 -1096.00000 -2192.00000 -4384.00000 -8768.00000
> seq(31) / 32
 [1] 0.03125 0.06250 0.09375 0.12500 0.15625 0.18750 0.21875 0.25000 0.28125
[10] 0.31250 0.34375 0.37500 0.40625 0.43750 0.46875 0.50000 0.53125 0.56250
[19] 0.59375 0.62500 0.65625 0.68750 0.71875 0.75000 0.78125 0.81250 0.84375
[28] 0.87500 0.90625 0.93750 0.96875
> 
> 
> 
> cleanEx()
> nameEx("rrar")
> ### * rrar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rrar
> ### Title: Nested Reduced-rank Autoregressive Models for Multiple Time
> ###   Series
> ### Aliases: rrar
> ### Keywords: ts regression models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D year <- seq(1961 + 1/12, 1972 + 10/12, by = 1/12)
> ##D par(mar = c(4, 4, 2, 2) + 0.1, mfrow = c(2, 2))
> ##D for (ii in 1:4) {
> ##D   plot(year, grain.us[, ii], main = names(grain.us)[ii], las = 1,
> ##D        type = "l", xlab = "", ylab = "", col = "blue")
> ##D   points(year, grain.us[, ii], pch = "*", col = "blue")
> ##D }
> ##D apply(grain.us, 2, mean)  # mu vector
> ##D cgrain <- scale(grain.us, scale = FALSE)  # Center the time series only
> ##D fit <- vglm(cgrain ~ 1, rrar(Ranks = c(4, 1)), trace = TRUE)
> ##D summary(fit)
> ##D 
> ##D print(fit@misc$Ak1, digits = 2)
> ##D print(fit@misc$Cmatrices, digits = 3)
> ##D print(fit@misc$Dmatrices, digits = 3)
> ##D print(fit@misc$omegahat, digits = 3)
> ##D print(fit@misc$Phimatrices, digits = 2)
> ##D 
> ##D par(mar = c(4, 4, 2, 2) + 0.1, mfrow = c(4, 1))
> ##D for (ii in 1:4) {
> ##D   plot(year, fit@misc$Z[, ii], main = paste("Z", ii, sep = ""),
> ##D        type = "l", xlab = "", ylab = "", las = 1, col = "blue")
> ##D   points(year, fit@misc$Z[, ii], pch = "*", col = "blue")
> ##D }
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rrvglm-class")
> ### * rrvglm-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rrvglm-class
> ### Title: Class "rrvglm"
> ### Aliases: rrvglm-class
> ### Keywords: classes
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  # Rank-1 stereotype model of Anderson (1984)
> ##D pneumo <- transform(pneumo, let = log(exposure.time),
> ##D                             x3  = runif(nrow(pneumo)))  # Noise
> ##D fit <- rrvglm(cbind(normal, mild, severe) ~ let + x3,
> ##D               multinomial, data = pneumo, Rank = 1)
> ##D Coef(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rrvglm")
> ### * rrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rrvglm
> ### Title: Fitting Reduced-Rank Vector Generalized Linear Models (RR-VGLMs)
> ###   and Doubly Constrained RR-VGLMs (DRR-VGLMs)
> ### Aliases: rrvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1: RR NB with Var(Y) = mu + delta1 * mu^delta2
> ##D nn <- 1000       # Number of observations
> ##D delta1 <- 3.0    # Specify this
> ##D delta2 <- 1.5    # Specify this; should be greater than 1
> ##D a21 <- 2 - delta2
> ##D mydata <- data.frame(x2 = runif(nn), x3 = runif(nn))
> ##D mydata <- transform(mydata, mu = exp(2 + 3 * x2 + 0 * x3))
> ##D mydata <- transform(mydata,
> ##D     y2 = rnbinom(nn, mu = mu, size = (1/delta1)*mu^a21))
> ##D plot(y2 ~ x2, mydata, pch = "+", col = 'blue', las = 1,
> ##D   main = paste0("Var(Y) = mu + ", delta1, " * mu^", delta2))
> ##D rrnb2 <- rrvglm(y2 ~ x2 + x3, negbinomial(zero = NULL),
> ##D                 data = mydata, trace = TRUE)
> ##D 
> ##D a21.hat <- (Coef(rrnb2)@A)["loglink(size)", 1]
> ##D beta11.hat <- Coef(rrnb2)@B1["(Intercept)", "loglink(mu)"]
> ##D beta21.hat <- Coef(rrnb2)@B1["(Intercept)", "loglink(size)"]
> ##D (delta1.hat <- exp(a21.hat * beta11.hat - beta21.hat))
> ##D (delta2.hat <- 2 - a21.hat)
> ##D # delta1.hat:
> ##D # exp(a21.hat * predict(rrnb2)[1,1] - predict(rrnb2)[1,2])
> ##D summary(rrnb2)
> ##D 
> ##D # Obtain a 95 percent CI for delta2:
> ##D se.a21.hat <- sqrt(vcov(rrnb2)["I(latvar.mat)", "I(latvar.mat)"])
> ##D ci.a21 <- a21.hat +  c(-1, 1) * 1.96 * se.a21.hat
> ##D (ci.delta2 <- 2 - rev(ci.a21))  # The 95 percent CI
> ##D 
> ##D Confint.rrnb(rrnb2)  # Quick way to get it
> ##D 
> ##D # Plot the abundances and fitted values vs the latent variable
> ##D plot(y2 ~ latvar(rrnb2), data = mydata, col = "blue",
> ##D      xlab = "Latent variable", las = 1)
> ##D ooo <- order(latvar(rrnb2))
> ##D lines(fitted(rrnb2)[ooo] ~ latvar(rrnb2)[ooo], col = "red")
> ##D 
> ##D # Example 2: stereotype model (RR multinomial logit model)
> ##D data(car.all)
> ##D scar <- subset(car.all,
> ##D     is.element(Country, c("Germany", "USA", "Japan", "Korea")))
> ##D fcols <- c(13,14,18:20,22:26,29:31,33,34,36)  # These are factors
> ##D scar[, -fcols] <- scale(scar[, -fcols])  # Stdze all numerical vars
> ##D ones <- CM.ones(3)  # matrix(1, 3, 1)
> ##D clist <- list("(Intercept)" = diag(3), Width = ones, Weight = ones,
> ##D               Disp. = diag(3), Tank = diag(3), Price = diag(3),
> ##D               Frt.Leg.Room = diag(3))
> ##D set.seed(111)
> ##D fit <- rrvglm(Country ~ Width + Weight + Disp. + Tank +
> ##D               Price + Frt.Leg.Room,
> ##D               multinomial, data = scar, Rank = 2, trace = TRUE,
> ##D               constraints = clist, noRRR = ~ 1 + Width + Weight,
> ##D #             Uncor = TRUE, Corner = FALSE,  # orig.
> ##D               Index.corner = c(1, 3),  # Less correlation
> ##D               Bestof = 3)
> ##D fit@misc$deviance  # A history of the fits
> ##D Coef(fit)
> ##D biplot(fit, chull = TRUE, scores = TRUE, clty = 2, Ccex = 2,
> ##D        ccol = "blue", scol = "orange", Ccol = "darkgreen",
> ##D        Clwd = 2, main = "1=Germany, 2=Japan, 3=Korea, 4=USA")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rrvglm.control")
> ### * rrvglm.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rrvglm.control
> ### Title: Control Function for rrvglm()
> ### Aliases: rrvglm.control
> ### Keywords: optimize models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(111)
> ##D pneumo <- transform(pneumo, let = log(exposure.time),
> ##D                             x3 = runif(nrow(pneumo)))  # Unrelated
> ##D fit <- rrvglm(cbind(normal, mild, severe) ~ let + x3,
> ##D               multinomial, pneumo, Rank = 1, Index.corner = 2)
> ##D constraints(fit)
> ##D vcov(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ruge")
> ### * ruge
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ruge
> ### Title: Rutherford-Geiger Polonium Data
> ### Aliases: ruge
> ### Keywords: datasets
> 
> ### ** Examples
> 
> lambdahat <- with(ruge, weighted.mean(number, w = counts))
> (N <- with(ruge, sum(counts)))
[1] 2608
> with(ruge, cbind(number, counts,
+                  fitted = round(N * dpois(number, lambdahat))))
      number counts fitted
 [1,]      0     57     54
 [2,]      1    203    210
 [3,]      2    383    407
 [4,]      3    525    525
 [5,]      4    532    508
 [6,]      5    408    394
 [7,]      6    273    254
 [8,]      7    139    141
 [9,]      8     45     68
[10,]      9     27     29
[11,]     10     10     11
[12,]     11      4      4
[13,]     12      0      1
[14,]     13      1      0
[15,]     14      1      0
> 
> 
> 
> cleanEx()
> nameEx("s")
> ### * s
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: s
> ### Title: Defining Smooths in VGAM Formulas
> ### Aliases: s
> ### Keywords: models regression smooth
> 
> ### ** Examples
> 
> # Nonparametric logistic regression
> fit1 <- vgam(agaaus ~ s(altitude, df = 2), binomialff, data = hunua)
> ## Not run:  plot(fit1, se = TRUE) 
> 
> # Bivariate logistic model with artificial data
> nn <- 300
> bdata <- data.frame(x1 = runif(nn), x2 = runif(nn))
> bdata <- transform(bdata,
+     y1 = rbinom(nn, size = 1, prob = logitlink(sin(2 * x2), inverse = TRUE)),
+     y2 = rbinom(nn, size = 1, prob = logitlink(sin(2 * x2), inverse = TRUE)))
> fit2 <- vgam(cbind(y1, y2) ~ x1 + s(x2, 3), trace = TRUE,
+              binom2.or(exchangeable = TRUE), data = bdata)
VGAM  s.vam  loop  1 :  deviance = 741.16592
VGAM  s.vam  loop  2 :  deviance = 734.07788
VGAM  s.vam  loop  3 :  deviance = 734.02669
VGAM  s.vam  loop  4 :  deviance = 734.02595
VGAM  s.vam  loop  5 :  deviance = 734.02598
VGAM  s.vam  loop  6 :  deviance = 734.02598
> coef(fit2, matrix = TRUE)  # Hard to interpret
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)      0.7703412      0.7703412       -0.266486
x1              -0.6810306     -0.6810306        0.000000
s(x2, 3)         0.7033183      0.7033183        0.000000
> ## Not run:  plot(fit2, se = TRUE, which.term = 2, scol = "blue") 
> 
> 
> 
> cleanEx()
> nameEx("sc.studentt2")
> ### * sc.studentt2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sc.studentt2
> ### Title: Scaled Student t Distribution with 2 df Family Function
> ### Aliases: sc.studentt2
> ### Keywords: models regression
> 
> ### ** Examples
> 
> set.seed(123); nn <- 1000
> kdata <- data.frame(x2 = sort(runif(nn)))
> kdata <- transform(kdata, mylocat = 1 + 3 * x2,
+                           myscale = 1)
> kdata <- transform(kdata, y = rsc.t2(nn, loc = mylocat, scale = myscale))
> fit  <- vglm(y ~ x2, sc.studentt2(perc = c(1, 50, 99)), data = kdata)
> fit2 <- vglm(y ~ x2,    studentt2(df = 2), data = kdata)  # 'same' as fit
> 
> coef(fit, matrix = TRUE)
             location loglink(scale)
(Intercept) 0.9681963     -0.0274607
x2          3.0650877      0.0000000
> head(fitted(fit))
         1%       50%      99%
1 -8.612957 0.9696226 10.55220
2 -8.612469 0.9701113 10.55269
3 -8.610731 0.9718487 10.55443
4 -8.603091 0.9794888 10.56207
5 -8.602441 0.9801389 10.56272
6 -8.601795 0.9807844 10.56336
> head(predict(fit))
      location loglink(scale)
[1,] 0.9696226     -0.0274607
[2,] 0.9701113     -0.0274607
[3,] 0.9718487     -0.0274607
[4,] 0.9794888     -0.0274607
[5,] 0.9801389     -0.0274607
[6,] 0.9807844     -0.0274607
> 
> # Nice plot of the results
> ## Not run: 
> ##D  plot(y ~ x2, data = kdata, col = "blue", las = 1,
> ##D      sub  = paste("n =", nn),
> ##D      main = "Fitted quantiles/expectiles using the sc.studentt2() distribution")
> ##D matplot(with(kdata, x2), fitted(fit), add = TRUE, type = "l", lwd = 3)
> ##D legend("bottomright", lty = 1:3, lwd = 3, legend = colnames(fitted(fit)),
> ##D        col = 1:3) 
> ## End(Not run)
> 
> fit@extra$percentile  # Sample quantiles
  1%  50%  99% 
 0.8 50.6 98.4 
> 
> 
> 
> cleanEx()
> nameEx("sc.t2UC")
> ### * sc.t2UC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Expectiles-sc.t2
> ### Title: Expectiles/Quantiles of the Scaled Student t Distribution with 2
> ###   Df
> ### Aliases: Expectiles-sc.t2 dsc.t2 psc.t2 qsc.t2 rsc.t2
> ### Keywords: distribution
> 
> ### ** Examples
> 
> my.p <- 0.25; y <- rsc.t2(nn <- 5000)
> (myexp <- qsc.t2(my.p))
[1] -1.154701
> sum(myexp - y[y <= myexp]) / sum(abs(myexp - y))  # Should be my.p
[1] 0.2583803
> # Equivalently:
> I1 <- mean(y <= myexp) * mean( myexp - y[y <= myexp])
> I2 <- mean(y >  myexp) * mean(-myexp + y[y >  myexp])
> I1 / (I1 + I2)  # Should be my.p
[1] 0.2583803
> # Or:
> I1 <- sum( myexp - y[y <= myexp])
> I2 <- sum(-myexp + y[y >  myexp])
> 
> # Non-standard Koenker distribution
> myloc <- 1; myscale <- 2
> yy <- rsc.t2(nn, myloc, myscale)
> (myexp <- qsc.t2(my.p, myloc, myscale))
[1] -1.309401
> sum(myexp - yy[yy <= myexp]) / sum(abs(myexp - yy))  # Should be my.p
[1] 0.2531129
> psc.t2(mean(yy), myloc, myscale)  # Should be 0.5
[1] 0.4900412
> abs(qsc.t2(0.5, myloc, myscale) - mean(yy))  # Should be 0
[1] 0.07968602
> abs(psc.t2(myexp, myloc, myscale) - my.p)  # Should be 0
[1] 0
> integrate(f = dsc.t2, lower = -Inf, upper = Inf,
+           locat = myloc, scale = myscale)  # Should be 1
1 with absolute error < 8e-08
> 
> y <- seq(-7, 7, len = 201)
> max(abs(dsc.t2(y) - dt(y / sqrt(2), df = 2) / sqrt(2)))  # Should be 0
[1] 8.326673e-17
> ## Not run: 
> ##D  plot(y, dsc.t2(y), type = "l", col = "blue", las = 1,
> ##D      ylim = c(0, 0.4), main = "Blue = Koenker; orange = N(0, 1)")
> ##D lines(y, dnorm(y), type = "l", col = "orange")
> ##D abline(h = 0, v = 0, lty = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("score.stat")
> ### * score.stat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: score.stat
> ### Title: Rao's Score Test Statistics Evaluated at the Null Values
> ### Aliases: score.stat score.stat.vlm
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> set.seed(1)
> pneumo <- transform(pneumo, let = log(exposure.time),
+                             x3 = rnorm(nrow(pneumo)))
> (pfit <- vglm(cbind(normal, mild, severe) ~ let + x3, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let + x3, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let            x3 
  -9.66744415  -10.57344562    2.58865720    0.08444356 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.763992 
Log-likelihood: -24.95885 
> score.stat(pfit)  # No HDE here; should be similar to the next line:
      let        x3 
8.3104839 0.5099257 
> coef(summary(pfit))[, "z value"]  # Wald statistics computed at the MLE
(Intercept):1 (Intercept):2           let            x3 
   -7.2264190    -7.7810263     6.7225483     0.5173993 
> summary(pfit, score0 = TRUE)
Call:
vglm(formula = cbind(normal, mild, severe) ~ let + x3, family = propodds, 
    data = pneumo)

Rao score test coefficients: 
    Estimate Std. Error z value Pr(>|z|)    
let  2.58866    0.19627    8.31   <2e-16 ***
x3   0.08444    0.16416    0.51     0.61    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 4.764 on 12 degrees of freedom

Log-likelihood: -24.9588 on 12 degrees of freedom

Number of Fisher scoring iterations: 4 


Exponentiated coefficients:
      let        x3 
13.311884  1.088111 
> 
> 
> 
> cleanEx()
> nameEx("seglines")
> ### * seglines
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: seglines
> ### Title: Hauck-Donner Effects: Segmented Lines Plot
> ### Aliases: seglines
> ### Keywords: models regression
> 
> ### ** Examples
> 
> deg <- 4  # myfun is a function that approximates the HDE
> myfun <- function(x, deriv = 0) switch(as.character(deriv),
+   '0' = x^deg * exp(-x),
+   '1' = (deg * x^(deg-1) - x^deg) * exp(-x),
+   '2' = (deg * (deg-1) * x^(deg-2) - 2*deg * x^(deg-1) +
+          x^deg) * exp(-x))
> ## Not run: 
> ##D curve(myfun, 0, 10, col = "white")
> ##D xgrid <- seq(0, 10, length = 101)
> ##D seglines(xgrid, myfun(xgrid), myfun(xgrid, deriv = 1),
> ##D          COPS0 = 2,
> ##D          myfun(xgrid, deriv = 2), pch.table = NULL,
> ##D          position = "bottom")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("seq2binomial")
> ### * seq2binomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: seq2binomial
> ### Title: The Two-stage Sequential Binomial Distribution Family Function
> ### Aliases: seq2binomial
> ### Keywords: models regression
> 
> ### ** Examples
> 
> sdata <- data.frame(mvector = round(rnorm(nn <- 100, m = 10, sd = 2)),
+                     x2 = runif(nn))
> sdata <- transform(sdata, prob1 = logitlink(+2 - x2, inverse = TRUE),
+                           prob2 = logitlink(-2 + x2, inverse = TRUE))
> sdata <- transform(sdata, successes1 = rbinom(nn, size = mvector,    prob = prob1))
> sdata <- transform(sdata, successes2 = rbinom(nn, size = successes1, prob = prob2))
> sdata <- transform(sdata, y1 = successes1 / mvector)
> sdata <- transform(sdata, y2 = successes2 / successes1)
> fit <- vglm(cbind(y1, y2) ~ x2, seq2binomial, weight = mvector,
+             data = sdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -305.34115
VGLM    linear loop  2 :  loglikelihood = -305.22864
VGLM    linear loop  3 :  loglikelihood = -305.22862
VGLM    linear loop  4 :  loglikelihood = -305.22862
> coef(fit)
(Intercept):1 (Intercept):2          x2:1          x2:2 
    2.0596186    -1.8856620    -1.1859472     0.7877133 
> coef(fit, matrix = TRUE)
            logitlink(prob1) logitlink(prob2)
(Intercept)         2.059619       -1.8856620
x2                 -1.185947        0.7877133
> head(fitted(fit))
      prob1     prob2
1 0.8509876 0.1577664
2 0.8581875 0.1527191
3 0.8094939 0.1856429
4 0.8507705 0.1579174
5 0.8635110 0.1489382
6 0.8091682 0.1858549
> head(depvar(fit))
         y1        y2
1 0.7777778 0.1428571
2 1.0000000 0.0000000
3 0.8750000 0.4285714
4 0.8461538 0.2727273
5 0.9090909 0.3000000
6 0.5000000 0.2500000
> head(weights(fit, type = "prior"))  # Same as with(sdata, mvector)
     [,1]
[1,]    9
[2,]   10
[3,]    8
[4,]   13
[5,]   11
[6,]    8
> # Number of first successes:
> head(depvar(fit)[, 1] * c(weights(fit, type = "prior")))
 1  2  3  4  5  6 
 7 10  7 11 10  4 
> # Number of second successes:
> head(depvar(fit)[, 2] * c(weights(fit, type = "prior")) *
+                           depvar(fit)[, 1])
1 2 3 4 5 6 
1 0 3 3 3 1 
> 
> 
> 
> cleanEx()
> nameEx("setup.smart")
> ### * setup.smart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: setup.smart
> ### Title: Smart Prediction Setup
> ### Aliases: setup.smart
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> ## Not run: 
> ##D setup.smart("write")  # Put at the beginning of lm
> ## End(Not run)
> 
> ## Not run: 
> ##D # Put at the beginning of predict.lm
> ##D setup.smart("read", smart.prediction = object$smart.prediction)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("simplex")
> ### * simplex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simplex
> ### Title: Simplex Distribution Family Function
> ### Aliases: simplex
> ### Keywords: models regression
> 
> ### ** Examples
> 
> sdata <- data.frame(x2 = runif(nn <- 1000))
> sdata <- transform(sdata, eta1 = 1 + 2 * x2,
+                           eta2 = 1 - 2 * x2)
> sdata <- transform(sdata, y = rsimplex(nn, mu = logitlink(eta1, inverse = TRUE),
+                                        dispersion = exp(eta2)))
> (fit <- vglm(y ~ x2, simplex(zero = NULL), data = sdata, trace = TRUE))
VGLM    linear loop  1 :  loglikelihood = 1439.766
VGLM    linear loop  2 :  loglikelihood = 1790.8455
VGLM    linear loop  3 :  loglikelihood = 2013.2252
VGLM    linear loop  4 :  loglikelihood = 2104.1144
VGLM    linear loop  5 :  loglikelihood = 2121.8423
VGLM    linear loop  6 :  loglikelihood = 2122.7209
VGLM    linear loop  7 :  loglikelihood = 2122.7317
VGLM    linear loop  8 :  loglikelihood = 2122.7318
VGLM    linear loop  9 :  loglikelihood = 2122.7318

Call:
vglm(formula = y ~ x2, family = simplex(zero = NULL), data = sdata, 
    trace = TRUE)


Coefficients:
(Intercept):1 (Intercept):2          x2:1          x2:2 
    1.0252945     0.9705606     1.9612338    -1.9041255 

Degrees of Freedom: 2000 Total; 1996 Residual
Log-likelihood: 2122.732 
> coef(fit, matrix = TRUE)
            logitlink(mu) loglink(sigma)
(Intercept)      1.025295      0.9705606
x2               1.961234     -1.9041255
> summary(fit)
Call:
vglm(formula = y ~ x2, family = simplex(zero = NULL), data = sdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  1.02529    0.02817   36.40   <2e-16 ***
(Intercept):2  0.97056    0.04475   21.69   <2e-16 ***
x2:1           1.96123    0.03356   58.45   <2e-16 ***
x2:2          -1.90413    0.07758  -24.55   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(mu), loglink(sigma)

Log-likelihood: 2122.732 on 1996 degrees of freedom

Number of Fisher scoring iterations: 9 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("simplexUC")
> ### * simplexUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Simplex 
> ### Title: Simplex Distribution
> ### Aliases: dsimplex rsimplex
> ### Keywords: distribution
> 
> ### ** Examples
> 
> sigma <- c(4, 2, 1)  # Dispersion parameter
> mymu  <- c(0.1, 0.5, 0.7); xxx <- seq(0, 1, len = 501)
> ## Not run: 
> ##D  par(mfrow = c(3, 3))  # Figure 2.1 of Song (2007)
> ##D for (iii in 1:3)
> ##D   for (jjj in 1:3) {
> ##D     plot(xxx, dsimplex(xxx, mymu[jjj], sigma[iii]),
> ##D          type = "l", col = "blue", xlab = "", ylab = "", main =
> ##D          paste("mu = ", mymu[jjj], ", sigma = ", sigma[iii], sep = "")) } 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("simulate.vlm")
> ### * simulate.vlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simulate.vlm
> ### Title: Simulate Responses for VGLMs and VGAMs
> ### Aliases: simulate.vlm
> ### Keywords: models datagen
> 
> ### ** Examples
> 
> nn <- 10; mysize <- 20; set.seed(123)
> bdata <- data.frame(x2 = rnorm(nn))
> bdata <- transform(bdata,
+   y1   = rbinom(nn, size = mysize, p = logitlink(1+x2, inverse = TRUE)),
+   y2   = rbinom(nn, size = mysize, p = logitlink(1+x2, inverse = TRUE)),
+   f1   = factor(as.numeric(rbinom(nn, size = 1,
+                                   p = logitlink(1+x2, inverse = TRUE)))))
> (fit1 <- vglm(cbind(y1, aaa = mysize - y1) ~ x2,  # Matrix response (2-colns)
+               binomialff, data = bdata))

Call:
vglm(formula = cbind(y1, aaa = mysize - y1) ~ x2, family = binomialff, 
    data = bdata)


Coefficients:
(Intercept)          x2 
  0.7631782   0.7719893 

Degrees of Freedom: 10 Total; 8 Residual
Residual deviance: 7.644748 
Log-likelihood: -19.68401 
> (fit2 <- vglm(f1 ~ x2, binomialff, model = TRUE, data = bdata)) # Factor response

Call:
vglm(formula = f1 ~ x2, family = binomialff, data = bdata, model = TRUE)


Coefficients:
(Intercept)          x2 
   3.202873    4.247620 

Degrees of Freedom: 10 Total; 8 Residual
Residual deviance: 5.485974 
Log-likelihood: -2.742987 
> 
> set.seed(123); simulate(fit1, nsim = 8)
   sim_1 sim_2 sim_3 sim_4 sim_5 sim_6 sim_7 sim_8
1   0.65  0.40  0.45  0.40  0.70  0.75  0.55  0.50
2   0.55  0.65  0.60  0.50  0.65  0.65  0.80  0.60
3   0.90  0.85  0.85  0.85  0.90  0.80  0.90  0.85
4   0.55  0.70  0.40  0.60  0.75  0.80  0.75  1.00
5   0.55  0.85  0.65  0.90  0.80  0.70  0.60  0.70
6   1.00  0.80  0.85  0.90  0.95  0.95  0.90  0.95
7   0.75  0.80  0.75  0.70  0.85  0.85  0.65  0.80
8   0.60  0.25  0.45  0.35  0.45  0.50  0.55  0.50
9   0.55  0.60  0.60  0.60  0.65  0.40  0.45  0.60
10  0.60  0.40  0.70  0.70  0.50  0.65  0.60  0.75
> set.seed(123); c(simulate(fit2, nsim = 3))  # Use c() when model = TRUE
$sim_1
 [1] 1 1 1 1 1 1 1 0 1 1
Levels: 0 1

$sim_2
 [1] 0 1 1 1 1 1 1 0 1 0
Levels: 0 1

$sim_3
 [1] 0 1 1 0 1 1 1 0 1 1
Levels: 0 1

> 
> # An n x N x F example
> set.seed(123); n <- 100
> bdata <- data.frame(x2 = runif(n), x3 = runif(n))
> bdata <- transform(bdata, y1 = rnorm(n, 1 + 2 * x2),
+                           y2 = rnorm(n, 3 + 4 * x2))
> fit1 <- vglm(cbind(y1, y2) ~ x2, binormal(eq.sd = TRUE), data = bdata)
> nsim <- 1000  # Number of simulations for each observation
> my.sims <- simulate(fit1, nsim = nsim)
> dim(my.sims)  # A data frame
[1]  200 1000
> aaa <- array(unlist(my.sims), c(n, nsim, ncol(fitted(fit1))))  # n by N by F
> summary(rowMeans(aaa[, , 1]) - fitted(fit1)[, 1])  # Should be all 0s
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-0.0659074 -0.0242755 -0.0003540  0.0009241  0.0238505  0.0802246 
> summary(rowMeans(aaa[, , 2]) - fitted(fit1)[, 2])  # Should be all 0s
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-0.054374 -0.015796  0.002093  0.004902  0.024698  0.090122 
> 
> # An n x F x N example
> n <- 100; set.seed(111); nsim <- 1000
> zdata <- data.frame(x2 = runif(n))
> zdata <- transform(zdata, lambda1 =  loglink(-0.5 + 2 * x2, inverse = TRUE),
+                           lambda2 =  loglink( 0.5 + 2 * x2, inverse = TRUE),
+                           pstr01  = logitlink( 0,            inverse = TRUE),
+                           pstr02  = logitlink(-1.0,          inverse = TRUE))
> zdata <- transform(zdata, y1 = rzipois(n, lambda = lambda1, pstr0 = pstr01),
+                           y2 = rzipois(n, lambda = lambda2, pstr0 = pstr02))
> zip.fit  <- vglm(cbind(y1, y2) ~ x2, zipoissonff, data = zdata, crit = "coef")
> my.sims <- simulate(zip.fit, nsim = nsim)
> dim(my.sims)  # A data frame
[1]  200 1000
> aaa <- array(unlist(my.sims), c(n, ncol(fitted(zip.fit)), nsim))  # n by F by N
> summary(rowMeans(aaa[, 1, ]) - fitted(zip.fit)[, 1])  # Should be all 0s
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-0.176434 -0.024770  0.001744 -0.002052  0.023224  0.113060 
> summary(rowMeans(aaa[, 2, ]) - fitted(zip.fit)[, 2])  # Should be all 0s
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-0.5001125 -0.0682611 -0.0030149 -0.0007428  0.0745908  0.3279039 
> 
> 
> 
> cleanEx()
> nameEx("sinmad")
> ### * sinmad
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sinmad
> ### Title: Singh-Maddala Distribution Family Function
> ### Aliases: sinmad
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D sdata <- data.frame(y = rsinmad(n = 1000, shape1 = exp(1),
> ##D                     scale = exp(2), shape3 = exp(0)))
> ##D fit <- vglm(y ~ 1, sinmad(lss = FALSE), sdata, trace = TRUE)
> ##D fit <- vglm(y ~ 1, sinmad(lss = FALSE, ishape1.a = exp(1)),
> ##D             sdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ##D 
> ##D # Harder problem (has the shape3.q parameter going to infinity)
> ##D 
> ##D set.seed(3)
> ##D sdata <- data.frame(y1 = rbeta(1000, 6, 6))
> ##D # hist(with(sdata, y1))
> ##D if (FALSE) {
> ##D # These struggle
> ##D   fit1 <- vglm(y1 ~ 1, sinmad(lss = FALSE), sdata, trace = TRUE)
> ##D   fit1 <- vglm(y1 ~ 1, sinmad(lss = FALSE), sdata, trace = TRUE,
> ##D                crit = "coef")
> ##D   Coef(fit1)
> ##D }
> ##D # Try this remedy:
> ##D fit2 <- vglm(y1 ~ 1, data = sdata, trace = TRUE, stepsize = 0.05, maxit = 99,
> ##D              sinmad(lss = FALSE, ishape3.q = 3, lshape3.q = "logloglink"))
> ##D              
> ##D coef(fit2, matrix = TRUE)
> ##D Coef(fit2)    
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sinmadUC")
> ### * sinmadUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Sinmad
> ### Title: The Singh-Maddala Distribution
> ### Aliases: Sinmad dsinmad psinmad qsinmad rsinmad
> ### Keywords: distribution
> 
> ### ** Examples
> 
> sdata <- data.frame(y = rsinmad(n = 3000, scale = exp(2),
+                                 shape1 = exp(1), shape3 = exp(1)))
> fit <- vglm(y ~ 1, sinmad(lss = FALSE, ishape1.a = 2.1), data = sdata,
+             trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
0.91276035, 2.12293427, 1.16346657
VGLM    linear loop  2 :  coefficients = 
0.98353898, 1.96442679, 0.94305039
VGLM    linear loop  3 :  coefficients = 
0.98715496, 1.97478397, 0.95486848
VGLM    linear loop  4 :  coefficients = 
0.98696677, 1.97554305, 0.95640611
VGLM    linear loop  5 :  coefficients = 
0.98694764, 1.97560299, 0.95651818
VGLM    linear loop  6 :  coefficients = 
0.98694625, 1.97560739, 0.95652641
VGLM    linear loop  7 :  coefficients = 
0.98694615, 1.97560771, 0.95652702
VGLM    linear loop  8 :  coefficients = 
0.98694615, 1.97560774, 0.95652706
> coef(fit, matrix = TRUE)
            loglink(shape1.a) loglink(scale) loglink(shape3.q)
(Intercept)         0.9869461       1.975608         0.9565271
> Coef(fit)
shape1.a    scale shape3.q 
2.683028 7.211001 2.602642 
> 
> 
> 
> cleanEx()
> nameEx("skellam")
> ### * skellam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: skellam
> ### Title: Skellam Distribution Family Function
> ### Aliases: skellam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D sdata <- data.frame(x2 = runif(nn <- 1000))
> ##D sdata <- transform(sdata, mu1 = exp(1 + x2), mu2 = exp(1 + x2))
> ##D sdata <- transform(sdata, y = rskellam(nn, mu1, mu2))
> ##D fit1 <- vglm(y ~ x2, skellam, data = sdata, trace = TRUE, crit = "coef")
> ##D fit2 <- vglm(y ~ x2, skellam(parallel = TRUE), data = sdata, trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ##D summary(fit1)
> ##D # Likelihood ratio test for equal means:
> ##D pchisq(2 * (logLik(fit1) - logLik(fit2)),
> ##D        df = df.residual(fit2) - df.residual(fit1), lower.tail = FALSE)
> ##D lrtest(fit1, fit2)  # Alternative
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("skellamUC")
> ### * skellamUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Skellam
> ### Title: The Skellam Distribution
> ### Aliases: Skellam dskellam rskellam
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  mu1 <- 1; mu2 <- 2; x <- (-7):7
> ##D plot(x, dskellam(x, mu1, mu2), type = "h", las = 1, col = "blue",
> ##D      main = paste("Density of Skellam distribution with mu1 = ", mu1,
> ##D                   " and mu2 = ", mu2, sep = "")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("skewnormUC")
> ### * skewnormUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: skewnorm
> ### Title: Skew-Normal Distribution
> ### Aliases: skewnorm dskewnorm rskewnorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  N <- 200  # Grid resolution
> ##D shape <- 7; x <- seq(-4, 4, len = N)
> ##D plot(x, dskewnorm(x, shape = shape), type = "l", col = "blue", las = 1,
> ##D      ylab = "", lty = 1, lwd = 2)
> ##D abline(v = 0, h = 0, col = "grey")
> ##D lines(x, dnorm(x), col = "orange", lty = 2, lwd = 2)
> ##D legend("topleft", leg = c(paste("Blue = dskewnorm(x, ", shape,")", sep = ""),
> ##D        "Orange = standard normal density"), lty = 1:2, lwd = 2,
> ##D        col = c("blue", "orange")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("skewnormal")
> ### * skewnormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: skewnormal
> ### Title: Univariate Skew-Normal Distribution Family Function
> ### Aliases: skewnormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> sdata <- data.frame(y1 = rskewnorm(nn <- 1000, shape = 5))
> fit1 <- vglm(y1 ~ 1, skewnormal, data = sdata, trace = TRUE)
Warning in checkwz(wz, M, trace = trace, wzepsilon = control$wzepsilon) :
  7 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  1 :  loglikelihood = -956.10675
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  1 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  2 :  loglikelihood = -922.7977
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  19 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  3 :  loglikelihood = -913.01104
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  67 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  4 :  loglikelihood = -911.93986
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  98 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  5 :  loglikelihood = -911.92511
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  104 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  6 :  loglikelihood = -911.92511
Warning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  104 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  7 :  loglikelihood = -911.92511
> coef(fit1, matrix = TRUE)
               shape
(Intercept) 4.452243
> head(fitted(fit1), 1)
       [,1]
1 0.7784896
> with(sdata, mean(y1))
[1] 0.803991
> ## Not run: 
> ##D  with(sdata, hist(y1, prob = TRUE))
> ##D x <- with(sdata, seq(min(y1), max(y1), len = 200))
> ##D with(sdata, lines(x, dskewnorm(x, shape = Coef(fit1)), col = "blue")) 
> ## End(Not run)
> 
> sdata <- data.frame(x2 = runif(nn))
> sdata <- transform(sdata, y2 = rskewnorm(nn, shape = 1 + 2*x2))
> fit2 <- vglm(y2 ~ x2, skewnormal, data = sdata, trace = TRUE, crit = "coef")
Warning in checkwz(wz, M, trace = trace, wzepsilon = control$wzepsilon) :
  5 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12
VGLM    linear loop  1 :  coefficients = 1.5557001, 0.4762808
VGLM    linear loop  2 :  coefficients = 1.2839881, 1.3685175
VGLM    linear loop  3 :  coefficients = 1.2567594, 1.5550340
VGLM    linear loop  4 :  coefficients = 1.2552008, 1.5649745
VGLM    linear loop  5 :  coefficients = 1.2551972, 1.5649985
VGLM    linear loop  6 :  coefficients = 1.2551972, 1.5649985
> summary(fit2)
Call:
vglm(formula = y2 ~ x2, family = skewnormal, data = sdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.2552     0.1795   6.992 2.72e-12 ***
x2            1.5650     0.4084   3.832 0.000127 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: shape 

Log-likelihood: -1097.367 on 998 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("slash")
> ### * slash
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: slash
> ### Title: Slash Distribution Family Function
> ### Aliases: slash
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D sdata <- data.frame(y = rslash(n = 1000, mu = 4, sigma = exp(2)))
> ##D fit <- vglm(y ~ 1, slash, data = sdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D Coef(fit)
> ##D summary(fit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("slashUC")
> ### * slashUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Slash
> ### Title: Slash Distribution
> ### Aliases: Slash dslash pslash rslash
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D curve(dslash, col = "blue", ylab = "f(x)", -5, 5, ylim = c(0, 0.4), las = 1,
> ##D      main = "Standard slash, normal and Cauchy densities", lwd = 2)
> ##D curve(dnorm, col = "black", lty = 2, lwd = 2, add = TRUE)
> ##D curve(dcauchy, col = "orange", lty = 3, lwd = 2, add = TRUE)
> ##D legend("topleft", c("slash", "normal", "Cauchy"), lty = 1:3,
> ##D        col = c("blue","black","orange"), lwd = 2)
> ##D 
> ##D curve(pslash, col = "blue", -5, 5, ylim = 0:1)
> ##D pslash(c(-Inf, -20000, 20000, Inf))  # Gives a warning
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sloglink")
> ### * sloglink
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sloglink
> ### Title: Square root-Log Link Mixtures
> ### Aliases: sloglink lcsloglink
> ### Keywords: math models regression
> 
> ### ** Examples
> 
> mu  <- seq(0.01, 3, length = 10)
> sloglink(mu)
 [1] -4.577258276 -1.002076937 -0.376030510  0.006651554  0.308344050
 [6]  0.571901705  0.812442769  1.036354822  1.246856665  1.445904973
> max(abs(sloglink(sloglink(mu), inv = TRUE) - mu))  # 0?
[1] 8.348877e-14
> 
> 
> 
> cleanEx()
> nameEx("sm.os")
> ### * sm.os
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sm.os
> ### Title: Defining O'Sullivan Spline Smooths in VGAM Formulas
> ### Aliases: sm.os
> ### Keywords: models regression smooth
> 
> ### ** Examples
> 
> sm.os(runif(20))
             2            3           4           5           6            7
1   0.01379606  0.400554880  0.24578273 -0.09471014 -0.15755647 -0.142596250
2  -0.06471526  0.007880263  0.50565867  0.08944052 -0.16740313 -0.151507958
3  -0.06308369 -0.126450712 -0.12156125  0.36427943  0.31429895 -0.128422686
4  -0.04239100 -0.084972393 -0.10250038 -0.07258085 -0.10965184 -0.007354401
5   0.24317517  0.453709309  0.02333355 -0.08834024 -0.13346040 -0.120788139
6  -0.04341279 -0.087020560 -0.10497104 -0.07433033 -0.11229338  0.022528184
7  -0.03832297 -0.076818053 -0.09266398 -0.06561566 -0.09912914 -0.069077607
8  -0.07154366 -0.143408641 -0.17299078 -0.04221134  0.47983484  0.087320188
9  -0.06945292 -0.139217767 -0.16686666  0.07917938  0.49455289 -0.035962932
10  0.01232136 -0.305317733 -0.37439357 -0.26513469 -0.40055340 -0.362520261
11  0.22110599  0.460286656  0.03459841 -0.08967740 -0.13548065 -0.122616561
12  0.38231387  0.383000202 -0.03005859 -0.08159056 -0.12326331 -0.111559273
13 -0.07174115 -0.143804509 -0.17346831 -0.09560314  0.38561041  0.229862447
14 -0.06417109 -0.024204419  0.49411232  0.13637978 -0.16594424 -0.150228909
15 -0.06417120 -0.128630608 -0.15516435 -0.10987237 -0.01274181  0.509531088
16 -0.05764293 -0.115477515  0.07310659  0.55733647 -0.01768796 -0.134946043
17 -0.07028004 -0.140875721 -0.16993537 -0.11705380  0.22533210  0.395098918
18 -0.02442463 -0.048958956 -0.05905814 -0.04181926 -0.06317863 -0.057087803
19 -0.06437796 -0.014028496  0.49932707  0.11975905 -0.16651834 -0.150713197
20 -0.06298118 -0.126245227 -0.15228692 -0.10783485 -0.03476650  0.501041194
             8           9          10
1  -0.08473205 -0.08122034 -0.04070159
2  -0.09002748 -0.08629630 -0.04324528
3  -0.08775479 -0.08411780 -0.04215358
4   0.42974574  0.36252865 -0.02798160
5  -0.07177347 -0.06879882 -0.03447685
6   0.46811224  0.28944689 -0.02900916
7   0.20460573  0.58817255  0.05656301
8  -0.09951129 -0.09539859 -0.04780668
9  -0.09661492 -0.09261073 -0.04640961
10 -0.21541300 -0.20648523 -0.10347504
11 -0.07285993 -0.06984026 -0.03499874
12 -0.06628959 -0.06354222 -0.03184263
13 -0.09602336 -0.09566193 -0.04793864
14 -0.08926745 -0.08556777 -0.04288020
15  0.09659118 -0.08443531 -0.04288027
16 -0.08018623 -0.07686292 -0.03851798
17 -0.06779666 -0.09371364 -0.04696230
18 -0.02428376  0.21150490  0.72982066
19 -0.08955522 -0.08584361 -0.04301843
20  0.13303431 -0.08125753 -0.04208508
attr(,"S.arg")
             [,1]         [,2]        [,3]        [,4]        [,5]         [,6]
 [1,]  1.30963224 -0.033751830  0.27664959  0.25022008  0.35062411  0.319949537
 [2,] -0.03375183  0.135677518 -0.06311171  0.01144048  0.01312542  0.009179189
 [3,]  0.27664959 -0.063111709  0.16935124 -0.01587997  0.06350550  0.077052480
 [4,]  0.25022008  0.011440481 -0.01587997  0.22509851 -0.03963728  0.078365581
 [5,]  0.35062411  0.013125417  0.06350550 -0.03963728  0.30576566 -0.046250514
 [6,]  0.31994954  0.009179189  0.07705248  0.07836558 -0.04625051  0.313121810
 [7,]  0.20244845  0.030172494  0.06855147  0.07531918  0.07596062 -0.032612313
 [8,]  0.13748181 -0.084484464 -0.07108943 -0.03578724 -0.04392336 -0.174536362
 [9,]  0.12436393  0.068848338  0.09849616  0.07703753  0.10958624  0.210050385
             [,7]        [,8]        [,9]
 [1,]  0.20244845  0.13748181  0.12436393
 [2,]  0.03017249 -0.08448446  0.06884834
 [3,]  0.06855147 -0.07108943  0.09849616
 [4,]  0.07531918 -0.03578724  0.07703753
 [5,]  0.07596062 -0.04392336  0.10958624
 [6,] -0.03261231 -0.17453636  0.21005039
 [7,]  0.48129035 -0.82577838  0.56345505
 [8,] -0.82577838  3.32254573 -2.48648843
 [9,]  0.56345505 -2.48648843  1.96059462
attr(,"knots")
                                             14.28571%  28.57143%  42.85714% 
0.05248507 0.05248507 0.05248507 0.05248507 0.20474811 0.37551445 0.50843555 
 57.14286%  71.42857%  85.71429%                                             
0.65627154 0.74746017 0.90119486 1.00120729 1.00120729 1.00120729 1.00120729 
attr(,"intKnots")
14.28571% 28.57143% 42.85714% 57.14286% 71.42857% 85.71429% 
0.2047481 0.3755144 0.5084355 0.6562715 0.7474602 0.9011949 
attr(,"spar")
[1] -1
attr(,"o.order")
[1] 2
attr(,"ps.int")
[1] NA
attr(,"all.knots")
[1] FALSE
attr(,"alg.niknots")
[1] "s"
attr(,"ridge.adj")
[1] 1e-05
attr(,"outer.ok")
[1] FALSE
attr(,"fixspar")
[1] FALSE
> 
> ## Not run: 
> ##D data("TravelMode", package = "AER")  # Need to install "AER" first
> ##D air.df <- subset(TravelMode, mode == "air")  # Form 4 smaller data frames
> ##D bus.df <- subset(TravelMode, mode == "bus")
> ##D trn.df <- subset(TravelMode, mode == "train")
> ##D car.df <- subset(TravelMode, mode == "car")
> ##D TravelMode2 <- data.frame(income     = air.df$income,
> ##D                           wait.air   = air.df$wait  - car.df$wait,
> ##D                           wait.trn   = trn.df$wait  - car.df$wait,
> ##D                           wait.bus   = bus.df$wait  - car.df$wait,
> ##D                           gcost.air  = air.df$gcost - car.df$gcost,
> ##D                           gcost.trn  = trn.df$gcost - car.df$gcost,
> ##D                           gcost.bus  = bus.df$gcost - car.df$gcost,
> ##D                           wait       = air.df$wait)  # Value is unimportant
> ##D TravelMode2$mode <- subset(TravelMode, choice == "yes")$mode  # The response
> ##D TravelMode2 <- transform(TravelMode2, incom.air = income, incom.trn = 0,
> ##D                                       incom.bus = 0)
> ##D set.seed(1)
> ##D TravelMode2 <- transform(TravelMode2,
> ##D                          junkx2 = runif(nrow(TravelMode2)))
> ##D 
> ##D tfit2 <-
> ##D   vgam(mode ~ sm.os(gcost.air, gcost.trn, gcost.bus) + ns(junkx2, 4) +
> ##D               sm.os(incom.air, incom.trn, incom.bus) + wait ,
> ##D        crit = "coef",
> ##D        multinomial(parallel = FALSE ~ 1), data = TravelMode2,
> ##D        xij = list(sm.os(gcost.air, gcost.trn, gcost.bus) ~
> ##D                   sm.os(gcost.air, gcost.trn, gcost.bus) +
> ##D                   sm.os(gcost.trn, gcost.bus, gcost.air) +
> ##D                   sm.os(gcost.bus, gcost.air, gcost.trn),
> ##D                   sm.os(incom.air, incom.trn, incom.bus) ~
> ##D                   sm.os(incom.air, incom.trn, incom.bus) +
> ##D                   sm.os(incom.trn, incom.bus, incom.air) +
> ##D                   sm.os(incom.bus, incom.air, incom.trn),
> ##D                   wait   ~  wait.air +  wait.trn +  wait.bus),
> ##D        form2 = ~  sm.os(gcost.air, gcost.trn, gcost.bus) +
> ##D                   sm.os(gcost.trn, gcost.bus, gcost.air) +
> ##D                   sm.os(gcost.bus, gcost.air, gcost.trn) +
> ##D                   wait +
> ##D                   sm.os(incom.air, incom.trn, incom.bus) +
> ##D                   sm.os(incom.trn, incom.bus, incom.air) +
> ##D                   sm.os(incom.bus, incom.air, incom.trn) +
> ##D                   junkx2 + ns(junkx2, 4) +
> ##D                   incom.air + incom.trn + incom.bus +
> ##D                   gcost.air + gcost.trn + gcost.bus +
> ##D                   wait.air +  wait.trn +  wait.bus)
> ##D par(mfrow = c(2, 2))
> ##D plot(tfit2, se = TRUE, lcol = "orange", scol = "blue", ylim = c(-4, 4))
> ##D summary(tfit2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sm.ps")
> ### * sm.ps
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sm.ps
> ### Title: Defining Penalized Spline Smooths in VGAM Formulas
> ### Aliases: sm.ps
> ### Keywords: models regression smooth
> 
> ### ** Examples
> 
> sm.ps(runif(20))
             2           3            4           5           6           7
1  -0.04403543  0.29473236  0.380613762 -0.08343575 -0.14284570 -0.17539075
2  -0.05412451 -0.08719033  0.426753380  0.27850663 -0.12767101 -0.16618009
3  -0.05724628 -0.13877612 -0.141544860  0.14292357  0.49981103 -0.07717245
4  -0.05187591 -0.12575730 -0.128891253 -0.10406014 -0.12972129 -0.10542841
5   0.06779164  0.52762391  0.089036227 -0.10689473 -0.13346785 -0.16387631
6  -0.05319127 -0.12894600 -0.132159413 -0.10669868 -0.13301049 -0.09054881
7  -0.04656933 -0.11289312 -0.115706487 -0.09341544 -0.11645161 -0.13088811
8  -0.06498487 -0.15753598 -0.161461872 -0.10803104  0.32391698  0.27232174
9  -0.06239108 -0.15124812 -0.155017320 -0.05611730  0.45409500  0.12654300
10  0.61404279  0.08662867 -0.119395486 -0.09643724 -0.12021859 -0.14760842
11  0.05499397  0.51981906  0.106824970 -0.10755949 -0.13456687 -0.16522572
12  0.16202107  0.53813343  0.003144408 -0.10069105 -0.12552139 -0.15411938
13 -0.06662559 -0.16151338 -0.165538398 -0.12827942  0.19365299  0.37698073
14 -0.05349317 -0.10146139  0.378726192  0.33797806 -0.11889994 -0.16424168
15 -0.06650787 -0.16122802 -0.165245924 -0.13341102 -0.10738689  0.38923791
16 -0.05199665 -0.12605000 -0.068255896  0.49273908  0.20814414 -0.15579095
17 -0.06765631 -0.16401207 -0.168099348 -0.13558318  0.04756847  0.45079342
18 -0.03839388 -0.09307422 -0.095393692 -0.07701594 -0.09600801 -0.11782799
19 -0.05370600 -0.09713012  0.395722598  0.31797828 -0.12223164 -0.16489512
20 -0.06605133 -0.16012127 -0.164111590 -0.13249522 -0.11918734  0.36331739
             8           9           10
1  -0.14069123 -0.09555742 -0.008954412
2  -0.13330282 -0.09053922 -0.008484171
3  -0.14099140 -0.09576130 -0.008973517
4   0.45587304  0.27058580 -0.002980828
5  -0.13145482 -0.08928406 -0.008366553
6   0.48443142  0.22047631 -0.005994022
7   0.31123408  0.45106792  0.025706842
8  -0.14064102 -0.10870638 -0.010186564
9  -0.15091297 -0.10436750 -0.009779980
10 -0.11840539 -0.08042089 -0.007536011
11 -0.13253726 -0.09001925 -0.008435446
12 -0.12362821 -0.08396823 -0.007868422
13 -0.11126003 -0.11145096 -0.010443750
14 -0.13174791 -0.08948312 -0.008385207
15  0.17964798 -0.10706593 -0.010425298
16 -0.12806213 -0.08697974 -0.008150623
17 -0.04203269 -0.11317515 -0.010605320
18  0.10861368  0.59789346  0.128635586
19 -0.13227207 -0.08983913 -0.008418568
20  0.21813974 -0.10340521 -0.010353734
attr(,"S.arg")
             [,1]         [,2]         [,3]         [,4]         [,5]
 [1,]  0.89906526 -0.510342425  0.305904858  0.131744209  0.154470334
 [2,] -0.51034243  0.801037334 -0.686949918  0.132708520 -0.053011524
 [3,]  0.30590486 -0.686949918  0.953861385 -0.584657239  0.182286781
 [4,]  0.13174421  0.132708520 -0.584657239  0.991976785 -0.576671091
 [5,]  0.15447033 -0.053011524  0.182286781 -0.576671091  0.973336114
 [6,]  0.17979108 -0.089022883  0.007438844  0.195785612 -0.605687174
 [7,]  0.15651917 -0.041597307  0.036523277  0.056383289  0.202494871
 [8,]  0.09778855 -0.048905081  0.003639720  0.021206465  0.010106312
 [9,]  0.01227440  0.002958713  0.008070474  0.008227521  0.008726216
              [,6]        [,7]        [,8]         [,9]
 [1,]  0.179791083  0.15651917  0.09778855  0.012274402
 [2,] -0.089022883 -0.04159731 -0.04890508  0.002958713
 [3,]  0.007438844  0.03652328  0.00363972  0.008070474
 [4,]  0.195785612  0.05638329  0.02120647  0.008227521
 [5,] -0.605687174  0.20249487  0.01010631  0.008726216
 [6,]  0.930900231 -0.59253460  0.15214372  0.009166754
 [7,] -0.592534596  0.99383150 -0.60772155  0.165530967
 [8,]  0.152143723 -0.60772155  0.77873463 -0.307531781
 [9,]  0.009166754  0.16553097 -0.30753178  0.157203203
attr(,"degree")
[1] 3
attr(,"knots")
 [1] -0.35411017 -0.21857842 -0.08304667  0.05248507  0.18801682  0.32354856
 [7]  0.45908031  0.59461206  0.73014380  0.86567555  1.00120729  1.13673904
[13]  1.27227078  1.40780253
attr(,"spar")
[1] -1
attr(,"p.order")
[1] 2
attr(,"ps.int")
[1] 7
attr(,"ridge.adj")
[1] 1e-05
attr(,"outer.ok")
[1] FALSE
attr(,"fixspar")
[1] FALSE
> sm.ps(runif(20), ps.int = 5)
             2           3            4           5           6            7
1  -0.05342309 -0.11220379 -0.130222031 -0.10967543  0.08473015  0.605253287
2   0.02276533  0.45404854 -0.009312254 -0.19716412 -0.19357600 -0.106278718
3  -0.09553158 -0.20064370 -0.204383957  0.31649766  0.25177866 -0.090933580
4   0.30384003  0.39449661 -0.151171409 -0.16515643 -0.16198539 -0.088934575
5  -0.06729963  0.32178901  0.162375994 -0.20164238 -0.20819762 -0.114306398
6  -0.10884764 -0.07750304  0.400327607 -0.04026386 -0.21920300 -0.120351706
7   0.58273980  0.02097169 -0.198843517 -0.16751624 -0.16429989 -0.090205301
8  -0.10888419 -0.06810257  0.401104119 -0.05067656 -0.21928184 -0.120392124
9  -0.06823602 -0.14331519 -0.166329461 -0.12984420  0.27390126  0.466044298
10 -0.10679945  0.06234727  0.362548701 -0.14077582 -0.21825292 -0.119827043
11 -0.10464283 -0.20384510  0.196965220  0.29051569 -0.18409539 -0.115702490
12 -0.09770788 -0.20521454 -0.142122719  0.44023628  0.06552925 -0.107246671
13 -0.10389283 -0.20783269  0.158966645  0.32741699 -0.17255672 -0.114873224
14  0.09121464  0.47320888 -0.071956612 -0.18818155 -0.18456842 -0.101333300
15 -0.07676452 -0.16122750 -0.187118208 -0.11788378  0.39515599  0.316492382
16 -0.09473838 -0.19897775 -0.214111256  0.26284426  0.30957368 -0.079311268
17 -0.08250822 -0.17329090 -0.201118812 -0.08523567  0.46326376  0.193756556
18  0.36811035  0.34739937 -0.161801666 -0.15992371 -0.15685314 -0.086116825
19 -0.09110576 -0.19134820 -0.221233103  0.07761782  0.45625692 -0.006000204
20 -0.10828816 -0.13075639  0.377436720  0.03881105 -0.21731932 -0.119733096
              8
1   0.137780698
2  -0.010000049
3  -0.009938848
4  -0.008368093
5  -0.010755395
6  -0.011324215
7  -0.008487659
8  -0.011328018
9   0.029820135
10 -0.011274848
11 -0.010886757
12 -0.010165264
13 -0.010808729
14 -0.009534722
15  0.001152372
16 -0.009856326
17 -0.007176914
18 -0.008102963
19 -0.009478397
20 -0.011266008
attr(,"S.arg")
            [,1]         [,2]         [,3]         [,4]       [,5]       [,6]
[1,]  1.25429609 -0.652607518  0.471259774  0.252254526  0.2060455  0.1304922
[2,] -0.65260752  1.071200310 -0.929154897  0.192248985 -0.1026552 -0.0198843
[3,]  0.47125977 -0.929154897  1.268417445 -0.742807175  0.1962901  0.0357218
[4,]  0.25225453  0.192248985 -0.742807175  1.389444172 -0.7815037  0.2724500
[5,]  0.20604554 -0.102655239  0.196290080 -0.781503716  1.2175281 -0.8161853
[6,]  0.13049216 -0.019884302  0.035721801  0.272450038 -0.8161853  1.0702843
[7,]  0.01256499 -0.001268956  0.004059845  0.006621464  0.2105241 -0.4136570
             [,7]
[1,]  0.012564990
[2,] -0.001268956
[3,]  0.004059845
[4,]  0.006621464
[5,]  0.210524093
[6,] -0.413657029
[7,]  0.208646339
attr(,"degree")
[1] 3
attr(,"knots")
 [1] -0.559667533 -0.371719294 -0.183771055  0.004177184  0.192125423
 [6]  0.380073663  0.568021902  0.755970141  0.943918380  1.131866619
[11]  1.319814858  1.507763098
attr(,"spar")
[1] -1
attr(,"p.order")
[1] 2
attr(,"ps.int")
[1] 5
attr(,"ridge.adj")
[1] 1e-05
attr(,"outer.ok")
[1] FALSE
attr(,"fixspar")
[1] FALSE
> 
> ## Not run: 
> ##D data("TravelMode", package = "AER")  # Need to install "AER" first
> ##D air.df <- subset(TravelMode, mode == "air")  # Form 4 smaller data frames
> ##D bus.df <- subset(TravelMode, mode == "bus")
> ##D trn.df <- subset(TravelMode, mode == "train")
> ##D car.df <- subset(TravelMode, mode == "car")
> ##D TravelMode2 <- data.frame(income     = air.df$income,
> ##D                           wait.air   = air.df$wait  - car.df$wait,
> ##D                           wait.trn   = trn.df$wait  - car.df$wait,
> ##D                           wait.bus   = bus.df$wait  - car.df$wait,
> ##D                           gcost.air  = air.df$gcost - car.df$gcost,
> ##D                           gcost.trn  = trn.df$gcost - car.df$gcost,
> ##D                           gcost.bus  = bus.df$gcost - car.df$gcost,
> ##D                           wait       = air.df$wait)  # Value is unimportant
> ##D TravelMode2$mode <- subset(TravelMode, choice == "yes")$mode  # The response
> ##D TravelMode2 <- transform(TravelMode2, incom.air = income, incom.trn = 0,
> ##D                                       incom.bus = 0)
> ##D set.seed(1)
> ##D TravelMode2 <- transform(TravelMode2,
> ##D                          junkx2 = runif(nrow(TravelMode2)))
> ##D 
> ##D tfit2 <-
> ##D   vgam(mode ~ sm.ps(gcost.air, gcost.trn, gcost.bus) + ns(junkx2, 4) +
> ##D               sm.ps(incom.air, incom.trn, incom.bus) + wait ,
> ##D        crit = "coef",
> ##D        multinomial(parallel = FALSE ~ 1), data = TravelMode2,
> ##D        xij = list(sm.ps(gcost.air, gcost.trn, gcost.bus) ~
> ##D                   sm.ps(gcost.air, gcost.trn, gcost.bus) +
> ##D                   sm.ps(gcost.trn, gcost.bus, gcost.air) +
> ##D                   sm.ps(gcost.bus, gcost.air, gcost.trn),
> ##D                   sm.ps(incom.air, incom.trn, incom.bus) ~
> ##D                   sm.ps(incom.air, incom.trn, incom.bus) +
> ##D                   sm.ps(incom.trn, incom.bus, incom.air) +
> ##D                   sm.ps(incom.bus, incom.air, incom.trn),
> ##D                   wait   ~  wait.air +  wait.trn +  wait.bus),
> ##D        form2 = ~  sm.ps(gcost.air, gcost.trn, gcost.bus) +
> ##D                   sm.ps(gcost.trn, gcost.bus, gcost.air) +
> ##D                   sm.ps(gcost.bus, gcost.air, gcost.trn) +
> ##D                   wait +
> ##D                   sm.ps(incom.air, incom.trn, incom.bus) +
> ##D                   sm.ps(incom.trn, incom.bus, incom.air) +
> ##D                   sm.ps(incom.bus, incom.air, incom.trn) +
> ##D                   junkx2 + ns(junkx2, 4) +
> ##D                   incom.air + incom.trn + incom.bus +
> ##D                   gcost.air + gcost.trn + gcost.bus +
> ##D                   wait.air +  wait.trn +  wait.bus)
> ##D par(mfrow = c(2, 2))
> ##D plot(tfit2, se = TRUE, lcol = "orange", scol = "blue", ylim = c(-4, 4))
> ##D summary(tfit2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("smart.expression")
> ### * smart.expression
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smart.expression
> ### Title: S Expression for Smart Functions
> ### Aliases: smart.expression
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> print(sm.min2)
function (x, .minx = min(x)) 
{
    x <- x
    if (smart.mode.is("read")) {
        return(eval(smart.expression))
    }
    else if (smart.mode.is("write")) 
        put.smart(list(.minx = .minx, match.call = match.call()))
    .minx
}
<bytecode: 0x556f6cc7c690>
<environment: namespace:VGAM>
attr(,"smart")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("smart.mode.is")
> ### * smart.mode.is
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smart.mode.is
> ### Title: Determine What Mode the Smart Prediction is In
> ### Aliases: smart.mode.is
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> print(sm.min1)
function (x) 
{
    x <- x
    minx <- min(x)
    if (smart.mode.is("read")) {
        smart <- get.smart()
        minx <- smart$minx
    }
    else if (smart.mode.is("write")) 
        put.smart(list(minx = minx))
    minx
}
<bytecode: 0x556f6bf06bc8>
<environment: namespace:VGAM>
attr(,"smart")
[1] TRUE
> smart.mode.is()  # Returns "neutral"
[1] "neutral"
> smart.mode.is(smart.mode.is())  # Returns TRUE
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("smartpred")
> ### * smartpred
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smartpred
> ### Title: Smart Prediction
> ### Aliases: smartpred sm.bs sm.ns sm.scale sm.scale.default sm.poly
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> # Create some data first
> n <- 20
> set.seed(86)  # For reproducibility of the random numbers
> ldata <- data.frame(x2 = sort(runif(n)), y = sort(runif(n)))
> library("splines")  # To get ns() in R
> 
> # This will work for R 1.6.0 and later
> fit <- lm(y ~ ns(x2, df = 5), data = ldata)
> ## Not run: 
> ##D plot(y ~ x2, data = ldata)
> ##D lines(fitted(fit) ~ x2, data = ldata)
> ##D new.ldata <- data.frame(x2 = seq(0, 1, len = n))
> ##D points(predict(fit, new.ldata) ~ x2, new.ldata, type = "b", col = 2, err = -1)
> ## End(Not run)
> 
> # The following fails for R 1.6.x and later. It can be
> # made to work with smart prediction provided
> # ns is changed to sm.ns and scale is changed to sm.scale:
> fit1 <- lm(y ~ ns(scale(x2), df = 5), data = ldata)
> ## Not run: 
> ##D plot(y ~ x2, data = ldata, main = "Safe prediction fails")
> ##D lines(fitted(fit1) ~ x2, data = ldata)
> ##D points(predict(fit1, new.ldata) ~ x2, new.ldata, type = "b", col = 2, err = -1)
> ## End(Not run)
> 
> # Fit the above using smart prediction
> ## Not run: 
> ##D library("VGAM")  # The following requires the VGAM package to be loaded 
> ##D fit2 <- vglm(y ~ sm.ns(sm.scale(x2), df = 5), uninormal, data = ldata)
> ##D fit2@smart.prediction
> ##D plot(y ~ x2, data = ldata, main = "Smart prediction")
> ##D lines(fitted(fit2) ~ x2, data = ldata)
> ##D points(predict(fit2, new.ldata, type = "response") ~ x2, data = new.ldata,
> ##D        type = "b", col = 2, err = -1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("specialsvglm")
> ### * specialsvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: specials
> ### Title: Special Values or Quantities in a Fitted Object
> ### Aliases: specials specialsvglm
> ### Keywords: models
> 
> ### ** Examples
> 
> abdata <- data.frame(y = 0:7, w = c(182, 41, 12, 2, 2, 0, 0, 1))
> fit1 <- vglm(y ~ 1, gaitdpoisson(a.mix = 0), data = abdata,
+              weight = w, subset = w > 0)
Warning in dgaitdpois(x, lambda.a, truncate = setdiff(allx.a, a.mix), max.support = max(a.mix)) :
  PMF > 1: too much inflation? NaNs produced
Warning in dgaitdpois(x, lambda.a, truncate = setdiff(allx.a, a.mix), max.support = max(a.mix)) :
  PMF > 1: too much inflation? NaNs produced
Warning in dgaitdpois(x, lambda.a, truncate = setdiff(allx.a, a.mix), max.support = max(a.mix)) :
  PMF > 1: too much inflation? NaNs produced
Warning in dgaitdpois(x, lambda.a, truncate = setdiff(allx.a, a.mix), max.support = max(a.mix)) :
  PMF > 1: too much inflation? NaNs produced
> specials(fit1)
$a.mix
[1] 0

$a.mlm
NULL

$i.mix
NULL

$i.mlm
NULL

$d.mix
NULL

$d.mlm
NULL

$truncate
NULL

$max.support
[1] Inf

> 
> 
> 
> cleanEx()
> nameEx("spikeplot")
> ### * spikeplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spikeplot
> ### Title: Spike Plot
> ### Aliases: spikeplot
> ### Keywords: distribution hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D spikeplot(with(marital.nz, age), col = "pink2", lwd = 2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("sratio")
> ### * sratio
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sratio
> ### Title: Ordinal Regression with Stopping Ratios
> ### Aliases: sratio
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let,
+              sratio(parallel = TRUE), data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = sratio(parallel = TRUE), 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
     8.733797      8.051302     -2.321359 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 7.626763 
Log-likelihood: -26.39023 
> coef(fit, matrix = TRUE)
            logitlink(P[Y=1|Y>=1]) logitlink(P[Y=2|Y>=2])
(Intercept)               8.733797               8.051302
let                      -2.321359              -2.321359
> constraints(fit)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> predict(fit)
  logitlink(P[Y=1|Y>=1]) logitlink(P[Y=2|Y>=2])
1              4.6531774              3.9706824
2              2.4474398              1.7649448
3              1.6117442              0.9292491
4              1.0403809              0.3578859
5              0.5822388             -0.1002563
6              0.1997827             -0.4827124
7             -0.1538548             -0.8363499
8             -0.4160301             -1.0985252
> predict(fit, untransform = TRUE)
  P[Y=1|Y>=1] P[Y=2|Y>=2]
1   0.9905587   0.9814886
2   0.9203740   0.8538279
3   0.8336534   0.7169229
4   0.7389235   0.5885286
5   0.6415824   0.4749569
6   0.5497802   0.3816118
7   0.4616120   0.3023041
8   0.3974671   0.2500163
> 
> 
> 
> cleanEx()
> nameEx("step4vglm")
> ### * step4vglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step4
> ### Title: Choose a model by AIC in a Stepwise Algorithm
> ### Aliases: step4 step4vglm
> ### Keywords: models
> 
> ### ** Examples
> 
> data("backPain2", package = "VGAM")
> summary(backPain2)
 x2     x3     x4                       pain   
 1:39   1:21   1:64   worse               : 5  
 2:62   2:52   2:37   same                :14  
        3:28          slight.improvement  :18  
                      moderate.improvement:20  
                      marked.improvement  :28  
                      complete.relief     :16  
> fit1 <- vglm(pain ~ x2 + x3 + x4 + x2:x3 + x2:x4 + x3:x4,
+              propodds, data = backPain2)
> spom1 <- step4(fit1)
Start:  AIC=340.44
pain ~ x2 + x3 + x4 + x2:x3 + x2:x4 + x3:x4

        Df Deviance    AIC
- x3:x4  2   312.79 336.79
- x2:x4  1   313.06 339.06
- x2:x3  2   316.00 339.99
<none>       312.44 340.44

Step:  AIC=336.79
pain ~ x2 + x3 + x4 + x2:x3 + x2:x4

        Df Deviance    AIC
- x2:x4  1   313.24 335.24
- x2:x3  2   316.28 336.28
<none>       312.79 336.79

Step:  AIC=335.24
pain ~ x2 + x3 + x4 + x2:x3

        Df Deviance    AIC
- x2:x3  2   316.40 334.40
<none>       313.24 335.24
- x4     1   320.79 340.79

Step:  AIC=334.4
pain ~ x2 + x3 + x4

       Df Deviance    AIC
<none>      316.40 334.40
- x3    2   321.53 335.53
- x4    1   322.58 338.58
- x2    1   330.48 346.48
> summary(spom1)
Call:
vglm(formula = pain ~ x2 + x3 + x4, family = propodds, data = backPain2)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   5.4102     0.7247   7.466 8.29e-14 ***
(Intercept):2   3.8365     0.5955   6.442 1.18e-10 ***
(Intercept):3   2.8387     0.5479   5.181 2.21e-07 ***
(Intercept):4   1.8598     0.5080   3.661 0.000251 ***
(Intercept):5   0.0968     0.4758   0.203 0.838768    
x22            -1.4657     0.3968  -3.694 0.000221 ***
x32            -1.0318     0.4839  -2.132 0.032975 *  
x33            -1.1021     0.5373  -2.051 0.040227 *  
x42            -0.9241     0.3804  -2.429 0.015130 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Number of linear predictors:  5 

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3]), 
logitlink(P[Y>=4]), logitlink(P[Y>=5]), logitlink(P[Y>=6])

Residual deviance: 316.4004 on 496 degrees of freedom

Log-likelihood: -158.2002 on 496 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates


Exponentiated coefficients:
      x22       x32       x33       x42 
0.2309154 0.3563712 0.3321658 0.3968965 
> spom1@post$anova
     Step Df  Deviance Resid. Df Resid. Dev      AIC
1         NA        NA       491   312.4383 340.4383
2 - x3:x4  2 0.3553795       493   312.7936 336.7936
3 - x2:x4  1 0.4467099       494   313.2403 335.2403
4 - x2:x3  2 3.1600873       496   316.4004 334.4004
> 
> 
> 
> cleanEx()
> nameEx("studentt")
> ### * studentt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: studentt
> ### Title: Student t Distribution
> ### Aliases: studentt studentt2 studentt3
> ### Keywords: models regression
> 
> ### ** Examples
> 
> tdata <- data.frame(x2 = runif(nn <- 1000))
> tdata <- transform(tdata, y1 = rt(nn, df = exp(exp(0.5 - x2))),
+                           y2 = rt(nn, df = exp(exp(0.5 - x2))))
> fit1 <- vglm(y1 ~ x2, studentt, data = tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1820.5417
VGLM    linear loop  2 :  loglikelihood = -1819.9838
VGLM    linear loop  3 :  loglikelihood = -1819.6347
VGLM    linear loop  4 :  loglikelihood = -1819.5815
VGLM    linear loop  5 :  loglikelihood = -1819.5634
VGLM    linear loop  6 :  loglikelihood = -1819.5593
VGLM    linear loop  7 :  loglikelihood = -1819.5581
VGLM    linear loop  8 :  loglikelihood = -1819.5578
VGLM    linear loop  9 :  loglikelihood = -1819.5577
VGLM    linear loop  10 :  loglikelihood = -1819.5577
VGLM    linear loop  11 :  loglikelihood = -1819.5577
> coef(fit1, matrix = TRUE)
            logloglink(df)
(Intercept)      0.6340559
x2              -1.4211882
> 
> # df inputted into studentt2() not quite right:
> fit2 <- vglm(y1 ~ x2, studentt2(df = exp(exp(0.5))), tdata)
> coef(fit2, matrix = TRUE)
               location loglink(scale)
(Intercept)  0.08531968      0.1501736
x2          -0.07866391      0.0000000
> 
> fit3 <- vglm(cbind(y1, y2) ~ x2, studentt3, tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3660.1955
VGLM    linear loop  2 :  loglikelihood = -3656.8654
VGLM    linear loop  3 :  loglikelihood = -3656.618
VGLM    linear loop  4 :  loglikelihood = -3656.598
VGLM    linear loop  5 :  loglikelihood = -3656.5964
VGLM    linear loop  6 :  loglikelihood = -3656.5963
VGLM    linear loop  7 :  loglikelihood = -3656.5963
> coef(fit3, matrix = TRUE)
              location1 loglink(scale1) logloglink(df1)   location2
(Intercept)  0.11018321     -0.09619751      -0.2781347  0.03721738
x2          -0.08308403      0.00000000       0.0000000 -0.19072656
            loglink(scale2) logloglink(df2)
(Intercept)     -0.03899694      -0.1333229
x2               0.00000000       0.0000000
> 
> 
> 
> cleanEx()
> nameEx("summarydrrvglm")
> ### * summarydrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.drrvglm
> ### Title: Summarizing Reduced Rank Vector Generalized Linear Model
> ###   (RR-VGLM) and Doubly constrained RR-VGLM Fits
> ### Aliases: summary.drrvglm summary.rrvglm show.summary.drrvglm
> ###   show.summary.rrvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D   # Fit a rank-1 RR-VGLM as a DRR-VGLM.
> ##D set.seed(1); n <- 1000; S <- 6  # S must be even
> ##D myrank <- 1
> ##D rdata <- data.frame(x1 = runif(n), x2 = runif(n),
> ##D            x3 = runif(n), x4 = runif(n))
> ##D dval <- ncol(rdata)  # Number of covariates
> ##D # Involves x1, x2, ... a rank-1 model:
> ##D ymatrix <- with(rdata,
> ##D   matrix(rpois(n*S, exp(3 + x1 - 0.5*x2)), n, S))
> ##D H.C <- vector("list", dval)  # Ordinary "rrvglm"
> ##D for (i in 1:dval) H.C[[i]] <- CM.free(myrank)
> ##D names(H.C) <- paste0("x", 1:dval)
> ##D H.A <- list(CM.free(S))  # rank-1
> ##D 
> ##D rfit1 <- rrvglm(ymatrix ~ x1 + x2 + x3 + x4,
> ##D            poissonff, rdata, trace = TRUE)
> ##D class(rfit1)
> ##D dfit1 <- rrvglm(ymatrix ~ x1 + x2 + x3 + x4,
> ##D            poissonff, rdata, trace = TRUE,
> ##D            H.A = H.A,    # drrvglm
> ##D            H.C = H.C)    # drrvglm
> ##D class(dfit1)
> ##D Coef(rfit1)  # The RR-VGLM is the same as
> ##D Coef(dfit1)  # the DRR-VGLM.
> ##D max(abs(predict(rfit1) - predict(dfit1)))  # 0
> ##D abs(logLik(rfit1) - logLik(dfit1))  # 0
> ##D summary(rfit1)
> ##D summary(dfit1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("summarypvgam")
> ### * summarypvgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summarypvgam
> ### Title: Summarizing Penalized Vector Generalized Additive Model Fits
> ### Aliases: summarypvgam show.summary.pvgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hfit2 <- vgam(agaaus ~ sm.os(altitude), binomialff, data = hunua)
> ##D coef(hfit2, matrix = TRUE)
> ##D summary(hfit2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("summaryvgam")
> ### * summaryvgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryvgam
> ### Title: Summarizing Vector Generalized Additive Model Fits
> ### Aliases: summaryvgam show.summary.vgam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> hfit <- vgam(agaaus ~ s(altitude, df = 2), binomialff, data = hunua)
> summary(hfit)

Call:
vgam(formula = agaaus ~ s(altitude, df = 2), family = binomialff, 
    data = hunua)

Name of additive predictor: logitlink(prob) 

(Default) Dispersion Parameter for binomialff family:   1

Residual deviance:  394.9298 on 389.167 degrees of freedom

Log-likelihood: -197.4649 on 389.167 degrees of freedom

Number of Fisher scoring iterations:  6 

DF for Terms and Approximate Chi-squares for Nonparametric Effects

                    Df Npar Df Npar Chisq     P(Chi)
(Intercept)          1                              
s(altitude, df = 2)  1     0.8     9.2773 0.00167449
> summary(hfit)@anova  # Table for (approximate) testing of linearity
                    Df Npar Df Npar Chisq      P(Chi)
(Intercept)          1      NA         NA          NA
s(altitude, df = 2)  1     0.8   9.277346 0.001674495
> 
> 
> 
> cleanEx()
> nameEx("summaryvglm")
> ### * summaryvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summaryvglm
> ### Title: Summarizing Vector Generalized Linear Model Fits
> ### Aliases: summaryvglm show.summary.vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## For examples see example(glm)
> pneumo <- transform(pneumo, let = log(exposure.time))
> (afit <- vglm(cbind(normal, mild, severe) ~ let, acat, data = pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = acat, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
   -8.9360297    -3.0390622     2.1653729     0.9020936 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 5.347382 
Log-likelihood: -25.25054 

This is an adjacent categories model with 3 levels
> coef(afit, matrix = TRUE)
            loglink(P[Y=2]/P[Y=1]) loglink(P[Y=3]/P[Y=2])
(Intercept)              -8.936030             -3.0390622
let                       2.165373              0.9020936
> summary(afit)  # Might suffer from the Hauck-Donner effect
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = acat, 
    data = pneumo)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  -8.9360     1.5804  -5.654 1.57e-08 ***
(Intercept):2  -3.0391     2.3761  -1.279    0.201    
let:1           2.1654     0.4575   4.733 2.21e-06 ***
let:2           0.9021     0.6690   1.348    0.178    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(P[Y=2]/P[Y=1]), loglink(P[Y=3]/P[Y=2])

Residual deviance: 5.3474 on 12 degrees of freedom

Log-likelihood: -25.2505 on 12 degrees of freedom

Number of Fisher scoring iterations: 4 

Warning: Hauck-Donner effect detected in the following estimate(s):
'(Intercept):1'


Exponentiated coefficients:
   let:1    let:2 
8.717852 2.464758 
> coef(summary(afit))
                Estimate Std. Error   z value     Pr(>|z|)
(Intercept):1 -8.9360297  1.5804347 -5.654159 1.566108e-08
(Intercept):2 -3.0390622  2.3760683 -1.279030 2.008866e-01
let:1          2.1653729  0.4574859  4.733202 2.210057e-06
let:2          0.9020936  0.6689816  1.348458 1.775111e-01
> summary(afit, lrt0 = TRUE, score0 = TRUE, wald0 = TRUE)
Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = acat, 
    data = pneumo)

Likelihood ratio test coefficients: 
      Estimate z value Pr(>|z|)    
let:1   2.1654   6.445 1.16e-10 ***
let:2   0.9021   1.364    0.173    

Rao score test coefficients: 
      Estimate Std. Error z value Pr(>|z|)    
let:1   2.1654     0.2260   5.654 1.57e-08 ***
let:2   0.9021     0.6576   1.364    0.172    

Wald (modified by IRLS iterations) coefficients: 
      Estimate Std. Error z value Pr(>|z|)    
let:1   2.1654     0.2260   9.583   <2e-16 ***
let:2   0.9021     0.6576   1.372     0.17    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(P[Y=2]/P[Y=1]), loglink(P[Y=3]/P[Y=2])

Residual deviance: 5.3474 on 12 degrees of freedom

Log-likelihood: -25.2505 on 12 degrees of freedom

Number of Fisher scoring iterations: 4 


Exponentiated coefficients:
   let:1    let:2 
8.717852 2.464758 
> 
> 
> 
> cleanEx()
> nameEx("tobit")
> ### * tobit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tobit
> ### Title: Tobit Regression
> ### Aliases: tobit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Here, fit1 is a standard Tobit model and fit2 is nonstandard
> tdata <- data.frame(x2 = seq(-1, 1, length = (nn <- 100)))
> set.seed(1)
> Lower <- 1; Upper <- 4  # For the nonstandard Tobit model
> tdata <- transform(tdata,
+                    Lower.vec = rnorm(nn, Lower, 0.5),
+                    Upper.vec = rnorm(nn, Upper, 0.5))
> meanfun1 <- function(x) 0 + 2*x
> meanfun2 <- function(x) 2 + 2*x
> meanfun3 <- function(x) 3 + 2*x
> tdata <- transform(tdata,
+   y1 = rtobit(nn, mean = meanfun1(x2)),  # Standard Tobit model
+   y2 = rtobit(nn, mean = meanfun2(x2), Lower = Lower, Upper = Upper),
+   y3 = rtobit(nn, mean = meanfun3(x2), Lower = Lower.vec,
+               Upper = Upper.vec),
+   y4 = rtobit(nn, mean = meanfun3(x2), Lower = Lower.vec,
+               Upper = Upper.vec))
> with(tdata, table(y1 == 0))  # How many censored values?

FALSE  TRUE 
   52    48 
> with(tdata, table(y2 == Lower | y2 == Upper))  # Ditto

FALSE  TRUE 
   66    34 
> with(tdata, table(attr(y2, "cenL")))

FALSE  TRUE 
   75    25 
> with(tdata, table(attr(y2, "cenU")))

FALSE  TRUE 
   91     9 
> 
> fit1 <- vglm(y1 ~ x2, tobit, data = tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -94.952489
VGLM    linear loop  2 :  loglikelihood = -90.601839
VGLM    linear loop  3 :  loglikelihood = -90.335693
VGLM    linear loop  4 :  loglikelihood = -90.334971
VGLM    linear loop  5 :  loglikelihood = -90.334971
> coef(fit1, matrix = TRUE)
                    mu loglink(sd)
(Intercept) 0.06794616 -0.06234739
x2          2.02595920  0.00000000
> summary(fit1)
Call:
vglm(formula = y1 ~ x2, family = tobit, data = tdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.06795    0.13614   0.499    0.618    
(Intercept):2 -0.06235    0.10273  -0.607    0.544    
x2             2.02596    0.23949   8.459   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: mu, loglink(sd)

Log-likelihood: -90.335 on 197 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> fit2 <- vglm(y2 ~ x2,
+              tobit(Lower = Lower, Upper = Upper, type.f = "cens"),
+              data = tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -117.40947
VGLM    linear loop  2 :  loglikelihood = -115.71547
VGLM    linear loop  3 :  loglikelihood = -115.66898
VGLM    linear loop  4 :  loglikelihood = -115.66819
VGLM    linear loop  5 :  loglikelihood = -115.66818
VGLM    linear loop  6 :  loglikelihood = -115.66818
> table(fit2@extra$censoredL)

FALSE  TRUE 
   75    25 
> table(fit2@extra$censoredU)

FALSE  TRUE 
   91     9 
> coef(fit2, matrix = TRUE)
                  mu loglink(sd)
(Intercept) 2.053261 -0.04893087
x2          1.880404  0.00000000
> 
> fit3 <- vglm(y3 ~ x2, tobit(Lower = with(tdata, Lower.vec),
+                             Upper = with(tdata, Upper.vec),
+                             type.f = "cens"),
+              data = tdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -118.69013
VGLM    linear loop  2 :  loglikelihood = -117.11926
VGLM    linear loop  3 :  loglikelihood = -117.07583
VGLM    linear loop  4 :  loglikelihood = -117.07438
VGLM    linear loop  5 :  loglikelihood = -117.07433
VGLM    linear loop  6 :  loglikelihood = -117.07433
VGLM    linear loop  7 :  loglikelihood = -117.07433
> table(fit3@extra$censoredL)

FALSE  TRUE 
   84    16 
> table(fit3@extra$censoredU)

FALSE  TRUE 
   77    23 
> coef(fit3, matrix = TRUE)
                  mu loglink(sd)
(Intercept) 2.904743  0.09173307
x2          2.178193  0.00000000
> 
> # fit4 is fit3 but with type.fitted = "uncen".
> fit4 <- vglm(cbind(y3, y4) ~ x2,
+              tobit(Lower = rep(with(tdata, Lower.vec), each = 2),
+                    Upper = rep(with(tdata, Upper.vec), each = 2),
+                    byrow.arg = TRUE),
+              data = tdata, crit = "coeff", trace = TRUE)
VGLM    linear loop  1 :  coefficients = 
2.89014257, 0.25304478, 2.96031053, 0.20508806, 2.12538698, 2.02573207
VGLM    linear loop  2 :  coefficients = 
2.885765205, 0.093423424, 2.942093302, 0.043317927, 2.232760342, 2.239483502
VGLM    linear loop  3 :  coefficients = 
2.905736069, 0.096906039, 2.951169180, 0.023582998, 2.181484799, 2.175403035
VGLM    linear loop  4 :  coefficients = 
2.904410822, 0.091524289, 2.950632485, 0.016355913, 2.179996601, 2.164449979
VGLM    linear loop  5 :  coefficients = 
2.904781633, 0.091888984, 2.950534264, 0.014991137, 2.178221623, 2.160988668
VGLM    linear loop  6 :  coefficients = 
2.904730135, 0.091715330, 2.950515287, 0.014622934, 2.178246473, 2.160270160
VGLM    linear loop  7 :  coefficients = 
2.904742983, 0.091733074, 2.950510499, 0.014541499, 2.178193053, 2.160085708
VGLM    linear loop  8 :  coefficients = 
2.904740949, 0.091727259, 2.950509419, 0.014521425, 2.178195929, 2.160043739
VGLM    linear loop  9 :  coefficients = 
2.904741408, 0.091728014, 2.950509157, 0.014516762, 2.178194244, 2.160033552
VGLM    linear loop  10 :  coefficients = 
2.904741329, 0.091727813, 2.950509096, 0.014515643, 2.178194396, 2.160031165
VGLM    linear loop  11 :  coefficients = 
2.904741346, 0.091727843, 2.950509081, 0.014515379, 2.178194341, 2.160030595
VGLM    linear loop  12 :  coefficients = 
2.904741343, 0.091727836, 2.950509078, 0.014515317, 2.178194347, 2.160030461
VGLM    linear loop  13 :  coefficients = 
2.904741344, 0.091727837, 2.950509077, 0.014515302, 2.178194345, 2.160030429
VGLM    linear loop  14 :  coefficients = 
2.904741344, 0.091727837, 2.950509077, 0.014515298, 2.178194346, 2.160030421
VGLM    linear loop  15 :  coefficients = 
2.904741344, 0.091727837, 2.950509077, 0.014515297, 2.178194346, 2.160030419
> head(fit4@extra$censoredL)  # A matrix
     y3    y4
1 FALSE FALSE
2 FALSE  TRUE
3  TRUE  TRUE
4  TRUE  TRUE
5  TRUE FALSE
6 FALSE FALSE
> head(fit4@extra$censoredU)  # A matrix
     y3    y4
1 FALSE FALSE
2 FALSE FALSE
3 FALSE FALSE
4 FALSE FALSE
5 FALSE FALSE
6 FALSE FALSE
> head(fit4@misc$Lower)       # A matrix
          [,1]      [,2]
[1,] 0.6867731 0.6867731
[2,] 1.0918217 1.0918217
[3,] 0.5821857 0.5821857
[4,] 1.7976404 1.7976404
[5,] 1.1647539 1.1647539
[6,] 0.5897658 0.5897658
> head(fit4@misc$Upper)       # A matrix
         [,1]     [,2]
[1,] 3.689817 3.689817
[2,] 4.021058 4.021058
[3,] 3.544539 3.544539
[4,] 4.079014 4.079014
[5,] 3.672708 3.672708
[6,] 4.883644 4.883644
> coef(fit4, matrix = TRUE)
                 mu1 loglink(sd1)      mu2 loglink(sd2)
(Intercept) 2.904741   0.09172784 2.950509    0.0145153
x2          2.178194   0.00000000 2.160030    0.0000000
> 
> ## Not run: 
> ##D  # Plot fit1--fit4
> ##D par(mfrow = c(2, 2))
> ##D 
> ##D plot(y1 ~ x2, tdata, las = 1, main = "Standard Tobit model",
> ##D      col = as.numeric(attr(y1, "cenL")) + 3,
> ##D      pch = as.numeric(attr(y1, "cenL")) + 1)
> ##D legend(x = "topleft", leg = c("censored", "uncensored"),
> ##D        pch = c(2, 1), col = c("blue", "green"))
> ##D legend(-1.0, 2.5, c("Truth", "Estimate", "Naive"), lwd = 2,
> ##D        col = c("purple", "orange", "black"), lty = c(1, 2, 2))
> ##D lines(meanfun1(x2) ~ x2, tdata, col = "purple", lwd = 2)
> ##D lines(fitted(fit1) ~ x2, tdata, col = "orange", lwd = 2, lty = 2)
> ##D lines(fitted(lm(y1 ~ x2, tdata)) ~ x2, tdata, col = "black",
> ##D       lty = 2, lwd = 2)  # This is simplest but wrong!
> ##D 
> ##D plot(y2 ~ x2, data = tdata, las = 1, main = "Tobit model",
> ##D      col = as.numeric(attr(y2, "cenL")) + 3 +
> ##D            as.numeric(attr(y2, "cenU")),
> ##D      pch = as.numeric(attr(y2, "cenL")) + 1 +
> ##D            as.numeric(attr(y2, "cenU")))
> ##D legend(x = "topleft", leg = c("censored", "uncensored"),
> ##D        pch = c(2, 1), col = c("blue", "green"))
> ##D legend(-1.0, 3.5, c("Truth", "Estimate", "Naive"), lwd = 2,
> ##D        col = c("purple", "orange", "black"), lty = c(1, 2, 2))
> ##D lines(meanfun2(x2) ~ x2, tdata, col = "purple", lwd = 2)
> ##D lines(fitted(fit2) ~ x2, tdata, col = "orange", lwd = 2, lty = 2)
> ##D lines(fitted(lm(y2 ~ x2, tdata)) ~ x2, tdata, col = "black",
> ##D       lty = 2, lwd = 2)  # This is simplest but wrong!
> ##D 
> ##D plot(y3 ~ x2, data = tdata, las = 1,
> ##D      main = "Tobit model with nonconstant censor levels",
> ##D      col = as.numeric(attr(y3, "cenL")) + 2 +
> ##D            as.numeric(attr(y3, "cenU") * 2),
> ##D      pch = as.numeric(attr(y3, "cenL")) + 1 +
> ##D            as.numeric(attr(y3, "cenU") * 2))
> ##D legend(x = "topleft", pch = c(2, 3, 1), col = c(3, 4, 2),
> ##D        leg = c("censoredL", "censoredU", "uncensored"))
> ##D legend(-1.0, 3.5, c("Truth", "Estimate", "Naive"), lwd = 2,
> ##D        col = c("purple", "orange", "black"), lty = c(1, 2, 2))
> ##D lines(meanfun3(x2) ~ x2, tdata, col = "purple", lwd = 2)
> ##D lines(fitted(fit3) ~ x2, tdata, col = "orange", lwd = 2, lty = 2)
> ##D lines(fitted(lm(y3 ~ x2, tdata)) ~ x2, tdata, col = "black",
> ##D       lty = 2, lwd = 2)  # This is simplest but wrong!
> ##D 
> ##D plot(y3 ~ x2, data = tdata, las = 1,
> ##D      main = "Tobit model with nonconstant censor levels",
> ##D      col = as.numeric(attr(y3, "cenL")) + 2 +
> ##D            as.numeric(attr(y3, "cenU") * 2),
> ##D      pch = as.numeric(attr(y3, "cenL")) + 1 +
> ##D            as.numeric(attr(y3, "cenU") * 2))
> ##D legend(x = "topleft", pch = c(2, 3, 1), col = c(3, 4, 2),
> ##D        leg = c("censoredL", "censoredU", "uncensored"))
> ##D legend(-1.0, 3.5, c("Truth", "Estimate", "Naive"), lwd = 2, 
> ##D        col = c("purple", "orange", "black"), lty = c(1, 2, 2))
> ##D lines(meanfun3(x2) ~ x2, data = tdata, col = "purple", lwd = 2)
> ##D lines(fitted(fit4)[, 1] ~ x2, tdata, col="orange", lwd = 2, lty = 2)
> ##D lines(fitted(lm(y3 ~ x2, tdata)) ~ x2, data = tdata, col = "black",
> ##D       lty = 2, lwd = 2)  # This is simplest but wrong!
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("tobitUC")
> ### * tobitUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tobit
> ### Title: The Tobit Distribution
> ### Aliases: Tobit dtobit ptobit qtobit rtobit
> ### Keywords: distribution
> 
> ### ** Examples
> 
> mu <- 0.5; x <- seq(-2, 4, by = 0.01)
> Lower <- -1; Upper <- 2.0
> 
> integrate(dtobit, lower = Lower, upper = Upper,
+           mean = mu, Lower = Lower, Upper = Upper)$value +
+ dtobit(Lower, mean = mu, Lower = Lower, Upper = Upper) +
+ dtobit(Upper, mean = mu, Lower = Lower, Upper = Upper)  # Adds to 1
[1] 1
> 
> ## Not run: 
> ##D plot(x, ptobit(x, m = mu, Lower = Lower, Upper = Upper),
> ##D      type = "l", ylim = 0:1, las = 1, col = "orange",
> ##D      ylab = paste("ptobit(m = ", mu, ", sd = 1, Lower =", Lower,
> ##D                   ", Upper =", Upper, ")"),
> ##D      main = "Orange is the CDF; blue is density",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0)
> ##D lines(x, dtobit(x, m = mu, L = Lower, U = Upper), col = "blue")
> ##D 
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qtobit(probs, m = mu, Lower = Lower, Upper = Upper)
> ##D lines(Q, ptobit(Q, m = mu, Lower = Lower, Upper = Upper),
> ##D       col = "purple", lty = "dashed", type = "h")
> ##D lines(Q, dtobit(Q, m = mu, Lower = Lower, Upper = Upper),
> ##D       col = "darkgreen", lty = "dashed", type = "h")
> ##D abline(h = probs, col = "purple", lty = "dashed")
> ##D max(abs(ptobit(Q, mu, L = Lower, U = Upper) - probs))  # Should be 0
> ##D 
> ##D epts <- c(Lower, Upper)  # Endpoints have a spike (not quite, actually)
> ##D lines(epts, dtobit(epts, m = mu, Lower = Lower, Upper = Upper),
> ##D       col = "blue", lwd = 3, type = "h")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("topple")
> ### * topple
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: topple
> ### Title: Topp-Leone Distribution Family Function
> ### Aliases: topple
> ### Keywords: models regression
> 
> ### ** Examples
> 
> tdata <- data.frame(y = rtopple(1000, logitlink(1, inverse = TRUE)))
> tfit <- vglm(y ~ 1, topple, tdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.0113613
VGLM    linear loop  2 :  coefficients = 1.0480927
VGLM    linear loop  3 :  coefficients = 1.0487841
VGLM    linear loop  4 :  coefficients = 1.0487843
VGLM    linear loop  5 :  coefficients = 1.0487843
> coef(tfit, matrix = TRUE)
            logitlink(shape)
(Intercept)         1.048784
> Coef(tfit)
    shape 
0.7405414 
> 
> 
> 
> cleanEx()
> nameEx("toppleUC")
> ### * toppleUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Topple
> ### Title: The Topp-Leone Distribution
> ### Aliases: Topple dtopple ptopple qtopple rtopple
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  shape <- 0.7; x <- seq(0.02, 0.999, length = 300)
> ##D plot(x, dtopple(x, shape = shape), type = "l", col = "blue",
> ##D      main = "Blue is density, orange is CDF", ylab = "", las = 1,
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, ptopple(x, shape = shape), type = "l", col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qtopple(probs, shape = shape)
> ##D lines(Q, dtopple(Q, shape), col = "purple", lty = 3, type = "h")
> ##D lines(Q, ptopple(Q, shape), col = "purple", lty = 3, type = "h")
> ##D abline(h = probs, col = "purple", lty = 3)
> ##D max(abs(ptopple(Q, shape) - probs))  # Should be zero
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("toxop")
> ### * toxop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: toxop
> ### Title: Toxoplasmosis Data
> ### Aliases: toxop
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  with(toxop, plot(rainfall, positive/ssize, col = "blue"))
> ##D plot(toxop, col = "blue") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("triangleUC")
> ### * triangleUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Triangle
> ### Title: The Triangle Distribution
> ### Aliases: Triangle dtriangle ptriangle qtriangle rtriangle
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  x <- seq(-0.1, 1.1, by = 0.01); theta <- 0.75
> ##D plot(x, dtriangle(x, theta = theta), type = "l", col = "blue", las = 1,
> ##D      main = "Blue is density, orange is the CDF",
> ##D      sub = "Purple lines are the 10,20,...,90 percentiles",
> ##D      ylim = c(0,2), ylab = "")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D lines(x, ptriangle(x, theta = theta), col = "orange")
> ##D probs <- seq(0.1, 0.9, by = 0.1)
> ##D Q <- qtriangle(probs, theta = theta)
> ##D lines(Q, dtriangle(Q, theta = theta), col = "purple", lty = 3, type = "h")
> ##D ptriangle(Q, theta = theta) - probs  # Should be all zero
> ##D abline(h = probs, col = "purple", lty = 3) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("trim.constraints")
> ### * trim.constraints
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trim.constraints
> ### Title: Trimmed Constraint Matrices
> ### Aliases: trim.constraints
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  data("xs.nz", package = "VGAMdata")
> ##D fit1 <-
> ##D   vglm(cbind(worry, worrier) ~ bs(age) + sex + ethnicity + cat + dog,
> ##D        binom2.or(zero = NULL), data = xs.nz, trace = TRUE)
> ##D summary(fit1, HDEtest = FALSE)  # 'cat' is not significant at all
> ##D dim(constraints(fit1, matrix = TRUE))
> ##D (tclist1 <- trim.constraints(fit1))  # No 'cat'
> ##D fit2 <-  # Delete 'cat' manually from the formula:
> ##D   vglm(cbind(worry, worrier) ~ bs(age) + sex + ethnicity +       dog,
> ##D        binom2.or(zero = NULL), data = xs.nz,
> ##D        constraints = tclist1, trace = TRUE)
> ##D summary(fit2, HDEtest = FALSE)  # A simplified model
> ##D dim(constraints(fit2, matrix = TRUE))  # Fewer regression coefficients
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("trinormal")
> ### * trinormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trinormal
> ### Title: Trivariate Normal Distribution Family Function
> ### Aliases: trinormal
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D   set.seed(123); nn <- 1000
> ##D tdata <- data.frame(x2 = runif(nn), x3 = runif(nn))
> ##D tdata <- transform(tdata, y1 = rnorm(nn, 1 + 2 * x2),
> ##D                           y2 = rnorm(nn, 3 + 4 * x2),
> ##D                           y3 = rnorm(nn, 4 + 5 * x2))
> ##D fit1 <- vglm(cbind(y1, y2, y3) ~ x2, data = tdata,
> ##D              trinormal(eq.sd = TRUE, eq.cor = TRUE), trace = TRUE)
> ##D coef(fit1, matrix = TRUE)
> ##D constraints(fit1)
> ##D summary(fit1)
> ##D # Try this when eq.sd = TRUE, eq.cor = TRUE:
> ##D fit2 <-
> ##D   vglm(cbind(y1, y2, y3) ~ x2, data = tdata, stepsize = 0.25,
> ##D        trinormal(eq.sd = TRUE, eq.cor = TRUE,
> ##D                  lrho12 = extlogitlink(min = -0.5),
> ##D                  lrho23 = extlogitlink(min = -0.5),
> ##D                  lrho13 = extlogitlink(min = -0.5)), trace = TRUE)
> ##D coef(fit2, matrix = TRUE)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("trinormalUC")
> ### * trinormalUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Trinorm
> ### Title: Trivariate Normal Distribution Density and Random Variates
> ### Aliases: Trinorm dtrinorm rtrinorm
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D nn <- 1000
> ##D tdata <- data.frame(x2 = sort(runif(nn)))
> ##D tdata <- transform(tdata, mean1 = 1 + 2 * x2,
> ##D                    mean2 = 3 + 1 * x2, mean3 = 4,
> ##D                    var1 = exp( 1), var2 = exp( 1), var3 = exp( 1),
> ##D                    rho12 = rhobitlink( 1, inverse = TRUE),
> ##D                    rho23 = rhobitlink( 1, inverse = TRUE),
> ##D                    rho13 = rhobitlink(-1, inverse = TRUE))
> ##D ymat <- with(tdata, rtrinorm(nn, mean1, mean2, mean3,
> ##D                              var1, var2, var3,
> ##D                              sqrt(var1)*sqrt(var1)*rho12,
> ##D                              sqrt(var2)*sqrt(var3)*rho23,
> ##D                              sqrt(var1)*sqrt(var3)*rho13))
> ##D pairs(ymat, col = "blue")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("trplot")
> ### * trplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trplot
> ### Title: Trajectory Plot
> ### Aliases: trplot
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  set.seed(123)
> ##D hspider[, 1:6] <- scale(hspider[, 1:6])  # Stdze environ. vars
> ##D p1cqo <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute,
> ##D                    Arctperi, Auloalbi, Pardlugu, Pardmont,
> ##D                    Pardnigr, Pardpull, Trocterr, Zoraspin) ~
> ##D             WaterCon + BareSand + FallTwig +
> ##D             CoveMoss + CoveHerb + ReflLux,
> ##D             poissonff, data = hspider, Crow1positive = FALSE)
> ##D 
> ##D nos <- ncol(depvar(p1cqo))
> ##D clr <- 1:nos  # OR (1:(nos+1))[-7]  to omit yellow
> ##D 
> ##D trplot(p1cqo, which.species = 1:3, log = "xy", lwd = 2,
> ##D        col = c("blue", "orange", "green"), label = TRUE) -> ii
> ##D legend(0.00005, 0.3, paste(ii$species[, 1], ii$species[, 2],
> ##D                            sep = " and "),
> ##D        lwd = 2, lty = 1, col = c("blue", "orange", "green"))
> ##D abline(a = 0, b = 1, lty = "dashed", col = "grey") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("trplot.qrrvglm")
> ### * trplot.qrrvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trplot.qrrvglm
> ### Title: Trajectory plot for QRR-VGLMs
> ### Aliases: trplot.qrrvglm
> ### Keywords: regression hplot nonlinear
> 
> ### ** Examples
> ## Not run: 
> ##D  set.seed(111)  # Leads to the global solution
> ##D # hspider[,1:6] <- scale(hspider[,1:6])  # Stdze the environ vars
> ##D p1 <- cqo(cbind(Alopacce, Alopcune, Alopfabr, Arctlute,
> ##D                 Arctperi, Auloalbi, Pardlugu, Pardmont,
> ##D                 Pardnigr, Pardpull, Trocterr, Zoraspin) ~
> ##D           WaterCon + BareSand + FallTwig + CoveMoss +
> ##D           CoveHerb + ReflLux,
> ##D           poissonff, data = hspider, trace = FALSE)
> ##D 
> ##D trplot(p1, which.species = 1:3, log = "xy", type = "b", lty = 1,
> ##D        main = "Trajectory plot of three hunting spiders species",
> ##D        col = c("blue","red","green"), lwd = 2, label = TRUE) -> ii
> ##D legend(0.00005, 0.3, lwd = 2, lty = 1,
> ##D        col = c("blue", "red", "green"),
> ##D        with(ii, paste(species.names[,1], species.names[,2],
> ##D                       sep = " and ")))
> ##D abline(a = 0, b = 1, lty = "dashed", col = "grey")  # Ref. line
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("truncparetoUC")
> ### * truncparetoUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Truncpareto
> ### Title: The Truncated Pareto Distribution
> ### Aliases: Truncpareto dtruncpareto ptruncpareto qtruncpareto
> ###   rtruncpareto
> ### Keywords: distribution
> 
> ### ** Examples
>  lower <- 3; upper <- 8; kay <- exp(0.5)
> ## Not run: 
> ##D  xx <- seq(lower - 0.5, upper + 0.5, len = 401)
> ##D plot(xx, dtruncpareto(xx, low = lower, upp = upper, shape = kay),
> ##D      main = "Truncated Pareto density split into 10 equal areas",
> ##D      type = "l", ylim = 0:1, xlab = "x")
> ##D abline(h = 0, col = "blue", lty = 2)
> ##D qq <- qtruncpareto(seq(0.1, 0.9, by = 0.1), low = lower, upp = upper,
> ##D                    shape = kay)
> ##D lines(qq, dtruncpareto(qq, low = lower, upp = upper, shape = kay),
> ##D       col = "purple", lty = 3, type = "h")
> ##D lines(xx, ptruncpareto(xx, low = lower, upp = upper, shape = kay),
> ##D       col = "orange") 
> ## End(Not run)
> pp <- seq(0.1, 0.9, by = 0.1)
> qq <- qtruncpareto(pp, lower = lower, upper = upper, shape = kay)
> 
> ptruncpareto(qq, lower = lower, upper = upper, shape = kay)
[1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
> qtruncpareto(ptruncpareto(qq, lower = lower, upper = upper, shape = kay),
+          lower = lower, upper = upper, shape = kay) - qq  # Should be all 0
[1] 0 0 0 0 0 0 0 0 0
> 
> 
> 
> cleanEx()
> nameEx("truncweibull")
> ### * truncweibull
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: truncweibull
> ### Title: Truncated Weibull Distribution Family Function
> ### Aliases: truncweibull
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D nn <- 5000; prop.lost <- 0.40   # Proportion lost to truncation
> ##D wdata <- data.frame(x2 = runif(nn))  # Complete Weibull data
> ##D wdata <- transform(wdata,
> ##D          Betaa = exp(1))  # > 2 okay (satisfies regularity conds)
> ##D wdata <- transform(wdata, Alpha = exp(0.5 - 1 * x2))
> ##D wdata <- transform(wdata, Shape = Betaa,
> ##D #                         aaa   = Betaa,
> ##D #                         bbb   = 1 / Alpha^(1 / Betaa),
> ##D                           Scale = 1 / Alpha^(1 / Betaa))
> ##D wdata <- transform(wdata, y2 = rweibull(nn, Shape, scale = Scale))
> ##D summary(wdata)
> ##D 
> ##D # Proportion lost:
> ##D lower.limit2 <- with(wdata, quantile(y2, prob = prop.lost))
> ##D # Smaller due to truncation:
> ##D wdata <- subset(wdata, y2 > lower.limit2)
> ##D 
> ##D fit1 <- vglm(y2 ~ x2, maxit = 100, trace = TRUE,
> ##D              truncweibull(lower.limit = lower.limit2), wdata)
> ##D coef(fit1, matrix = TRUE)
> ##D summary(fit1)
> ##D vcov(fit1)
> ##D head(fit1@extra$lower.limit)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ucberk")
> ### * ucberk
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ucberk
> ### Title: University California Berkeley Graduate Admissions
> ### Aliases: ucberk
> ### Keywords: datasets
> 
> ### ** Examples
> 
> summary(ucberk)
 dept      m.deny         m.admit           w.deny          w.admit      
 A:1   Min.   :138.0   Min.   : 22.00   Min.   :  8.00   Min.   : 17.00  
 B:1   1st Qu.:205.5   1st Qu.: 69.75   1st Qu.: 75.25   1st Qu.: 40.25  
 C:1   Median :242.5   Median :129.50   Median :271.50   Median : 91.50  
 D:1   Mean   :248.7   Mean   :199.83   Mean   :213.00   Mean   : 92.83  
 E:1   3rd Qu.:304.2   3rd Qu.:299.50   3rd Qu.:312.50   3rd Qu.:121.75  
 F:1   Max.   :351.0   Max.   :512.00   Max.   :391.00   Max.   :202.00  
> 
> 
> 
> cleanEx()
> nameEx("uninormal")
> ### * uninormal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: uninormal
> ### Title: Univariate Normal Distribution
> ### Aliases: uninormal gaussianff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> udata <- data.frame(x2 = rnorm(nn <- 200))
> udata <- transform(udata,
+            y1  = rnorm(nn, m = 1 - 3*x2, sd = exp(1 + 0.2*x2)),
+            y2a = rnorm(nn, m = 1 + 2*x2, sd = exp(1 + 2.0*x2)^0.5),
+            y2b = rnorm(nn, m = 1 + 2*x2, sd = exp(1 + 2.0*x2)^0.5))
> fit1 <- vglm(y1 ~ x2, uninormal(zero = NULL), udata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -506.96106
VGLM    linear loop  2 :  loglikelihood = -488.28819
VGLM    linear loop  3 :  loglikelihood = -486.22046
VGLM    linear loop  4 :  loglikelihood = -486.195
VGLM    linear loop  5 :  loglikelihood = -486.19495
VGLM    linear loop  6 :  loglikelihood = -486.19495
> coef(fit1, matrix = TRUE)
                 mean loglink(sd)
(Intercept)  1.099548   1.0068426
x2          -3.067171   0.1461352
> fit2 <- vglm(cbind(y2a, y2b) ~ x2, data = udata, trace = TRUE,
+              uninormal(var = TRUE, parallel = TRUE ~ x2,
+                        zero = NULL))
VGLM    linear loop  1 :  loglikelihood = -1205.1644
VGLM    linear loop  2 :  loglikelihood = -1113.1504
VGLM    linear loop  3 :  loglikelihood = -1056.8573
VGLM    linear loop  4 :  loglikelihood = -1011.3604
VGLM    linear loop  5 :  loglikelihood = -940.31073
VGLM    linear loop  6 :  loglikelihood = -853.42373
VGLM    linear loop  7 :  loglikelihood = -813.49511
VGLM    linear loop  8 :  loglikelihood = -810.85062
VGLM    linear loop  9 :  loglikelihood = -810.84236
VGLM    linear loop  10 :  loglikelihood = -810.84236
> coef(fit2, matrix = TRUE)
               mean1 loglink(var1)    mean2 loglink(var2)
(Intercept) 1.063906      1.063906 1.063906      1.063906
x2          2.102699      2.102699 2.102699      2.102699
> 
> # Generate data from N(mu=theta=10, sigma=theta) and estimate theta.
> theta <- 10
> udata <- data.frame(y3 = rnorm(100, m = theta, sd = theta))
> fit3a <- vglm(y3 ~ 1, uninormal(lsd = "identitylink"), data = udata,
+              constraints = list("(Intercept)" = rbind(1, 1)))
> fit3b <- vglm(y3 ~ 1, uninormal(lsd = "identitylink",
+                         parallel = TRUE ~ 1, zero = NULL), udata)
> coef(fit3a, matrix = TRUE)
                mean       sd
(Intercept) 10.59571 10.59571
> coef(fit3b, matrix = TRUE)  # Same as fit3a
                mean       sd
(Intercept) 10.59571 10.59571
> 
> 
> 
> cleanEx()
> nameEx("vcovvlm")
> ### * vcovvlm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vcovvlm
> ### Title: Calculate Variance-Covariance Matrix for a Fitted VLM or RR-VGLM
> ###   or DRR-VGLM or QRR-VGLM Object
> ### Aliases: vcovvlm vcovrrvglm vcovdrrvglm vcovqrrvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D ndata <- data.frame(x2 = runif(nn <- 300))
> ##D ndata <- transform(ndata, y1 = rnbinom(nn, mu = exp(3+x2), exp(1)),
> ##D                           y2 = rnbinom(nn, mu = exp(2-x2), exp(0)))
> ##D fit1 <- vglm(cbind(y1, y2) ~ x2, negbinomial, ndata, trace = TRUE)
> ##D fit2 <- rrvglm(y1 ~ x2, negbinomial(zero = NULL), data = ndata)
> ##D coef(fit1, matrix = TRUE)
> ##D vcov(fit1)
> ##D vcov(fit2)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("venice")
> ### * venice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: venice
> ### Title: Venice Maximum Sea Levels Data
> ### Aliases: venice venice90
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D matplot(venice[["year"]], venice[, -1], xlab = "Year",
> ##D         ylab = "Sea level (cm)", type = "l")
> ##D 
> ##D ymat <- as.matrix(venice[, paste("r", 1:10, sep = "")])
> ##D fit1 <- vgam(ymat ~ s(year, df = 3), gumbel(R = 365, mpv = TRUE),
> ##D              venice, trace = TRUE, na.action = na.pass)
> ##D head(fitted(fit1))
> ##D 
> ##D par(mfrow = c(2, 1), xpd = TRUE)
> ##D plot(fit1, se = TRUE, lcol = "blue", llwd = 2, slty = "dashed")
> ##D 
> ##D par(mfrow = c(1,1), bty = "l", xpd = TRUE, las = 1)
> ##D qtplot(fit1, mpv = TRUE, lcol = c(1, 2, 5), tcol = c(1, 2, 5),
> ##D        llwd = 2, pcol = "blue", tadj = 0.1)
> ##D 
> ##D plot(sealevel ~ Year, data = venice90, type = "h", col = "blue")
> ##D summary(venice90)
> ##D dim(venice90)
> ##D round(100 * nrow(venice90)/((2009-1940+1)*365.26*24), dig = 3)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vgam-class")
> ### * vgam-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vgam-class
> ### Title: Class "vgam"
> ### Aliases: vgam-class
> ### Keywords: classes models regression smooth
> 
> ### ** Examples
> 
> # Fit a nonparametric proportional odds model
> pneumo <- transform(pneumo, let = log(exposure.time))
> vgam(cbind(normal, mild, severe) ~ s(let),
+      cumulative(parallel = TRUE), data = pneumo)

Call:
vgam(formula = cbind(normal, mild, severe) ~ s(let), family = cumulative(parallel = TRUE), 
    data = pneumo)


Degrees of Freedom: 16 Total; 10.35 Residual
Residual deviance: 2.363363 
Log-likelihood: -23.75853 
> 
> 
> 
> cleanEx()
> nameEx("vgam")
> ### * vgam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vgam
> ### Title: Fitting Vector Generalized Additive Models
> ### Aliases: vgam
> ### Keywords: models regression smooth
> 
> ### ** Examples
> # Nonparametric proportional odds model
> pneumo <- transform(pneumo, let = log(exposure.time))
> vgam(cbind(normal, mild, severe) ~ s(let),
+      cumulative(parallel = TRUE), data = pneumo, trace = TRUE)
VGAM  s.vam  loop  1 :  deviance = 2.884449
VGAM  s.vam  loop  2 :  deviance = 2.490356
VGAM  s.vam  loop  3 :  deviance = 2.385685
VGAM  s.vam  loop  4 :  deviance = 2.364548
VGAM  s.vam  loop  5 :  deviance = 2.363368
VGAM  s.vam  loop  6 :  deviance = 2.363363
VGAM  s.vam  loop  7 :  deviance = 2.363363

Call:
vgam(formula = cbind(normal, mild, severe) ~ s(let), family = cumulative(parallel = TRUE), 
    data = pneumo, trace = TRUE)


Degrees of Freedom: 16 Total; 10.35 Residual
Residual deviance: 2.363363 
Log-likelihood: -23.75853 
> 
> # Nonparametric logistic regression
> hfit <- vgam(agaaus ~ s(altitude, df = 2), binomialff, hunua)
> ## Not run:  plot(hfit, se = TRUE) 
> phfit <- predict(hfit, type = "terms", raw = TRUE, se = TRUE)
> names(phfit)
[1] "fitted.values" "se.fit"        "df"            "sigma"        
> head(phfit$fitted)
  s(altitude, df = 2)
1        -0.006159306
2        -0.048137984
3        -0.191079032
4        -0.243027764
5        -0.243027764
6        -0.191079032
> head(phfit$se.fit)
  s(altitude, df = 2)
1          0.09530237
2          0.10615371
3          0.15381343
4          0.17420471
5          0.17420471
6          0.15381343
> phfit$df
[1] 389.1674
> phfit$sigma
[1] 1
> 
> ## Not run: 
> ##D    # Fit two species simultaneously
> ##D hfit2 <- vgam(cbind(agaaus, kniexc) ~ s(altitude, df = c(2, 3)),
> ##D               binomialff(multiple.responses = TRUE), data = hunua)
> ##D coef(hfit2, matrix = TRUE)  # Not really interpretable
> ##D plot(hfit2, se = TRUE, overlay = TRUE, lcol = 3:4, scol = 3:4)
> ##D ooo <- with(hunua, order(altitude))
> ##D with(hunua, matplot(altitude[ooo], fitted(hfit2)[ooo,],
> ##D       ylim = c(0, 0.8), las = 1,type = "l", lwd = 2,
> ##D      xlab = "Altitude (m)", ylab = "Probability of presence",
> ##D      main = "Two plant species' response curves"))
> ##D with(hunua, rug(altitude))
> ##D 
> ##D # The 'subset' argument does not work here. Use subset() instead.
> ##D set.seed(1)
> ##D zdata <- data.frame(x2 = runif(nn <- 500))
> ##D zdata <- transform(zdata, y = rbinom(nn, 1, 0.5))
> ##D zdata <- transform(zdata, subS = runif(nn) < 0.7)
> ##D sub.zdata <- subset(zdata, subS)  # Use this instead
> ##D if (FALSE)
> ##D   fit4a <- vgam(cbind(y, y) ~ s(x2, df = 2),
> ##D                 binomialff(multiple.responses = TRUE),
> ##D                 data = zdata, subset = subS)  # This fails!!!
> ##D fit4b <- vgam(cbind(y, y) ~ s(x2, df = 2),
> ##D               binomialff(multiple.responses = TRUE),
> ##D               data = sub.zdata)  # This succeeds!!!
> ##D fit4c <- vgam(cbind(y, y) ~ sm.os(x2),
> ##D               binomialff(multiple.responses = TRUE),
> ##D               data = sub.zdata)  # This succeeds!!!
> ##D par(mfrow = c(2, 2))
> ##D plot(fit4b, se = TRUE, shade = TRUE, shcol = "pink")
> ##D plot(fit4c, se = TRUE, shade = TRUE, shcol = "pink")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vgam.control")
> ### * vgam.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vgam.control
> ### Title: Control Function for vgam()
> ### Aliases: vgam.control
> ### Keywords: optimize models
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> vgam(cbind(normal, mild, severe) ~ s(let, df = 2), multinomial,
+      data = pneumo, trace = TRUE, eps = 1e-4, maxit = 10)
VGAM  s.vam  loop  1 :  deviance = 3.582
VGAM  s.vam  loop  2 :  deviance = 3.405
VGAM  s.vam  loop  3 :  deviance = 3.386
VGAM  s.vam  loop  4 :  deviance = 3.385
VGAM  s.vam  loop  5 :  deviance = 3.385

Call:
vgam(formula = cbind(normal, mild, severe) ~ s(let, df = 2), 
    family = multinomial, data = pneumo, trace = TRUE, eps = 1e-04, 
    maxit = 10)


Degrees of Freedom: 16 Total; 10.53 Residual
Residual deviance: 3.385485 
Log-likelihood: -24.26959 

This is a multinomial logit model with 3 levels
> 
> 
> 
> cleanEx()
> nameEx("vglm-class")
> ### * vglm-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vglm-class
> ### Title: Class "vglm"
> ### Aliases: vglm-class
> ### Keywords: classes
> 
> ### ** Examples
> 
> # Multinomial logit model
> pneumo <- transform(pneumo, let = log(exposure.time))
> vglm(cbind(normal, mild, severe) ~ let, multinomial, data = pneumo)

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = multinomial, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
   11.9750920     3.0390622    -3.0674665    -0.9020936 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 5.347382 
Log-likelihood: -25.25054 

This is a multinomial logit model with 3 levels
> 
> 
> 
> cleanEx()
> nameEx("vglm")
> ### * vglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vglm
> ### Title: Fitting Vector Generalized Linear Models
> ### Aliases: vglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1. See help(glm)
> (d.AD <- data.frame(treatment = gl(3, 3),
+                     outcome = gl(3, 1, 9),
+                     counts = c(18,17,15,20,10,20,25,13,12)))
  treatment outcome counts
1         1       1     18
2         1       2     17
3         1       3     15
4         2       1     20
5         2       2     10
6         2       3     20
7         3       1     25
8         3       2     13
9         3       3     12
> vglm.D93 <- vglm(counts ~ outcome + treatment, poissonff,
+                  data = d.AD, trace = TRUE)
VGLM    linear loop  1 :  deviance = 5.181115
VGLM    linear loop  2 :  deviance = 5.129147
VGLM    linear loop  3 :  deviance = 5.129141
VGLM    linear loop  4 :  deviance = 5.129141
> summary(vglm.D93)
Call:
vglm(formula = counts ~ outcome + treatment, family = poissonff, 
    data = d.AD, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.045e+00  1.709e-01  17.815   <2e-16 ***
outcome2    -4.543e-01  2.022e-01  -2.247   0.0246 *  
outcome3    -2.930e-01  1.927e-01  -1.520   0.1285    
treatment2   6.796e-16  2.000e-01   0.000   1.0000    
treatment3   1.332e-16  2.000e-01   0.000   1.0000    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(lambda) 

Residual deviance: 5.1291 on 4 degrees of freedom

Log-likelihood: -23.3807 on 4 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> 
> # Example 2. Multinomial logit model
> pneumo <- transform(pneumo, let = log(exposure.time))
> vglm(cbind(normal, mild, severe) ~ let, multinomial, pneumo)

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = multinomial, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
   11.9750920     3.0390622    -3.0674665    -0.9020936 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 5.347382 
Log-likelihood: -25.25054 

This is a multinomial logit model with 3 levels
> 
> 
> # Example 3. Proportional odds model
> fit3 <- vglm(cbind(normal, mild, severe) ~ let, propodds, pneumo)
> coef(fit3, matrix = TRUE)
            logitlink(P[Y>=2]) logitlink(P[Y>=3])
(Intercept)          -9.676093         -10.581725
let                   2.596807           2.596807
> constraints(fit3)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

$let
     [,1]
[1,]    1
[2,]    1

> model.matrix(fit3, type = "lm")  # LM model matrix
  (Intercept)      let
1           1 1.757858
2           1 2.708050
3           1 3.068053
4           1 3.314186
5           1 3.511545
6           1 3.676301
7           1 3.828641
8           1 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1

attr(,"assign")$let
[1] 2

attr(,"orig.assign.lm")
[1] 0 1
> model.matrix(fit3)               # Larger VGLM (or VLM) matrix
    (Intercept):1 (Intercept):2      let
1:1             1             0 1.757858
1:2             0             1 1.757858
2:1             1             0 2.708050
2:2             0             1 2.708050
3:1             1             0 3.068053
3:2             0             1 3.068053
4:1             1             0 3.314186
4:2             0             1 3.314186
5:1             1             0 3.511545
5:2             0             1 3.511545
6:1             1             0 3.676301
6:2             0             1 3.676301
7:1             1             0 3.828641
7:2             0             1 3.828641
8:1             1             0 3.941582
8:2             0             1 3.941582
attr(,"assign")
attr(,"assign")$`(Intercept)`
[1] 1 2

attr(,"assign")$let
[1] 3

attr(,"vassign")
attr(,"vassign")$`(Intercept):1`
[1] 1

attr(,"vassign")$`(Intercept):2`
[1] 2

attr(,"vassign")$let
[1] 3

attr(,"constraints")
attr(,"constraints")$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1

attr(,"constraints")$let
     [,1]
[1,]    1
[2,]    1

attr(,"orig.assign.lm")
[1] 0 1
> 
> 
> # Example 4. Bivariate logistic model
> fit4 <- vglm(cbind(nBnW, nBW, BnW, BW) ~ age, binom2.or, coalminers)
> coef(fit4, matrix = TRUE)
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)     -6.5858766    -4.23347003        2.832535
age              0.1029391     0.06534392        0.000000
> depvar(fit4)  # Response are proportions
       nBnW        nBW         BnW          BW
1 0.9431352 0.04866803 0.003586066 0.004610656
2 0.9235064 0.05862647 0.005025126 0.012841988
3 0.8816848 0.08376716 0.008991955 0.025556081
4 0.8469278 0.09234639 0.017247575 0.043478261
5 0.7818821 0.12005277 0.023746702 0.074318382
6 0.7154200 0.13539490 0.036773924 0.112411199
7 0.6334928 0.11722488 0.055980861 0.193301435
8 0.5525714 0.12857143 0.086857143 0.232000000
9 0.4630282 0.11619718 0.093309859 0.327464789
> weights(fit4, type = "prior")
  [,1]
1 1952
2 1791
3 2113
4 2783
5 2274
6 2393
7 2090
8 1750
9 1136
> 
> 
> # Example 5. The use of the xij argument (simple case).
> # The constraint matrix for 'op' has one column.
> nn <- 1000
> eyesdat <- round(data.frame(lop = runif(nn),
+                             rop = runif(nn),
+                              op = runif(nn)), digits = 2)
> eyesdat <- transform(eyesdat, eta1 = -1 + 2 * lop,
+                               eta2 = -1 + 2 * lop)
> eyesdat <- transform(eyesdat,
+            leye = rbinom(nn, 1, prob = logitlink(eta1, inv = TRUE)),
+            reye = rbinom(nn, 1, prob = logitlink(eta2, inv = TRUE)))
> head(eyesdat)
   lop  rop   op  eta1  eta2 leye reye
1 0.27 0.53 0.87 -0.46 -0.46    1    0
2 0.37 0.68 0.97 -0.26 -0.26    0    0
3 0.57 0.38 0.87  0.14  0.14    1    1
4 0.91 0.95 0.44  0.82  0.82    1    1
5 0.20 0.12 0.19 -0.60 -0.60    1    1
6 0.90 0.04 0.08  0.80  0.80    0    1
> fit5 <- vglm(cbind(leye, reye) ~ op,
+              binom2.or(exchangeable = TRUE, zero = 3),
+              data = eyesdat, trace = TRUE,
+              xij = list(op ~ lop + rop + fill1(lop)),
+              form2 = ~  op + lop + rop + fill1(lop))
VGLM    linear loop  1 :  deviance = 2749.1477
VGLM    linear loop  2 :  deviance = 2748.4518
VGLM    linear loop  3 :  deviance = 2748.4296
VGLM    linear loop  4 :  deviance = 2748.4289
VGLM    linear loop  5 :  deviance = 2748.4289
VGLM    linear loop  6 :  deviance = 2748.4289
> coef(fit5)
(Intercept):1 (Intercept):2            op 
 -0.407559825  -0.005341593   0.745863264 
> coef(fit5, matrix = TRUE)
            logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)     -0.4075598     -0.4075598    -0.005341593
op               0.7458633      0.7458633     0.000000000
> constraints(fit5)
$`(Intercept)`
     [,1] [,2]
[1,]    1    0
[2,]    1    0
[3,]    0    1

$op
     [,1]
[1,]    1
[2,]    1
[3,]    0

> fit5@control$xij
[[1]]
op ~ lop + rop + fill1(lop)

> head(model.matrix(fit5))
    (Intercept):1 (Intercept):2   op
1:1             1             0 0.27
1:2             1             0 0.53
1:3             0             1 0.00
2:1             1             0 0.37
2:2             1             0 0.68
2:3             0             1 0.00
> 
> 
> # Example 6. The use of the 'constraints' argument.
> as.character(~ bs(year,df=3))  # Get the white spaces right
[1] "~"                "bs(year, df = 3)"
> clist <- list("(Intercept)"      = diag(3),
+               "bs(year, df = 3)" = rbind(1, 0, 0))
> fit1 <- vglm(r1 ~ bs(year,df=3), gev(zero = NULL),
+              data = venice, constraints = clist, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -217.53311
VGLM    linear loop  2 :  loglikelihood = -215.92358
VGLM    linear loop  3 :  loglikelihood = -215.79841
VGLM    linear loop  4 :  loglikelihood = -215.79279
VGLM    linear loop  5 :  loglikelihood = -215.79236
VGLM    linear loop  6 :  loglikelihood = -215.79233
VGLM    linear loop  7 :  loglikelihood = -215.79232
VGLM    linear loop  8 :  loglikelihood = -215.79232
> coef(fit1, matrix = TRUE)  # Check
                  location loglink(scale) logofflink(shape, offset = 0.5)
(Intercept)       93.45545       2.664918                      -0.7177728
bs(year, df = 3)1 25.60150       0.000000                       0.0000000
bs(year, df = 3)2 10.42492       0.000000                       0.0000000
bs(year, df = 3)3 36.32781       0.000000                       0.0000000
> 
> 
> 
> cleanEx()
> nameEx("vglm.control")
> ### * vglm.control
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vglm.control
> ### Title: Control Function for vglm()
> ### Aliases: vglm.control
> ### Keywords: optimize models
> 
> ### ** Examples
> 
> # Example 1.
> pneumo <- transform(pneumo, let = log(exposure.time))
> vglm(cbind(normal, mild, severe) ~ let, multinomial, pneumo,
+      crit = "coef", step = 0.5, trace = TRUE, epsil = 1e-8,
+      maxit = 40)
Taking a modified step.
VGLM    linear loop  2 :  coefficients = 
11.861219020,  3.067638352, -3.037467770, -0.909802643
Taking a modified step.
VGLM    linear loop  3 :  coefficients = 
11.917320655,  3.054144695, -3.052234942, -0.906168654
Taking a modified step.
VGLM    linear loop  4 :  coefficients = 
11.945991247,  3.046813945, -3.059790886, -0.904189533
Taking a modified step.
VGLM    linear loop  5 :  coefficients = 
11.96048702,  3.04299230, -3.06361350, -0.90315661
Taking a modified step.
VGLM    linear loop  6 :  coefficients = 
11.967775745,  3.041041032, -3.065536165, -0.902628927
Taking a modified step.
VGLM    linear loop  7 :  coefficients = 
11.971430414,  3.040055106, -3.066500366, -0.902362232
Taking a modified step.
VGLM    linear loop  8 :  coefficients = 
11.973260336,  3.039559547, -3.066983186, -0.902228164
Taking a modified step.
VGLM    linear loop  9 :  coefficients = 
11.97417595,  3.03931112, -3.06722478, -0.90216095
Taking a modified step.
VGLM    linear loop  10 :  coefficients = 
11.974633912,  3.039186738, -3.067345617, -0.902127297
Taking a modified step.
VGLM    linear loop  11 :  coefficients = 
11.974862936,  3.039124507, -3.067406049, -0.902110459
Taking a modified step.
VGLM    linear loop  12 :  coefficients = 
11.974977458,  3.039093382, -3.067436267, -0.902102038
Taking a modified step.
VGLM    linear loop  13 :  coefficients = 
11.975034722,  3.039077817, -3.067451377, -0.902097826
Taking a modified step.
VGLM    linear loop  14 :  coefficients = 
11.97506335,  3.03907003, -3.06745893, -0.90209572
Taking a modified step.
VGLM    linear loop  15 :  coefficients = 
11.975077671,  3.039066142, -3.067462710, -0.902094667
Taking a modified step.
VGLM    linear loop  16 :  coefficients = 
11.975084829,  3.039064196, -3.067464599, -0.902094141
Taking a modified step.
VGLM    linear loop  17 :  coefficients = 
11.975088408,  3.039063223, -3.067465544, -0.902093878
Taking a modified step.
VGLM    linear loop  18 :  coefficients = 
11.975090198,  3.039062736, -3.067466016, -0.902093746
Taking a modified step.
VGLM    linear loop  19 :  coefficients = 
11.97509109,  3.03906249, -3.06746625, -0.90209368
Taking a modified step.
VGLM    linear loop  20 :  coefficients = 
11.975091540,  3.039062371, -3.067466370, -0.902093647
Taking a modified step.
VGLM    linear loop  21 :  coefficients = 
11.975091764,  3.039062311, -3.067466429, -0.902093631
Taking a modified step.
VGLM    linear loop  22 :  coefficients = 
11.975091875,  3.039062280, -3.067466458, -0.902093623
Taking a modified step.
VGLM    linear loop  23 :  coefficients = 
11.975091931,  3.039062265, -3.067466473, -0.902093618

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = multinomial, 
    data = pneumo, crit = "coef", step = 0.5, trace = TRUE, epsil = 1e-08, 
    maxit = 40)


Coefficients:
(Intercept):1 (Intercept):2         let:1         let:2 
   11.9750919     3.0390623    -3.0674665    -0.9020936 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 5.347382 
Log-likelihood: -25.25054 

This is a multinomial logit model with 3 levels
> 
> 
> # Example 2. The use of the xij argument (simple case).
> ymat <- rdiric(n <- 1000, shape = rep(exp(2), len = 4))
> mydat <- data.frame(x1 = runif(n), x2 = runif(n), x3 = runif(n),
+                     x4 = runif(n),
+                     z1 = runif(n), z2 = runif(n), z3 = runif(n),
+                     z4 = runif(n))
> mydat <- transform(mydat, X = x1, Z = z1)
> mydat <- round(mydat, digits = 2)
> fit2 <- vglm(ymat ~ X + Z,
+              dirichlet(parallel = TRUE), mydat, trace = TRUE,
+              xij = list(Z ~ z1 + z2 + z3 + z4,
+                         X ~ x1 + x2 + x3 + x4),
+              form2 = ~  Z + z1 + z2 + z3 + z4 +
+                         X + x1 + x2 + x3 + x4)
VGLM    linear loop  1 :  loglikelihood = 1270.9236
VGLM    linear loop  2 :  loglikelihood = 12098.086
VGLM    linear loop  3 :  loglikelihood = 47281.282
VGLM    linear loop  4 :  loglikelihood = 121169.37
VGLM    linear loop  5 :  loglikelihood = 193688.67
VGLM    linear loop  6 :  loglikelihood = 214232.05
VGLM    linear loop  7 :  loglikelihood = 215088.55
VGLM    linear loop  8 :  loglikelihood = 215089.84
VGLM    linear loop  9 :  loglikelihood = 215089.84
> head(model.matrix(fit2, type =  "lm"))  # LM model matrix
  (Intercept)    X    Z
1           1 0.20 0.51
2           1 0.57 0.42
3           1 0.43 0.01
4           1 0.26 0.30
5           1 0.72 0.66
6           1 0.62 0.05
> head(model.matrix(fit2, type = "vlm"))  # Big VLM model matrix
    (Intercept)    X    Z
1:1           1 0.20 0.51
1:2           1 0.03 0.57
1:3           1 0.90 0.64
1:4           1 0.48 0.96
2:1           1 0.57 0.42
2:2           1 0.88 0.53
> coef(fit2)
(Intercept)           X           Z 
 1.98973856  0.04428208 -0.01004415 
> coef(fit2, matrix = TRUE)
            loglink(shape1) loglink(shape2) loglink(shape3) loglink(shape4)
(Intercept)      1.98973856      1.98973856      1.98973856      1.98973856
X                0.04428208      0.04428208      0.04428208      0.04428208
Z               -0.01004415     -0.01004415     -0.01004415     -0.01004415
> max(abs(predict(fit2)-predict(fit2, new = mydat)))  # Predicts ok
[1] 6.217249e-15
> summary(fit2)
Call:
vglm(formula = ymat ~ X + Z, family = dirichlet(parallel = TRUE), 
    data = mydat, form2 = ~Z + z1 + z2 + z3 + z4 + X + x1 + x2 + 
        x3 + x4, trace = TRUE, xij = list(Z ~ z1 + z2 + z3 + 
        z4, X ~ x1 + x2 + x3 + x4))

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.98974    0.02957  67.295   <2e-16 ***
X            0.04428    0.02220   1.994   0.0461 *  
Z           -0.01004    0.02208  -0.455   0.6492    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2), loglink(shape3), 
loglink(shape4)

Log-likelihood: 215089.8 on 3997 degrees of freedom

Number of Fisher scoring iterations: 9 

No Hauck-Donner effect found in any of the estimates

> ## Not run: 
> ##D # plotvgam(fit2, se = TRUE, xlab = "x1", which.term = 1)  # Bug!
> ##D # plotvgam(fit2, se = TRUE, xlab = "z1", which.term = 2)  # Bug!
> ##D plotvgam(fit2, xlab = "x1")  # Correct
> ##D plotvgam(fit2, xlab = "z1")  # Correct
> ## End(Not run)
> 
> 
> # Example 3. The use of the xij argument (complex case).
> set.seed(123)
> coalminers <-
+   transform(coalminers,
+             Age = (age - 42) / 5,
+             dum1 = round(runif(nrow(coalminers)), digits = 2),
+             dum2 = round(runif(nrow(coalminers)), digits = 2),
+             dum3 = round(runif(nrow(coalminers)), digits = 2),
+             dumm = round(runif(nrow(coalminers)), digits = 2))
> BS <- function(x, ..., df = 3)
+   sm.bs(c(x,...), df = df)[1:length(x),,drop = FALSE]
> NS <- function(x, ..., df = 3)
+   sm.ns(c(x,...), df = df)[1:length(x),,drop = FALSE]
> 
> # Equivalently...
> BS <- function(x, ..., df = 3)
+   head(sm.bs(c(x,...), df = df), length(x), drop = FALSE)
> NS <- function(x, ..., df = 3)
+   head(sm.ns(c(x,...), df = df), length(x), drop = FALSE)
> 
> fit3 <- vglm(cbind(nBnW,nBW,BnW,BW) ~ Age + NS(dum1, dum2),
+              fam = binom2.or(exchangeable = TRUE, zero = 3),
+              xij = list(NS(dum1, dum2) ~ NS(dum1, dum2) +
+                                          NS(dum2, dum1) +
+                                          fill1(NS( dum1))),
+              form2 = ~  NS(dum1, dum2) + NS(dum2, dum1) +
+                         fill1(NS(dum1)) +
+                         dum1 + dum2 + dum3 + Age + age + dumm,
+              data = coalminers, trace = TRUE)
VGLM    linear loop  1 :  deviance = 928.5861
VGLM    linear loop  2 :  deviance = 826.005
VGLM    linear loop  3 :  deviance = 825.1369
VGLM    linear loop  4 :  deviance = 825.1364
VGLM    linear loop  5 :  deviance = 825.1364
> head(model.matrix(fit3, type = "lm"))   # LM model matrix
  (Intercept) Age NS(dum1, dum2)1 NS(dum1, dum2)2 NS(dum1, dum2)3
1           1  -4    -0.079043356       0.4484001     -0.30608181
2           1  -3     0.472889436       0.3152905      0.16988482
3           1  -2     0.036567135       0.5310701     -0.36251309
4           1  -1     0.187156496       0.3663381      0.44213521
5           1   0    -0.054404487       0.4175134      0.63682278
6           1   1    -0.006642409       0.0209409     -0.01429444
> head(model.matrix(fit3, type = "vlm"))  # Big VLM model matrix
    (Intercept):1 (Intercept):2 Age NS(dum1, dum2)1 NS(dum1, dum2)2
1:1             1             0  -4     -0.07904336       0.4484001
1:2             1             0  -4      0.13364445       0.5231724
1:3             0             1   0      0.00000000       0.0000000
2:1             1             0  -3      0.47288944       0.3152905
2:2             1             0  -3     -0.13828760       0.4357007
2:3             0             1   0      0.00000000       0.0000000
    NS(dum1, dum2)3
1:1      -0.3060818
1:2      -0.3570089
1:3       0.0000000
2:1       0.1698848
2:2       0.7025869
2:3       0.0000000
> coef(fit3)
  (Intercept):1   (Intercept):2             Age NS(dum1, dum2)1 NS(dum1, dum2)2 
     -1.7854997       2.5196178       0.3930814       0.2481514      -0.1308625 
NS(dum1, dum2)3 
     -0.1768814 
> coef(fit3, matrix = TRUE)
                logitlink(mu1) logitlink(mu2) loglink(oratio)
(Intercept)         -1.7854997     -1.7854997        2.519618
Age                  0.3930814      0.3930814        0.000000
NS(dum1, dum2)1      0.2481514      0.2481514        0.000000
NS(dum1, dum2)2     -0.1308625     -0.1308625        0.000000
NS(dum1, dum2)3     -0.1768814     -0.1768814        0.000000
> ## Not run: 
> ##D plotvgam(fit3, se = TRUE, lcol = 2, scol = 4, xlab = "dum1")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vglmff-class")
> ### * vglmff-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vglmff-class
> ### Title: Class "vglmff"
> ### Aliases: vglmff-class
> ### Keywords: classes
> 
> ### ** Examples
> 
> cratio()
Family:  cratio 
Informal classes: cratio, VGAMordinal, VGAMcategorical 

Continuation ratio model

Links:    logitlink(P[Y>j|Y>=j])
Variance: mu[,j]*(1-mu[,j]); -mu[,j]*mu[,k]
> cratio(link = "clogloglink")
Family:  cratio 
Informal classes: cratio, VGAMordinal, VGAMcategorical 

Continuation ratio model

Links:    clogloglink(P[Y>j|Y>=j])
Variance: mu[,j]*(1-mu[,j]); -mu[,j]*mu[,k]
> cratio(link = "clogloglink", reverse = TRUE)
Family:  cratio 
Informal classes: cratio, VGAMordinal, VGAMcategorical 

Continuation ratio model

Links:    clogloglink(P[Y<j+1|Y<=j+1])
Variance: mu[,j]*(1-mu[,j]); -mu[,j]*mu[,k]
> 
> 
> 
> cleanEx()
> nameEx("vonmises")
> ### * vonmises
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vonmises
> ### Title: von Mises Distribution Family Function
> ### Aliases: vonmises
> ### Keywords: models regression
> 
> ### ** Examples
> 
> vdata <- data.frame(x2 = runif(nn <- 1000))
> vdata <- transform(vdata,
+                    y = rnorm(nn, 2+x2, exp(0.2)))  # Bad data!!
> fit <- vglm(y  ~ x2, vonmises(zero = 2), vdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 211.35576
VGLM    linear loop  2 :  loglikelihood = 211.72181
VGLM    linear loop  3 :  loglikelihood = 211.72184
VGLM    linear loop  4 :  loglikelihood = 211.72184
> coef(fit, matrix = TRUE)
            extlogitlink(location, min = 0, max = 6.28318530717959)
(Intercept)                                              -0.7519089
x2                                                        0.6184658
            loglink(scale)
(Intercept)    0.003508577
x2             0.000000000
> Coef(fit)
(Intercept):1 (Intercept):2            x2 
 -0.751908936   0.003508577   0.618465830 
> with(vdata, range(y))  # Original data
[1] -1.567956  6.963336
> range(depvar(fit))     # Processed data is in [0,2*pi)
[1] 0.02476508 6.27442258
> 
> 
> 
> cleanEx()
> nameEx("vplot.profile")
> ### * vplot.profile
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vplot.profile
> ### Title: Plotting Functions for 'profile' Objects
> ### Aliases: vplot.profile vpairs.profile
> ### Keywords: regression hplot
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> fit1 <- vglm(cbind(normal, mild, severe) ~ let, acat,
+              trace = TRUE, data = pneumo)
VGLM    linear loop  1 :  deviance = 5.407271
VGLM    linear loop  2 :  deviance = 5.34745
VGLM    linear loop  3 :  deviance = 5.347382
VGLM    linear loop  4 :  deviance = 5.347382
> pfit1 <- profile(fit1, trace = FALSE)
> ## Not run: 
> ##D vplot.profile(pfit1)
> ##D vpairs.profile(pfit1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vsmooth.spline")
> ### * vsmooth.spline
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vsmooth.spline
> ### Title: Vector Cubic Smoothing Spline
> ### Aliases: vsmooth.spline
> ### Keywords: regression smooth
> 
> ### ** Examples
> 
> nn <- 20; x <- 2 + 5*(nn:1)/nn
> x[2:4] <- x[5:7]  # Allow duplication
> y1 <- sin(x) + rnorm(nn, sd = 0.13)
> y2 <- cos(x) + rnorm(nn, sd = 0.13)
> y3 <- 1 + sin(x) + rnorm(nn, sd = 0.13)  # For constraints
> y <- cbind(y1, y2, y3)
> ww <- cbind(rep(3, nn), 4, (1:nn)/nn)
> 
> (fit <- vsmooth.spline(x, y, w = ww, df = 5))
Call:
vsmooth.spline(x = x, y = y, w = ww, df = 5)

Smoothing Parameter (Spar): 0.5534466, 0.5534466, 0.5359578 

Equivalent Degrees of Freedom (Df): 4.999474, 4.999474, 4.999402 
> ## Not run: 
> ##D plot(fit)  # The 1st & 3rd functions dont differ by a constant
> ## End(Not run)
> 
> mat <- matrix(c(1,0,1, 0,1,0), 3, 2)
> (fit2 <- vsmooth.spline(x, y, w = ww, df = 5, i.constr = mat,
+                         x.constr = mat))
Call:
vsmooth.spline(x = x, y = y, w = ww, df = 5, i.constraint = mat, 
    x.constraint = mat)

Smoothing Parameter (Spar): 0.5535428, 0.5534466 

Equivalent Degrees of Freedom (Df): 4.999470, 4.999474 

Constraint matrices:
$`(Intercepts)`
     [,1] [,2]
[1,]    1    0
[2,]    0    1
[3,]    1    0

$x
     [,1] [,2]
[1,]    1    0
[2,]    0    1
[3,]    1    0

> # The 1st and 3rd functions do differ by a constant:
> mycols <- c("orange", "blue", "orange")
> ## Not run:  plot(fit2, lcol = mycols, pcol = mycols, las = 1) 
> 
> p <- predict(fit, x = model.matrix(fit, type = "lm"), deriv = 0)
> max(abs(depvar(fit) - with(p, y)))  # Should be 0
[1] 5.759593e-11
> 
> par(mfrow = c(3, 1))
> ux <- seq(1, 8, len = 100)
> for (dd in 1:3) {
+   pp <- predict(fit, x = ux, deriv = dd)
+ ## Not run: 
+ ##D with(pp, matplot(x, y, type = "l", main = paste("deriv =", dd),
+ ##D                  lwd = 2, ylab = "", cex.axis = 1.5,
+ ##D                  cex.lab = 1.5, cex.main = 1.5)) 
+ ## End(Not run)
+ }
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("waitakere")
> ### * waitakere
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: waitakere
> ### Title: Waitakere Ranges Data
> ### Aliases: waitakere
> ### Keywords: datasets
> 
> ### ** Examples
> 
> fit <- vgam(agaaus ~ s(altitude, df = 2), binomialff, waitakere)
> head(predict(fit, waitakere, type = "response"))
[1] 0.2491610 0.2924107 0.1921006 0.1921006 0.2491610 0.2924107
> ## Not run:  plot(fit, se = TRUE, lcol = "orange", scol = "blue") 
> 
> 
> 
> cleanEx()
> nameEx("wald.stat")
> ### * wald.stat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wald.stat
> ### Title: Wald Test Statistics Evaluated at the Null Values
> ### Aliases: wald.stat wald.stat.vlm
> ### Keywords: models regression htest
> 
> ### ** Examples
> 
> set.seed(1)
> pneumo <- transform(pneumo, let = log(exposure.time),
+                             x3 = rnorm(nrow(pneumo)))
> (fit <- vglm(cbind(normal, mild, severe) ~ let + x3, propodds, pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let + x3, family = propodds, 
    data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let            x3 
  -9.66744415  -10.57344562    2.58865720    0.08444356 

Degrees of Freedom: 16 Total; 12 Residual
Residual deviance: 4.763992 
Log-likelihood: -24.95885 
> wald.stat(fit)  # No HDE here
       let         x3 
13.1890479  0.5144113 
> summary(fit, wald0 = TRUE)  # See them here
Call:
vglm(formula = cbind(normal, mild, severe) ~ let + x3, family = propodds, 
    data = pneumo)

Wald (modified by IRLS iterations) coefficients: 
    Estimate Std. Error z value Pr(>|z|)    
let  2.58866    0.19627  13.189   <2e-16 ***
x3   0.08444    0.16416   0.514    0.607    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(P[Y>=2]), logitlink(P[Y>=3])

Residual deviance: 4.764 on 12 degrees of freedom

Log-likelihood: -24.9588 on 12 degrees of freedom

Number of Fisher scoring iterations: 4 


Exponentiated coefficients:
      let        x3 
13.311884  1.088111 
> coef(summary(fit))  # Usual Wald statistics evaluated at the MLE
                  Estimate Std. Error    z value     Pr(>|z|)
(Intercept):1  -9.66744415  1.3377918 -7.2264190 4.958962e-13
(Intercept):2 -10.57344562  1.3588755 -7.7810263 7.193855e-15
let             2.58865720  0.3850708  6.7225483 1.785736e-11
x3              0.08444356  0.1632077  0.5173993 6.048774e-01
> wald.stat(fit, orig.SE = TRUE)  # Same as previous line
      let        x3 
6.7225483 0.5173993 
> 
> 
> 
> cleanEx()
> nameEx("waldff")
> ### * waldff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: waldff
> ### Title: Wald Distribution Family Function
> ### Aliases: waldff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> wdata <- data.frame(y = rinv.gaussian(1000, mu =  1, exp(1)))
> wfit <- vglm(y ~ 1, waldff(ilambda = 0.2), wdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1094.0251
VGLM    linear loop  2 :  loglikelihood = -814.24222
VGLM    linear loop  3 :  loglikelihood = -699.60294
VGLM    linear loop  4 :  loglikelihood = -684.26816
VGLM    linear loop  5 :  loglikelihood = -684.02814
VGLM    linear loop  6 :  loglikelihood = -684.02808
VGLM    linear loop  7 :  loglikelihood = -684.02808
> coef(wfit, matrix = TRUE)
            loglink(lambda)
(Intercept)        0.934627
> Coef(wfit)
  lambda 
2.546263 
> summary(wfit)
Call:
vglm(formula = y ~ 1, family = waldff(ilambda = 0.2), data = wdata, 
    trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.93463    0.04472    20.9   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(lambda) 

Log-likelihood: -684.0281 on 999 degrees of freedom

Number of Fisher scoring iterations: 7 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("weibull.mean")
> ### * weibull.mean
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: weibull.mean
> ### Title: Weibull Distribution Family Function, Parameterized by the Mean
> ### Aliases: weibull.mean
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D wdata <- data.frame(x2 = runif(nn <- 1000))  # Complete data
> ##D wdata <- transform(wdata, mu     = exp(-1 + 1 * x2),
> ##D                           x3     = rnorm(nn),
> ##D                           shape1 = exp(1),
> ##D                           shape2 = exp(2))
> ##D wdata <- transform(wdata,
> ##D   y1 = rweibull(nn, shape1, scale = mu / gamma(1 + 1/shape1)),
> ##D   y2 = rweibull(nn, shape2, scale = mu / gamma(1 + 1/shape2)))
> ##D fit <- vglm(cbind(y1, y2) ~ x2 + x3, weibull.mean, wdata,
> ##D             trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D sqrt(diag(vcov(fit)))  # SEs
> ##D summary(fit, presid = FALSE)   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("weibullR")
> ### * weibullR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: weibullR
> ### Title: Weibull Distribution Family Function
> ### Aliases: weibullR
> ### Keywords: models regression
> 
> ### ** Examples
> 
> wdata <- data.frame(x2 = runif(nn <- 1000))  # Complete data
> wdata <- transform(wdata,
+             y1 = rweibull(nn, exp(1), scale = exp(-2 + x2)),
+             y2 = rweibull(nn, exp(2), scale = exp( 1 - x2)))
> fit <- vglm(cbind(y1, y2) ~ x2, weibullR, wdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = 587.77584
VGLM    linear loop  2 :  loglikelihood = 912.20436
VGLM    linear loop  3 :  loglikelihood = 1066.5667
VGLM    linear loop  4 :  loglikelihood = 1104.9041
VGLM    linear loop  5 :  loglikelihood = 1107.3962
VGLM    linear loop  6 :  loglikelihood = 1107.4118
VGLM    linear loop  7 :  loglikelihood = 1107.4119
VGLM    linear loop  8 :  loglikelihood = 1107.4119
> coef(fit, matrix = TRUE)
            loglink(scale1) loglink(shape1) loglink(scale2) loglink(shape2)
(Intercept)       -2.016572       0.9820561       0.9917845        2.014665
x2                 1.066166       0.0000000      -0.9752416        0.000000
> vcov(fit)
              (Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4
(Intercept):1  5.771080e-04  9.626509e-05  0.000000e+00  0.000000e+00
(Intercept):2  9.626509e-05  6.079271e-04  0.000000e+00  0.000000e+00
(Intercept):3  0.000000e+00  0.000000e+00  7.317231e-05  3.427786e-05
(Intercept):4  0.000000e+00  0.000000e+00  3.427786e-05  6.079271e-04
x2:1          -8.436884e-04 -3.505613e-20  0.000000e+00  0.000000e+00
x2:2           0.000000e+00  0.000000e+00 -1.069724e-04 -1.691993e-20
                       x2:1          x2:2
(Intercept):1 -8.436884e-04  0.000000e+00
(Intercept):2 -3.505613e-20  0.000000e+00
(Intercept):3  0.000000e+00 -1.069724e-04
(Intercept):4  0.000000e+00 -1.691993e-20
x2:1           1.688418e-03  0.000000e+00
x2:2           0.000000e+00  2.140768e-04
> summary(fit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = weibullR, data = wdata, 
    trace = TRUE)

Coefficients: 
               Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -2.016572   0.024023  -83.94   <2e-16 ***
(Intercept):2  0.982056   0.024656   39.83   <2e-16 ***
(Intercept):3  0.991785   0.008554  115.94   <2e-16 ***
(Intercept):4  2.014665   0.024656   81.71   <2e-16 ***
x2:1           1.066166   0.041090   25.95   <2e-16 ***
x2:2          -0.975242   0.014631  -66.65   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(scale1), loglink(shape1), loglink(scale2), 
loglink(shape2)

Log-likelihood: 1107.412 on 3994 degrees of freedom

Number of Fisher scoring iterations: 8 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("weightsvglm")
> ### * weightsvglm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: weightsvglm
> ### Title: Prior and Working Weights of a VGLM fit
> ### Aliases: weightsvglm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> pneumo <- transform(pneumo, let = log(exposure.time))
> (fit <- vglm(cbind(normal, mild, severe) ~ let,
+              cumulative(parallel = TRUE, reverse = TRUE), pneumo))

Call:
vglm(formula = cbind(normal, mild, severe) ~ let, family = cumulative(parallel = TRUE, 
    reverse = TRUE), data = pneumo)


Coefficients:
(Intercept):1 (Intercept):2           let 
    -9.676093    -10.581725      2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual deviance: 5.026826 
Log-likelihood: -25.09026 
> depvar(fit)  # These are sample proportions
     normal       mild     severe
1 1.0000000 0.00000000 0.00000000
2 0.9444444 0.03703704 0.01851852
3 0.7906977 0.13953488 0.06976744
4 0.7291667 0.10416667 0.16666667
5 0.6274510 0.19607843 0.17647059
6 0.6052632 0.18421053 0.21052632
7 0.4285714 0.21428571 0.35714286
8 0.3636364 0.18181818 0.45454545
> weights(fit, type = "prior", matrix = FALSE)  # No. of observations
[1] 98 54 43 48 51 38 28 11
> 
> # Look at the working residuals
> nn <- nrow(model.matrix(fit, type = "lm"))
> M <- ncol(predict(fit))
> 
> wwt <- weights(fit, type="working", deriv=TRUE)  # Matrix-band format
> wz <- m2a(wwt$weights, M = M)  # In array format
> wzinv <- array(apply(wz, 3, solve), c(M, M, nn))
> wresid <- matrix(NA, nn, M)  # Working residuals
> for (ii in 1:nn)
+   wresid[ii, ] <- wzinv[, , ii, drop = TRUE] %*% wwt$deriv[ii, ]
> max(abs(c(resid(fit, type = "work")) - c(wresid)))  # Should be 0
[1] 1.437464e-05
> 
> (zedd <- predict(fit) + wresid)  # Adjusted dependent vector
  logitlink(P[Y>=2]) logitlink(P[Y>=3])
1         -6.1173043         -7.0193456
2         -2.8183563         -3.8962823
3         -1.2774948         -2.5900103
4         -0.9888700         -1.5560630
5         -0.5211240         -1.5385773
6         -0.4224572         -1.3017459
7          0.2876506         -0.5873817
8          0.5596158         -0.1803636
> 
> 
> 
> cleanEx()
> nameEx("wine")
> ### * wine
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wine
> ### Title: Bitterness in Wine Data
> ### Aliases: wine
> ### Keywords: datasets
> 
> ### ** Examples
> 
> wine
  temp contact bitter1 bitter2 bitter3 bitter4 bitter5
1 cold      no       4       9       5       0       0
2 cold     yes       1       7       8       2       0
3 warm      no       0       5       8       3       2
4 warm     yes       0       1       5       7       5
> summary(wine)
   temp   contact    bitter1        bitter2       bitter3       bitter4   
 cold:2   no :2   Min.   :0.00   Min.   :1.0   Min.   :5.0   Min.   :0.0  
 warm:2   yes:2   1st Qu.:0.00   1st Qu.:4.0   1st Qu.:5.0   1st Qu.:1.5  
                  Median :0.50   Median :6.0   Median :6.5   Median :2.5  
                  Mean   :1.25   Mean   :5.5   Mean   :6.5   Mean   :3.0  
                  3rd Qu.:1.75   3rd Qu.:7.5   3rd Qu.:8.0   3rd Qu.:4.0  
                  Max.   :4.00   Max.   :9.0   Max.   :8.0   Max.   :7.0  
    bitter5    
 Min.   :0.00  
 1st Qu.:0.00  
 Median :1.00  
 Mean   :1.75  
 3rd Qu.:2.75  
 Max.   :5.00  
> 
> 
> 
> cleanEx()
> nameEx("wrapup.smart")
> ### * wrapup.smart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wrapup.smart
> ### Title: Cleans Up After Smart Prediction
> ### Aliases: wrapup.smart
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Place this inside modelling functions such as lm, glm, vglm.
> ##D wrapup.smart()  # Put at the end of lm
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("yeo.johnson")
> ### * yeo.johnson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: yeo.johnson
> ### Title: Yeo-Johnson Transformation
> ### Aliases: yeo.johnson
> ### Keywords: models regression
> 
> ### ** Examples
> 
> y <- seq(-4, 4, len = (nn <- 200))
> ltry <- c(0, 0.5, 1, 1.5, 2)  # Try these values of lambda
> lltry <- length(ltry)
> psi <- matrix(NA_real_, nn, lltry)
> for (ii in 1:lltry)
+   psi[, ii] <- yeo.johnson(y, lambda = ltry[ii])
> 
> ## Not run: 
> ##D matplot(y, psi, type = "l", ylim = c(-4, 4), lwd = 2,
> ##D         lty = 1:lltry, col = 1:lltry, las = 1,
> ##D         ylab = "Yeo-Johnson transformation", 
> ##D         main = "Yeo-Johnson transformation with some lambda values")
> ##D abline(v = 0, h = 0)
> ##D legend(x = 1, y = -0.5, lty = 1:lltry, legend = as.character(ltry),
> ##D        lwd = 2, col = 1:lltry) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("yulesimon")
> ### * yulesimon
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: yulesimon
> ### Title: Yule-Simon Family Function
> ### Aliases: yulesimon
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ydata <- data.frame(x2 = runif(nn <- 1000))
> ydata <- transform(ydata, y = ryules(nn, shape = exp(1.5 - x2)))
> with(ydata, table(y))
y
  1   2   3   4   5   6   7   8  12  14  15  18  22  26  27  29 
726 146  67  22  17   4   6   3   2   1   1   1   1   1   1   1 
> fit <- vglm(y ~ x2, yulesimon, data = ydata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -985.79716
VGLM    linear loop  2 :  loglikelihood = -984.86166
VGLM    linear loop  3 :  loglikelihood = -984.85647
VGLM    linear loop  4 :  loglikelihood = -984.85647
> coef(fit, matrix = TRUE)
            loglink(shape)
(Intercept)      1.4929064
x2              -0.9855115
> summary(fit)
Call:
vglm(formula = y ~ x2, family = yulesimon, data = ydata, trace = TRUE)

Coefficients: 
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.4929     0.1214  12.300  < 2e-16 ***
x2           -0.9855     0.1922  -5.127 2.95e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Name of linear predictor: loglink(shape) 

Log-likelihood: -984.8565 on 998 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("yulesimonUC")
> ### * yulesimonUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Yules
> ### Title: Yule-Simon Distribution
> ### Aliases: Yules dyules pyules qyules ryules
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dyules(1:20, 2.1)
 [1] 0.6774193548 0.1652242329 0.0647938168 0.0318658116 0.0179525699
 [6] 0.0110818333 0.0073067033 0.0050640518 0.0036497670 0.0027147028
[11] 0.0020722922 0.0016166818 0.0012847803 0.0010374002 0.0008493335
[16] 0.0007038676 0.0005896273 0.0004986897 0.0004254225 0.0003657479
> ryules(20, 2.1)
 [1] 3 1 1 1 1 1 2 1 3 1 2 1 1 2 1 1 1 2 2 1
> 
> round(1000 * dyules(1:8, 2))
[1] 667 167  67  33  19  12   8   6
> table(ryules(1000, 2))

  1   2   3   4   5   6   7   8   9  10  11  12  14  16  17  19  20  22  29  30 
653 176  66  31  26  16   7   5   1   3   1   1   1   1   3   1   1   1   1   1 
 35  39  54  63 
  1   1   1   1 
> 
> ## Not run: 
> ##D  x <- 0:6
> ##D plot(x, dyules(x, shape = 2.2), type = "h", las = 1, col = "blue")
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("zabinomUC")
> ### * zabinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zabinom
> ### Title: Zero-Altered Binomial Distribution
> ### Aliases: Zabinom dzabinom pzabinom qzabinom rzabinom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> size <- 10; prob <- 0.15; pobs0 <- 0.05; x <- (-1):7
> dzabinom(x, size = size, prob = prob, pobs0 = pobs0)
[1] 0.0000000000 0.0500000000 0.4109620590 0.3263522233 0.1535775169
[6] 0.0474283508 0.0100436508 0.0014770075 0.0001489419
> table(rzabinom(100, size = size, prob = prob, pobs0 = pobs0))

 0  1  2  3  4  5 
 2 41 33 21  2  1 
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D barplot(rbind(dzabinom(x, size = size, prob = prob, pobs0 = pobs0),
> ##D                 dbinom(x, size = size, prob = prob)),
> ##D   beside = TRUE, col = c("blue", "orange"), cex.main = 0.7, las = 1,
> ##D   ylab = "Probability", names.arg = as.character(x),
> ##D   main = paste("ZAB(size = ", size, ", prob = ", prob, ", pobs0 = ", pobs0,
> ##D                ") [blue] vs",  " Binom(size = ", size, ", prob = ", prob,
> ##D                ") [orange] densities", sep = "")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zabinomial")
> ### * zabinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zabinomial
> ### Title: Zero-Altered Binomial Distribution
> ### Aliases: zabinomial zabinomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(x2 = runif(nn <- 1000))
> zdata <- transform(zdata, size  = 10,
+                           prob  = logitlink(-2 + 3*x2, inverse = TRUE),
+                           pobs0 = logitlink(-1 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata,
+                    y1 = rzabinom(nn, size = size, prob = prob, pobs0 = pobs0))
> with(zdata, table(y1))
y1
  0   1   2   3   4   5   6   7   8   9  10 
513  90 111  82  71  43  42  30  14   1   3 
> 
> zfit <- vglm(cbind(y1, size - y1) ~ x2, zabinomial(zero = NULL),
+              data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1520.9188
VGLM    linear loop  2 :  loglikelihood = -1435.5695
VGLM    linear loop  3 :  loglikelihood = -1435.0265
VGLM    linear loop  4 :  loglikelihood = -1435.0263
VGLM    linear loop  5 :  loglikelihood = -1435.0263
> coef(zfit, matrix = TRUE)
            logitlink(pobs0) logitlink(prob)
(Intercept)        -1.047551       -1.959983
x2                  2.212457        2.813151
> head(fitted(zfit))
       [,1]
1 0.1517226
2 0.1648118
3 0.1851144
4 0.1782186
5 0.1442223
6 0.1792495
> head(predict(zfit))
  logitlink(pobs0) logitlink(prob)
1       -0.4601240      -1.2130668
2       -0.2242424      -0.9131420
3        0.2198629      -0.3484597
4        0.9618202       0.5949431
5       -0.6013379      -1.3926211
6        0.9400980       0.5673232
> summary(zfit)
Call:
vglm(formula = cbind(y1, size - y1) ~ x2, family = zabinomial(zero = NULL), 
    data = zdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.04755    0.13573  -7.718 1.18e-14 ***
(Intercept):2 -1.95998    0.07409 -26.453  < 2e-16 ***
x2:1           2.21246    0.23970   9.230  < 2e-16 ***
x2:2           2.81315    0.13237  21.252  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pobs0), logitlink(prob)

Log-likelihood: -1435.026 on 1996 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("zageomUC")
> ### * zageomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zageom
> ### Title: Zero-Altered Geometric Distribution
> ### Aliases: Zageom dzageom pzageom qzageom rzageom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> prob <- 0.35; pobs0 <- 0.05; x <- (-1):7
> dzageom(x, prob = prob, pobs0 = pobs0)
[1] 0.00000000 0.05000000 0.33250000 0.21612500 0.14048125 0.09131281 0.05935333
[8] 0.03857966 0.02507678
> table(rzageom(100, prob = prob, pobs0 = pobs0))

 0  1  2  3  4  5  6  7  8 12 
 2 31 23 16 13  7  4  2  1  1 
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D barplot(rbind(dzageom(x, prob = prob, pobs0 = pobs0),
> ##D                 dgeom(x, prob = prob)), las = 1,
> ##D         beside = TRUE, col = c("blue", "orange"), cex.main = 0.7,
> ##D         ylab = "Probability", names.arg = as.character(x),
> ##D         main = paste("ZAG(prob = ", prob, ", pobs0 = ", pobs0,
> ##D                    ") [blue] vs",  " Geometric(prob = ", prob,
> ##D                    ") [orange] densities", sep = "")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zageometric")
> ### * zageometric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zageometric
> ### Title: Zero-Altered Geometric Distribution
> ### Aliases: zageometric zageometricff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(x2 = runif(nn <- 1000))
> zdata <- transform(zdata, pobs0 = logitlink(-1 + 2*x2, inverse = TRUE),
+                           prob  = logitlink(-2 + 3*x2, inverse = TRUE))
> zdata <- transform(zdata, y1 = rzageom(nn, prob = prob, pobs0 = pobs0),
+                           y2 = rzageom(nn, prob = prob, pobs0 = pobs0))
> with(zdata, table(y1))
y1
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 
513 173  90  61  37  32  19  10  12  10   7   7   7   5   2   3   2   1   1   4 
 21  26  30 
  2   1   1 
> 
> fit <- vglm(cbind(y1, y2) ~ x2, zageometric, data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -3373.6262
VGLM    linear loop  2 :  loglikelihood = -3261.0641
VGLM    linear loop  3 :  loglikelihood = -3238.698
VGLM    linear loop  4 :  loglikelihood = -3237.7278
VGLM    linear loop  5 :  loglikelihood = -3237.725
VGLM    linear loop  6 :  loglikelihood = -3237.725
> coef(fit, matrix = TRUE)
            logitlink(pobs01) logitlink(prob1) logitlink(pobs02)
(Intercept)         -1.047551        -1.999426        -0.9275495
x2                   2.212457         3.201455         1.8229594
            logitlink(prob2)
(Intercept)        -1.853596
x2                  2.771101
> head(fitted(fit))
         y1        y2
1 2.5480154 2.4718903
2 1.8028879 1.8410100
3 0.9706274 1.0852799
4 0.3880224 0.4933850
5 3.1470765 2.9593591
6 0.3977668 0.5040418
> head(predict(fit))
     logitlink(pobs01) logitlink(prob1) logitlink(pobs02) logitlink(prob2)
[1,]        -0.4601240       -1.1494120        -0.4435380       -1.1178445
[2,]        -0.2242424       -0.8080881        -0.2491828       -0.8224029
[3,]         0.2198629       -0.1654617         0.1167389       -0.2661613
[4,]         0.9618202        0.9081604         0.7280764        0.6631397
[5,]        -0.6013379       -1.3537504        -0.5598916       -1.2947149
[6,]         0.9400980        0.8767282         0.7101784        0.6359328
> summary(fit)
Call:
vglm(formula = cbind(y1, y2) ~ x2, family = zageometric, data = zdata, 
    trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.04755    0.13573  -7.718 1.18e-14 ***
(Intercept):2 -1.99943    0.09623 -20.777  < 2e-16 ***
(Intercept):3 -0.92755    0.13365  -6.940 3.92e-12 ***
(Intercept):4 -1.85360    0.09588 -19.332  < 2e-16 ***
x2:1           2.21246    0.23970   9.230  < 2e-16 ***
x2:2           3.20146    0.23463  13.644  < 2e-16 ***
x2:3           1.82296    0.23309   7.821 5.24e-15 ***
x2:4           2.77110    0.21973  12.611  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pobs01), logitlink(prob1), 
logitlink(pobs02), logitlink(prob2)

Log-likelihood: -3237.725 on 3992 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("zanegbinUC")
> ### * zanegbinUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zanegbin
> ### Title: Zero-Altered Negative Binomial Distribution
> ### Aliases: Zanegbin dzanegbin pzanegbin qzanegbin rzanegbin
> ### Keywords: distribution
> 
> ### ** Examples
> 
> munb <- 3; size <- 4; pobs0 <- 0.3; x <- (-1):7
> dzanegbin(x, munb = munb, size = size, pobs0 = pobs0)
[1] 0.0 0.3  NA  NA  NA  NA  NA  NA  NA
> table(rzanegbin(100, munb = munb, size = size, pobs0 = pobs0))

 0  1  2  3  4  5  6  7  8 
27 11 24  8 11 11  6  1  1 
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D barplot(rbind(dzanegbin(x, munb = munb, size = size, pobs0 = pobs0),
> ##D                 dnbinom(x, mu   = munb, size = size)),
> ##D         beside = TRUE, col = c("blue", "green"), cex.main = 0.7,
> ##D         ylab = "Probability", names.arg = as.character(x), las = 1,
> ##D         main = paste0("ZANB(munb = ", munb, ", size = ", size,",
> ##D                pobs0 = ", pobs0,
> ##D                ") [blue] vs",  " NB(mu = ", munb, ", size = ", size,
> ##D                ") [green] densities")) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zanegbinomial")
> ### * zanegbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zanegbinomial
> ### Title: Zero-Altered Negative Binomial Distribution
> ### Aliases: zanegbinomial zanegbinomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D zdata <- data.frame(x2 = runif(nn <- 2000))
> ##D zdata <- transform(zdata, pobs0 = logitlink(-1 + 2*x2, inverse = TRUE))
> ##D zdata <- transform(zdata,
> ##D          y1 = rzanegbin(nn, munb = exp(0+2*x2), size = exp(1), pobs0 = pobs0),
> ##D          y2 = rzanegbin(nn, munb = exp(1+2*x2), size = exp(1), pobs0 = pobs0))
> ##D with(zdata, table(y1))
> ##D with(zdata, table(y2))
> ##D 
> ##D fit <- vglm(cbind(y1, y2) ~ x2, zanegbinomial, data = zdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D head(fitted(fit))
> ##D head(predict(fit))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zapoisUC")
> ### * zapoisUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zapois
> ### Title: Zero-Altered Poisson Distribution
> ### Aliases: Zapois dzapois pzapois qzapois rzapois
> ### Keywords: distribution
> 
> ### ** Examples
> 
> lambda <- 3; pobs0 <- 0.2; x <- (-1):7
> (ii <- dzapois(x, lambda, pobs0))
[1] 0.00000000 0.20000000 0.12574967 0.18862451 0.18862451 0.14146838 0.08488103
[8] 0.04244051 0.01818879
> max(abs(cumsum(ii) - pzapois(x, lambda, pobs0)))  # Should be 0
[1] 1.110223e-16
> table(rzapois(100, lambda, pobs0))

 0  1  2  3  4  5  6 
16  8 25 15 21 12  3 
> table(qzapois(runif(100), lambda, pobs0))

 0  1  2  3  4  5  6  7 
26 18 21 15  7  5  7  1 
> round(dzapois(0:10, lambda, pobs0) * 100)  # Should be similar
 [1] 20 13 19 19 14  8  4  2  1  0  0
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D barplot(rbind(dzapois(x, lambda, pobs0), dpois(x, lambda)),
> ##D         beside = TRUE, col = c("blue", "green"), las = 1,
> ##D         main = paste0("ZAP(", lambda, ", pobs0 = ", pobs0, ") [blue]",
> ##D                       "vs Poisson(", lambda, ") [green] densities"),
> ##D         names.arg = as.character(x), ylab = "Probability") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zapoisson")
> ### * zapoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zapoisson
> ### Title: Zero-Altered Poisson Distribution
> ### Aliases: zapoisson zapoissonff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(x2 = runif(nn <- 1000))
> zdata <- transform(zdata, pobs0  = logitlink( -1 + 1*x2, inverse = TRUE),
+                           lambda = loglink(-0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata, y = rgaitdpois(nn, lambda, pobs.mlm = pobs0,
+                                         a.mlm = 0))
> 
> with(zdata, table(y))
y
  0   1   2   3   4   5   6   7   8   9  10 
392 252 164  87  51  29  10   9   4   1   1 
> fit <- vglm(y ~ x2, zapoisson, data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -1480.1365
VGLM    linear loop  2 :  loglikelihood = -1470.2963
VGLM    linear loop  3 :  loglikelihood = -1470.0869
VGLM    linear loop  4 :  loglikelihood = -1470.0867
VGLM    linear loop  5 :  loglikelihood = -1470.0867
> fit <- vglm(y ~ x2, zapoisson, data = zdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 
-1.26719374, -0.19231554,  1.48470252,  1.73445500
VGLM    linear loop  2 :  coefficients = 
-1.10221184, -0.47044238,  1.30046692,  2.02307546
VGLM    linear loop  3 :  coefficients = 
-1.10671338, -0.52549185,  1.30631012,  2.08840988
VGLM    linear loop  4 :  coefficients = 
-1.10671638, -0.52713269,  1.30631410,  2.09084351
VGLM    linear loop  5 :  coefficients = 
-1.10671638, -0.52716628,  1.30631410,  2.09091049
VGLM    linear loop  6 :  coefficients = 
-1.10671638, -0.52716764,  1.30631410,  2.09091309
VGLM    linear loop  7 :  coefficients = 
-1.10671638, -0.52716769,  1.30631410,  2.09091319
> head(fitted(fit))
      [,1]
1 1.090674
2 1.155420
3 1.340800
4 1.930196
5 1.060292
6 1.906733
> head(predict(fit))
  logitlink(pobs0) loglink(lambda)
1      -0.75987867      0.02798788
2      -0.62060568      0.25091108
3      -0.35838995      0.67061896
4       0.07968826      1.37181596
5      -0.84325643     -0.10546828
6       0.06686273      1.35128715
> head(predict(fit, untransform = TRUE))
      pobs0   lambda
1 0.3186726 1.028383
2 0.3496437 1.285196
3 0.4113494 1.955447
4 0.5199115 3.942504
5 0.3008494 0.899903
6 0.5167095 3.862394
> coef(fit, matrix = TRUE)
            logitlink(pobs0) loglink(lambda)
(Intercept)        -1.106716      -0.5271677
x2                  1.306314       2.0909132
> summary(fit)
Call:
vglm(formula = y ~ x2, family = zapoisson, data = zdata, trace = TRUE, 
    crit = "coef")

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.10672    0.13779  -8.032 9.59e-16 ***
(Intercept):2 -0.52717    0.09016  -5.847 5.00e-09 ***
x2:1           1.30631    0.23167   5.639 1.71e-08 ***
x2:2           2.09091    0.12658  16.519  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pobs0), loglink(lambda)

Log-likelihood: -1470.087 on 1996 degrees of freedom

Number of Fisher scoring iterations: 7 

No Hauck-Donner effect found in any of the estimates

> 
> # Another example ------------------------------
> # Data from Angers and Biswas (2003)
> abdata <- data.frame(y = 0:7, w = c(182, 41, 12, 2, 2, 0, 0, 1))
> abdata <- subset(abdata, w > 0)
> Abdata <- data.frame(yy = with(abdata, rep(y, w)))
> fit3 <- vglm(yy ~ 1, zapoisson, data = Abdata, trace = TRUE, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.25650524, 0.12608894
VGLM    linear loop  2 :  coefficients =  1.14002466, -0.14124406
VGLM    linear loop  3 :  coefficients =  1.14356045, -0.16530315
VGLM    linear loop  4 :  coefficients =  1.14356368, -0.16572625
VGLM    linear loop  5 :  coefficients =  1.14356368, -0.16572636
VGLM    linear loop  6 :  coefficients =  1.14356368, -0.16572636
> coef(fit3, matrix = TRUE)
            logitlink(pobs0) loglink(lambda)
(Intercept)         1.143564      -0.1657264
> Coef(fit3)  # Estimate lambda (they get 0.6997 with SE 0.1520)
    pobs0    lambda 
0.7583333 0.8472781 
> head(fitted(fit3), 1)
       [,1]
1 0.3583333
> with(Abdata, mean(yy))  # Compare this with fitted(fit3)
[1] 0.3583333
> 
> 
> 
> cleanEx()
> nameEx("zero")
> ### * zero
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zero
> ### Title: The zero Argument in VGAM Family Functions
> ### Aliases: zero
> ### Keywords: models regression programming
> 
> ### ** Examples
> 
> args(multinomial)
function (zero = NULL, parallel = FALSE, nointercept = NULL, 
    refLevel = "(Last)", ynames = FALSE, imethod = 1, imu = NULL, 
    byrow.arg = FALSE, Thresh = NULL, Trev = FALSE, Tref = if (Trev) "M" else 1, 
    whitespace = FALSE) 
NULL
> args(binom2.or)
function (lmu = "logitlink", lmu1 = lmu, lmu2 = lmu, loratio = "loglink", 
    imu1 = NULL, imu2 = NULL, ioratio = NULL, zero = "oratio", 
    exchangeable = FALSE, tol = 0.001, more.robust = FALSE) 
NULL
> args(gpd)
function (threshold = 0, lscale = "loglink", lshape = logofflink(offset = 0.5), 
    percentiles = c(90, 95), iscale = NULL, ishape = NULL, tolshape0 = 0.001, 
    type.fitted = c("percentiles", "mean"), imethod = 1, zero = "shape") 
NULL
> 
> #LMS quantile regression example
> fit <- vglm(BMI ~ sm.bs(age, df = 4), lms.bcg(zero = c(1, 3)),
+             data = bmi.nz, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2003.6436
VGLM    linear loop  2 :  loglikelihood = -2009.2245
Taking a modified step.
VGLM    linear loop  2 :  loglikelihood = -1996.7124
VGLM    linear loop  3 :  loglikelihood = -1996.3429
VGLM    linear loop  4 :  loglikelihood = -1996.5846
Taking a modified step.
VGLM    linear loop  4 :  loglikelihood = -1995.9549
VGLM    linear loop  5 :  loglikelihood = -1995.9509
VGLM    linear loop  6 :  loglikelihood = -1995.9569
Taking a modified step.
VGLM    linear loop  6 :  loglikelihood = -1995.9381
VGLM    linear loop  7 :  loglikelihood = -1995.9378
VGLM    linear loop  8 :  loglikelihood = -1995.9379
Taking a modified step.
VGLM    linear loop  8 :  loglikelihood = -1995.9375
VGLM    linear loop  9 :  loglikelihood = -1995.9375
VGLM    linear loop  10 :  loglikelihood = -1995.9375
Taking a modified step.
VGLM    linear loop  10 :  loglikelihood = -1995.9375
VGLM    linear loop  11 :  loglikelihood = -1995.9375
> coef(fit, matrix = TRUE)
                       lambda         mu loglink(sigma)
(Intercept)         -1.836875 24.6068044      -1.851626
sm.bs(age, df = 4)1  0.000000 -0.3320701       0.000000
sm.bs(age, df = 4)2  0.000000  2.8342499       0.000000
sm.bs(age, df = 4)3  0.000000  2.8573231       0.000000
sm.bs(age, df = 4)4  0.000000 -3.7988440       0.000000
> 
> 
> 
> cleanEx()
> nameEx("zeta")
> ### * zeta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zeta
> ### Title: Riemann's (and the Hurwitz) Zeta Function, With Derivatives
> ### Aliases: zeta
> ### Keywords: math
> 
> ### ** Examples
> 
> zeta(2:10)
[1] 1.644934 1.202057 1.082323 1.036928 1.017343 1.008349 1.004077 1.002008
[9] 1.000995
> 
> ## Not run: 
> ##D curve(zeta, -13, 0.8, xlim = c(-12, 10), ylim = c(-1, 4), col = "orange",
> ##D       las = 1, main = expression({zeta}(x)))
> ##D curve(zeta, 1.2,  12, add = TRUE, col = "orange")
> ##D abline(v = 0, h = c(0, 1), lty = "dashed", col = "gray")
> ##D 
> ##D curve(zeta, -14, -0.4, col = "orange", main = expression({zeta}(x)))
> ##D abline(v = 0, h = 0, lty = "dashed", col = "gray")  # Close up plot
> ##D 
> ##D x <- seq(0.04, 0.8, len = 100)  # Plot of the first derivative
> ##D plot(x, zeta(x, deriv = 1), type = "l", las = 1, col = "blue",
> ##D      xlim = c(0.04, 3), ylim = c(-6, 0), main = "zeta'(x)")
> ##D x <- seq(1.2, 3, len = 100)
> ##D lines(x, zeta(x, deriv = 1), col = "blue")
> ##D abline(v = 0, h = 0, lty = "dashed", col = "gray") 
> ## End(Not run)
> 
> zeta(2) - pi^2 / 6     # Should be 0
[1] 0
> zeta(4) - pi^4 / 90    # Should be 0
[1] 4.440892e-16
> zeta(6) - pi^6 / 945   # Should be 0
[1] 0
> zeta(8) - pi^8 / 9450  # Should be 0
[1] 4.440892e-16
> zeta(0, deriv = 1) + 0.5 * log(2*pi)  # Should be 0
[1] 0
> gamma0 <-  0.5772156649
> gamma1 <- -0.07281584548
> zeta(0, deriv = 2) -
+   gamma1 + 0.5 * (log(2*pi))^2 + pi^2/24 - gamma0^2 / 2  # Should be 0
[1] -2.791795e-12
> zeta(0.5, deriv = 1) + 3.92264613  # Should be 0
[1] -9.209153e-09
> zeta(2.0, deriv = 1) + 0.93754825431  # Should be 0
[1] -5.84377e-12
> 
> 
> 
> cleanEx()
> nameEx("zetaUC")
> ### * zetaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zeta
> ### Title: The Zeta Distribution
> ### Aliases: Zeta dzeta pzeta qzeta rzeta
> ### Keywords: distribution
> 
> ### ** Examples
> 
> dzeta(1:20, shape = 2)
 [1] 0.8319073726 0.1039884216 0.0308113842 0.0129985527 0.0066552590
 [6] 0.0038514230 0.0024253859 0.0016248191 0.0011411624 0.0008319074
[11] 0.0006250243 0.0004814279 0.0003786561 0.0003031732 0.0002464911
[16] 0.0002031024 0.0001693278 0.0001426453 0.0001212870 0.0001039884
> myshape <- 0.5
> max(abs(pzeta(1:200, myshape) -
+     cumsum(1/(1:200)^(1+myshape)) / zeta(myshape+1)))  # Should be 0
[1] 2.220446e-16
> 
> ## Not run: 
> ##D  plot(1:6, dzeta(1:6, 2), type = "h", las = 1,
> ##D                col = "orange", ylab = "Probability",
> ##D  main = "zeta probability function; orange: shape = 2; blue: shape = 1")
> ##D points(0.10 + 1:6, dzeta(1:6, 1), type = "h", col = "blue") 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zetaff")
> ### * zetaff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zetaff
> ### Title: Zeta Distribution Family Function
> ### Aliases: zetaff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(y = 1:5, w =  c(63, 14, 5, 1, 2))  # Knight, p.304
> fit <- vglm(y ~ 1, zetaff, data = zdata, trace = TRUE, weight = w, crit = "c")
VGLM    linear loop  1 :  coefficients = 0.49363036
VGLM    linear loop  2 :  coefficients = 0.51981018
VGLM    linear loop  3 :  coefficients = 0.5203144
VGLM    linear loop  4 :  coefficients = 0.52031458
VGLM    linear loop  5 :  coefficients = 0.52031458
> (phat <- Coef(fit))  # 1.682557
   shape 
1.682557 
> with(zdata, cbind(round(dzeta(y, phat) * sum(w), 1), w))
           w
[1,] 66.4 63
[2,] 10.3 14
[3,]  3.5  5
[4,]  1.6  1
[5,]  0.9  2
> 
> with(zdata, weighted.mean(y, w))
[1] 1.411765
> fitted(fit, matrix = FALSE)
      [,1]
1 1.633302
2 1.633302
3 1.633302
4 1.633302
5 1.633302
> predict(fit)
     loglink(shape)
[1,]      0.5203146
[2,]      0.5203146
[3,]      0.5203146
[4,]      0.5203146
[5,]      0.5203146
> 
> # The following should be zero at the MLE:
> with(zdata, mean(log(rep(y, w))) + zeta(1+phat, deriv = 1) / zeta(1+phat))
        shape 
-2.775558e-17 
> 
> 
> 
> cleanEx()
> nameEx("zibinomUC")
> ### * zibinomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zibinom
> ### Title: Zero-Inflated Binomial Distribution
> ### Aliases: Zibinom dzibinom pzibinom qzibinom rzibinom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> prob <- 0.2; size <- 10; pstr0 <- 0.5
> (ii <- dzibinom(0:size, size, prob, pstr0 = pstr0))
 [1] 0.5536870912 0.1342177280 0.1509949440 0.1006632960 0.0440401920
 [6] 0.0132120576 0.0027525120 0.0003932160 0.0000368640 0.0000020480
[11] 0.0000000512
> max(abs(cumsum(ii) - pzibinom(0:size, size, prob, pstr0 = pstr0)))  # 0?
[1] 1.110223e-16
> table(rzibinom(100, size, prob, pstr0 = pstr0))

 0  1  2  3  4  5 
55 12 19 11  2  1 
> 
> table(qzibinom(runif(100), size, prob, pstr0 = pstr0))

 0  1  2  3  4  5 
53 17 12 13  2  3 
> round(dzibinom(0:10, size, prob, pstr0 = pstr0) * 100)  # Similar?
 [1] 55 13 15 10  4  1  0  0  0  0  0
> 
> ## Not run: 
> ##D  x <- 0:size
> ##D barplot(rbind(dzibinom(x, size, prob, pstr0 = pstr0),
> ##D                 dbinom(x, size, prob)),
> ##D     beside = TRUE, col = c("blue", "green"), ylab = "Probability",
> ##D     main = paste0("ZIB(", size, ", ", prob, ", pstr0 = ", pstr0, ")",
> ##D                  " (blue) vs Binomial(", size, ", ", prob, ") (green)"),
> ##D     names.arg = as.character(x), las = 1, lwd = 2) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zibinomial")
> ### * zibinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zibinomial
> ### Title: Zero-Inflated Binomial Distribution Family Function
> ### Aliases: zibinomial zibinomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> size <- 10  # Number of trials; N in the notation above
> nn <- 200
> zdata <- data.frame(pstr0 = logitlink( 0, inverse = TRUE),  # 0.50
+                     mubin = logitlink(-1, inverse = TRUE),  # Mean of usual binomial
+                     sv    = rep(size, length = nn))
> zdata <- transform(zdata,
+                    y = rzibinom(nn, size = sv, prob = mubin, pstr0 = pstr0))
> with(zdata, table(y))
y
  0   1   2   3   4   5   6 
103  15  31  26  19   3   3 
> fit <- vglm(cbind(y, sv - y) ~ 1, zibinomialff, data = zdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -340.84327
VGLM    linear loop  2 :  loglikelihood = -391.00566
Taking a modified step.
VGLM    linear loop  2 :  loglikelihood = -302.32981
VGLM    linear loop  3 :  loglikelihood = -292.28305
VGLM    linear loop  4 :  loglikelihood = -290.60106
VGLM    linear loop  5 :  loglikelihood = -290.59947
VGLM    linear loop  6 :  loglikelihood = -290.59947
> fit <- vglm(cbind(y, sv - y) ~ 1, zibinomialff, data = zdata, trace = TRUE,
+             stepsize = 0.5)
Taking a modified step.
VGLM    linear loop  2 :  loglikelihood = -302.32981
Taking a modified step.
VGLM    linear loop  3 :  loglikelihood = -292.22145
Taking a modified step.
VGLM    linear loop  4 :  loglikelihood = -290.9489
Taking a modified step.
VGLM    linear loop  5 :  loglikelihood = -290.68191
Taking a modified step.
VGLM    linear loop  6 :  loglikelihood = -290.61956
Taking a modified step.
VGLM    linear loop  7 :  loglikelihood = -290.60443
Taking a modified step.
VGLM    linear loop  8 :  loglikelihood = -290.60071
Taking a modified step.
VGLM    linear loop  9 :  loglikelihood = -290.59978
Taking a modified step.
VGLM    linear loop  10 :  loglikelihood = -290.59955
Taking a modified step.
VGLM    linear loop  11 :  loglikelihood = -290.59949
Taking a modified step.
VGLM    linear loop  12 :  loglikelihood = -290.59948
Taking a modified step.
VGLM    linear loop  13 :  loglikelihood = -290.59948
Taking a modified step.
VGLM    linear loop  14 :  loglikelihood = -290.59947
> 
> coef(fit, matrix = TRUE)
            logitlink(prob) logitlink(onempstr0)
(Intercept)       -1.053886           0.04258395
> Coef(fit)  # Useful for intercept-only models
     prob onempstr0 
0.2584796 0.5106444 
> head(fitted(fit, type = "pobs0"))  # Estimate of P(Y = 0)
       [,1]
1 0.5150211
2 0.5150211
3 0.5150211
4 0.5150211
5 0.5150211
6 0.5150211
> head(fitted(fit))
       [,1]
1 0.1319911
2 0.1319911
3 0.1319911
4 0.1319911
5 0.1319911
6 0.1319911
> with(zdata, mean(y))  # Compare this with fitted(fit)
[1] 1.32
> summary(fit)
Call:
vglm(formula = cbind(y, sv - y) ~ 1, family = zibinomialff, data = zdata, 
    trace = TRUE, stepsize = 0.5)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1 -1.05389    0.07915 -13.315   <2e-16 ***
(Intercept):2  0.04258    0.15054   0.283    0.777    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(prob), logitlink(onempstr0)

Log-likelihood: -290.5995 on 398 degrees of freedom

Number of Fisher scoring iterations: 14 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("zigeomUC")
> ### * zigeomUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zigeom
> ### Title: Zero-Inflated Geometric Distribution
> ### Aliases: Zigeom dzigeom pzigeom qzigeom rzigeom
> ### Keywords: distribution
> 
> ### ** Examples
> 
> prob <- 0.5; pstr0 <- 0.2; x <- (-1):20
> (ii <- dzigeom(x, prob, pstr0))
 [1] 0.000000e+00 6.000000e-01 2.000000e-01 1.000000e-01 5.000000e-02
 [6] 2.500000e-02 1.250000e-02 6.250000e-03 3.125000e-03 1.562500e-03
[11] 7.812500e-04 3.906250e-04 1.953125e-04 9.765625e-05 4.882813e-05
[16] 2.441406e-05 1.220703e-05 6.103516e-06 3.051758e-06 1.525879e-06
[21] 7.629395e-07 3.814697e-07
> max(abs(cumsum(ii) - pzigeom(x, prob, pstr0)))  # Should be 0
[1] 2.220446e-16
> table(rzigeom(1000, prob, pstr0))

  0   1   2   3   4   5   6   7  10  13 
606 196  88  57  27  10  10   4   1   1 
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D barplot(rbind(dzigeom(x, prob, pstr0), dgeom(x, prob)),
> ##D    beside = TRUE, col = c("blue","orange"),
> ##D    ylab = "P[Y = y]", xlab = "y", las = 1,
> ##D    main = paste0("zigeometric(", prob, ", pstr0 = ", pstr0,
> ##D                  ") (blue) vs", " geometric(", prob, ") (orange)"),
> ##D    names.arg = as.character(x)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zigeometric")
> ### * zigeometric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zigeometric
> ### Title: Zero-Inflated Geometric Distribution Family Function
> ### Aliases: zigeometric zigeometricff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> gdata <- data.frame(x2 = runif(nn <- 1000) - 0.5)
> gdata <- transform(gdata, x3 = runif(nn) - 0.5,
+                           x4 = runif(nn) - 0.5)
> gdata <- transform(gdata, eta1 =  1.0 - 1.0 * x2 + 2.0 * x3,
+                           eta2 = -1.0,
+                           eta3 =  0.5)
> gdata <- transform(gdata, prob1 = logitlink(eta1, inverse = TRUE),
+                           prob2 = logitlink(eta2, inverse = TRUE),
+                           prob3 = logitlink(eta3, inverse = TRUE))
> gdata <- transform(gdata, y1 = rzigeom(nn, prob1, pstr0 = prob3),
+                           y2 = rzigeom(nn, prob2, pstr0 = prob3),
+                           y3 = rzigeom(nn, prob2, pstr0 = prob3))
> with(gdata, table(y1))
y1
  0   1   2   3   4   5   7 
872  81  31  11   3   1   1 
> with(gdata, table(y2))
y2
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  18 
711  70  51  42  37  22  19  15   6  10   6   3   2   3   2   1 
> with(gdata, table(y3))
y3
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17 
721  79  51  40  33  15  21  13  11   1   4   2   2   2   2   1   1   1 
> head(gdata)
           x2          x3         x4       eta1 eta2 eta3     prob1     prob2
1 -0.23449134  0.03080879  0.3718050  1.2961089   -1  0.5 0.7851794 0.2689414
2 -0.12787610  0.18486090  0.4671970  1.4975979   -1  0.5 0.8172159 0.2689414
3  0.07285336 -0.11671661  0.3669163  0.6937134   -1  0.5 0.6667925 0.2689414
4  0.40820779  0.45498800 -0.0622847  1.5017682   -1  0.5 0.8178380 0.2689414
5 -0.29831807 -0.38164342 -0.3080622  0.5350312   -1  0.5 0.6306558 0.2689414
6  0.39838968 -0.46089994 -0.4177056 -0.3201896   -1  0.5 0.4206295 0.2689414
      prob3 y1 y2 y3
1 0.6224593  0  0  0
2 0.6224593  0  0  0
3 0.6224593  0  0  0
4 0.6224593  0  0  3
5 0.6224593  0  6  0
6 0.6224593  3  0  0
> 
> fit1 <- vglm(y1 ~ x2 + x3 + x4, zigeometric(zero = 1), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -530.34927
VGLM    linear loop  2 :  loglikelihood = -507.97209
VGLM    linear loop  3 :  loglikelihood = -495.05366
VGLM    linear loop  4 :  loglikelihood = -492.48838
VGLM    linear loop  5 :  loglikelihood = -492.41168
VGLM    linear loop  6 :  loglikelihood = -492.41135
VGLM    linear loop  7 :  loglikelihood = -492.41135
> coef(fit1, matrix = TRUE)
            logitlink(pstr0) logitlink(prob)
(Intercept)        0.1141337       1.0852219
x2                 0.0000000      -1.2132864
x3                 0.0000000       1.8077364
x4                 0.0000000      -0.1156748
> head(fitted(fit1, type = "pstr0"))
          [,1]
[1,] 0.5285025
[2,] 0.5285025
[3,] 0.5285025
[4,] 0.5285025
[5,] 0.5285025
[6,] 0.5285025
> 
> fit2 <- vglm(cbind(y2, y3) ~ 1, zigeometric(zero = 1), data = gdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -2427.7434
VGLM    linear loop  2 :  loglikelihood = -2426.5368
VGLM    linear loop  3 :  loglikelihood = -2426.5338
VGLM    linear loop  4 :  loglikelihood = -2426.5338
> coef(fit2, matrix = TRUE)
            logitlink(pstr01) logitlink(prob1) logitlink(pstr02)
(Intercept)         0.4468288        -1.050168         0.4663284
            logitlink(prob2)
(Intercept)       -0.9632017
> summary(fit2)
Call:
vglm(formula = cbind(y2, y3) ~ 1, family = zigeometric(zero = 1), 
    data = gdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1  0.44683    0.08636   5.174 2.29e-07 ***
(Intercept):2 -1.05017    0.06834 -15.366  < 2e-16 ***
(Intercept):3  0.46633    0.08857   5.265 1.40e-07 ***
(Intercept):4 -0.96320    0.07037 -13.687  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pstr01), logitlink(prob1), 
logitlink(pstr02), logitlink(prob2)

Log-likelihood: -2426.534 on 3996 degrees of freedom

Number of Fisher scoring iterations: 4 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("zinegbinUC")
> ### * zinegbinUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zinegbin
> ### Title: Zero-Inflated Negative Binomial Distribution
> ### Aliases: Zinegbin dzinegbin pzinegbin qzinegbin rzinegbin
> ### Keywords: distribution
> 
> ### ** Examples
> 
> munb <- 3; pstr0 <- 0.2; size <- k <- 10; x <- 0:10
> (ii <- dzinegbin(x, pstr0 = pstr0, mu = munb, size = k))
 [1] 0.258030520 0.133916585 0.169971050 0.156896354 0.117672266 0.076034387
 [7] 0.043865993 0.023138106 0.011346571 0.005236879 0.002296170
> max(abs(cumsum(ii) - pzinegbin(x, pstr0 = pstr0, mu = munb, size = k)))
[1] 2.220446e-16
> table(rzinegbin(100, pstr0 = pstr0, mu = munb, size = k))

 0  1  2  3  4  5  6  7  9 
21 16 18 16 15  9  3  1  1 
> 
> table(qzinegbin(runif(1000), pstr0 = pstr0, mu = munb, size = k))

  0   1   2   3   4   5   6   7   8   9  10  12  14 
254 144 185 140 105  70  57  22  11   9   1   1   1 
> round(dzinegbin(x, pstr0 = pstr0, mu = munb, size = k) * 1000)  # Similar?
 [1] 258 134 170 157 118  76  44  23  11   5   2
> 
> ## Not run: 
> ##D barplot(rbind(dzinegbin(x, pstr0 = pstr0, mu = munb, size = k),
> ##D                 dnbinom(x, mu = munb, size = k)), las = 1,
> ##D     beside = TRUE, col = c("blue", "green"), ylab = "Probability",
> ##D     main = paste("ZINB(mu = ", munb, ", k = ", k, ", pstr0 = ", pstr0,
> ##D                  ") (blue) vs NB(mu = ", munb,
> ##D                  ", size = ", k, ") (green)", sep = ""),
> ##D     names.arg = as.character(x)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zinegbinomial")
> ### * zinegbinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zinegbinomial
> ### Title: Zero-Inflated Negative Binomial Distribution Family Function
> ### Aliases: zinegbinomial zinegbinomialff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Example 1
> ##D ndata <- data.frame(x2 = runif(nn <- 1000))
> ##D ndata <- transform(ndata, pstr0 = logitlink(-0.5 + 1 * x2, inverse = TRUE),
> ##D                           munb  =   exp( 3   + 1 * x2),
> ##D                           size  =   exp( 0   + 2 * x2))
> ##D ndata <- transform(ndata,
> ##D                    y1 = rzinegbin(nn, mu = munb, size = size, pstr0 = pstr0))
> ##D with(ndata, table(y1)["0"] / sum(table(y1)))
> ##D nfit <- vglm(y1 ~ x2, zinegbinomial(zero = NULL), data = ndata)
> ##D coef(nfit, matrix = TRUE)
> ##D summary(nfit)
> ##D head(cbind(fitted(nfit), with(ndata, (1 - pstr0) * munb)))
> ##D round(vcov(nfit), 3)
> ##D 
> ##D 
> ##D # Example 2: RR-ZINB could also be called a COZIVGLM-ZINB-2
> ##D ndata <- data.frame(x2 = runif(nn <- 2000))
> ##D ndata <- transform(ndata, x3 = runif(nn))
> ##D ndata <- transform(ndata, eta1 =          3   + 1   * x2 + 2 * x3)
> ##D ndata <- transform(ndata, pstr0  = logitlink(-1.5 + 0.5 * eta1, inverse = TRUE),
> ##D                           munb = exp(eta1),
> ##D                           size = exp(4))
> ##D ndata <- transform(ndata,
> ##D                    y1 = rzinegbin(nn, pstr0 = pstr0, mu = munb, size = size))
> ##D with(ndata, table(y1)["0"] / sum(table(y1)))
> ##D rrzinb <- rrvglm(y1 ~ x2 + x3, zinegbinomial(zero = NULL), data = ndata,
> ##D                  Index.corner = 2, str0 = 3, trace = TRUE)
> ##D coef(rrzinb, matrix = TRUE)
> ##D Coef(rrzinb)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zipebcom")
> ### * zipebcom
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zipebcom
> ### Title: Exchangeable Bivariate cloglog Odds-ratio Model From a
> ###   Zero-inflated Poisson Distribution
> ### Aliases: zipebcom
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D zdata <- data.frame(x2 = seq(0, 1, len = (nsites <- 2000)))
> ##D zdata <- transform(zdata, eta1 =  -3 + 5 * x2,
> ##D                          phi1 = logitlink(-1, inverse = TRUE),
> ##D                          oratio = exp(2))
> ##D zdata <- transform(zdata, mu12 = clogloglink(eta1, inverse = TRUE) * (1-phi1))
> ##D tmat <-  with(zdata, rbinom2.or(nsites, mu1 = mu12, oratio = oratio, exch = TRUE))
> ##D zdata <- transform(zdata, ybin1 = tmat[, 1], ybin2 = tmat[, 2])
> ##D 
> ##D with(zdata, table(ybin1, ybin2)) / nsites  # For interest only
> ##D # Various plots of the data, for interest only
> ##D par(mfrow = c(2, 2))
> ##D plot(jitter(ybin1) ~ x2, data = zdata, col = "blue")
> ##D 
> ##D plot(jitter(ybin2) ~ jitter(ybin1), data = zdata, col = "blue")
> ##D 
> ##D plot(mu12 ~ x2, data = zdata, col = "blue", type = "l", ylim = 0:1,
> ##D      ylab = "Probability", main = "Marginal probability and phi")
> ##D with(zdata, abline(h = phi1[1], col = "red", lty = "dashed"))
> ##D 
> ##D tmat2 <- with(zdata, dbinom2.or(mu1 = mu12, oratio = oratio, exch = TRUE))
> ##D with(zdata, matplot(x2, tmat2, col = 1:4, type = "l", ylim = 0:1,
> ##D      ylab = "Probability", main = "Joint probabilities"))
> ##D 
> ##D # Now fit the model to the data.
> ##D fit <- vglm(cbind(ybin1, ybin2) ~ x2, zipebcom, data = zdata, trace = TRUE)
> ##D coef(fit, matrix = TRUE)
> ##D summary(fit)
> ##D vcov(fit)   
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zipf")
> ### * zipf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zipf
> ### Title: Zipf Distribution Family Function
> ### Aliases: zipf
> ### Keywords: models regression
> 
> ### ** Examples
> 
> zdata <- data.frame(y = 1:5, ofreq = c(63, 14, 5, 1, 2))
> zfit <- vglm(y ~ 1, zipf, data = zdata, trace = TRUE, weight = ofreq)
VGLM    linear loop  1 :  loglikelihood = -70.96903
VGLM    linear loop  2 :  loglikelihood = -70.96903
> zfit <- vglm(y ~ 1, zipf(lshape = "identitylink", ishape = 3.4), data = zdata,
+             trace = TRUE, weight = ofreq, crit = "coef")
VGLM    linear loop  1 :  coefficients = 1.733347
VGLM    linear loop  2 :  coefficients = 2.2463516
VGLM    linear loop  3 :  coefficients = 2.3406871
VGLM    linear loop  4 :  coefficients = 2.3439
VGLM    linear loop  5 :  coefficients = 2.3439037
VGLM    linear loop  6 :  coefficients = 2.3439037
> zfit@misc$N
[1] 5
> (shape.hat <- Coef(zfit))
   shape 
2.343904 
> with(zdata, weighted.mean(y, ofreq))
[1] 1.411765
> fitted(zfit, matrix = FALSE)
      [,1]
1 1.417752
2 1.417752
3 1.417752
4 1.417752
5 1.417752
> 
> 
> 
> cleanEx()
> nameEx("zipfUC")
> ### * zipfUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zipf
> ### Title: The Zipf Distribution
> ### Aliases: Zipf dzipf pzipf qzipf rzipf
> ### Keywords: distribution
> 
> ### ** Examples
> 
> N <- 10; shape <- 0.5; y <- 1:N
> proby <- dzipf(y, N = N, shape = shape)
> ## Not run: 
> ##D  plot(proby ~ y, type = "h", col = "blue",
> ##D    ylim = c(0, 0.2), ylab = "Probability", lwd = 2, las = 1,
> ##D    main = paste0("Zipf(N = ", N, ", shape = ", shape, ")")) 
> ## End(Not run)
> sum(proby)  # Should be 1
[1] 1
> max(abs(cumsum(proby) - pzipf(y, N = N, shape = shape)))  # 0?
[1] 1.110223e-16
> 
> 
> 
> cleanEx()
> nameEx("zipfmbUC")
> ### * zipfmbUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zipfmb
> ### Title: The Zipf-Mandelbrot Distribution
> ### Aliases: Zipfmb dzipfmb pzipfmb qzipfmb rzipfmb
> ### Keywords: distribution
> 
> ### ** Examples
> 
> aa <- 1:10
> (pp <- pzipfmb(aa, shape = 0.5, start = 1))
 [1] 0.5000000 0.6250000 0.6875000 0.7265625 0.7539062 0.7744141 0.7905273
 [8] 0.8036194 0.8145294 0.8238029
> cumsum(dzipfmb(aa, shape = 0.5, start = 1))  # Should be same
 [1] 0.5000000 0.6250000 0.6875000 0.7265625 0.7539062 0.7744141 0.7905273
 [8] 0.8036194 0.8145294 0.8238029
> qzipfmb(pp, shape = 0.5, start = 1) - aa  # Should be  all 0s
 [1] 0 0 0 0 0 0 0 0 0 0
> 
> rdiffzeta(30, 0.5)
 [1]     1     2     5   118     1    96   326     8     7     1     1     1
[13]    10     2    18     3    12 15264     2    20   234     1     8     1
[25]     1     2     1     2    58     2
> 
> ## Not run: 
> ##D x <- 1:10
> ##D plot(x, dzipfmb(x, shape = 0.5), type = "h", ylim = 0:1,
> ##D      sub = "shape=0.5", las = 1, col = "blue", ylab = "Probability",
> ##D      main = "Zipf-Mandelbrot distribution: blue=PMF; orange=CDF")
> ##D lines(x+0.1, pzipfmb(x, shape = 0.5), col = "red", lty = 3, type = "h")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zipoisUC")
> ### * zipoisUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zipois
> ### Title: Zero-Inflated Poisson Distribution
> ### Aliases: Zipois dzipois pzipois qzipois rzipois
> ### Keywords: distribution
> 
> ### ** Examples
> 
> lambda <- 3; pstr0 <- 0.2; x <- (-1):7
> (ii <- dzipois(x, lambda, pstr0 = pstr0))
[1] 0.00000000 0.23982965 0.11948896 0.17923345 0.17923345 0.13442508 0.08065505
[8] 0.04032753 0.01728323
> max(abs(cumsum(ii) - pzipois(x, lambda, pstr0 = pstr0)))  # 0?
[1] 2.220446e-16
> table(rzipois(100, lambda, pstr0 = pstr0))

 0  1  2  3  4  5  6  8 
18 13 23 17 16  9  3  1 
> 
> table(qzipois(runif(100), lambda, pstr0))

 0  1  2  3  4  5  6  7  8 
21 12 19 20 12 11  1  2  2 
> round(dzipois(0:10, lambda, pstr0 = pstr0) * 100)  # Similar?
 [1] 24 12 18 18 13  8  4  2  1  0  0
> 
> ## Not run: 
> ##D  x <- 0:10
> ##D par(mfrow = c(2, 1))  # Zero-inflated Poisson
> ##D barplot(rbind(dzipois(x, lambda, pstr0 = pstr0), dpois(x, lambda)),
> ##D         beside = TRUE, col = c("blue", "orange"),
> ##D         main = paste0("ZIP(", lambda,
> ##D                       ", pstr0 = ", pstr0, ") (blue) vs",
> ##D                       " Poisson(", lambda, ") (orange)"),
> ##D         names.arg = as.character(x))
> ##D 
> ##D deflat.limit <- -1 / expm1(lambda)  # Zero-deflated Poisson
> ##D newpstr0 <- round(deflat.limit / 1.5, 3)
> ##D barplot(rbind(dzipois(x, lambda, pstr0 = newpstr0),
> ##D                 dpois(x, lambda)),
> ##D         beside = TRUE, col = c("blue","orange"),
> ##D         main = paste0("ZDP(", lambda, ", pstr0 = ", newpstr0, ")",
> ##D                      " (blue) vs Poisson(", lambda, ") (orange)"),
> ##D         names.arg = as.character(x)) 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("zipoisson")
> ### * zipoisson
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zipoisson
> ### Title: Zero-Inflated Poisson Distribution Family Function
> ### Aliases: zipoisson zipoissonff
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # Example 1: simulated ZIP data
> zdata <- data.frame(x2 = runif(nn <- 1000))
> zdata <- transform(zdata,
+            pstr01  = logitlink(-0.5 + 1*x2, inverse = TRUE),
+            pstr02  = logitlink( 0.5 - 1*x2, inverse = TRUE),
+            Ps01    = logitlink(-0.5       , inverse = TRUE),
+            Ps02    = logitlink( 0.5       , inverse = TRUE),
+            lambda1 =   loglink(-0.5 + 2*x2, inverse = TRUE),
+            lambda2 =   loglink( 0.5 + 2*x2, inverse = TRUE))
> zdata <- transform(zdata, y1 = rzipois(nn, lambda1, pstr0 = Ps01),
+                           y2 = rzipois(nn, lambda2, pstr0 = Ps02))
> 
> with(zdata, table(y1))  # Eyeball the data
y1
  0   1   2   3   4   5   6   7   8   9  10 
525 186 103  80  46  32  13   9   4   1   1 
> with(zdata, table(y2))
y2
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19  24 
658  25  37  44  55  35  31  29  12  16  19  15   5   7   4   2   4   1   1 
> fit1 <- vglm(y1 ~ x2, zipoisson(zero = 1), zdata, crit = "coef")
> fit2 <- vglm(y2 ~ x2, zipoisson(zero = 1), zdata, crit = "coef")
> coef(fit1, matrix = TRUE)  # Should agree with the above values
            logitlink(pstr0) loglink(lambda)
(Intercept)       -0.4291536      -0.5373543
x2                 0.0000000       2.0909158
> coef(fit2, matrix = TRUE)  # Should agree with the above values
            logitlink(pstr0) loglink(lambda)
(Intercept)        0.6043945       0.6262089
x2                 0.0000000       1.8432000
> 
> # Fit all two simultaneously, using a different parameterization:
> fit12 <- vglm(cbind(y1, y2) ~ x2, zipoissonff, zdata, crit = "coef")
> coef(fit12, matrix = TRUE)  # Should agree with the above values
            loglink(lambda1) logitlink(onempstr01) loglink(lambda2)
(Intercept)       -0.5373543             0.4291536        0.6262089
x2                 2.0909158             0.0000000        1.8432000
            logitlink(onempstr02)
(Intercept)            -0.6043945
x2                      0.0000000
> 
> # For the first observation compute the probability that y1 is
> # due to a structural zero.
> (fitted(fit1, type = "pstr0") / fitted(fit1, type = "pobs0"))[1]
[1] 0.6430915
> 
> 
> # Example 2: McKendrick (1925). From 223 Indian village households
> cholera <- data.frame(ncases = 0:4,  # Number of cholera cases,
+                       wfreq  = c(168, 32, 16, 6, 1))  # Frequencies
> fit <- vglm(ncases ~ 1, zipoisson, wei = wfreq, cholera)
> coef(fit, matrix = TRUE)
            logitlink(pstr0) loglink(lambda)
(Intercept)         0.419289     -0.02821652
> with(cholera, cbind(actual = wfreq,
+                     fitted = round(dzipois(ncases, Coef(fit)[2],
+                                            pstr0 = Coef(fit)[1]) *
+                                    sum(wfreq), digits = 2)))
     actual fitted
[1,]    168 168.00
[2,]     32  32.53
[3,]     16  15.81
[4,]      6   5.12
[5,]      1   1.25
> 
> # Example 3: data from Angers and Biswas (2003)
> abdata <- data.frame(y = 0:7, w = c(182, 41, 12, 2, 2, 0, 0, 1))
> abdata <- subset(abdata, w > 0)
> fit3 <- vglm(y ~ 1, zipoisson(lpstr0 = probitlink, ipstr0 = 0.8),
+              data = abdata, weight = w, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -191.2889
VGLM    linear loop  2 :  loglikelihood = -190.4574
VGLM    linear loop  3 :  loglikelihood = -190.437
VGLM    linear loop  4 :  loglikelihood = -190.437
VGLM    linear loop  5 :  loglikelihood = -190.437
> fitted(fit3, type = "pobs0")  # Estimate of P(Y = 0)
       [,1]
1 0.7583333
2 0.7583333
3 0.7583333
4 0.7583333
5 0.7583333
8 0.7583333
> coef(fit3, matrix = TRUE)
            probitlink(pstr0) loglink(lambda)
(Intercept)         0.1944214      -0.1657264
> Coef(fit3)  # Estimate of pstr0 and lambda
    pstr0    lambda 
0.5770771 0.8472781 
> fitted(fit3)
       [,1]
1 0.3583333
2 0.3583333
3 0.3583333
4 0.3583333
5 0.3583333
8 0.3583333
> with(abdata, weighted.mean(y, w))  # Compare this with fitted(fit)
[1] 0.3583333
> summary(fit3)
Call:
vglm(formula = y ~ 1, family = zipoisson(lpstr0 = probitlink, 
    ipstr0 = 0.8), data = abdata, weights = w, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)
(Intercept):1   0.1944     0.1741   1.117    0.264
(Intercept):2  -0.1657     0.1786  -0.928    0.353

Names of linear predictors: probitlink(pstr0), loglink(lambda)

Log-likelihood: -190.437 on 10 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates

> 
> # Example 4: zero-deflated (ZDP) model for intercept-only data
> zdata <- transform(zdata, lambda3 = loglink(0.0, inverse = TRUE))
> zdata <- transform(zdata, deflat.limit=-1/expm1(lambda3))  # Bndy
> # The 'pstr0' parameter is negative and in parameter space:
> # Not too near the boundary:
> zdata <- transform(zdata, usepstr0 = deflat.limit / 2)
> zdata <- transform(zdata,
+                    y3 = rzipois(nn, lambda3, pstr0 = usepstr0))
> head(zdata)
         x2    pstr01    pstr02      Ps01      Ps02   lambda1   lambda2 y1 y2
1 0.2655087 0.4416443 0.5583557 0.3775407 0.6224593 1.0315034  2.803917  0  3
2 0.3721239 0.4680745 0.5319255 0.3775407 0.6224593 1.2766606  3.470323  1  6
3 0.5728534 0.5182053 0.4817947 0.3775407 0.6224593 1.9073345  5.184673  0  6
4 0.9082078 0.6006581 0.3993419 0.3775407 0.6224593 3.7300274 10.139266  7  0
5 0.2016819 0.4259687 0.5740313 0.3775407 0.6224593 0.9078863  2.467891  0  0
6 0.8983897 0.5983007 0.4016993 0.3775407 0.6224593 3.6574982  9.942111  0  0
  lambda3 deflat.limit   usepstr0 y3
1       1   -0.5819767 -0.2909884  2
2       1   -0.5819767 -0.2909884  1
3       1   -0.5819767 -0.2909884  0
4       1   -0.5819767 -0.2909884  1
5       1   -0.5819767 -0.2909884  2
6       1   -0.5819767 -0.2909884  3
> with(zdata, table(y3))  # A lot of deflation
y3
  0   1   2   3   4   5 
202 446 239  83  24   6 
> fit4 <- vglm(y3 ~ 1, data = zdata, trace = TRUE, crit = "coef",
+              zipoisson(lpstr0 = "identitylink"))
VGLM    linear loop  1 :  coefficients = -0.13420005,  0.14653094
VGLM    linear loop  2 :  coefficients = -0.210558533,  0.068402289
VGLM    linear loop  3 :  coefficients = -0.215396497,  0.066518643
VGLM    linear loop  4 :  coefficients = -0.215399403,  0.066521989
VGLM    linear loop  5 :  coefficients = -0.215399403,  0.066521988
> coef(fit4, matrix = TRUE)
                 pstr0 loglink(lambda)
(Intercept) -0.2153994      0.06652199
> # Check how accurate it was:
> zdata[1, "usepstr0"]  # Answer
[1] -0.2909884
> coef(fit4)[1]         # Estimate
(Intercept):1 
   -0.2153994 
> Coef(fit4)
     pstr0     lambda 
-0.2153994  1.0687845 
> vcov(fit4)  # Is positive-definite
              (Intercept):1 (Intercept):2
(Intercept):1   0.001179867   0.001186164
(Intercept):2   0.001186164   0.001745769
> 
> # Example 5: RR-ZIP
> set.seed(123)
> rrzip <- rrvglm(Alopacce ~ sm.bs(WaterCon, df = 3),
+                 zipoisson(zero = NULL),
+                 data = hspider, trace = TRUE, Index.corner = 2)
RR-VGLM    linear loop  1 :  loglikelihood = -102.86636
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  138.30995311 
   Alternating iteration 2 ,   Convergence criterion  =  0.0001542638 
    ResSS  =  138.28862019 
   Alternating iteration 3 ,   Convergence criterion  =  7.621223e-06 
    ResSS  =  138.28756627 
   Alternating iteration 4 ,   Convergence criterion  =  3.7527e-07 
    ResSS  =  138.28751437 
   Alternating iteration 5 ,   Convergence criterion  =  1.846195e-08 
    ResSS  =  138.28751182 
   Alternating iteration 6 ,   Convergence criterion  =  9.080769e-10 
    ResSS  =  138.28751169 
RR-VGLM    linear loop  2 :  loglikelihood = -87.831645
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  181.06956442 
   Alternating iteration 2 ,   Convergence criterion  =  0.001857977 
    ResSS  =  180.7337653 
   Alternating iteration 3 ,   Convergence criterion  =  0.0006632735 
    ResSS  =  180.61396884 
   Alternating iteration 4 ,   Convergence criterion  =  0.0001544903 
    ResSS  =  180.58607004 
   Alternating iteration 5 ,   Convergence criterion  =  2.791617e-05 
    ResSS  =  180.58102891 
   Alternating iteration 6 ,   Convergence criterion  =  4.467694e-06 
    ResSS  =  180.58022214 
   Alternating iteration 7 ,   Convergence criterion  =  6.792301e-07 
    ResSS  =  180.58009948 
   Alternating iteration 8 ,   Convergence criterion  =  1.011703e-07 
    ResSS  =  180.58008121 
   Alternating iteration 9 ,   Convergence criterion  =  1.494939e-08 
    ResSS  =  180.58007851 
   Alternating iteration 10 ,   Convergence criterion  =  2.2022e-09 
    ResSS  =  180.58007811 
RR-VGLM    linear loop  3 :  loglikelihood = -87.470201
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  218.39987417 
   Alternating iteration 2 ,   Convergence criterion  =  0.0001399765 
    ResSS  =  218.3693076 
   Alternating iteration 3 ,   Convergence criterion  =  3.694569e-05 
    ResSS  =  218.36124009 
   Alternating iteration 4 ,   Convergence criterion  =  1.042708e-05 
    ResSS  =  218.35896324 
   Alternating iteration 5 ,   Convergence criterion  =  3.041633e-06 
    ResSS  =  218.35829908 
   Alternating iteration 6 ,   Convergence criterion  =  9.026038e-07 
    ResSS  =  218.35810199 
   Alternating iteration 7 ,   Convergence criterion  =  2.70307e-07 
    ResSS  =  218.35804296 
   Alternating iteration 8 ,   Convergence criterion  =  8.135131e-08 
    ResSS  =  218.3580252 
   Alternating iteration 9 ,   Convergence criterion  =  2.454947e-08 
    ResSS  =  218.35801984 
   Alternating iteration 10 ,   Convergence criterion  =  7.419252e-09 
    ResSS  =  218.35801822 
RR-VGLM    linear loop  4 :  loglikelihood = -87.287992
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  242.65958249 
   Alternating iteration 2 ,   Convergence criterion  =  2.403423e-08 
    ResSS  =  242.65957666 
   Alternating iteration 3 ,   Convergence criterion  =  9.243684e-09 
    ResSS  =  242.65957442 
RR-VGLM    linear loop  5 :  loglikelihood = -87.285868
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  233.47629539 
   Alternating iteration 2 ,   Convergence criterion  =  1.405686e-08 
    ResSS  =  233.47629211 
   Alternating iteration 3 ,   Convergence criterion  =  5.230029e-09 
    ResSS  =  233.47629089 
RR-VGLM    linear loop  6 :  loglikelihood = -87.285863
   Alternating iteration 1 ,   Convergence criterion  =  1 
    ResSS  =  233.49036584 
   Alternating iteration 2 ,   Convergence criterion  =  7.776123e-11 
    ResSS  =  233.49036583 
RR-VGLM    linear loop  7 :  loglikelihood = -87.285862
> coef(rrzip, matrix = TRUE)
                         logitlink(pstr0) loglink(lambda)
(Intercept)                    -2.5900978       2.7264820
sm.bs(WaterCon, df = 3)1        0.8885286      -0.4154601
sm.bs(WaterCon, df = 3)2       -0.7412230       0.3465826
sm.bs(WaterCon, df = 3)3        5.6708444      -2.6515856
> Coef(rrzip)
A matrix:
                    latvar
logitlink(pstr0) -2.138662
loglink(lambda)   1.000000

C matrix:
                             latvar
sm.bs(WaterCon, df = 3)1 -0.4155134
sm.bs(WaterCon, df = 3)2  0.3466581
sm.bs(WaterCon, df = 3)3 -2.6516382

B1 matrix:
            logitlink(pstr0) loglink(lambda)
(Intercept)        -2.590098        2.726482
> summary(rrzip)
Call:
rrvglm(formula = Alopacce ~ sm.bs(WaterCon, df = 3), family = zipoisson(zero = NULL), 
    data = hspider, trace = TRUE, Index.corner = 2)

Coefficients: 
                         Estimate Std. Error z value Pr(>|z|)    
I(latvar.mat)            -2.13860    1.69884 -1.2589  0.10404    
(Intercept):1            -2.59010    1.38756 -1.8667  0.03097 *  
(Intercept):2             2.72648    0.16208 16.8217  < 2e-16 ***
sm.bs(WaterCon, df = 3)1 -0.41546    1.01494 -0.4093  0.34114    
sm.bs(WaterCon, df = 3)2  0.34658    1.39927  0.2477  0.40219    
sm.bs(WaterCon, df = 3)3 -2.65159    1.39118 -1.9060  0.02833 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: logitlink(pstr0), loglink(lambda)

Log-likelihood: -87.28586 on 49 degrees of freedom

Number of Fisher scoring iterations: 7 

> ## Not run: plotvgam(rrzip, lcol = "blue")
> 
> 
> 
> cleanEx()
> nameEx("zoabetaR")
> ### * zoabetaR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zoabetaR
> ### Title: Zero- and One-Inflated Beta Distribution Family Function
> ### Aliases: zoabetaR
> ### Keywords: regression
> 
> ### ** Examples
> 
> nn <- 1000; set.seed(1)
> bdata <- data.frame(x2 = runif(nn))
> bdata <- transform(bdata,
+   pobs0 = logitlink(-2 + x2, inverse = TRUE),
+   pobs1 = logitlink(-2 + x2, inverse = TRUE))
> bdata <- transform(bdata,
+   y1 = rzoabeta(nn, shape1 = exp(1 + x2), shape2 = exp(2 - x2),
+                 pobs0 = pobs0, pobs1 = pobs1))
> summary(bdata)
       x2               pobs0            pobs1              y1        
 Min.   :0.001315   Min.   :0.1193   Min.   :0.1193   Min.   :0.0000  
 1st Qu.:0.258129   1st Qu.:0.1491   1st Qu.:0.1491   1st Qu.:0.1807  
 Median :0.483260   Median :0.1799   Median :0.1799   Median :0.4567  
 Mean   :0.499692   Mean   :0.1863   Mean   :0.1863   Mean   :0.4733  
 3rd Qu.:0.746932   3rd Qu.:0.2222   3rd Qu.:0.2222   3rd Qu.:0.7384  
 Max.   :0.999931   Max.   :0.2689   Max.   :0.2689   Max.   :1.0000  
> fit1 <- vglm(y1 ~ x2, zoabetaR(parallel.pobs = TRUE),
+              data = bdata, trace = TRUE)
VGLM    linear loop  1 :  loglikelihood = -667.3399
VGLM    linear loop  2 :  loglikelihood = -629.01907
VGLM    linear loop  3 :  loglikelihood = -626.34081
VGLM    linear loop  4 :  loglikelihood = -626.32467
VGLM    linear loop  5 :  loglikelihood = -626.32467
VGLM    linear loop  6 :  loglikelihood = -626.32467
> coef(fit1, matrix = TRUE)
            loglink(shape1) loglink(shape2) logitlink(pobs0) logitlink(pobs1)
(Intercept)       0.8680077       1.8387140        -2.058975        -2.058975
x2                1.2421898      -0.6163044         1.275530         1.275530
> summary(fit1)
Call:
vglm(formula = y1 ~ x2, family = zoabetaR(parallel.pobs = TRUE), 
    data = bdata, trace = TRUE)

Coefficients: 
              Estimate Std. Error z value Pr(>|z|)    
(Intercept):1   0.8680     0.1039   8.352  < 2e-16 ***
(Intercept):2   1.8387     0.1073  17.133  < 2e-16 ***
(Intercept):3  -2.0590     0.1119 -18.395  < 2e-16 ***
x2:1            1.2422     0.2014   6.168 6.92e-10 ***
x2:2           -0.6163     0.2021  -3.050  0.00229 ** 
x2:3            1.2755     0.1690   7.549 4.37e-14 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Names of linear predictors: loglink(shape1), loglink(shape2), 
logitlink(pobs0), logitlink(pobs1)

Log-likelihood: -626.3247 on 3994 degrees of freedom

Number of Fisher scoring iterations: 6 

No Hauck-Donner effect found in any of the estimates

> 
> 
> 
> cleanEx()
> nameEx("zoabetaUC")
> ### * zoabetaUC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Zoabeta
> ### Title: The Zero/One-Inflated Beta Distribution
> ### Aliases: Zoabeta dzoabeta pzoabeta qzoabeta rzoabeta
> ### Keywords: distribution
> 
> ### ** Examples
> 
> ## Not run: 
> ##D N <- 1000; y <- rzoabeta(N, 2, 3, 0.2, 0.2)
> ##D hist(y, probability = TRUE, border = "blue", las = 1,
> ##D      main = "Blue = 0- and 1-altered; orange = ordinary beta")
> ##D sum(y == 0) / N  # Proportion of 0s
> ##D sum(y == 1) / N  # Proportion of 1s
> ##D Ngrid <- 1000
> ##D lines(seq(0, 1, length = Ngrid),
> ##D       dbeta(seq(0, 1, length = Ngrid), 2, 3), col = "orange")
> ##D lines(seq(0, 1, length = Ngrid), col = "blue",
> ##D       dzoabeta(seq(0, 1, length = Ngrid), 2 , 3, 0.2, 0.2))
> ## End(Not run)
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  25.386 5.485 24.511 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
