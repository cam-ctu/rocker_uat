
R version 4.4.2 (2024-10-31) -- "Pile of Leaves"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "Hmisc"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('Hmisc')

Attaching package: ‘Hmisc’

The following objects are masked from ‘package:base’:

    format.pval, units

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Cs")
> ### * Cs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Cs
> ### Title: Character strings from unquoted names
> ### Aliases: Cs .q
> ### Keywords: character utilities
> 
> ### ** Examples
> 
> Cs(a,cat,dog)
[1] "a"   "cat" "dog"
> # subset.data.frame <- dataframe[,Cs(age,sex,race,bloodpressure,height)]
> .q(a, b, c, 'this and that')
[1] "a"             "b"             "c"             "this and that"
> .q(dog=a, giraffe=b, cat=c)
    dog giraffe     cat 
    "a"     "b"     "c" 
> 
> 
> 
> cleanEx()
> nameEx("Ecdf")
> ### * Ecdf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Ecdf
> ### Title: Empirical Cumulative Distribution Plot
> ### Aliases: Ecdf Ecdf.default Ecdf.data.frame Ecdf.formula panel.Ecdf
> ###   prepanel.Ecdf
> ### Keywords: nonparametric hplot methods distribution
> 
> ### ** Examples
> 
> set.seed(1)
> ch <- rnorm(1000, 200, 40)
> Ecdf(ch, xlab="Serum Cholesterol")
> scat1d(ch)                       # add rug plot
> histSpike(ch, add=TRUE, frac=.15)   # add spike histogram
> # Better: add a data density display automatically:
> Ecdf(ch, datadensity='density')
> 
> 
> label(ch) <- "Serum Cholesterol"
> Ecdf(ch)
> other.ch <- rnorm(500, 220, 20)
> Ecdf(other.ch,add=TRUE,lty=2)
> 
> 
> sex <- factor(sample(c('female','male'), 1000, TRUE))
> Ecdf(ch, q=c(.25,.5,.75))  # show quartiles
> Ecdf(ch, group=sex,
+      label.curves=list(method='arrow'))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> 
> 
> # Example showing how to draw multiple ECDFs from paired data
> pre.test <- rnorm(100,50,10)
> post.test <- rnorm(100,55,10)
> x <- c(pre.test, post.test)
> g <- c(rep('Pre',length(pre.test)),rep('Post',length(post.test)))
> Ecdf(x, group=g, xlab='Test Results', label.curves=list(keys=1:2))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> # keys=1:2 causes symbols to be drawn periodically on top of curves
> 
> 
> # Draw a matrix of ECDFs for a data frame
> m <- data.frame(pre.test, post.test, 
+                 sex=sample(c('male','female'),100,TRUE))
> Ecdf(m, group=m$sex, datadensity='rug')
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in par(oldmf) : argument 1 does not name a graphical parameter
> 
> 
> freqs <- sample(1:10, 1000, TRUE)
> Ecdf(ch, weights=freqs)  # weighted estimates
> 
> 
> # Trellis/Lattice examples:
> 
> 
> region <- factor(sample(c('Europe','USA','Australia'),100,TRUE))
> year <- factor(sample(2001:2002,1000,TRUE))
> Ecdf(~ch | region*year, groups=sex)
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> Key()           # draw a key for sex at the default location
> # Key(locator(1)) # user-specified positioning of key
> age <- rnorm(1000, 50, 10)
> Ecdf(~ch | lattice::equal.count(age), groups=sex)  # use overlapping shingles
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values
> Ecdf(~ch | sex, datadensity='hist', side=3)  # add spike histogram at top
> 
> 
> 
> cleanEx()
> nameEx("GiniMd")
> ### * GiniMd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GiniMd
> ### Title: Gini's Mean Difference
> ### Aliases: GiniMd
> ### Keywords: robust univar
> 
> ### ** Examples
> 
> set.seed(1)
> x <- rnorm(40)
> # Test GiniMd against a brute-force solution
> gmd <- function(x) {
+   n <- length(x)
+   sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)
+   }
> GiniMd(x)
[1] 0.9954155
> gmd(x)
[1] 0.9954155
> 
> z <- c(rep(0,17), rep(1,6))
> n <- length(z)
> GiniMd(z)
[1] 0.4031621
> 2*mean(z)*(1-mean(z))*n/(n-1)
[1] 0.4031621
> 
> a <- 12; b <- 13; c <- 7; n <- a + b + c
> A <- -.123; B <- -.707; C <- 0.523
> xx <- c(rep(A, a), rep(B, b), rep(C, c))
> GiniMd(xx)
[1] 0.518746
> 2*(a*b*abs(A-B) + a*c*abs(A-C) + b*c*abs(B-C))/n/(n-1)
[1] 0.518746
> 
> 
> 
> cleanEx()
> nameEx("Lag")
> ### * Lag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Lag
> ### Title: Lag a Numeric, Character, or Factor Vector
> ### Aliases: Lag
> ### Keywords: manip
> 
> ### ** Examples
> 
> Lag(1:5,2)
[1] NA NA  1  2  3
> Lag(letters[1:4],2)
[1] ""  ""  "a" "b"
> Lag(factor(letters[1:4]),-2)
[1] c    d    <NA> <NA>
Levels: a b c d
> # Find which observations are the first for a given subject
> id <- c('a','a','b','b','b','c')
> id != Lag(id)
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE
> !duplicated(id)
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE
> 
> 
> 
> cleanEx()
> nameEx("Merge")
> ### * Merge
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Merge
> ### Title: Merge Multiple Data Frames or Data Tables
> ### Aliases: Merge
> 
> ### ** Examples
> 
> ## Not run: 
> ##D a <- data.frame(sid=1:3, age=c(20,30,40))
> ##D b <- data.frame(sid=c(1,2,2), bp=c(120,130,140))
> ##D d <- data.frame(sid=c(1,3,4), wt=c(170,180,190))
> ##D all <- Merge(a, b, d, id = ~ sid)
> ##D # First file should be the master file and must
> ##D # contain all ids that ever occur.  ids not in the master will
> ##D # not be merged from other datasets.
> ##D a <- data.table(a); setkey(a, sid)
> ##D # data.table also does not allow duplicates without allow.cartesian=TRUE
> ##D b <- data.table(sid=1:2, bp=c(120,130)); setkey(b, sid)
> ##D d <- data.table(d); setkey(d, sid)
> ##D all <- Merge(a, b, d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("Misc")
> ### * Misc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Misc
> ### Title: Miscellaneous Functions
> ### Aliases: clowess confbar getLatestSource grType prType htmlSpecialType
> ###   inverseFunction james.stein keepHattrib km.quick latexBuild
> ###   lm.fit.qr.bare matxv makeSteps nomiss outerText plotlyParm plotp
> ###   rendHTML restoreHattrib sepUnitsTrans strgraphwrap tobase64image
> ###   trap.rule trellis.strip.blank unPaste whichClosest whichClosePW
> ###   whichClosek xless
> ### Keywords: programming utilities iplot
> 
> ### ** Examples
> 
> 
> 
> trap.rule(1:100,1:100)
[1] 4999.5
> 
> unPaste(c('a;b or c','ab;d','qr;s'), ';')
[[1]]
[1] "a"  "ab" "qr"

[[2]]
[1] "b or c" "d"      "s"     

> 
> sepUnitsTrans(c('3 days','4 months','2 years','7'))
[1]   3.00 121.75 730.50   7.00
attr(,"units")
[1] "day"
> 
> set.seed(1)
> whichClosest(1:100, 3:5)
[1] 3 4 5
> whichClosest(1:100, rep(3,20))
 [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
> 
> whichClosePW(1:100, rep(3,20))
 [1]  3  3  5  8  2  8  9  6  6  1  2  2  6  4  7  5  6 11  4  7
> whichClosePW(1:100, rep(3,20), f=.05)
 [1] 4 2 3 2 2 3 1 3 4 3 3 3 3 2 4 3 4 2 4 3
> whichClosePW(1:100, rep(3,20), f=1e-10)
 [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
> 
> x <- seq(-1, 1, by=.01)
> y <- x^2
> h <- inverseFunction(x,y)
> formals(h)$turns   # vertex
[1] 0.005
> a <- seq(0, 1, by=.01)
> plot(0, 0, type='n', xlim=c(-.5,1.5))
> lines(a, h(a)[,1])            ## first inverse
> lines(a, h(a)[,2], col='red') ## second inverse
> a <- c(-.1, 1.01, 1.1, 1.2)
> points(a, h(a)[,1])
> 
> d <- data.frame(x=1:2, y=3:4, z=5:6)
> d <- upData(d, labels=c(x='X', z='Z lab'), units=c(z='mm'))
Input object size:	 1008 bytes;	 3 variables	 2 observations
New object size:	2272 bytes;	3 variables	2 observations
> a <- keepHattrib(d)
> 
> d <- data.frame(x=1:2, y=3:4, z=5:6)
> d2 <- restoreHattrib(d, a)
> sapply(d2, attributes)
$x
$x$label
[1] "X"

$x$class
[1] "labelled" "integer" 


$y
NULL

$z
$z$label
[1] "Z lab"

$z$class
[1] "labelled" "integer" 

$z$units
[1] "mm"


> 
> ## Not run: 
> ##D getLatestSource(recent=5)  # source() most recent 5 revised files in Hmisc
> ##D getLatestSource('cut2')    # fetch and source latest cut2.s
> ##D getLatestSource('all')     # get everything
> ##D getLatestSource(avail=TRUE) # list available files and latest versions
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("R2Measures")
> ### * R2Measures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: R2Measures
> ### Title: R2Measures
> ### Aliases: R2Measures
> 
> ### ** Examples
> 
> x <- c(rep(0, 50), rep(1, 50))
> y <- x
> # f <- lrm(y ~ x)
> # f   # Nagelkerke R^2=1.0
> # lr <- f$stats['Model L.R.']
> # 1 - exp(- lr / 100)  # Maddala-Cox-Snell (MCS) 0.75
> lr <- 138.6267  # manually so don't need rms package
> 
> R2Measures(lr, 1, 100, c(50, 50))  # 0.84 Effective n=75
  R2(100) R2(1,100)    R2(75)  R2(1,75) 
0.7499932 0.7474805 0.8425041 0.8403901 
> R2Measures(lr, 1, 100, 50)         # 0.94
  R2(100) R2(1,100)    R2(50)  R2(1,50) 
0.7499932 0.7474805 0.9374966 0.9362339 
> # MCS requires unreasonable effective sample size = minimum outcome
> # frequency to get close to the 1.0 that Nagelkerke R^2 achieves
> 
> 
> 
> cleanEx()
> nameEx("Save")
> ### * Save
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Save
> ### Title: Faciliate Use of save and load to Remote Directories
> ### Aliases: Save Load
> ### Keywords: data file utilities
> 
> ### ** Examples
> 
> ## Not run: 
> ##D d <- data.frame(x=1:3, y=11:13)
> ##D options(LoadPath='../data/rda')
> ##D Save(d)   # creates ../data/rda/d.rda
> ##D Load(d)   # reads   ../data/rda/d.rda
> ##D Save(d, 'D')   # creates object D and saves it in .../D.rda
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("abs.error.pred")
> ### * abs.error.pred
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: abs.error.pred
> ### Title: Indexes of Absolute Prediction Error for Linear Models
> ### Aliases: abs.error.pred print.abs.error.pred
> ### Keywords: robust regression models
> 
> ### ** Examples
> 
> set.seed(1)         # so can regenerate results
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> y  <- exp(x1+x2+rnorm(100))
> f <- lm(log(y) ~ x1 + poly(x2,3), y=TRUE)
> abs.error.pred(lp=exp(fitted(f)), y=y)

Mean/Median |Differences|

                             Mean    Median
|Yi hat - median(Y hat)| 1.983447 0.8651185
|Yi hat - Yi|            2.184563 0.5436367
|Yi - median(Y)|         2.976277 1.0091661


Ratios of Mean/Median |Differences|

                                               Mean    Median
|Yi hat - median(Y hat)|/|Yi - median(Y)| 0.6664189 0.8572607
|Yi hat - Yi|/|Yi - median(Y)|            0.7339920 0.5386989
> rm(x1,x2,y,f)
> 
> 
> 
> cleanEx()
> nameEx("addMarginal")
> ### * addMarginal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: addMarginal
> ### Title: Add Marginal Observations
> ### Aliases: addMarginal
> ### Keywords: utilities manip
> 
> ### ** Examples
> 
> d <- expand.grid(sex=c('female', 'male'), country=c('US', 'Romania'),
+                  reps=1:2)
> addMarginal(d, sex, country)
      sex country reps  .marginal.
1  female      US    1            
2    male      US    1            
3  female Romania    1            
4    male Romania    1            
5  female      US    2            
6    male      US    2            
7  female Romania    2            
8    male Romania    2            
9     All      US    1         sex
10    All      US    1         sex
11    All Romania    1         sex
12    All Romania    1         sex
13    All      US    2         sex
14    All      US    2         sex
15    All Romania    2         sex
16    All Romania    2         sex
17 female     All    1     country
18   male     All    1     country
19 female     All    1     country
20   male     All    1     country
21 female     All    2     country
22   male     All    2     country
23 female     All    2     country
24   male     All    2     country
25    All     All    1 sex,country
26    All     All    1 sex,country
27    All     All    1 sex,country
28    All     All    1 sex,country
29    All     All    2 sex,country
30    All     All    2 sex,country
31    All     All    2 sex,country
32    All     All    2 sex,country
> 
> # Example of nested variables
> d <- data.frame(state=c('AL', 'AL', 'GA', 'GA', 'GA'),
+                 city=c('Mobile', 'Montgomery', 'Valdosto',
+                        'Augusta', 'Atlanta'),
+                 x=1:5, stringsAsFactors=TRUE)
> addMarginal(d, state, nested=city) # cite set to 'All' when state is
   state       city x .marginal.
1     AL     Mobile 1           
2     AL Montgomery 2           
3     GA   Valdosto 3           
4     GA    Augusta 4           
5     GA    Atlanta 5           
6    All        All 1      state
7    All        All 2      state
8    All        All 3      state
9    All        All 4      state
10   All        All 5      state
> 
> 
> 
> cleanEx()
> nameEx("all.is.numeric")
> ### * all.is.numeric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: all.is.numeric
> ### Title: Check if All Elements in Character Vector are Numeric
> ### Aliases: all.is.numeric
> ### Keywords: character
> 
> ### ** Examples
> 
> all.is.numeric(c('1','1.2','3'))
[1] TRUE
> all.is.numeric(c('1','1.2','3a'))
[1] FALSE
> all.is.numeric(c('1','1.2','3'),'vector')
[1] 1.0 1.2 3.0
> all.is.numeric(c('1','1.2','3a'),'vector')
[1] "1"   "1.2" "3a" 
> all.is.numeric(c('1','',' .'),'vector')
[1]  1 NA NA
> all.is.numeric(c('1', '1.2', '3a'), 'nonnum')
[1] "3a"
> 
> 
> 
> cleanEx()
> nameEx("approxExtrap")
> ### * approxExtrap
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: approxExtrap
> ### Title: Linear Extrapolation
> ### Aliases: approxExtrap
> ### Keywords: arith dplot
> 
> ### ** Examples
> 
> approxExtrap(1:3,1:3,xout=c(0,4))
$x
[1] 0 4

$y
[1] 0 4

> 
> 
> 
> cleanEx()
> nameEx("areg")
> ### * areg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: areg
> ### Title: Additive Regression with Optimal Transformations on Both Sides
> ###   using Canonical Variates
> ### Aliases: areg print.areg predict.areg plot.areg
> ### Keywords: smooth regression multivariate models
> 
> ### ** Examples
> 
> set.seed(1)
> 
> ns <- c(30,300,3000)
> for(n in ns) {
+   y <- sample(1:5, n, TRUE)
+   x <- abs(y-3) + runif(n)
+   par(mfrow=c(3,4))
+   for(k in c(0,3:5)) {
+     z <- areg(x, y, ytype='c', nk=k)
+     plot(x, z$tx)
+ 	title(paste('R2=',format(z$rsquared)))
+     tapply(z$ty, y, range)
+     a <- tapply(x,y,mean)
+     b <- tapply(z$ty,y,mean)
+     plot(a,b)
+ 	abline(lsfit(a,b))
+     # Should get same result to within linear transformation if reverse x and y
+     w <- areg(y, x, xtype='c', nk=k)
+     plot(z$ty, w$tx)
+     title(paste('R2=',format(w$rsquared)))
+     abline(lsfit(z$ty, w$tx))
+  }
+ }
> 
> par(mfrow=c(2,2))
> # Example where one category in y differs from others but only in variance of x
> n <- 50
> y <- sample(1:5,n,TRUE)
> x <- rnorm(n)
> x[y==1] <- rnorm(sum(y==1), 0, 5)
> z <- areg(x,y,xtype='l',ytype='c')
> z

N: 50 	 0  observations with NAs deleted.
R^2: 0.155	nk: 4	Mean and Median |error|: 2.2, 2


  type d.f.
x    l    1

y type: c 	d.f.: 4 

> plot(z)
> z <- areg(x,y,ytype='c')
> z

N: 50 	 0  observations with NAs deleted.
R^2: 0.756	nk: 4	Mean and Median |error|: 2.2, 2


  type d.f.
x    s    3

y type: c 	d.f.: 4 

> plot(z)
> 
> ## Not run: 
> ##D 		
> ##D # Examine overfitting when true transformations are linear
> ##D par(mfrow=c(4,3))
> ##D for(n in c(200,2000)) {
> ##D   x <- rnorm(n); y <- rnorm(n) + x
> ##D     for(nk in c(0,3,5)) {
> ##D     z <- areg(x, y, nk=nk, crossval=10, B=100)
> ##D     print(z)
> ##D     plot(z)
> ##D     title(paste('n=',n))
> ##D   }
> ##D }
> ##D par(mfrow=c(1,1))
> ##D 
> ##D # Underfitting when true transformation is quadratic but overfitting
> ##D # when y is allowed to be transformed
> ##D set.seed(49)
> ##D n <- 200
> ##D x <- rnorm(n); y <- rnorm(n) + .5*x^2
> ##D #areg(x, y, nk=0, crossval=10, B=100)
> ##D #areg(x, y, nk=4, ytype='l', crossval=10, B=100)
> ##D z <- areg(x, y, nk=4) #, crossval=10, B=100)
> ##D z
> ##D # Plot x vs. predicted value on original scale.  Since y-transform is
> ##D # not monotonic, there are multiple y-inverses
> ##D xx <- seq(-3.5,3.5,length=1000)
> ##D yhat <- predict(z, xx, type='fitted')
> ##D plot(x, y, xlim=c(-3.5,3.5))
> ##D for(j in 1:ncol(yhat)) lines(xx, yhat[,j], col=j)
> ##D # Plot a random sample of possible y inverses
> ##D yhats <- predict(z, xx, type='fitted', what='sample')
> ##D points(xx, yhats, pch=2)
> ## End(Not run)
> 
> # True transformation of x1 is quadratic, y is linear
> n <- 200
> x1 <- rnorm(n); x2 <- rnorm(n); y <- rnorm(n) + x1^2
> z <- areg(cbind(x1,x2),y,xtype=c('s','l'),nk=3)
> par(mfrow=c(2,2))
> plot(z)
> 
> # y transformation is inverse quadratic but areg gets the same answer by
> # making x1 quadratic
> n <- 5000
> x1 <- rnorm(n); x2 <- rnorm(n); y <- (x1 + rnorm(n))^2
> z <- areg(cbind(x1,x2),y,nk=5)
> par(mfrow=c(2,2))
> plot(z)
> 
> # Overfit 20 predictors when no true relationships exist
> n <- 1000
> x <- matrix(runif(n*20),n,20)
> y <- rnorm(n)
> z <- areg(x, y, nk=5)  # add crossval=4 to expose the problem
> 
> # Test predict function
> n <- 50
> x <- rnorm(n)
> y <- rnorm(n) + x
> g <- sample(1:3, n, TRUE)
> z <- areg(cbind(x,g),y,xtype=c('s','c'))
> range(predict(z, cbind(x,g)) - z$linear.predictors)
[1] 0 0
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("aregImpute")
> ### * aregImpute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: aregImpute
> ### Title: Multiple Imputation using Additive Regression, Bootstrapping,
> ###   and Predictive Mean Matching
> ### Aliases: aregImpute print.aregImpute plot.aregImpute reformM
> ### Keywords: smooth regression multivariate methods models
> 
> ### ** Examples
> 
> # Check that aregImpute can almost exactly estimate missing values when
> # there is a perfect nonlinear relationship between two variables
> # Fit restricted cubic splines with 4 knots for x1 and x2, linear for x3
> set.seed(3)
> x1 <- rnorm(200)
> x2 <- x1^2
> x3 <- runif(200)
> m <- 30
> x2[1:m] <- NA
> a <- aregImpute(~x1+x2+I(x3), n.impute=5, nk=4, match='closest')
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 
> a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~x1 + x2 + I(x3), n.impute = 5, nk = 4, 
    match = "closest")

n: 200 	p: 3 	Imputations: 5  	nk: 4 

Number of NAs:
x1 x2 x3 
 0 30  0 

   type d.f.
x1    s    3
x2    s    1
x3    l    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x2 
0.984 
> matplot(x1[1:m]^2, a$imputed$x2)
> abline(a=0, b=1, lty=2)
> 
> x1[1:m]^2
 [1] 0.925315897 0.085571299 0.066971341 1.327407883 0.038330915 0.000907452
 [7] 0.007296189 1.246818367 1.485613400 1.606223478 0.554699626 1.279655455
[13] 0.513169486 0.063833220 0.023117897 0.094652479 0.908242033 0.420218743
[19] 1.498943851 0.039924679 0.334643416 0.887930672 0.041505171 2.777138392
[25] 0.234696753 0.549188688 1.347028987 1.024279865 0.005195306 1.292273993
> a$imputed$x2
          [,1]        [,2]         [,3]         [,4]       [,5]
1  0.958996702 0.930577069 9.762484e-01 9.799896e-01 0.93057707
2  0.149052972 0.146963621 1.661947e-01 1.353046e-01 0.12388046
3  0.056748811 0.056748811 7.148054e-02 2.631055e-02 0.12140884
4  1.278855629 1.262597928 1.379068e+00 1.379068e+00 1.26259793
5  0.035671951 0.035671951 4.655437e-02 3.567195e-02 0.04379575
6  0.004982439 0.035671951 1.463217e-03 4.099071e-05 0.05595187
7  0.035671951 0.034669484 4.099071e-05 4.099071e-05 0.04379575
8  1.262597928 1.262597928 1.262598e+00 1.262598e+00 1.18470752
9  1.379068228 1.379068228 1.499170e+00 1.591427e+00 1.37906823
10 1.661538043 1.667583069 1.661538e+00 1.667583e+00 1.48316856
11 0.618593060 0.618593060 6.164482e-01 6.164482e-01 0.51992134
12 1.262597928 1.184707523 1.355782e+00 1.354657e+00 1.18470752
13 0.594890076 0.594890076 5.948901e-01 5.948901e-01 0.55889449
14 0.026310553 0.046554372 3.883560e-02 5.108231e-02 0.10438988
15 0.020493742 0.002514871 3.466948e-02 2.514871e-03 0.06568317
16 0.166194673 0.168263518 1.682635e-01 1.469636e-01 0.14905297
17 0.930577069 0.930577069 9.589967e-01 9.698616e-01 0.91224047
18 0.497752140 0.497752140 4.659193e-01 4.505450e-01 0.45543010
19 1.479656909 1.591426720 1.479657e+00 1.543022e+00 1.37906823
20 0.042739772 0.034669484 4.273977e-02 4.982439e-03 0.10438988
21 0.422393723 0.422393723 4.223937e-01 4.223937e-01 0.38112108
22 0.930577069 0.912240467 9.589967e-01 9.589967e-01 0.86794372
23 0.104389875 0.078255416 4.379575e-02 7.133446e-02 0.10438988
24 2.562633045 2.228492325 2.921218e+00 2.973363e+00 2.66392498
25 0.250603132 0.250603132 3.289231e-01 3.289231e-01 0.26743794
26 0.618593060 0.618593060 6.300569e-01 6.300569e-01 0.51992134
27 1.337073813 1.354657372 1.337074e+00 1.355782e+00 1.27885563
28 0.980088775 1.035227246 9.799896e-01 9.799896e-01 0.97998959
29 0.034669484 0.034669484 4.273977e-02 1.634461e-02 0.02460284
30 1.262597928 1.262597928 1.355782e+00 1.354657e+00 1.26259793
> 
> 
> # Multiple imputation and estimation of variances and covariances of
> # regression coefficient estimates accounting for imputation
> # Example 1: large sample size, much missing data, no overlap in
> # NAs across variables
> x1 <- factor(sample(c('a','b','c'),1000,TRUE))
> x2 <- (x1=='b') + 3*(x1=='c') + rnorm(1000,0,2)
> x3 <- rnorm(1000)
> y  <- x2 + 1*(x1=='c') + .2*x3 + rnorm(1000,0,2)
> orig.x1 <- x1[1:250]
> orig.x2 <- x2[251:350]
> x1[1:250] <- NA
> x2[251:350] <- NA
> d <- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
> # Find value of nk that yields best validating imputation models
> # tlinear=FALSE means to not force the target variable to be linear
> f <- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), tlinear=FALSE,
+                 data=d, B=10) # normally B=75
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> f

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~y + x1 + x2 + x3, data = d, nk = c(0, 3:5), 
    tlinear = FALSE, B = 10)

n: 1000 	p: 4 	Imputations: 5  	nk: 0 

Number of NAs:
  y  x1  x2  x3 
  0 250 100   0 

   type d.f.
y     s    1
x1    c    2
x2    s    1
x3    s    1

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x1    x2 
0.331 0.611 

Resampling results for determining the complexity of imputation models

Variable being imputed: x1 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.327 0.332 0.351 0.346
10-fold cross-validated  R^2            0.340 0.351 0.351 0.342
Bootstrap bias-corrected mean   |error| 1.072 1.069 1.062 1.067
10-fold cross-validated  mean   |error| 0.489 0.486 0.494 0.498
Bootstrap bias-corrected median |error| 1.000 1.000 1.000 1.000
10-fold cross-validated  median |error| 0.000 0.200 0.000 0.200

Variable being imputed: x2 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.652 0.638 0.635 0.647
10-fold cross-validated  R^2            0.635 0.643 0.635 0.634
Bootstrap bias-corrected mean   |error| 1.140 1.196 1.200 1.193
10-fold cross-validated  mean   |error| 1.712 1.203 1.202 1.202
Bootstrap bias-corrected median |error| 0.984 0.962 0.997 0.985
10-fold cross-validated  median |error| 1.436 0.980 1.007 0.995


> # Try forcing target variable (x1, then x2) to be linear while allowing
> # predictors to be nonlinear (could also say tlinear=TRUE)
> f <- aregImpute(~y + x1 + x2 + x3, nk=c(0,3:5), data=d, B=10)
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> f

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~y + x1 + x2 + x3, data = d, nk = c(0, 3:5), 
    B = 10)

n: 1000 	p: 4 	Imputations: 5  	nk: 0 

Number of NAs:
  y  x1  x2  x3 
  0 250 100   0 

   type d.f.
y     s    1
x1    c    2
x2    s    1
x3    s    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x1    x2 
0.358 0.621 

Resampling results for determining the complexity of imputation models

Variable being imputed: x1 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.334 0.336 0.341 0.342
10-fold cross-validated  R^2            0.329 0.334 0.337 0.336
Bootstrap bias-corrected mean   |error| 1.058 1.061 1.052 1.052
10-fold cross-validated  mean   |error| 0.492 0.486 0.485 0.488
Bootstrap bias-corrected median |error| 1.000 1.000 1.000 1.000
10-fold cross-validated  median |error| 0.100 0.300 0.100 0.100

Variable being imputed: x2 
                                         nk=0  nk=3  nk=4  nk=5
Bootstrap bias-corrected R^2            0.633 0.631 0.635 0.618
10-fold cross-validated  R^2            0.629 0.629 0.626 0.627
Bootstrap bias-corrected mean   |error| 1.165 1.168 1.174 1.199
10-fold cross-validated  mean   |error| 1.736 1.731 1.742 1.737
Bootstrap bias-corrected median |error| 1.005 1.007 1.010 1.020
10-fold cross-validated  median |error| 1.450 1.445 1.454 1.464


> 
> ## Not run: 
> ##D # Use 100 imputations to better check against individual true values
> ##D f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)
> ##D f
> ##D par(mfrow=c(2,1))
> ##D plot(f)
> ##D modecat <- function(u) {
> ##D  tab <- table(u)
> ##D  as.numeric(names(tab)[tab==max(tab)][1])
> ##D }
> ##D table(orig.x1,apply(f$imputed$x1, 1, modecat))
> ##D par(mfrow=c(1,1))
> ##D plot(orig.x2, apply(f$imputed$x2, 1, mean))
> ##D fmi <- fit.mult.impute(y ~ x1 + x2 + x3, lm, f, 
> ##D                        data=d)
> ##D sqrt(diag(vcov(fmi)))
> ##D fcc <- lm(y ~ x1 + x2 + x3)
> ##D summary(fcc)   # SEs are larger than from mult. imputation
> ## End(Not run)
> ## Not run: 
> ##D # Example 2: Very discriminating imputation models,
> ##D # x1 and x2 have some NAs on the same rows, smaller n
> ##D set.seed(5)
> ##D x1 <- factor(sample(c('a','b','c'),100,TRUE))
> ##D x2 <- (x1=='b') + 3*(x1=='c') + rnorm(100,0,.4)
> ##D x3 <- rnorm(100)
> ##D y  <- x2 + 1*(x1=='c') + .2*x3 + rnorm(100,0,.4)
> ##D orig.x1 <- x1[1:20]
> ##D orig.x2 <- x2[18:23]
> ##D x1[1:20] <- NA
> ##D x2[18:23] <- NA
> ##D #x2[21:25] <- NA
> ##D d <- data.frame(x1,x2,x3,y, stringsAsFactors=TRUE)
> ##D n <- naclus(d)
> ##D plot(n); naplot(n)  # Show patterns of NAs
> ##D # 100 imputations to study them; normally use 5 or 10
> ##D f  <- aregImpute(~y + x1 + x2 + x3, n.impute=100, nk=0, data=d)
> ##D par(mfrow=c(2,3))
> ##D plot(f, diagnostics=TRUE, maxn=2)
> ##D # Note: diagnostics=TRUE makes graphs similar to those made by:
> ##D # r <- range(f$imputed$x2, orig.x2)
> ##D # for(i in 1:6) {  # use 1:2 to mimic maxn=2
> ##D #   plot(1:100, f$imputed$x2[i,], ylim=r,
> ##D #        ylab=paste("Imputations for Obs.",i))
> ##D #   abline(h=orig.x2[i],lty=2)
> ##D # }
> ##D 
> ##D table(orig.x1,apply(f$imputed$x1, 1, modecat))
> ##D par(mfrow=c(1,1))
> ##D plot(orig.x2, apply(f$imputed$x2, 1, mean))
> ##D 
> ##D 
> ##D fmi <- fit.mult.impute(y ~ x1 + x2, lm, f, 
> ##D                        data=d)
> ##D sqrt(diag(vcov(fmi)))
> ##D fcc <- lm(y ~ x1 + x2)
> ##D summary(fcc)   # SEs are larger than from mult. imputation
> ## End(Not run)
> 
> ## Not run: 
> ##D # Study relationship between smoothing parameter for weighting function
> ##D # (multiplier of mean absolute distance of transformed predicted
> ##D # values, used in tricube weighting function) and standard deviation
> ##D # of multiple imputations.  SDs are computed from average variances
> ##D # across subjects.  match="closest" same as match="weighted" with
> ##D # small value of fweighted.
> ##D # This example also shows problems with predicted mean
> ##D # matching almost always giving the same imputed values when there is
> ##D # only one predictor (regression coefficients change over multiple
> ##D # imputations but predicted values are virtually 1-1 functions of each
> ##D # other)
> ##D 
> ##D set.seed(23)
> ##D x <- runif(200)
> ##D y <- x + runif(200, -.05, .05)
> ##D r <- resid(lsfit(x,y))
> ##D rmse <- sqrt(sum(r^2)/(200-2))   # sqrt of residual MSE
> ##D 
> ##D y[1:20] <- NA
> ##D d <- data.frame(x,y)
> ##D f <- aregImpute(~ x + y, n.impute=10, match='closest', data=d)
> ##D # As an aside here is how to create a completed dataset for imputation
> ##D # number 3 as fit.mult.impute would do automatically.  In this degenerate
> ##D # case changing 3 to 1-2,4-10 will not alter the results.
> ##D imputed <- impute.transcan(f, imputation=3, data=d, list.out=TRUE,
> ##D                            pr=FALSE, check=FALSE)
> ##D sd <- sqrt(mean(apply(f$imputed$y, 1, var)))
> ##D 
> ##D ss <- c(0, .01, .02, seq(.05, 1, length=20))
> ##D sds <- ss; sds[1] <- sd
> ##D 
> ##D for(i in 2:length(ss)) {
> ##D   f <- aregImpute(~ x + y, n.impute=10, fweighted=ss[i])
> ##D   sds[i] <- sqrt(mean(apply(f$imputed$y, 1, var)))
> ##D }
> ##D 
> ##D plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
> ##D      type='b')
> ##D abline(v=.2,  lty=2)  # default value of fweighted
> ##D abline(h=rmse, lty=2)  # root MSE of residuals from linear regression
> ## End(Not run)
> 
> ## Not run: 
> ##D # Do a similar experiment for the Titanic dataset
> ##D getHdata(titanic3)
> ##D h <- lm(age ~ sex + pclass + survived, data=titanic3)
> ##D rmse <- summary(h)$sigma
> ##D set.seed(21)
> ##D f <- aregImpute(~ age + sex + pclass + survived, n.impute=10,
> ##D                 data=titanic3, match='closest')
> ##D sd <- sqrt(mean(apply(f$imputed$age, 1, var)))
> ##D 
> ##D ss <- c(0, .01, .02, seq(.05, 1, length=20))
> ##D sds <- ss; sds[1] <- sd
> ##D 
> ##D for(i in 2:length(ss)) {
> ##D   f <- aregImpute(~ age + sex + pclass + survived, data=titanic3,
> ##D                   n.impute=10, fweighted=ss[i])
> ##D   sds[i] <- sqrt(mean(apply(f$imputed$age, 1, var)))
> ##D }
> ##D 
> ##D plot(ss, sds, xlab='Smoothing Parameter', ylab='SD of Imputed Values',
> ##D      type='b')
> ##D abline(v=.2,   lty=2)  # default value of fweighted
> ##D abline(h=rmse, lty=2)  # root MSE of residuals from linear regression
> ## End(Not run)
> 
> 
> set.seed(2)
> d <- data.frame(x1=runif(50), x2=c(rep(NA, 10), runif(40)),
+                 x3=c(runif(4), rep(NA, 11), runif(35)))
> reformM(~ x1 + x2 + x3, data=d)
Recommended number of imputations: 30 
~x3 + x2 + x1
<environment: 0x559654c9e748>
> reformM(~ x1 + x2 + x3, data=d, nperm=2)
Recommended number of imputations: 30 
[[1]]
~x1 + x2 + x3
<environment: 0x559654cd1aa8>

[[2]]
~x1 + x3 + x2
<environment: 0x559654cd1aa8>

> # Give result or one of the results as the first argument to aregImpute
> 
> # Constrain imputed values for two variables
> # Require imputed values for x2 to be above 0.2
> # Assume x1 is never missing and require imputed values for
> # x3 to be less than the recipient's value of x1
> a <- aregImpute(~ x1 + x2 + x3, data=d,
+                 constraint=list(x2 = expression(d$x2 > 0.2),
+                                 x3 = expression(d$x3 < r$x1)))
Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 
> a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~x1 + x2 + x3, data = d, constraint = list(x2 = expression(d$x2 > 
    0.2), x3 = expression(d$x3 < r$x1)))

n: 50 	p: 3 	Imputations: 5  	nk: 3 

Number of NAs:
x1 x2 x3 
 0 10 11 

   type d.f.
x1    s    2
x2    s    2
x3    s    1

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
   x2    x3 
0.128 0.100 

Frequency distributions of number of potential donor observations
meeting constraints

x2 

32 
10 

x3 

 6  7  9 16 21 29 32 35 
 1  1  1  1  3  1  1  2 

> 
> 
> 
> cleanEx()
> nameEx("biVar")
> ### * biVar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biVar
> ### Title: Bivariate Summaries Computed Separately by a Series of
> ###   Predictors
> ### Aliases: biVar print.biVar plot.biVar spearman2 spearman2.default
> ###   spearman2.formula spearman spearman.test chiSquare
> ### Keywords: nonparametric htest category
> 
> ### ** Examples
> 
> x <- c(-2, -1, 0, 1, 2)
> y <- c(4,   1, 0, 1, 4)
> z <- c(1,   2, 3, 4, NA)
> v <- c(1,   2, 3, 4, 5)
> 
> spearman2(x, y)
         rho2             F           df1           df2             P 
    0.0000000     0.0000000     1.0000000     3.0000000     1.0000000 
            n Adjusted rho2 
    5.0000000    -0.3333333 
> plot(spearman2(z ~ x + y + v, p=2))
> 
> f <- chiSquare(z ~ x + y + v)
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(x, y) :
  Chi-squared approximation may be incorrect
> f

Pearson Chi-square Tests    Response variable:z

  chisquare df chisquare-df      P n
x         8  6            2 0.2381 4
y         8  6            2 0.2381 4
v         8  6            2 0.2381 4
> 
> 
> 
> cleanEx()
> nameEx("binconf")
> ### * binconf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: binconf
> ### Title: Confidence Intervals for Binomial Probabilities
> ### Aliases: binconf
> ### Keywords: category htest
> 
> ### ** Examples
> 
> binconf(0:10,10,include.x=TRUE,include.n=TRUE)
  X  N PointEst       Lower     Upper
  0 10      0.0 0.000000000 0.2775328
  1 10      0.1 0.005129329 0.4041500
  2 10      0.2 0.056682151 0.5098375
  3 10      0.3 0.107791267 0.6032219
  4 10      0.4 0.168180330 0.6873262
  5 10      0.5 0.236593091 0.7634069
  6 10      0.6 0.312673770 0.8318197
  7 10      0.7 0.396778147 0.8922087
  8 10      0.8 0.490162472 0.9433178
  9 10      0.9 0.595849973 0.9948707
 10 10      1.0 0.722467200 1.0000000
> binconf(46,50,method="all")
           PointEst     Lower     Upper
Exact          0.92 0.8076572 0.9777720
Wilson         0.92 0.8116175 0.9684505
Asymptotic     0.92 0.8448027 0.9951973
> 
> 
> 
> cleanEx()
> nameEx("bootkm")
> ### * bootkm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootkm
> ### Title: Bootstrap Kaplan-Meier Estimates
> ### Aliases: bootkm
> ### Keywords: survival nonparametric
> 
> ### ** Examples
> 
> # Compute 0.95 nonparametric confidence interval for the difference in
> # median survival time between females and males (two-sample problem)
> set.seed(1)
> library(survival)
> S <- Surv(runif(200))      # no censoring
> sex <- c(rep('female',100),rep('male',100))
> med.female <- bootkm(S[sex=='female',], B=100) # normally B=500
10 20 30 40 50 60 70 80 90 100 
> med.male   <- bootkm(S[sex=='male',],   B=100)
10 20 30 40 50 60 70 80 90 100 
> describe(med.female-med.male)
med.female - med.male 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
     100        0       87        1 -0.01575 -0.01962  0.08495 -0.12179 
     .10      .25      .50      .75      .90      .95 
-0.11216 -0.08030 -0.01734  0.01819  0.09412  0.15126 

lowest : -0.139027 -0.136873 -0.136775 -0.122804 -0.121741
highest: 0.151802  0.152362  0.154363  0.157939  0.160357 
> quantile(med.female-med.male, c(.025,.975), na.rm=TRUE)
      2.5%      97.5% 
-0.1301387  0.1534129 
> # na.rm needed because some bootstrap estimates of median survival
> # time may be missing when a bootstrap sample did not include the
> # longer survival times
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("bpower")
> ### * bpower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bpower
> ### Title: Power and Sample Size for Two-Sample Binomial Test
> ### Aliases: bpower bsamsize ballocation bpower.sim
> ### Keywords: htest category
> 
> ### ** Examples
> 
> bpower(.1, odds.ratio=.9, n=1000, alpha=c(.01,.05))
    Power1     Power2 
0.01953539 0.07780432 
> bpower.sim(.1, odds.ratio=.9, n=1000)
     Power      Lower      Upper 
0.07920000 0.07390701 0.08449299 
> bsamsize(.1, .05, power=.95)
      n1       n2 
718.2381 718.2381 
> ballocation(.1, .5, n=100)
   fraction.group1.min.var.diff   fraction.group1.min.var.ratio 
                      0.3750000                       0.7500000 
fraction.group1.min.var.logodds       fraction.group1.max.power 
                      0.6250000                       0.4745255 
> 
> 
> # Plot power vs. n for various odds ratios  (base prob.=.1)
> n  <- seq(10, 1000, by=10)
> OR <- seq(.2,.9,by=.1)
> plot(0, 0, xlim=range(n), ylim=c(0,1), xlab="n", ylab="Power", type="n")
> for(or in OR) {
+   lines(n, bpower(.1, odds.ratio=or, n=n))
+   text(350, bpower(.1, odds.ratio=or, n=350)-.02, format(or))
+ }
> 
> 
> # Another way to plot the same curves, but letting labcurve do the
> # work, including labeling each curve at points of maximum separation
> pow <- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
+               n=n)
> names(pow) <- format(OR)
> labcurve(pow, pl=TRUE, xlab='n', ylab='Power')
> 
> 
> # Contour graph for various probabilities of outcome in the control
> # group, fixing the odds ratio at .8 ([p2/(1-p2) / p1/(1-p1)] = .8)
> # n is varied also
> p1 <- seq(.01,.99,by=.01)
> n  <- seq(100,5000,by=250)
> pow <- outer(p1, n, function(p1,n) bpower(p1, n=n, odds.ratio=.8))
> # This forms a length(p1)*length(n) matrix of power estimates
> contour(p1, n, pow)
> 
> 
> 
> cleanEx()
> nameEx("bpplot")
> ### * bpplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bpplot
> ### Title: Box-percentile plots
> ### Aliases: bpplot
> ### Keywords: nonparametric hplot
> 
> ### ** Examples
> 
> set.seed(1)
> x1 <- rnorm(500)
> x2 <- runif(500, -2, 2)
> x3 <- abs(rnorm(500))-2
> bpplot(x1, x2, x3)
> g <- sample(1:2, 500, replace=TRUE)
> bpplot(split(x2, g), name=c('Group 1','Group 2'))
> rm(x1,x2,x3,g)
> 
> 
> 
> cleanEx()
> nameEx("bystats")
> ### * bystats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bystats
> ### Title: Statistics by Categories
> ### Aliases: bystats print.bystats latex.bystats bystats2 print.bystats2
> ###   latex.bystats2
> ### Keywords: category
> 
> ### ** Examples
> 
> ## Not run: 
> ##D bystats(sex==2, county, city)
> ##D bystats(death, race)
> ##D bystats(death, cut2(age,g=5), race)
> ##D bystats(cholesterol, cut2(age,g=4), sex, fun=median)
> ##D bystats(cholesterol, sex, fun=quantile)
> ##D bystats(cholesterol, sex, fun=function(x)c(Mean=mean(x),Median=median(x)))
> ##D latex(bystats(death,race,nmiss=FALSE,subset=sex=="female"), digits=2)
> ##D f <- function(y) c(Hazard=sum(y[,2])/sum(y[,1]))
> ##D # f() gets the hazard estimate for right-censored data from exponential dist.
> ##D bystats(cbind(d.time, death), race, sex, fun=f)
> ##D bystats(cbind(pressure, cholesterol), age.decile, 
> ##D         fun=function(y) c(Median.pressure   =median(y[,1]),
> ##D                           Median.cholesterol=median(y[,2])))
> ##D y <- cbind(pressure, cholesterol)
> ##D bystats(y, age.decile, 
> ##D         fun=function(y) apply(y, 2, median))   # same result as last one
> ##D bystats(y, age.decile, fun=function(y) apply(y, 2, quantile, c(.25,.75)))
> ##D # The last one computes separately the 0.25 and 0.75 quantiles of 2 vars.
> ##D latex(bystats2(death, race, sex, fun=table))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("capitalize")
> ### * capitalize
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: capitalize
> ### Title: capitalize the first letter of a string
> ### Aliases: capitalize
> ### Keywords: manip character
> 
> ### ** Examples
> 
> capitalize(c("Hello", "bob", "daN"))
[1] "Hello" "Bob"   "DaN"  
> 
> 
> 
> cleanEx()
> nameEx("ciapower")
> ### * ciapower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ciapower
> ### Title: Power of Interaction Test for Exponential Survival
> ### Aliases: ciapower
> ### Keywords: survival htest
> 
> ### ** Examples
> 
> # Find the power of a race x treatment test.  25% of patients will
> # be non-white and the total sample size is 14000.  
> # Accrual is for 1.5 years and minimum follow-up is 5y.
> # Reduction in 5-year mortality is 15% for whites, 0% or -5% for
> # non-whites.  5-year mortality for control subjects if assumed to
> # be 0.18 for whites, 0.23 for non-whites.
> n <- 14000
> for(nonwhite.reduction in c(0,-5)) {
+   cat("\n\n\n% Reduction in 5-year mortality for non-whites:",
+       nonwhite.reduction, "\n\n")
+   pow <- ciapower(5,  .75*n, .25*n,  .18, .23,  15, nonwhite.reduction,  
+                   1.5, 5)
+   cat("\n\nPower:",format(pow),"\n")
+ }



% Reduction in 5-year mortality for non-whites: 0 


Accrual duration: 1.5 y  Minimum follow-up: 5 y

Sample size Stratum 1: 10500   Stratum 2: 3500 

Alpha= 0.05 

5-year Mortalities
          Control Intervention
Stratum 1    0.18        0.153
Stratum 2    0.23        0.230

Hazard Rates
             Control Intervention
Stratum 1 0.03969019   0.03321092
Stratum 2 0.05227295   0.05227295

Probabilities of an Event During Study
            Control Intervention
Stratum 1 0.2039322    0.1737512
Stratum 2 0.2594139    0.2594139

Expected Number of Events
          Control Intervention
Stratum 1  1070.6        912.2
Stratum 2   454.0        454.0

Ratio of hazard ratios: 0.8367538 
Standard deviation of log ratio of ratios: 0.08022351 


Power: 0.6032173 



% Reduction in 5-year mortality for non-whites: -5 


Accrual duration: 1.5 y  Minimum follow-up: 5 y

Sample size Stratum 1: 10500   Stratum 2: 3500 

Alpha= 0.05 

5-year Mortalities
          Control Intervention
Stratum 1    0.18       0.1530
Stratum 2    0.23       0.2415

Hazard Rates
             Control Intervention
Stratum 1 0.03969019   0.03321092
Stratum 2 0.05227295   0.05528250

Probabilities of an Event During Study
            Control Intervention
Stratum 1 0.2039322    0.1737512
Stratum 2 0.2594139    0.2720973

Expected Number of Events
          Control Intervention
Stratum 1  1070.6        912.2
Stratum 2   454.0        476.2

Ratio of hazard ratios: 0.7912015 
Standard deviation of log ratio of ratios: 0.07958098 


Power: 0.8371925 
> 
> 
> 
> cleanEx()
> nameEx("cnvrt.coords")
> ### * cnvrt.coords
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cnvrt.coords
> ### Title: Convert between the 5 different coordinate sytems on a graphical
> ###   device
> ### Aliases: cnvrt.coords
> ### Keywords: dplot aplot
> 
> ### ** Examples
> 
> 
> old.par <- par(no.readonly=TRUE)
> 
> par(mfrow=c(2,2),xpd=NA)
> 
> # generate some sample data
> tmp.x <- rnorm(25, 10, 2)
> tmp.y <- rnorm(25, 50, 10)
> tmp.z <- rnorm(25, 0, 1)
> 
> plot( tmp.x, tmp.y)
> 
> # draw a diagonal line across the plot area
> tmp1 <- cnvrt.coords( c(0,1), c(0,1), input='plt' )
> lines(tmp1$usr, col='blue')
> 
> # draw a diagonal line accross figure region
> tmp2 <- cnvrt.coords( c(0,1), c(1,0), input='fig')
> lines(tmp2$usr, col='red')
> 
> # save coordinate of point 1 and y value near top of plot for future plots
> tmp.point1 <- cnvrt.coords(tmp.x[1], tmp.y[1])
> tmp.range1 <- cnvrt.coords(NA, 0.98, input='plt')
> 
> # make a second plot and draw a line linking point 1 in each plot
> plot(tmp.y, tmp.z)
> 
> tmp.point2 <- cnvrt.coords( tmp.point1$dev, input='dev' )
> arrows( tmp.y[1], tmp.z[1], tmp.point2$usr$x, tmp.point2$usr$y,
+  col='green')
> 
> # draw another plot and add rectangle showing same range in 2 plots
> 
> plot(tmp.x, tmp.z)
> tmp.range2 <- cnvrt.coords(NA, 0.02, input='plt')
> tmp.range3 <- cnvrt.coords(NA, tmp.range1$dev$y, input='dev')
> rect( 9, tmp.range2$usr$y, 11, tmp.range3$usr$y, border='yellow')
> 
> # put a label just to the right of the plot and
> #  near the top of the figure region.
> text( cnvrt.coords(1.05, NA, input='plt')$usr$x,
+ 	cnvrt.coords(NA, 0.75, input='fig')$usr$y,
+ 	"Label", adj=0)
> 
> par(mfrow=c(1,1))
> 
> ## create a subplot within another plot (see also subplot)
> 
> plot(1:10, 1:10)
> 
> tmp <- cnvrt.coords( c( 1, 4, 6, 9), c(6, 9, 1, 4) )
> 
> par(plt = c(tmp$dev$x[1:2], tmp$dev$y[1:2]), new=TRUE)
> hist(rnorm(100))
> 
> par(fig = c(tmp$dev$x[3:4], tmp$dev$y[3:4]), new=TRUE)
> hist(rnorm(100))
> 
> par(old.par)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("combine.levels")
> ### * combine.levels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: combine.levels
> ### Title: combine.levels
> ### Aliases: combine.levels
> 
> ### ** Examples
> 
> x <- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1))
> combine.levels(x, m=3)
 [1] OTHER B     B     B     C     C     C     C     OTHER OTHER
Levels: OTHER B C
> combine.levels(x, m=3, plevels=TRUE)
 [1] A,D,E B     B     B     C     C     C     C     A,D,E A,D,E
Levels: A,D,E B C
> combine.levels(x, ord=TRUE, m=3)
 [1] A,B   A,B   A,B   A,B   C,D,E C,D,E C,D,E C,D,E C,D,E C,D,E
Levels: A,B < C,D,E
> x <- c(rep('A', 1), rep('B', 3), rep('C', 4), rep('D',1), rep('E',1),
+        rep('F',1))
> combine.levels(x, ord=TRUE, m=3)
 [1] A,B   A,B   A,B   A,B   C     C     C     C     D,E,F D,E,F D,E,F
Levels: A,B < C < D,E,F
> 
> 
> 
> cleanEx()
> nameEx("combplotp")
> ### * combplotp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: combplotp
> ### Title: Combination Plot
> ### Aliases: combplotp
> 
> ### ** Examples
> 
> if (requireNamespace("plotly")) {
+   g <- function() sample(0:1, n, prob=c(1 - p, p), replace=TRUE)
+   set.seed(2); n <- 100; p <- 0.5
+   x1 <- g(); label(x1) <- 'A long label for x1 that describes it'
+   x2 <- g()
+   x3 <- g(); label(x3) <- 'This is<br>a label for x3'
+   x4 <- g()
+   combplotp(~ x1 + x2 + x3 + x4, showno=TRUE, includenone=TRUE)
+ 
+   n <- 1500; p <- 0.05
+   pain       <- g()
+   anxiety    <- g()
+   depression <- g()
+   soreness   <- g()
+   numbness   <- g()
+   tiredness  <- g()
+   sleepiness <- g()
+   combplotp(~ pain + anxiety + depression + soreness + numbness +
+             tiredness + sleepiness, showno=TRUE)
+ }
Loading required namespace: plotly
Warning: Ignoring 3 observations
Warning: Ignoring 4 observations
Input to asJSON(keep_vec_names=TRUE) is a named vector. In a future version of jsonlite, this option will not be supported, and named vectors will be translated into arrays instead of objects. If you want JSON object output, please use a named list instead. See ?toJSON.
Input to asJSON(keep_vec_names=TRUE) is a named vector. In a future version of jsonlite, this option will not be supported, and named vectors will be translated into arrays instead of objects. If you want JSON object output, please use a named list instead. See ?toJSON.
> 
> 
> 
> cleanEx()
> nameEx("completer")
> ### * completer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: completer
> ### Title: completer
> ### Aliases: completer
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mtcars$hp[1:5]    <- NA
> ##D mtcars$wt[1:10]   <- NA
> ##D myrform <- ~ wt + hp + I(carb)
> ##D mytranscan  <- transcan( myrform,  data = mtcars, imputed = TRUE,
> ##D   pl = FALSE, pr = FALSE, trantab = TRUE, long = TRUE)
> ##D myareg      <- aregImpute(myrform, data = mtcars, x=TRUE, n.impute = 5)
> ##D completer(mytranscan)                    # single completed dataset
> ##D completer(myareg, 3, oneimpute = TRUE)
> ##D # single completed dataset based on the `n.impute`th set of multiple imputation
> ##D completer(myareg, 3)
> ##D # list of completed datasets based on first `nimpute` sets of multiple imputation
> ##D completer(myareg)
> ##D # list of completed datasets based on all available sets of multiple imputation
> ##D # To get a stacked data frame of all completed datasets use
> ##D # do.call(rbind, completer(myareg, data=mydata))
> ##D # or use rbindlist in data.table
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("consolidate")
> ### * consolidate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: consolidate
> ### Title: Element Merging
> ### Aliases: consolidate consolidate<- consolidate.default
> ### Keywords: utilities
> 
> ### ** Examples
> 
> x <- 1:5
> names(x) <- LETTERS[x]
> 
> y <- 6:10
> names(y) <- LETTERS[y-2]
> 
> x                  # c(A=1,B=2,C=3,D=4,E=5)
A B C D E 
1 2 3 4 5 
> y                  # c(D=6,E=7,F=8,G=9,H=10)
 D  E  F  G  H 
 6  7  8  9 10 
> 
> consolidate(x, y)      # c(A=1,B=2,C=3,D=6,E=7,F=8,G=9,H=10)
 A  B  C  D  E  F  G  H 
 1  2  3  6  7  8  9 10 
> consolidate(x, y, protect=TRUE)      # c(A=1,B=2,C=3,D=4,E=5,F=8,G=9,H=10)
 A  B  C  D  E  F  G  H 
 1  2  3  4  5  8  9 10 
> 
> 
> 
> 
> cleanEx()
> nameEx("contents")
> ### * contents
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: contents
> ### Title: Metadata for a Data Frame
> ### Aliases: contents contents.data.frame print.contents.data.frame
> ###   html.contents.data.frame contents.list print.contents.list
> ### Keywords: data interface
> 
> ### ** Examples
> 
> set.seed(1)
> dfr <- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE),
+                   stringsAsFactors=TRUE)
> contents(dfr)

Data frame:dfr	400 observations and 2 variables    Maximum # NAs:0


  Levels Storage
x         double
y      2 integer

+--------+-----------+
|Variable|Levels     |
+--------+-----------+
|    y   |female,male|
+--------+-----------+
> dfr <- upData(dfr, labels=c(x='Label for x', y='Label for y'))
Input object size:	 6160 bytes;	 2 variables	 400 observations
New object size:	6992 bytes;	2 variables	400 observations
> attr(dfr$x, 'longlabel') <-
+  'A very long label for x that can continue onto multiple long lines of text'
> 
> k <- contents(dfr)
> print(k, sort='names', prlevels=FALSE)

Data frame:dfr	400 observations and 2 variables    Maximum # NAs:0


       Labels Levels   Class Storage
x Label for x        numeric  double
y Label for y      2         integer
+--------+------------------------------------------------------------------+
|Variable|                            Long Label                            |
+--------+------------------------------------------------------------------+
|x       |A very long label for x that can continue onto multiple long lines|
|        |                                                           of text|
+--------+------------------------------------------------------------------+
> ## Not run: 
> ##D html(k)
> ##D html(contents(dfr))            # same result
> ##D latex(k$contents)              # latex.default just the main information
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cpower")
> ### * cpower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cpower
> ### Title: Power of Cox/log-rank Two-Sample Test
> ### Aliases: cpower
> ### Keywords: htest survival
> 
> ### ** Examples
> 
> #In this example, 4 plots are drawn on one page, one plot for each
> #combination of noncompliance percentage.  Within a plot, the
> #5-year mortality % in the control group is on the x-axis, and
> #separate curves are drawn for several % reductions in mortality
> #with the intervention.  The accrual period is 1.5y, with all
> #patients followed at least 5y and some 6.5y.
> 
> 
> par(mfrow=c(2,2),oma=c(3,0,3,0))
> 
> 
> morts <- seq(10,25,length=50)
> red <- c(10,15,20,25)
> 
> 
> for(noncomp in c(0,10,15,-1)) {
+   if(noncomp>=0) nc.i <- nc.c <- noncomp else {nc.i <- 25; nc.c <- 15}
+   z <- paste("Drop-in ",nc.c,"%, Non-adherence ",nc.i,"%",sep="")
+   plot(0,0,xlim=range(morts),ylim=c(0,1),
+            xlab="5-year Mortality in Control Patients (%)",
+            ylab="Power",type="n")
+   title(z)
+   cat(z,"\n")
+   lty <- 0
+   for(r in red) {
+         lty <- lty+1
+         power <- morts
+         i <- 0
+         for(m in morts) {
+           i <- i+1
+           power[i] <- cpower(5, 14000, m/100, r, 1.5, 5, nc.c, nc.i, pr=FALSE)
+         }
+         lines(morts, power, lty=lty)
+   }
+   if(noncomp==0)legend(18,.55,rev(paste(red,"% reduction",sep="")),
+            lty=4:1,bty="n")
+ }
Drop-in 0%, Non-adherence 0% 
Drop-in 10%, Non-adherence 10% 
Drop-in 15%, Non-adherence 15% 
Drop-in 15%, Non-adherence 25% 
> mtitle("Power vs Non-Adherence for Main Comparison",
+            ll="alpha=.05, 2-tailed, Total N=14000",cex.l=.8)
> #
> # Point sample size requirement vs. mortality reduction
> # Root finder (uniroot()) assumes needed sample size is between
> # 1000 and 40000
> #
> nc.i <- 25; nc.c <- 15; mort <- .18
> red <- seq(10,25,by=.25)
> samsiz <- red
> 
> 
> i <- 0
> for(r in red) {
+   i <- i+1
+   samsiz[i] <- uniroot(function(x) cpower(5, x, mort, r, 1.5, 5,
+                                           nc.c, nc.i, pr=FALSE) - .8,
+                        c(1000,40000))$root
+ }
> 
> 
> samsiz <- samsiz/1000
> par(mfrow=c(1,1))
> plot(red, samsiz, xlab='% Reduction in 5-Year Mortality',
+ 	 ylab='Total Sample Size (Thousands)', type='n')
> lines(red, samsiz, lwd=2)
> title('Sample Size for Power=0.80\nDrop-in 15%, Non-adherence 25%')
> title(sub='alpha=0.05, 2-tailed', adj=0)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("csv.get")
> ### * csv.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: csv.get
> ### Title: Read Comma-Separated Text Data Files
> ### Aliases: csv.get
> ### Keywords: manip IO file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat <- csv.get('myfile.csv')
> ##D 
> ##D # Read a csv file with junk in the first row, variable names in the
> ##D # second, long variable labels in the third, and junk in the 4th row
> ##D dat <- csv.get('myfile.csv', vnames=2, labels=3, skip=4)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("curveRep")
> ### * curveRep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: curveRep
> ### Title: Representative Curves
> ### Aliases: curveRep print.curveRep plot.curveRep curveSmooth
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Simulate 200 curves with per-curve sample sizes ranging from 1 to 10
> ##D # Make curves with odd-numbered IDs have an x-distribution that is random
> ##D # uniform [0,1] and those with even-numbered IDs have an x-dist. that is
> ##D # half as wide but still centered at 0.5.  Shift y values higher with
> ##D # increasing IDs
> ##D set.seed(1)
> ##D N <- 200
> ##D nc <- sample(1:10, N, TRUE)
> ##D id <- rep(1:N, nc)
> ##D x <- y <- id
> ##D for(i in 1:N) {
> ##D   x[id==i] <- if(i %% 2) runif(nc[i]) else runif(nc[i], c(.25, .75))
> ##D   y[id==i] <- i + 10*(x[id==i] - .5) + runif(nc[i], -10, 10)
> ##D }
> ##D 
> ##D w <- curveRep(x, y, id, kxdist=2, p=10)
> ##D w
> ##D par(ask=TRUE, mfrow=c(4,5))
> ##D plot(w)                # show everything, profiles going across
> ##D par(mfrow=c(2,5))
> ##D plot(w,1)              # show n=1 results
> ##D # Use a color assignment table, assigning low curves to green and
> ##D # high to red.  Unique curve (subject) IDs are the names of the vector.
> ##D cols <- c(rep('green', N/2), rep('red', N/2))
> ##D names(cols) <- as.character(1:N)
> ##D plot(w, 3, idcol=cols)
> ##D par(ask=FALSE, mfrow=c(1,1))
> ##D 
> ##D plot(w, 1, 'lattice')  # show n=1 results
> ##D plot(w, 3, 'lattice')  # show n=4-5 results
> ##D plot(w, 3, 'lattice', idcol=cols)  # same but different color mapping
> ##D plot(w, 3, 'lattice', m=1)  # show a single "representative" curve
> ##D # Show median, 10th, and 90th percentiles of supposedly representative curves
> ##D plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9))
> ##D # Same plot but with much less grouping of x variable
> ##D plot(w, 3, 'lattice', m='quantiles', probs=c(.5,.1,.9), nx=2)
> ##D 
> ##D # Use ggplot2 for one sample size interval
> ##D z <- plot(w, 2, 'data')
> ##D require(ggplot2)
> ##D ggplot(z, aes(x, y, color=curve)) + geom_line() +
> ##D        facet_grid(distribution ~ cluster) +
> ##D        theme(legend.position='none') +
> ##D        labs(caption=z$ninterval[1])
> ##D 
> ##D 
> ##D # Smooth data before profiling.  This allows later plotting to plot
> ##D # smoothed representative curves rather than raw curves (which
> ##D # specifying smooth=TRUE to curveRep would do, if curveSmooth was not used)
> ##D d <- curveSmooth(x, y, id)
> ##D w <- with(d, curveRep(x, y, id))
> ##D 
> ##D # Example to show that curveRep can cluster profiles correctly when
> ##D # there is no noise.  In the data there are four profiles - flat, flat
> ##D # at a higher mean y, linearly increasing then flat, and flat at the
> ##D # first height except for a sharp triangular peak
> ##D 
> ##D set.seed(1)
> ##D x <- 0:100
> ##D m <- length(x)
> ##D profile <- matrix(NA, nrow=m, ncol=4)
> ##D profile[,1] <- rep(0, m)
> ##D profile[,2] <- rep(3, m)
> ##D profile[,3] <- c(0:3, rep(3, m-4))
> ##D profile[,4] <- c(0,1,3,1,rep(0,m-4))
> ##D col <- c('black','blue','green','red')
> ##D matplot(x, profile, type='l', col=col)
> ##D xeval <- seq(0, 100, length.out=5)
> ##D s <- x ##D 
> ##D matplot(x[s], profile[s,], type='l', col=col)
> ##D 
> ##D id <- rep(1:100, each=m)
> ##D X <- Y <- id
> ##D cols <- character(100)
> ##D names(cols) <- as.character(1:100)
> ##D for(i in 1:100) {
> ##D   s <- id==i
> ##D   X[s] <- x
> ##D   j <- sample(1:4,1)
> ##D   Y[s] <- profile[,j]
> ##D   cols[i] <- col[j]
> ##D }
> ##D table(cols)
> ##D yl <- c(-1,4)
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=4)
> ##D plot(w, 1, 'lattice', idcol=cols, ylim=yl)
> ##D # Found 4 clusters but two have same profile
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=3)
> ##D plot(w, 1, 'lattice', idcol=cols, freq=cols, plotfreq=TRUE, ylim=yl)
> ##D # Incorrectly combined black and red because default value p=5 did
> ##D # not result in different profiles at x=xeval
> ##D w <- curveRep(X, Y, id, kn=1, kxdist=1, k=4, p=40)
> ##D plot(w, 1, 'lattice', idcol=cols, ylim=yl)
> ##D # Found correct clusters because evaluated curves at 40 equally
> ##D # spaced points and could find the sharp triangular peak in profile 4
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cut2")
> ### * cut2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cut2
> ### Title: Cut a Numeric Variable into Intervals
> ### Aliases: cut2
> ### Keywords: category nonparametric
> 
> ### ** Examples
> 
> set.seed(1)
> x <- runif(1000, 0, 100)
> z <- cut2(x, c(10,20,30))
> table(z)
z
[  0.131, 10.000) [ 10.000, 20.000) [ 20.000, 30.000) [ 30.000, 99.993] 
               96               104                93               707 
> table(cut2(x, g=10))      # quantile groups

[ 0.131, 10.5) [10.505, 20.2) [20.168, 31.2) [31.204, 39.8) [39.784, 48.4) 
           100            100            100            100            100 
[48.435, 59.6) [59.645, 70.7) [70.666, 79.7) [79.731, 91.0) [91.037,100.0] 
           100            100            100            100            100 
> table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

[ 0.131,  5.52) [ 5.516, 10.51) [10.505, 15.48) [15.483, 20.17) [20.168, 25.82) 
             50              50              50              50              50 
[25.817, 31.20) [31.204, 35.32) [35.320, 39.78) [39.784, 44.15) [44.146, 48.43) 
             50              50              50              50              50 
[48.435, 52.78) [52.778, 59.64) [59.645, 65.09) [65.087, 70.67) [70.666, 74.76) 
             50              50              50              50              50 
[74.764, 79.73) [79.731, 85.51) [85.508, 91.04) [91.037, 95.37) [95.373, 99.99] 
             50              50              50              50              50 
> 
> 
> 
> cleanEx()
> nameEx("data.frame.create.modify.check")
> ### * data.frame.create.modify.check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: data.frame.create.modify.check
> ### Title: Tips for Creating, Modifying, and Checking Data Frames
> ### Aliases: data.frame.create.modify.check
> ### Keywords: data manip programming interface htest
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # First, we do steps that create or manipulate the data
> ##D # frame in its entirety.  For S-Plus, these are done with
> ##D # .Data in search position one (the default at the
> ##D # start of the session).
> ##D #
> ##D # -----------------------------------------------------------------------
> ##D # Step 1: Create initial draft of data frame
> ##D # 
> ##D # We usually begin by importing a dataset from
> ##D # # another application.  ASCII files may be imported
> ##D # using the scan and read.table functions.  SAS
> ##D # datasets may be imported using the Hmisc sas.get
> ##D # function (which will carry more attributes from
> ##D # SAS than using File \dots  Import) from the GUI
> ##D # menus.  But for most applications (especially
> ##D # Excel), File \dots Import will suffice.  If using
> ##D # the GUI, it is often best to provide variable
> ##D # names during the import process, using the Options
> ##D # tab, rather than renaming all fields later Of
> ##D # course, if the data to be imported already have
> ##D # field names (e.g., in Excel), let S use those
> ##D # automatically.  If using S-Plus, you can use a
> ##D # command to execute File \dots  Import, e.g.:
> ##D 
> ##D 
> ##D import.data(FileName = "/windows/temp/fev.asc",
> ##D             FileType = "ASCII", DataFrame = "FEV")
> ##D 
> ##D 
> ##D # Here we name the new data frame FEV rather than
> ##D # fev, because we wanted to distinguish a variable
> ##D # in the data frame named fev from the data frame
> ##D # name.  For S-Plus the command will look
> ##D # instead like the following:
> ##D 
> ##D 
> ##D FEV <- importData("/tmp/fev.asc")
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 2: Clean up data frame / make it be more
> ##D # efficiently stored
> ##D # 
> ##D # Unless using sas.get to import your dataset
> ##D # (sas.get already stores data efficiently), it is
> ##D # usually a good idea to run the data frame through
> ##D # the Hmisc cleanup.import function to change
> ##D # numeric variables that are always whole numbers to
> ##D # be stored as integers, the remaining numerics to
> ##D # single precision, strange values from Excel to
> ##D # NAs, and character variables that always contain
> ##D # legal numeric values to numeric variables.
> ##D # cleanup.import typically halves the size of the
> ##D # data frame.  If you do not specify any parameters
> ##D # to cleanup.import, the function assumes that no
> ##D # numeric variable needs more than 7 significant
> ##D # digits of precision, so all non-integer-valued
> ##D # variables will be converted to single precision.
> ##D 
> ##D 
> ##D FEV <- cleanup.import(FEV)
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 3: Make global changes to the data frame
> ##D # 
> ##D # A data frame has attributes that are "external" to
> ##D # its variables.  There are the vector of its
> ##D # variable names ("names" attribute), the
> ##D # observation identifiers ("row.names"), and the
> ##D # "class" (whose value is "data.frame").  The
> ##D # "names" attribute is the one most commonly in need
> ##D # of modification.  If we had wanted to change all
> ##D # the variable names to lower case, we could have
> ##D # specified lowernames=TRUE to the cleanup.import
> ##D # invocation above, or type
> ##D 
> ##D 
> ##D names(FEV) <- casefold(names(FEV))
> ##D 
> ##D 
> ##D # The upData function can also be used to change
> ##D # variable names in two ways (see below).
> ##D # To change names in a non-systematic way we use
> ##D # other options.  Under Windows/NT the most
> ##D # straigtforward approach is to change the names
> ##D # interactively.  Click on the data frame in the
> ##D # left panel of the Object Browser, then in the
> ##D # right pane click twice (slowly) on a variable.
> ##D # Use the left arrow and other keys to edit the
> ##D # name.  Click outside that name field to commit the
> ##D # change.  You can also rename columns while in a
> ##D # Data Sheet.  To instead use programming commands
> ##D # to change names, use something like:
> ##D 
> ##D 
> ##D names(FEV)[6] <- 'smoke'   # assumes you know the positions!  
> ##D names(FEV)[names(FEV)=='smoking'] <- 'smoke' 
> ##D names(FEV) <- edit(names(FEV))
> ##D 
> ##D 
> ##D # The last example is useful if you are changing
> ##D # many names.  But none of the interactive
> ##D # approaches such as edit() are handy if you will be
> ##D # re-importing the dataset after it is updated in
> ##D # its original application.  This problem can be
> ##D # addressed by saving the new names in a permanent
> ##D # vector in .Data:
> ##D 
> ##D 
> ##D new.names <- names(FEV)
> ##D 
> ##D 
> ##D # Then if the data are re-imported, you can type
> ##D 
> ##D 
> ##D names(FEV) <- new.names
> ##D 
> ##D 
> ##D # to rename the variables.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 4: Delete unneeded variables
> ##D # 
> ##D # To delete some of the variables, you can
> ##D # right-click on variable names in the Object
> ##D # Browser's right pane, then select Delete.  You can
> ##D # also set variables to have NULL values, which
> ##D # causes the system to delete them.  We don't need
> ##D # to delete any variables from FEV but suppose we
> ##D # did need to delete some from mydframe.
> ##D 
> ##D 
> ##D mydframe$x1 <- NULL 
> ##D mydframe$x2 <- NULL
> ##D mydframe[c('age','sex')] <- NULL   # delete 2 variables 
> ##D mydframe[Cs(age,sex)]    <- NULL   # same thing
> ##D 
> ##D 
> ##D # The last example uses the Hmisc short-cut quoting
> ##D # function Cs.  See also the drop parameter to upData.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 5: Make changes to individual variables
> ##D #         within the data frame
> ##D # 
> ##D # After importing data, the resulting variables are
> ##D # seldom self - documenting, so we commonly need to
> ##D # change or enhance attributes of individual
> ##D # variables within the data frame.
> ##D # 
> ##D # If you are only changing a few variables, it is
> ##D # efficient to change them directly without
> ##D # attaching the entire data frame.
> ##D 
> ##D 
> ##D FEV$sex   <- factor(FEV$sex,   0:1, c('female','male')) 
> ##D FEV$smoke <- factor(FEV$smoke, 0:1, 
> ##D                     c('non-current smoker','current smoker')) 
> ##D units(FEV$age)    <- 'years'
> ##D units(FEV$fev)    <- 'L' 
> ##D label(FEV$fev)    <- 'Forced Expiratory Volume' 
> ##D units(FEV$height) <- 'inches'
> ##D 
> ##D 
> ##D # When changing more than one or two variables it is
> ##D # more convenient change the data frame using the
> ##D # Hmisc upData function.
> ##D 
> ##D 
> ##D FEV2 <- upData(FEV,
> ##D   rename=c(smoking='smoke'), 
> ##D   # omit if renamed above
> ##D   drop=c('var1','var2'),
> ##D   levels=list(sex  =list(female=0,male=1),
> ##D               smoke=list('non-current smoker'=0,
> ##D                          'current smoker'=1)),
> ##D   units=list(age='years', fev='L', height='inches'),
> ##D   labels=list(fev='Forced Expiratory Volume'))
> ##D 
> ##D 
> ##D # An alternative to levels=list(\dots) is for example
> ##D # upData(FEV, sex=factor(sex,0:1,c('female','male'))).
> ##D # 
> ##D # Note that we saved the changed data frame into a
> ##D # new data frame FEV2.  If we were confident of the
> ##D # correctness of our changes we could have stored
> ##D # the new data frame on top of the old one, under
> ##D # the original name FEV.
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 6:  Check the data frame
> ##D # 
> ##D # The Hmisc describe function is perhaps the first
> ##D # function that should be used on the new data
> ##D # frame.  It provides documentation of all the
> ##D # variables and the frequency tabulation, counts of
> ##D # NAs,  and 5 largest and smallest values are
> ##D # helpful in detecting data errors.  Typing
> ##D # describe(FEV) will write the results to the
> ##D # current output window.  To put the results in a
> ##D # new window that can persist, even upon exiting
> ##D # S, we use the page function.  The describe
> ##D # output can be minimized to an icon but kept ready
> ##D # for guiding later steps of the analysis.
> ##D 
> ##D 
> ##D page(describe(FEV2), multi=TRUE) 
> ##D # multi=TRUE allows that window to persist while
> ##D # control is returned to other windows
> ##D 
> ##D 
> ##D # The new data frame is OK.  Store it on top of the
> ##D # old FEV and then use the graphical user interface
> ##D # to delete FEV2 (click on it and hit the Delete
> ##D # key) or type rm(FEV2) after the next statement.
> ##D 
> ##D 
> ##D FEV <- FEV2
> ##D 
> ##D 
> ##D # Next, we can use a variety of other functions to
> ##D # check and describe all of the variables.  As we
> ##D # are analyzing all or almost all of the variables,
> ##D # this is best done without attaching the data
> ##D # frame.  Note that plot.data.frame plots inverted
> ##D # CDFs for continuous variables and dot plots
> ##D # showing frequency distributions of categorical
> ##D # ones.
> ##D 
> ##D 
> ##D summary(FEV)
> ##D # basic summary function (summary.data.frame) 
> ##D 
> ##D 
> ##D plot(FEV)                # plot.data.frame 
> ##D datadensity(FEV)         
> ##D # rug plots and freq. bar charts for all var.
> ##D 
> ##D 
> ##D hist.data.frame(FEV)     
> ##D # for variables having > 2 values 
> ##D 
> ##D 
> ##D by(FEV, FEV$smoke, summary)  
> ##D # use basic summary function with stratification
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Step 7:  Do detailed analyses involving individual
> ##D #          variables
> ##D # 
> ##D # Analyses based on the formula language can use
> ##D # data= so attaching the data frame may not be
> ##D # required.  This saves memory.  Here we use the
> ##D # Hmisc summary.formula function to compute 5
> ##D # statistics on height, stratified separately by age
> ##D # quartile and by sex.
> ##D 
> ##D 
> ##D options(width=80) 
> ##D summary(height ~ age + sex, data=FEV,
> ##D         fun=function(y)c(smean.sd(y),
> ##D                          smedian.hilow(y,conf.int=.5)))
> ##D # This computes mean height, S.D., median, outer quartiles
> ##D 
> ##D 
> ##D fit <- lm(height ~ age*sex, data=FEV) 
> ##D summary(fit)
> ##D 
> ##D 
> ##D # For this analysis we could also have attached the
> ##D # data frame in search position 2.  For other
> ##D # analyses, it is mandatory to attach the data frame
> ##D # unless FEV$ prefixes each variable name.
> ##D # Important: DO NOT USE attach(FEV, 1) or
> ##D # attach(FEV, pos=1, \dots) if you are only analyzing
> ##D # and not changing the variables, unless you really
> ##D # need to avoid conflicts with variables in search
> ##D # position 1 that have the same names as the
> ##D # variables in FEV.  Attaching into search position
> ##D # 1 will cause S-Plus to be more of a memory hog.
> ##D 
> ##D 
> ##D attach(FEV)
> ##D # Use e.g. attach(FEV[,Cs(age,sex)]) if you only
> ##D # want to analyze a small subset of the variables
> ##D # Use e.g. attach(FEV[FEV$sex=='male',]) to
> ##D # analyze a subset of the observations
> ##D 
> ##D 
> ##D summary(height ~ age + sex,
> ##D         fun=function(y)c(smean.sd(y),
> ##D           smedian.hilow(y,conf.int=.5)))
> ##D fit <- lm(height ~ age*sex)
> ##D 
> ##D 
> ##D # Run generic summary function on height and fev, 
> ##D # stratified by sex
> ##D by(data.frame(height,fev), sex, summary)
> ##D 
> ##D 
> ##D # Cross-classify into 4 sex x smoke groups
> ##D by(FEV, list(sex,smoke), summary)
> ##D 
> ##D 
> ##D # Plot 5 quantiles
> ##D s <- summary(fev ~ age + sex + height,
> ##D               fun=function(y)quantile(y,c(.1,.25,.5,.75,.9)))
> ##D 
> ##D 
> ##D plot(s, which=1:5, pch=c(1,2,15,2,1), #pch=c('=','[','o',']','='), 
> ##D      main='A Discovery', xlab='FEV')
> ##D 
> ##D 
> ##D # Use the nonparametric bootstrap to compute a 
> ##D # 0.95 confidence interval for the population mean fev
> ##D smean.cl.boot(fev)    # in Hmisc
> ##D 
> ##D 
> ##D # Use the Statistics \dots Compare Samples \dots One Sample 
> ##D # keys to get a normal-theory-based C.I.  Then do it 
> ##D # more manually.  The following method assumes that 
> ##D # there are no NAs in fev
> ##D 
> ##D 
> ##D sd <- sqrt(var(fev))
> ##D xbar <- mean(fev)
> ##D xbar
> ##D sd
> ##D n <- length(fev)
> ##D qt(.975,n-1)     
> ##D # prints 0.975 critical value of t dist. with n-1 d.f.
> ##D 
> ##D 
> ##D xbar + c(-1,1)*sd/sqrt(n)*qt(.975,n-1)   
> ##D # prints confidence limits
> ##D 
> ##D 
> ##D # Fit a linear model
> ##D # fit <- lm(fev ~ other variables \dots)
> ##D 
> ##D 
> ##D detach()
> ##D 
> ##D 
> ##D # The last command is only needed if you want to
> ##D # start operating on another data frame and you want
> ##D # to get FEV out of the way.
> ##D 
> ##D 
> ##D 
> ##D 
> ##D # -----------------------------------------------------------------------
> ##D # Creating data frames from scratch
> ##D # 
> ##D # Data frames can be created from within S.  To
> ##D # create a small data frame containing ordinary
> ##D # data, you can use something like
> ##D 
> ##D 
> ##D dframe <- data.frame(age=c(10,20,30), 
> ##D                      sex=c('male','female','male'),
> ##D                      stringsAsFactors=TRUE)
> ##D 
> ##D 
> ##D # You can also create a data frame using the Data
> ##D # Sheet.  Create an empty data frame with the
> ##D # correct variable names and types, then edit in the
> ##D # data.
> ##D 
> ##D 
> ##D dd <- data.frame(age=numeric(0),sex=character(0),
> ##D                  stringsAsFactors=TRUE)
> ##D 
> ##D 
> ##D # The sex variable will be stored as a factor, and
> ##D # levels will be automatically added to it as you
> ##D # define new values for sex in the Data Sheet's sex
> ##D # column.
> ##D # 
> ##D # When the data frame you need to create is defined
> ##D # by systematically varying variables (e.g., all
> ##D # possible combinations of values of each variable),
> ##D # the expand.grid function is useful for quickly
> ##D # creating the data.  Then you can add
> ##D # non-systematically-varying variables to the object
> ##D # created by expand.grid, using programming
> ##D # statements or editing the Data Sheet.  This
> ##D # process is useful for creating a data frame
> ##D # representing all the values in a printed table.
> ##D # In what follows we create a data frame
> ##D # representing the combinations of values from an 8
> ##D # x 2 x 2 x 2 (event x method x sex x what) table,
> ##D # and add a non-systematic variable percent to the
> ##D # data.
> ##D 
> ##D 
> ##D jcetable <- expand.grid(
> ##D  event=c('Wheezing at any time',
> ##D          'Wheezing and breathless',
> ##D          'Wheezing without a cold',
> ##D          'Waking with tightness in the chest',
> ##D          'Waking with shortness of breath',
> ##D          'Waking with an attack of cough',
> ##D          'Attack of asthma',
> ##D          'Use of medication'),
> ##D  method=c('Mail','Telephone'), 
> ##D  sex=c('Male','Female'),
> ##D  what=c('Sensitivity','Specificity'))
> ##D 
> ##D 
> ##D jcetable$percent <- 
> ##D c(756,618,706,422,356,578,289,333,
> ##D   576,421,789,273,273,212,212,212,
> ##D   613,763,713,403,377,541,290,226,
> ##D   613,684,632,290,387,613,258,129,
> ##D   656,597,438,780,732,679,938,919,
> ##D   714,600,494,877,850,703,963,987,
> ##D   755,420,480,794,779,647,956,941,
> ##D   766,423,500,833,833,604,955,986) / 10
> ##D 
> ##D 
> ##D # In jcetable, event varies most rapidly, then
> ##D # method, then sex, and what.
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("dataRep")
> ### * dataRep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dataRep
> ### Title: Representativeness of Observations in a Data Set
> ### Aliases: dataRep print.dataRep predict.dataRep print.predict.dataRep
> ###   roundN [.roundN
> ### Keywords: datasets category cluster manip models
> 
> ### ** Examples
> 
> set.seed(13)
> num.symptoms <- sample(1:4, 1000,TRUE)
> sex <- factor(sample(c('female','male'), 1000,TRUE))
> x    <- runif(1000)
> x[1] <- NA
> table(num.symptoms, sex, .25*round(x/.25))
, ,  = 0

            sex
num.symptoms female male
           1     15   11
           2     22   13
           3     14   18
           4     16   18

, ,  = 0.25

            sex
num.symptoms female male
           1     37   28
           2     33   24
           3     32   35
           4     27   32

, ,  = 0.5

            sex
num.symptoms female male
           1     30   29
           2     36   35
           3     20   34
           4     27   36

, ,  = 0.75

            sex
num.symptoms female male
           1     38   28
           2     23   25
           3     32   29
           4     30   31

, ,  = 1

            sex
num.symptoms female male
           1     19   24
           2     25   18
           3     17   15
           4      6   17

> 
> 
> d <- dataRep(~ num.symptoms + sex + roundN(x,.25))
> print(d, long=TRUE)

Data Representativeness    n=999

dataRep(formula = ~num.symptoms + sex + roundN(x, 0.25))

Frequencies of Missing Values Due to Each Variable
   num.symptoms             sex roundN(x, 0.25) 
              0               0               1 

Specifications for Matching

                          Type      Parameters
num.symptoms     exact numeric                
sex          exact categorical     female male
x                        round to nearest 0.25

Unique Combinations of Descriptor Variables

   num.symptoms    sex    x Frequency
1             1 female 0.00        15
2             2 female 0.00        22
3             3 female 0.00        14
4             4 female 0.00        16
5             1   male 0.00        11
6             2   male 0.00        13
7             3   male 0.00        18
8             4   male 0.00        18
9             1 female 0.25        37
10            2 female 0.25        33
11            3 female 0.25        32
12            4 female 0.25        27
13            1   male 0.25        28
14            2   male 0.25        24
15            3   male 0.25        35
16            4   male 0.25        32
17            1 female 0.50        30
18            2 female 0.50        36
19            3 female 0.50        20
20            4 female 0.50        27
21            1   male 0.50        29
22            2   male 0.50        35
23            3   male 0.50        34
24            4   male 0.50        36
25            1 female 0.75        38
26            2 female 0.75        23
27            3 female 0.75        32
28            4 female 0.75        30
29            1   male 0.75        28
30            2   male 0.75        25
31            3   male 0.75        29
32            4   male 0.75        31
33            1 female 1.00        19
34            2 female 1.00        25
35            3 female 1.00        17
36            4 female 1.00         6
37            1   male 1.00        24
38            2   male 1.00        18
39            3   male 1.00        15
40            4   male 1.00        17
> 
> 
> predict(d, data.frame(num.symptoms=1:3, sex=c('male','male','female'),
+                       x=c(.03,.5,1.5)))
Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) :
  collapsing to unique 'x' values

Descriptor Variable Values, Estimated Frequency in Original Dataset,
and Minimum Marginal Frequency for any Variable

  num.symptoms    sex    x Frequency Marginal.Freq
1            1   male 0.03        11           127
2            2   male 0.50        35           247
3            3 female 1.50         0             0


Percentiles for Continuous Descriptor Variables,
Percentage in Category for Categorical Variables

  num.symptoms sex   x
1           12  50   3
2           38  50  50
3           64  50 100
> 
> 
> 
> cleanEx()
> nameEx("deff")
> ### * deff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deff
> ### Title: Design Effect and Intra-cluster Correlation
> ### Aliases: deff
> ### Keywords: htest
> 
> ### ** Examples
> 
> set.seed(1)
> blood.pressure <- rnorm(1000, 120, 15)
> clinic <- sample(letters, 1000, replace=TRUE)
> deff(blood.pressure, clinic)
            n      clusters           rho          deff 
 1.000000e+03  2.600000e+01 -7.080501e-03  7.289867e-01 
> 
> 
> 
> cleanEx()
> nameEx("describe")
> ### * describe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: describe
> ### Title: Concise Statistical Description of a Vector, Matrix, Data Frame,
> ###   or Formula
> ### Aliases: describe describe.default describe.vector describe.matrix
> ###   describe.formula describe.data.frame plot.describe print.describe
> ###   print.describe.single [.describe latex.describe latex.describe.single
> ###   html.describe html.describe.single formatdescribeSingle
> ### Keywords: interface nonparametric category distribution robust models
> ###   hplot
> 
> ### ** Examples
> 
> set.seed(1)
> describe(runif(200),dig=2)    #single variable, continuous
runif(200) 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
     200        0      200        1     0.52     0.52     0.31    0.084 
     .10      .25      .50      .75      .90      .95 
   0.142    0.294    0.505    0.742    0.881    0.927 

lowest : 0.0130776 0.0133903 0.0233312 0.0355406 0.0589344
highest: 0.976171  0.985095  0.991839  0.991906  0.992684 
>                               #get quantiles .05,.10,\dots
> 
> dfr <- data.frame(x=rnorm(400),y=sample(c('male','female'),400,TRUE))
> describe(dfr)
dfr 

 2  Variables      400  Observations
--------------------------------------------------------------------------------
x 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
     400        0      400        1  -0.0463 -0.04444    1.223 -1.79403 
     .10      .25      .50      .75      .90      .95 
-1.43026 -0.82824 -0.01549  0.68933  1.34958  1.77431 

lowest : -2.99695 -2.93977 -2.59611 -2.51443 -2.44231
highest: 2.25188  2.32133  2.34949  2.67574  3.05574 
--------------------------------------------------------------------------------
y 
       n  missing distinct 
     400        0        2 
                        
Value      female   male
Frequency     213    187
Proportion  0.532  0.468
--------------------------------------------------------------------------------
> 
> ## Not run: 
> ##D options(grType='plotly')
> ##D d <- describe(mydata)
> ##D p <- plot(d)   # create plots for both types of variables
> ##D p[[1]]; p[[2]] # or p$Categorical; p$Continuous
> ##D plotly::subplot(p[[1]], p[[2]], nrows=2)  # plot both in one
> ##D plot(d, which='categorical')    # categorical ones
> ##D 
> ##D d <- sas.get(".","mydata",special.miss=TRUE,recode=TRUE)
> ##D describe(d)      #describe entire data frame
> ##D attach(d, 1)
> ##D describe(relig)  #Has special missing values .D .F .M .R .T
> ##D                  #attr(relig,"label") is "Religious preference"
> ##D 
> ##D #relig : Religious preference  Format:relig
> ##D #    n missing  D  F M R T distinct 
> ##D # 4038     263 45 33 7 2 1        8
> ##D #
> ##D #0:none (251, 6%), 1:Jewish (372, 9%), 2:Catholic (1230, 30%) 
> ##D #3:Jehovah's Witnes (25, 1%), 4:Christ Scientist (7, 0%) 
> ##D #5:Seventh Day Adv (17, 0%), 6:Protestant (2025, 50%), 7:other (111, 3%) 
> ##D 
> ##D 
> ##D # Method for describing part of a data frame:
> ##D  describe(death.time ~ age*sex + rcs(blood.pressure))
> ##D  describe(~ age+sex)
> ##D  describe(~ age+sex, weights=freqs)  # weighted analysis
> ##D 
> ##D  fit <- lrm(y ~ age*sex + log(height))
> ##D  describe(formula(fit))
> ##D  describe(y ~ age*sex, na.action=na.delete)   
> ##D # report on number deleted for each variable
> ##D  options(na.detail.response=TRUE)  
> ##D # keep missings separately for each x, report on dist of y by x=NA
> ##D  describe(y ~ age*sex)
> ##D  options(na.fun.response="quantile")
> ##D  describe(y ~ age*sex)   # same but use quantiles of y by x=NA
> ##D 
> ##D  d <- describe(my.data.frame)
> ##D  d$age                   # print description for just age
> ##D  d[c('age','sex')]       # print description for two variables
> ##D  d[sort(names(d))]       # print in alphabetic order by var. names
> ##D  d2 <- d[20:30]          # keep variables 20-30
> ##D  page(d2)                # pop-up window for these variables
> ##D 
> ##D # Test date/time formats and suppression of times when they don't vary
> ##D  library(chron)
> ##D  d <- data.frame(a=chron((1:20)+.1),
> ##D                  b=chron((1:20)+(1:20)/100),
> ##D                  d=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
> ##D                                hour=rep(11,20),min=rep(17,20),sec=rep(11,20)),
> ##D                  f=ISOdatetime(year=rep(2003,20),month=rep(4,20),day=1:20,
> ##D                                hour=1:20,min=1:20,sec=1:20),
> ##D                  g=ISOdate(year=2001:2020,month=rep(3,20),day=1:20))
> ##D  describe(d)
> ##D 
> ##D # Make a function to run describe, latex.describe, and use the kdvi
> ##D # previewer in Linux to view the result and easily make a pdf file
> ##D 
> ##D  ldesc <- function(data) {
> ##D   options(xdvicmd='kdvi')
> ##D   d <- describe(data, desc=deparse(substitute(data)))
> ##D   dvi(latex(d, file='/tmp/z.tex'), nomargins=FALSE, width=8.5, height=11)
> ##D  }
> ##D 
> ##D  ldesc(d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("discrete")
> ### * discrete
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: discrete
> ### Title: Discrete Vector tools
> ### Aliases: as.discrete as.discrete.default discrete [<-.discrete
> ###   [.discrete [[.discrete is.discrete is.na<-.discrete length<-.discrete
> ### Keywords: manip
> 
> ### ** Examples
> 
> a <- discrete(1:25)
> a
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
attr(,"levels")
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
attr(,"class")
[1] "discrete"
> 
> is.discrete(a)
[1] TRUE
> 
> b <- as.discrete(2:4)
> b
[1] 2 3 4
attr(,"levels")
[1] 2 3 4
attr(,"class")
[1] "discrete"
> 
> 
> 
> cleanEx()
> nameEx("dotchart2")
> ### * dotchart2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchart2
> ### Title: Enhanced Dot Chart
> ### Aliases: dotchart2
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(135)
> maj <- factor(c(rep('North',13),rep('South',13)))
> g <- paste('Category',rep(letters[1:13],2))
> n <- sample(1:15000, 26, replace=TRUE)
> y1 <- runif(26)
> y2 <- pmax(0, y1 - runif(26, 0, .1))
> dotchart2(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y')
> dotchart2(y2, g, groups=maj, pch=17, add=TRUE)
> ## Compare with dotchart function (no superpositioning or auxdata allowed):
> ## dotchart(y1, g, groups=maj, xlab='Y')
> 
> ## To plot using a transformed scale add for example
> ## axisat=sqrt(pretty(y)), axislabels=pretty(y)
> 
> 
> 
> cleanEx()
> nameEx("dotchart3")
> ### * dotchart3
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchart3
> ### Title: Enhanced Version of dotchart Function
> ### Aliases: dotchart3 dotchartp summaryD summaryDp
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(135)
> maj <- factor(c(rep('North',13),rep('South',13)))
> g <- paste('Category',rep(letters[1:13],2))
> n <- sample(1:15000, 26, replace=TRUE)
> y1 <- runif(26)
> y2 <- pmax(0, y1 - runif(26, 0, .1))
> dotchart3(cbind(y1,y2), g, groups=maj, auxdata=n, auxtitle='n',
+           xlab='Y', pch=c(1,17))
> ## Compare with dotchart function (no superpositioning or auxdata allowed):
> ## dotchart(y1, g, groups=maj, xlab='Y')
> 
> ## Not run: 
> ##D dotchartp(cbind(y1, y2), g, groups=maj, auxdata=n, auxtitle='n',
> ##D           xlab='Y', gdata=cbind(c(0,.1), c(.23,.44)), auxgdata=c(-1,-2),
> ##D           symbol=c('circle', 'line-ns-open'))
> ##D 
> ##D summaryDp(sbp ~ region + sex + race + cut2(age, g=5), data=mydata)
> ## End(Not run)
> 
> ## Put options(grType='plotly') to have the following use dotchartp
> ## (rlegend will not apply)
> ## Add argument auxwhere='hover' to summaryD or dotchartp to put
> ## aux info in hover text instead of right margin
> summaryD(y1 ~ maj + g, xlab='Mean')
> summaryD(y1 ~ maj + g, groupsummary=FALSE)
> summaryD(y1 ~ g, fmtvals=function(x) sprintf('%4.2f', x))
> Y <- cbind(y1, y2)   # summaryD cannot handle cbind(...) ~ ...
> summaryD(Y  ~ maj + g, fun=function(y) y[1,], symbol=c(1,17))
> rlegend(.1, 26, c('y1','y2'), pch=c(1,17))
> 
> summaryD(y1 ~ maj, fun=function(y) c(Mean=mean(y), n=length(y)),
+          auxvar='n', auxtitle='N')
> 
> 
> 
> cleanEx()
> nameEx("dotchartpl")
> ### * dotchartpl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dotchartpl
> ### Title: Enhanced Version of dotchart Function for plotly
> ### Aliases: dotchartpl
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1)
> ##D d <- expand.grid(major=c('Alabama', 'Alaska', 'Arkansas'),
> ##D                  minor=c('East', 'West'),
> ##D                  group=c('Female', 'Male'),
> ##D                  city=0:2)
> ##D n <- nrow(d)
> ##D d$num   <- round(100*runif(n))
> ##D d$denom <- d$num + round(100*runif(n))
> ##D d$x     <- d$num / d$denom
> ##D d$lower <- d$x - runif(n)
> ##D d$upper <- d$x + runif(n)
> ##D 
> ##D with(d,
> ##D  dotchartpl(x, major, minor, group, city, lower=lower, upper=upper,
> ##D             big=city==0, num=num, denom=denom, xlab='x'))
> ##D 
> ##D # Show half-width confidence intervals for Female - Male differences
> ##D # after subsetting the data to have only one record per
> ##D # state/region/group
> ##D d <- subset(d, city == 0)
> ##D with(d,
> ##D  dotchartpl(x, major, minor, group, num=num, denom=denom,
> ##D             lower=lower, upper=upper, refgroup='Male')
> ##D )
> ##D 
> ##D n <- 500
> ##D set.seed(1)
> ##D d <- data.frame(
> ##D   race         = sample(c('Asian', 'Black/AA', 'White'), n, TRUE),
> ##D   sex          = sample(c('Female', 'Male'), n, TRUE),
> ##D   treat        = sample(c('A', 'B'), n, TRUE),
> ##D   smoking      = sample(c('Smoker', 'Non-smoker'), n, TRUE),
> ##D   hypertension = sample(c('Hypertensive', 'Non-Hypertensive'), n, TRUE),
> ##D   region       = sample(c('North America','Europe','South America',
> ##D                           'Europe', 'Asia', 'Central America'), n, TRUE))
> ##D 
> ##D d <- upData(d, labels=c(race='Race', sex='Sex'))
> ##D 
> ##D dm <- addMarginal(d, region)
> ##D s <- summaryP(race + sex + smoking + hypertension ~
> ##D                 region + treat,  data=dm)
> ##D 
> ##D s$region <- ifelse(s$region == 'All', 'All Regions', as.character(s$region))
> ##D 
> ##D with(s, 
> ##D  dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
> ##D             big=region == 'All Regions', num=freq, denom=denom)
> ##D )
> ##D 
> ##D s2 <- s[- attr(s, 'rows.to.exclude1'), ]
> ##D with(s2, 
> ##D      dotchartpl(freq / denom, major=var, minor=val, group=treat, mult=region,
> ##D                 big=region == 'All Regions', num=freq, denom=denom)
> ##D )
> ##D # Note these plots can be created by plot.summaryP when options(grType='plotly')
> ##D 
> ##D # Plot hazard rates and ratios with confidence limits, on log scale
> ##D d <- data.frame(tx=c('a', 'a', 'b', 'b'),
> ##D                 event=c('MI', 'stroke', 'MI', 'stroke'),
> ##D                 count=c(10, 5, 5, 2),
> ##D                 exposure=c(1000, 1000, 900, 900))
> ##D # There were no zero event counts in this dataset.  In general we
> ##D # want to handle that, hence the 0.5 below
> ##D d <- upData(d, hazard = pmax(0.5, count) / exposure,
> ##D                selog  = sqrt(1. / pmax(0.5, count)),
> ##D                lower  = log(hazard) - 1.96 * selog,
> ##D                upper  = log(hazard) + 1.96 * selog)
> ##D with(d,
> ##D      dotchartpl(log(hazard), minor=event, group=tx, num=count, denom=exposure,
> ##D                 lower=lower, upper=upper,
> ##D                 fun=exp, ifun=log, op='/',
> ##D                 numlabel='events', denomlabel='years',
> ##D                 refgroup='a', xlab='Events Per Person-Year')
> ##D )
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ebpcomp")
> ### * ebpcomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ebpcomp
> ### Title: ebpcomp
> ### Aliases: ebpcomp
> 
> ### ** Examples
> 
> ebpcomp(1:1000)
$segments
$segments$x
   50%    25%    75% 
500.50 250.75 750.25 

$segments$y1
[1] -1.0000000 -0.6666667 -0.6666667

$segments$y2
[1] 1.0000000 0.6666667 0.6666667


$lines
$lines$x
     5%   12.5%   12.5%     25%     25%   37.5%   37.5%   62.5%   62.5%     75% 
 50.950 125.875 125.875 250.750 250.750 375.625 375.625 625.375 625.375 750.250 
    75%   87.5%   87.5%     95%     95%   87.5%   87.5%     75%     75%   62.5% 
750.250 875.125 875.125 950.050 950.050 875.125 875.125 750.250 750.250 625.375 
  62.5%   37.5%   37.5%     25%     25%   12.5%   12.5%      5%      5% 
625.375 375.625 375.625 250.750 250.750 125.875 125.875  50.950  50.950 

$lines$y
 [1]  0.1333333  0.1333333  0.3333333  0.3333333  0.6666667  0.6666667
 [7]  1.0000000  1.0000000  0.6666667  0.6666667  0.3333333  0.3333333
[13]  0.1333333  0.1333333 -0.1333333 -0.1333333 -0.3333333 -0.3333333
[19] -0.6666667 -0.6666667 -1.0000000 -1.0000000 -0.6666667 -0.6666667
[25] -0.3333333 -0.3333333 -0.1333333 -0.1333333  0.1333333


$points
$points$x
[1] 500.5

$points$y
[1] 0

$points$N
[1] 1000


$points2
$points2$x
    1%    99% 
 10.99 990.01 

$points2$y
[1] 0


> 
> 
> 
> cleanEx()
> nameEx("ecdfSteps")
> ### * ecdfSteps
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ecdfSteps
> ### Title: ecdfSteps
> ### Aliases: ecdfSteps
> 
> ### ** Examples
> 
> ecdfSteps(0:10)
$x
 [1] -0.5  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0 10.5

$y
 [1] 0.00000000 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545
 [7] 0.54545455 0.63636364 0.72727273 0.81818182 0.90909091 1.00000000
[13] 1.00000000

> ## Not run: 
> ##D # Use data.table for obtaining ECDFs by country and region
> ##D w <- d[, ecdfSteps(z, extend=c(1,11)), by=.(country, region)]  # d is a DT
> ##D # Use ggplot2 to make one graph with multiple regions' ECDFs
> ##D # and use faceting for countries
> ##D ggplot(w, aes(x, y, color=region)) + geom_step() +
> ##D        facet_wrap(~ country)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("epi")
> ### * epi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mhgr
> ### Title: Miscellaneous Functions for Epidemiology
> ### Aliases: mhgr print.mhgr lrcum print.lrcum
> ### Keywords: category htest
> 
> ### ** Examples
> 
> # Greate Migraine dataset used in Example 28.6 in the SAS PROC FREQ guide
> d <- expand.grid(response=c('Better','Same'),
+                  treatment=c('Active','Placebo'),
+                  sex=c('female','male'))
> d$count <- c(16, 11, 5, 20, 12, 16, 7, 19)
> d
  response treatment    sex count
1   Better    Active female    16
2     Same    Active female    11
3   Better   Placebo female     5
4     Same   Placebo female    20
5   Better    Active   male    12
6     Same    Active   male    16
7   Better   Placebo   male     7
8     Same   Placebo   male    19
> # Expand data frame to represent raw data
> r <- rep(1:8, d$count)
> d <- d[r,]
> with(d, mhgr(response=='Better', treatment, sex))
Mantel-Haenszel Risk Ratio and 0.95 Greenland-Robins Confidence Interval

Common Relative Risk: 2.163597 CI: 1.233568 3.794806 

N in Each Group

group
 Active Placebo 
     55      51 
> 
> # Discrete survival time example, to get Cox-Mantel relative risk and CL
> # From Stokes ME, Davis CS, Koch GG, Categorical Data Analysis Using the
> # SAS System, 2nd Edition, Sectino 17.3, p. 596-599
> #
> # Input data in Table 17.5
> d <- expand.grid(treatment=c('A','P'), center=1:3)
> d$healed2w    <- c(15,15,17,12, 7, 3)
> d$healed4w    <- c(17,17,17,13,17,17)
> d$notHealed4w <- c( 2, 7,10,15,16,18)
> d
  treatment center healed2w healed4w notHealed4w
1         A      1       15       17           2
2         P      1       15       17           7
3         A      2       17       17          10
4         P      2       12       13          15
5         A      3        7       17          16
6         P      3        3       17          18
> # Reformat to the way most people would collect raw data
> d1 <- d[rep(1:6, d$healed2w),]
> d1$time <- '2'
> d1$y <- 1
> d2 <- d[rep(1:6, d$healed4w),]
> d2$time <- '4'
> d2$y <- 1
> d3 <- d[rep(1:6, d$notHealed4w),]
> d3$time <- '4'
> d3$y <- 0
> d <- rbind(d1, d2, d3)
> d$healed2w <- d$healed4w <- d$notHealed4w <- NULL
> d
      treatment center time y
1             A      1    2 1
1.1           A      1    2 1
1.2           A      1    2 1
1.3           A      1    2 1
1.4           A      1    2 1
1.5           A      1    2 1
1.6           A      1    2 1
1.7           A      1    2 1
1.8           A      1    2 1
1.9           A      1    2 1
1.10          A      1    2 1
1.11          A      1    2 1
1.12          A      1    2 1
1.13          A      1    2 1
1.14          A      1    2 1
2             P      1    2 1
2.1           P      1    2 1
2.2           P      1    2 1
2.3           P      1    2 1
2.4           P      1    2 1
2.5           P      1    2 1
2.6           P      1    2 1
2.7           P      1    2 1
2.8           P      1    2 1
2.9           P      1    2 1
2.10          P      1    2 1
2.11          P      1    2 1
2.12          P      1    2 1
2.13          P      1    2 1
2.14          P      1    2 1
3             A      2    2 1
3.1           A      2    2 1
3.2           A      2    2 1
3.3           A      2    2 1
3.4           A      2    2 1
3.5           A      2    2 1
3.6           A      2    2 1
3.7           A      2    2 1
3.8           A      2    2 1
3.9           A      2    2 1
3.10          A      2    2 1
3.11          A      2    2 1
3.12          A      2    2 1
3.13          A      2    2 1
3.14          A      2    2 1
3.15          A      2    2 1
3.16          A      2    2 1
4             P      2    2 1
4.1           P      2    2 1
4.2           P      2    2 1
4.3           P      2    2 1
4.4           P      2    2 1
4.5           P      2    2 1
4.6           P      2    2 1
4.7           P      2    2 1
4.8           P      2    2 1
4.9           P      2    2 1
4.10          P      2    2 1
4.11          P      2    2 1
5             A      3    2 1
5.1           A      3    2 1
5.2           A      3    2 1
5.3           A      3    2 1
5.4           A      3    2 1
5.5           A      3    2 1
5.6           A      3    2 1
6             P      3    2 1
6.1           P      3    2 1
6.2           P      3    2 1
11            A      1    4 1
1.17          A      1    4 1
1.21          A      1    4 1
1.31          A      1    4 1
1.41          A      1    4 1
1.51          A      1    4 1
1.61          A      1    4 1
1.71          A      1    4 1
1.81          A      1    4 1
1.91          A      1    4 1
1.101         A      1    4 1
1.111         A      1    4 1
1.121         A      1    4 1
1.131         A      1    4 1
1.141         A      1    4 1
1.15          A      1    4 1
1.16          A      1    4 1
21            P      1    4 1
2.17          P      1    4 1
2.21          P      1    4 1
2.31          P      1    4 1
2.41          P      1    4 1
2.51          P      1    4 1
2.61          P      1    4 1
2.71          P      1    4 1
2.81          P      1    4 1
2.91          P      1    4 1
2.101         P      1    4 1
2.111         P      1    4 1
2.121         P      1    4 1
2.131         P      1    4 1
2.141         P      1    4 1
2.15          P      1    4 1
2.16          P      1    4 1
31            A      2    4 1
3.17          A      2    4 1
3.21          A      2    4 1
3.31          A      2    4 1
3.41          A      2    4 1
3.51          A      2    4 1
3.61          A      2    4 1
3.71          A      2    4 1
3.81          A      2    4 1
3.91          A      2    4 1
3.101         A      2    4 1
3.111         A      2    4 1
3.121         A      2    4 1
3.131         A      2    4 1
3.141         A      2    4 1
3.151         A      2    4 1
3.161         A      2    4 1
41            P      2    4 1
4.15          P      2    4 1
4.21          P      2    4 1
4.31          P      2    4 1
4.41          P      2    4 1
4.51          P      2    4 1
4.61          P      2    4 1
4.71          P      2    4 1
4.81          P      2    4 1
4.91          P      2    4 1
4.101         P      2    4 1
4.111         P      2    4 1
4.12          P      2    4 1
51            A      3    4 1
5.17          A      3    4 1
5.21          A      3    4 1
5.31          A      3    4 1
5.41          A      3    4 1
5.51          A      3    4 1
5.61          A      3    4 1
5.7           A      3    4 1
5.8           A      3    4 1
5.9           A      3    4 1
5.10          A      3    4 1
5.11          A      3    4 1
5.12          A      3    4 1
5.13          A      3    4 1
5.14          A      3    4 1
5.15          A      3    4 1
5.16          A      3    4 1
61            P      3    4 1
6.18          P      3    4 1
6.21          P      3    4 1
6.3           P      3    4 1
6.4           P      3    4 1
6.5           P      3    4 1
6.6           P      3    4 1
6.7           P      3    4 1
6.8           P      3    4 1
6.9           P      3    4 1
6.10          P      3    4 1
6.11          P      3    4 1
6.12          P      3    4 1
6.13          P      3    4 1
6.14          P      3    4 1
6.15          P      3    4 1
6.16          P      3    4 1
12            A      1    4 0
1.18          A      1    4 0
22            P      1    4 0
2.18          P      1    4 0
2.22          P      1    4 0
2.32          P      1    4 0
2.42          P      1    4 0
2.52          P      1    4 0
2.62          P      1    4 0
32            A      2    4 0
3.18          A      2    4 0
3.22          A      2    4 0
3.32          A      2    4 0
3.42          A      2    4 0
3.52          A      2    4 0
3.62          A      2    4 0
3.72          A      2    4 0
3.82          A      2    4 0
3.92          A      2    4 0
42            P      2    4 0
4.16          P      2    4 0
4.22          P      2    4 0
4.32          P      2    4 0
4.42          P      2    4 0
4.52          P      2    4 0
4.62          P      2    4 0
4.72          P      2    4 0
4.82          P      2    4 0
4.92          P      2    4 0
4.102         P      2    4 0
4.112         P      2    4 0
4.121         P      2    4 0
4.13          P      2    4 0
4.14          P      2    4 0
52            A      3    4 0
5.18          A      3    4 0
5.22          A      3    4 0
5.32          A      3    4 0
5.42          A      3    4 0
5.52          A      3    4 0
5.62          A      3    4 0
5.71          A      3    4 0
5.81          A      3    4 0
5.91          A      3    4 0
5.101         A      3    4 0
5.111         A      3    4 0
5.121         A      3    4 0
5.131         A      3    4 0
5.141         A      3    4 0
5.151         A      3    4 0
62            P      3    4 0
6.19          P      3    4 0
6.22          P      3    4 0
6.31          P      3    4 0
6.41          P      3    4 0
6.51          P      3    4 0
6.61          P      3    4 0
6.71          P      3    4 0
6.81          P      3    4 0
6.91          P      3    4 0
6.101         P      3    4 0
6.111         P      3    4 0
6.121         P      3    4 0
6.131         P      3    4 0
6.141         P      3    4 0
6.151         P      3    4 0
6.161         P      3    4 0
6.17          P      3    4 0
> # Finally, duplicate appropriate observations to create 2 and 4-week
> # risk sets.  Healed and not healed at 4w need to be in the 2-week
> # risk set as not healed
> d2w      <- subset(d, time=='4')
> d2w$time <- '2'
> d2w$y    <- 0
> d24      <- rbind(d, d2w)
> with(d24, table(y, treatment, time, center))
, , time = 2, center = 1

   treatment
y    A  P
  0 19 24
  1 15 15

, , time = 4, center = 1

   treatment
y    A  P
  0  2  7
  1 17 17

, , time = 2, center = 2

   treatment
y    A  P
  0 27 28
  1 17 12

, , time = 4, center = 2

   treatment
y    A  P
  0 10 15
  1 17 13

, , time = 2, center = 3

   treatment
y    A  P
  0 33 35
  1  7  3

, , time = 4, center = 3

   treatment
y    A  P
  0 16 18
  1 17 17

> # Matches Table 17.6
> 
> with(d24, mhgr(y, treatment, interaction(center, time, sep=';')))
Mantel-Haenszel Risk Ratio and 0.95 Greenland-Robins Confidence Interval

Common Relative Risk: 1.255945 CI: 1.012105 1.558532 

N in Each Group

group
  A   P 
197 204 
> 
> # Get cumulative likelihood ratios and their 0.95 confidence intervals
> # based on the following two tables
> #
> #          Disease       Disease
> #          +     -       +     -
> # Test +   39    3       20    5
> # Test -   21   17       22   15
> 
> lrcum(c(39,20), c(3,5), c(21,22), c(17,15))
   LR+ Lower 0.95 Upper 0.95 Cum. LR+ Lower 0.95 Upper 0.95
 4.333      1.502     12.503    4.333      1.502     12.503
 1.905      0.837      4.336    8.254      2.158     31.571

   LR- Lower 0.95 Upper 0.95 Cum. LR- Lower 0.95 Upper 0.95
 0.412      0.279      0.609    0.412      0.279      0.609
 0.698      0.476      1.025    0.288      0.166      0.497
> 
> 
> 
> cleanEx()
> nameEx("equalBins")
> ### * equalBins
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: equalBins
> ### Title: Multicolumn Formating
> ### Aliases: equalBins
> ### Keywords: print
> 
> ### ** Examples
> 
> mcols <- c("Group 1", "Group 2")
> mwidth <- nchar(mcols, type="width")
> spancols <- c(3,3)
> ccols <- c("a", "deer", "ad", "cat", "help", "bob")
> cwidth <- nchar(ccols, type="width")
> 
> subwidths <- partition.vector(cwidth, spancols)
> 
> equalBins(mwidth, subwidths)
11 12 13 21 22 23 
 1  4  2  3  4  3 
> 
> 
> 
> cleanEx()
> nameEx("errbar")
> ### * errbar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: errbar
> ### Title: Plot Error Bars
> ### Aliases: errbar
> ### Keywords: hplot
> 
> ### ** Examples
> 
> set.seed(1)
> x <- 1:10
> y <- x + rnorm(10)
> delta <- runif(10)
> errbar( x, y, y + delta, y - delta )
> 
> 
> # Show bootstrap nonparametric CLs for 3 group means and for
> # pairwise differences on same graph
> group <- sample(c('a','b','d'), 200, TRUE)
> y     <- runif(200) + .25*(group=='b') + .5*(group=='d')
> cla <- smean.cl.boot(y[group=='a'],B=100,reps=TRUE)  # usually B=1000
> a   <- attr(cla,'reps')
> clb <- smean.cl.boot(y[group=='b'],B=100,reps=TRUE)
> b   <- attr(clb,'reps')
> cld <- smean.cl.boot(y[group=='d'],B=100,reps=TRUE)
> d   <- attr(cld,'reps')
> a.b <- quantile(a-b,c(.025,.975))
> a.d <- quantile(a-d,c(.025,.975))
> b.d <- quantile(b-d,c(.025,.975))
> errbar(c('a','b','d','a - b','a - d','b - d'),
+        c(cla[1],clb[1],cld[1],cla[1]-clb[1],cla[1]-cld[1],clb[1]-cld[1]),
+        c(cla[3],clb[3],cld[3],a.b[2],a.d[2],b.d[2]),
+        c(cla[2],clb[2],cld[2],a.b[1],a.d[1],b.d[1]),
+        Type=c(1,1,1,2,2,2), xlab='', ylab='')
>        
> 
> 
> 
> cleanEx()
> nameEx("escapeRegex")
> ### * escapeRegex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: escapeRegex
> ### Title: Escapes any characters that would have special meaning in a
> ###   reqular expression.
> ### Aliases: escapeRegex escapeBS
> ### Keywords: manip character programming
> 
> ### ** Examples
> 
> string <- "this\\(system) {is} [full]."
> escapeRegex(string)
[1] "this\\\\\\(system\\) \\{is\\} \\[full\\]\\."
> 
> escapeBS(string)
[1] "this\\\\(system) {is} [full]."
> 
> ## Don't show: 
> if(!any(grep(escapeRegex(string), string))) {
+   stop("function escapeRegex failed test")
+ }
> 
> if(escapeBS(string) != "this\\\\(system) {is} [full].") {
+   stop("function escapeBS failed test")
+ }
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("estSeqSim")
> ### * estSeqSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: estSeqSim
> ### Title: estSeqSim
> ### Aliases: estSeqSim
> 
> ### ** Examples
> 
> if (requireNamespace("rms", quietly = TRUE)) {
+   # Run 100 simulations, 5 looks, 2 true parameter values
+   # Total simulation time: 2s
+   lfit <- function(x, y) {
+   f <- rms::lrm.fit(x, y)
+     k <- length(coef(f))
+     c(coef(f)[k], vcov(f)[k, k])
+   }
+   gdat <- function(beta, n1, n2) {
+     # Cell probabilities for a 7-category ordinal outcome for the control group
+     p <- c(2, 1, 2, 7, 8, 38, 42) / 100
+ 
+     # Compute cell probabilities for the treated group
+     p2 <- pomodm(p=p, odds.ratio=exp(beta))
+     y1 <- sample(1 : 7, n1, p,  replace=TRUE)
+     y2 <- sample(1 : 7, n2, p2, replace=TRUE)
+     list(y1=y1, y2=y2)
+   }
+ 
+   set.seed(1)
+   est <- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
+                     gendat=gdat,
+                     fitter=lfit, nsim=100)
+   head(est)
+ }
  sim  parameter look         est       vest
1   1  0.0000000   50 -0.07834606 0.27969284
2   1  0.0000000   75 -0.48653077 0.18703883
3   1  0.0000000   95 -0.30167932 0.14752954
4   1  0.0000000  100 -0.26988751 0.14021910
5   1  0.0000000  200 -0.43421591 0.07182057
6   1 -0.3566749   50  0.10563461 0.27823395
> 
> 
> 
> cleanEx()
> nameEx("event.chart")
> ### * event.chart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.chart
> ### Title: Flexible Event Chart for Time-to-Event Data
> ### Aliases: event.chart
> ### Keywords: hplot survival
> 
> ### ** Examples
> 
> # The sample data set is an augmented CDC AIDS dataset (ASCII)
> # which is used in the examples in the help file.  This dataset is 
> # described in Kalbfleisch and Lawless (JASA, 1989).
> # Here, we have included only children 4 years old and younger.
> # We have also added a new field, dethdate, which
> # represents a fictitious death date for each patient.  There was
> # no recording of death date on the original dataset.  In addition, we have
> # added a fictitious viral load reading (copies/ml) for each patient at time of AIDS diagnosis,
> # noting viral load was also not part of the original dataset.
> #   
> # All dates are julian with julian=0 being 
> # January 1, 1960, and julian=14000 being 14000 days beyond
> # January 1, 1960 (i.e., May 1, 1998).
> 
> 
> cdcaids <- data.frame(
+ age=c(4,2,1,1,2,2,2,4,2,1,1,3,2,1,3,2,1,2,4,2,2,1,4,2,4,1,4,2,1,1,3,3,1,3),
+ infedate=c(
+ 7274,7727,7949,8037,7765,8096,8186,7520,8522,8609,8524,8213,8455,8739,
+ 8034,8646,8886,8549,8068,8682,8612,9007,8461,8888,8096,9192,9107,9001,
+ 9344,9155,8800,8519,9282,8673),
+ diagdate=c(
+ 8100,8158,8251,8343,8463,8489,8554,8644,8713,8733,8854,8855,8863,8983,
+ 9035,9037,9132,9164,9186,9221,9224,9252,9274,9404,9405,9433,9434,9470,
+ 9470,9472,9489,9500,9585,9649),
+ diffdate=c(
+ 826,431,302,306,698,393,368,1124,191,124,330,642,408,244,1001,391,246,
+ 615,1118,539,612,245,813,516,1309,241,327,469,126,317,689,981,303,976),
+ dethdate=c(
+ 8434,8304,NA,8414,8715,NA,8667,9142,8731,8750,8963,9120,9005,9028,9445,
+ 9180,9189,9406,9711,9453,9465,9289,9640,9608,10010,9488,9523,9633,9667,
+ 9547,9755,NA,9686,10084),
+ censdate=c(
+ NA,NA,8321,NA,NA,8519,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,
+ NA,NA,NA,NA,NA,NA,NA,NA,NA,10095,NA,NA),
+ viralload=c(
+ 13000,36000,70000,90000,21000,110000,75000,12000,125000,110000,13000,39000,79000,135000,14000,
+ 42000,123000,20000,12000,18000,16000,140000,16000,58000,11000,120000,85000,31000,24000,115000,
+ 17000,13100,72000,13500)
+ )
> 
> cdcaids <- upData(cdcaids,
+  labels=c(age     ='Age, y', infedate='Date of blood transfusion',
+           diagdate='Date of AIDS diagnosis',
+           diffdate='Incubation period (days from HIV to AIDS)',
+           dethdate='Fictitious date of death',
+           censdate='Fictitious censoring date',
+ 	  viralload='Fictitious viral load'))
Input object size:	 3416 bytes;	 7 variables	 34 observations
New object size:	6264 bytes;	7 variables	34 observations
> 
> 
> # Note that the style options listed with these
> # examples are best suited for output to a postscript file (i.e., using
> # the postscript function with horizontal=TRUE) as opposed to a graphical
> # window (e.g., motif).
> 
> 
> # To produce simple calendar event chart (with internal legend):
> # postscript('example1.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'observation dates',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data calendar event chart 1',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   legend.point.at = list(c(7210, 8100), c(35, 27)), legend.bty='o')
> 
> 
> # To produce simple interval event chart (with internal legend):
> # postscript('example2.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data interval event chart 1',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))
> 
> 
> # To produce simple interval event chart (with internal legend),
> # but now with flexible diagdate symbol size based on viral load variable:
> # postscript('example2a.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)',
+   y.lab='patients (sorted by AIDS diagnosis date)',
+   titl='AIDS data interval event chart 1a, with viral load at diagdate represented',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   point.cex.mult = 0.00002, point.cex.mult.var = 'viralload', extra.points.no.mult = c(1,NA,1,1), 
+   legend.plot=TRUE, legend.location='i', legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1400, 1950), c(7, -1)))
> 
> 
> # To produce more complicated interval chart which is
> # referenced by infection date, and sorted by age and incubation period:
> # postscript('example3.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since diagnosis of AIDS (in days)',
+   y.lab='patients (sorted by age and incubation length)',
+   titl='AIDS data interval event chart 2 (sorted by age, incubation)',
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8),
+   legend.plot=TRUE, legend.location='i',legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='diagdate', x.julian=TRUE, sort.by=c('age','diffdate'),
+   line.by='age', line.lty=c(1,3,2,4), line.lwd=rep(1,4), line.col=rep(1,4),
+   legend.bty='o', legend.point.at = list(c(-1350, -800), c(7, -1)),
+   legend.line.at = list(c(-1350, -800), c(16, 8)),
+   legend.line.text=c('age = 1', '       = 2', '       = 3', '       = 4'))
> 
> 
> # To produce the Goldman chart:
> # postscript('example4.ps', horizontal=TRUE)
>  event.chart(cdcaids,
+   subset.c=c('infedate','diagdate','dethdate','censdate'),
+   x.lab = 'time since transfusion (in days)', y.lab='dates of observation',
+   titl='AIDS data Goldman event chart 1',
+   y.var = c('infedate'), y.var.type='d', now.line=TRUE, y.jitter=FALSE,
+   point.pch=c(1,2,15,0), point.cex=c(1,1,0.8,0.8), mgp = c(3.1,1.6,0),
+   legend.plot=TRUE, legend.location='i',legend.cex=1.0,
+   legend.point.text=c('transfusion','AIDS diagnosis','death','censored'),
+   x.reference='infedate', x.julian=TRUE,
+   legend.bty='o', legend.point.at = list(c(1500, 2800), c(9300, 10000)))
> 
> 
> # To convert coded time-to-event data, then, draw an event chart:
> surv.time <- c(5,6,3,1,2)
> cens.ind   <- c(1,0,1,1,0)
> surv.data  <- cbind(surv.time,cens.ind)
> event.data <- event.convert(surv.data)
> event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
> 
> 
> 
> cleanEx()
> nameEx("event.convert")
> ### * event.convert
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.convert
> ### Title: Event Conversion for Time-to-Event Data
> ### Aliases: event.convert
> ### Keywords: hplot survival
> 
> ### ** Examples
> 
> # To convert coded time-to-event data, then, draw an event chart:
> surv.time <- c(5,6,3,1,2)
> cens.ind   <- c(1,0,1,1,0)
> surv.data  <- cbind(surv.time,cens.ind)
> event.data <- event.convert(surv.data)
> event.chart(cbind(rep(0,5),event.data),x.julian=TRUE,x.reference=1)
> 
> 
> 
> cleanEx()
> nameEx("event.history")
> ### * event.history
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: event.history
> ### Title: Produces event.history graph for survival data
> ### Aliases: event.history
> ### Keywords: survival
> 
> ### ** Examples
> 
> # Code to produce event history graphs for SIM paper
> #
> # before generating plots, some pre-processing needs to be performed,
> #  in order to get dataset in proper form for event.history function;
> #  need to create one line per subject and sort by time under observation, 
> #  with those experiencing event coming before those tied with censoring time;
> require('survival')
Loading required package: survival
> data(heart)
> 
> # creation of event.history version of heart dataset (call heart.one):
> 
> heart.one <- matrix(nrow=length(unique(heart$id)), ncol=8)
> for(i in 1:length(unique(heart$id)))
+  {
+   if(length(heart$id[heart$id==i]) == 1)
+    heart.one[i,] <- as.numeric(unlist(heart[heart$id==i, ]))
+   else if(length(heart$id[heart$id==i]) == 2)
+    heart.one[i,] <- as.numeric(unlist(heart[heart$id==i,][2,]))
+  }
> 
> heart.one[,3][heart.one[,3] == 0] <- 2 	## converting censored events to 2, from 0
> if(is.factor(heart$transplant))
+  heart.one[,7] <- heart.one[,7] - 1
>  ## getting back to correct transplantation coding
> heart.one <- as.data.frame(heart.one[order(unlist(heart.one[,2]), unlist(heart.one[,3])),])
> names(heart.one) <- names(heart)
> # back to usual censoring indicator:
> heart.one[,3][heart.one[,3] == 2] <- 0 
> # note: transplant says 0 (for no transplants) or 1 (for one transplant)
> #        and event = 1 is death, while event = 0 is censored
> 
> # plot single Kaplan-Meier curve from heart data, first creating survival object
> heart.surv <- survfit(Surv(stop, event) ~ 1, data=heart.one, conf.int = FALSE)
> 
> # figure 3: traditional Kaplan-Meier curve
> # postscript('ehgfig3.ps', horiz=TRUE)
> # omi <- par(omi=c(0,1.25,0.5,1.25))
>  plot(heart.surv, ylab='estimated survival probability',
+       xlab='observation time (in days)')
>  title('Figure 3: Kaplan-Meier curve for Stanford data', cex=0.8)
> # dev.off()
> 
> ## now, draw event history graph for Stanford heart data; use as Figure 4
> 
> # postscript('ehgfig4.ps', horiz=TRUE, colors = seq(0, 1, len=20))
> # par(omi=c(0,1.25,0.5,1.25))
>  event.history(heart.one, 
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),
+ 		x.lab = 'time under observation (in days)',
+ 		title='Figure 4: Event history graph for\nStanford data',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 30.0, cens.mark.cex = 0.85)
> # dev.off()
> 
> 
> 
> # now, draw age-stratified event history graph for Stanford heart data; 
> #  use as Figure 5
> 
> # two plots, stratified by age status
> # postscript('c:\temp\ehgfig5.ps', horiz=TRUE, colors = seq(0, 1, len=20))
> # par(omi=c(0,1.25,0.5,1.25))
>  par(mfrow=c(1,2))
> 
>  event.history(data=heart.one, subset.rows = (heart.one[,4] < 0),
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),  
+ 		x.lab = 'time under observation\n(in days)',
+ 		title = 'Figure 5a:\nStanford data\n(age < 48)',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
+ 		xlim=c(0,1900))
> 
>  event.history(data=heart.one, subset.rows = (heart.one[,4] >= 0),
+ 		survtime.col=heart.one[,2], surv.col=heart.one[,3],
+ 		covtime.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,1]),
+ 		cov.cols = cbind(rep(0, dim(heart.one)[1]), heart.one[,7]),
+ 		num.colors=2, colors=c(6,10),
+ 		x.lab = 'time under observation\n(in days)',
+ 		title = 'Figure 5b:\nStanford data\n(age >= 48)',
+ 		cens.mark.right =TRUE, cens.mark = '-', 
+ 		cens.mark.ahead = 40.0, cens.mark.cex = 0.85,
+ 		xlim=c(0,1900))
> # dev.off()
> # par(omi=omi)
> 
> # we will not show liver cirrhosis data manipulation, as it was 
> #  a bit detailed; however, here is the 
> #  event.history code to produce Figure 7 / Plate 1
> 
> # Figure 7 / Plate 1 : prothrombin ehg with color
> ## Not run: 
> ##D second.arg <- 1				### second.arg is for shading
> ##D third.arg <- c(rep(1,18),0,1)		### third.arg is for intensity
> ##D 
> ##D # postscript('c:\temp\ehgfig7.ps', horiz=TRUE, 
> ##D # colors = cbind(seq(0, 1, len = 20), second.arg, third.arg)) 
> ##D # par(omi=c(0,1.25,0.5,1.25), col=19)
> ##D  event.history(cirrhos2.eh, subset.rows = NULL,
> ##D                survtime.col=cirrhos2.eh$time, surv.col=cirrhos2.eh$event,
> ##D 		covtime.cols = as.matrix(cirrhos2.eh[, ((2:18)*2)]),
> ##D 		cov.cols = as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
> ##D 		cut.cov =  as.numeric(quantile(as.matrix(cirrhos2.eh[, ((2:18)*2) + 1]),
> ##D 				c(0,.2,.4,.6,.8,1), na.rm=TRUE) + c(-1,0,0,0,0,1)),	
> ##D  		colors=c(20,4,8,11,14),
> ##D 		x.lab = 'time under observation (in days)',
> ##D 		title='Figure 7: Event history graph for liver cirrhosis data (color)',
> ##D 		cens.mark.right =TRUE, cens.mark = '-', 
> ##D 		cens.mark.ahead = 100.0, cens.mark.cex = 0.85)
> ##D # dev.off()
> ## End(Not run)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:survival’

> nameEx("extractlabs")
> ### * extractlabs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extractlabs
> ### Title: extractlabs
> ### Aliases: extractlabs
> 
> ### ** Examples
> 
> d <- data.frame(x=1:10, y=(1:10)/10)
> d <- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
> d2 <- d
> units(d2$x) <- 'cm'
> LabelsUnits <- extractlabs(d, d2)
Warning in extractlabs(d, d2) :
  1 variables have conflicting labels/units from different datasets
Variable names with inconsistent attributes:
Key: <name>
     name  label  units
   <char> <char> <char>
1:      x      X   mmHg
2:      x      X     cm

> LabelsUnits
Key: <name>
     name  label  units
   <char> <char> <char>
1:      x      X   mmHg
2:      x      X     cm
3:      y      Y       
> 
> 
> 
> cleanEx()
> nameEx("fImport")
> ### * fImport
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fImport
> ### Title: fImport
> ### Aliases: fImport
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Get a Stata dataset
> ##D d <- fImport('http://www.principlesofeconometrics.com/stata/alcohol.dta')
> ##D contents(d)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("find.matches")
> ### * find.matches
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: find.matches
> ### Title: Find Close Matches
> ### Aliases: find.matches summary.find.matches print.find.matches
> ###   matchCases
> ### Keywords: math multivariate htest
> 
> ### ** Examples
> 
> y <- rbind(c(.1, .2),c(.11, .22), c(.3, .4), c(.31, .41), c(.32, 5))
> x <- rbind(c(.09,.21), c(.29,.39))
> y
     [,1] [,2]
[1,] 0.10 0.20
[2,] 0.11 0.22
[3,] 0.30 0.40
[4,] 0.31 0.41
[5,] 0.32 5.00
> x
     [,1] [,2]
[1,] 0.09 0.21
[2,] 0.29 0.39
> w <- find.matches(x, y, maxmatch=5, tol=c(.05,.05))
> 
> 
> set.seed(111)       # so can replicate results
> x <- matrix(runif(500), ncol=2)
> y <- matrix(runif(2000), ncol=2)
> w <- find.matches(x, y, maxmatch=5, tol=c(.02,.03))
> w$matches[1:5,]
     Match #1 Match #2 Match #3 Match #4 Match #5
[1,]      999      694        0        0        0
[2,]        0        0        0        0        0
[3,]      235        0        0        0        0
[4,]      964      139        0        0        0
[5,]      906      427      204        0        0
> w$distance[1:5,]
     Distance #1 Distance #2 Distance #3 Distance #4 Distance #5
[1,]   0.1042884   0.1562084          NA          NA          NA
[2,]          NA          NA          NA          NA          NA
[3,]   0.7272258          NA          NA          NA          NA
[4,]   0.2815041   0.7973284          NA          NA          NA
[5,]   0.6135293   0.7162828   0.7189297          NA          NA
> # Find first x with 3 or more y-matches
> num.match <- apply(w$matches, 1, function(x)sum(x > 0))
> j <- ((1:length(num.match))[num.match > 2])[1]
> x[j,]
[1] 0.3776632 0.6833354
> y[w$matches[j,],]
          [,1]      [,2]
[1,] 0.3708767 0.7045144
[2,] 0.3687917 0.7049588
[3,] 0.3821378 0.7078709
> 
> 
> summary(w)
Frequency table of number of matches found per observation

m
 0  1  2  3  4  5 
27 53 64 54 32 20 

Median minimum distance by number of matches

        1         2         3         4         5 
0.5859325 0.3376432 0.1917933 0.1407859 0.1398928 

Observations selected first more than once (with frequencies)


 57  73  91 101 116 165 191 251 256 292 415 422 438 443 467 552 592 593 650 691 
  2   2   2   2   2   3   2   2   2   3   2   2   2   2   2   2   2   2   2   2 
719 733 747 754 818 820 824 849 871 926 945 964 970 
  2   2   2   2   2   2   3   2   2   2   2   2   2 
> 
> 
> # For many applications would do something like this:
> # attach(df1)
> # x <- cbind(age, sex) # Just do as.matrix(df1) if df1 has no factor objects
> # attach(df2)
> # y <- cbind(age, sex)
> # mat <- find.matches(x, y, tol=c(5,0)) # exact match on sex, 5y on age
> 
> 
> # Demonstrate matchCases
> xcase     <- c(1,3,5,12)
> xcontrol  <- 1:6
> idcase    <- c('A','B','C','D')
> idcontrol <- c('a','b','c','d','e','f')
> ycase     <- c(11,33,55,122)
> ycontrol  <- c(11,22,33,44,55,66)
> matchCases(xcase, ycase, idcase,
+            xcontrol, ycontrol, idcontrol, tol=1)

Frequencies of Number of Matched Controls per Case:

matches
0 2 3 
1 1 2 

   idcase    type id x  y
1       A    case  A 1 11
2       A control  a 1 11
3       A control  b 2 22
4       B    case  B 3 33
5       B control  b 2 22
6       B control  c 3 33
7       B control  d 4 44
8       C    case  C 5 55
9       C control  d 4 44
10      C control  e 5 55
11      C control  f 6 66
> 
> 
> # If y is a binary response variable, the following code
> # will produce a Mantel-Haenszel summary odds ratio that 
> # utilizes the matching.
> # Standard variance formula will not work here because
> # a control will match more than one case
> # WARNING: The M-H procedure exemplified here is suspect 
> # because of the small strata and widely varying number
> # of controls per case.
> 
> 
> x    <- c(1, 2, 3, 3, 3, 6, 7, 12,  1, 1:7)
> y    <- c(0, 0, 0, 1, 0, 1, 1,  1,  1, 0, 0, 0, 0, 1, 1, 1)
> case <- c(rep(TRUE, 8), rep(FALSE, 8))
> id   <- 1:length(x)
> 
> 
> m <- matchCases(x[case],  y[case],  id[case],
+                 x[!case], y[!case], id[!case], tol=1)

Frequencies of Number of Matched Controls per Case:

matches
0 2 3 4 
1 1 5 1 

> iscase <- m$type=='case'
> # Note: the first tapply on insures that event indicators are
> # sorted by case id.  The second actually does something.
> event.case    <- tapply(m$y[iscase],  m$idcase[iscase],  sum)
> event.control <- tapply(m$y[!iscase], m$idcase[!iscase], sum)
> n.control     <- tapply(!iscase,      m$idcase,          sum)
> n             <- tapply(m$y,          m$idcase,          length)
> or <- sum(event.case * (n.control - event.control) / n) /
+       sum(event.control * (1 - event.case) / n)
> or
[1] 1.666667
> 
> 
> # Bootstrap this estimator by sampling with replacement from
> # subjects.  Assumes id is unique when combine cases+controls
> # (id was constructed this way above).  The following algorithms
> # puts all sampled controls back with the cases to whom they were
> # originally matched.
> 
> 
> ids <- unique(m$id)
> idgroups <- split(1:nrow(m), m$id)
> B   <- 50   # in practice use many more
> ors <- numeric(B)
> # Function to order w by ids, leaving unassigned elements zero
> align <- function(ids, w) {
+   z <- structure(rep(0, length(ids)), names=ids)
+   z[names(w)] <- w
+   z
+ }
> for(i in 1:B) {
+   j <- sample(ids, replace=TRUE)
+   obs <- unlist(idgroups[j])
+   u <- m[obs,]
+   iscase <- u$type=='case'
+   n.case <- align(ids, tapply(u$type, u$idcase, 
+                               function(v)sum(v=='case')))
+   n.control <- align(ids, tapply(u$type, u$idcase,
+                                  function(v)sum(v=='control')))
+   event.case <- align(ids, tapply(u$y[iscase],  u$idcase[iscase],  sum))
+   event.control <- align(ids, tapply(u$y[!iscase], u$idcase[!iscase], sum))
+   n <- n.case + n.control
+   # Remove sets having 0 cases or 0 controls in resample
+   s             <- n.case > 0 & n.control > 0
+   denom <- sum(event.control[s] * (n.case[s] - event.case[s]) / n[s])
+   or <- if(denom==0) NA else 
+    sum(event.case[s] * (n.control[s] - event.control[s]) / n[s]) / denom
+   ors[i] <- or
+ }
> describe(ors)
ors 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
      25       25       14    0.936    1.442        1    1.935   0.0000 
     .10      .25      .50      .75      .90      .95 
  0.0000   0.0000   0.9375   1.6667   3.6000   5.0667 

0 (10, 0.40), 0.5 (1, 0.04), 0.777777777777778 (1, 0.04), 0.9375 (1, 0.04),
1.16666666666667 (1, 0.04), 1.2 (1, 0.04), 1.5 (2, 0.08), 1.66666666666667 (2,
0.08), 2 (1, 0.04), 2.4 (1, 0.04), 3 (1, 0.04), 4 (1, 0.04), 5.33333333333333
(1, 0.04), 8.4 (1, 0.04)

For the frequency table, variable is rounded to the nearest 0
> 
> 
> 
> cleanEx()
> nameEx("first.word")
> ### * first.word
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: first.word
> ### Title: First Word in a String or Expression
> ### Aliases: first.word
> ### Keywords: character manip
> 
> ### ** Examples
> 
> first.word(expr=expression(y ~ x + log(w)))
[1] "y"
> 
> 
> 
> cleanEx()
> nameEx("format.df")
> ### * format.df
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: format.df
> ### Title: Format a Data Frame or Matrix for LaTeX or HTML
> ### Aliases: format.df
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- data.frame(a=1:2, b=3:4)
> ##D x$m <- 10000*matrix(5:8,nrow=2)
> ##D names(x)
> ##D dim(x)
> ##D x
> ##D format.df(x, big.mark=",")
> ##D dim(format.df(x))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("format.pval")
> ### * format.pval
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: format.pval
> ### Title: Format P Values
> ### Aliases: format.pval
> ### Keywords: print
> 
> ### ** Examples
> 
> format.pval(c(runif(5), pi^-100, NA))
[1] "0.26551" "0.37212" "0.57285" "0.90821" "0.20168" "< 2e-16" "NA"     
> format.pval(c(0.1, 0.0001, 1e-27))
[1] "1e-01"  "1e-04"  "<2e-16"
> format.pval(c(0.1, 1e-27), nsmall=3)
[1] "0.100"  "<2e-16"
> 
> 
> 
> cleanEx()
> nameEx("gbayes")
> ### * gbayes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbayes
> ### Title: Gaussian Bayesian Posterior and Predictive Distributions
> ### Aliases: gbayes plot.gbayes gbayes2 gbayesMixPredNoData gbayesMixPost
> ###   gbayesMixPowerNP gbayes1PowerNP
> ### Keywords: htest
> 
> ### ** Examples
> 
> # Compare 2 proportions using the var stabilizing transformation
> # arcsin(sqrt((x+3/8)/(n+3/4))) (Anscombe), which has variance 
> # 1/[4(n+.5)]
> 
> 
> m1 <- 100;     m2 <- 150
> deaths1 <- 10; deaths2 <- 30
> 
> 
> f <- function(events,n) asin(sqrt((events+3/8)/(n+3/4)))
> stat <- f(deaths1,m1) - f(deaths2,m2)
> var.stat <- function(m1, m2) 1/4/(m1+.5) + 1/4/(m2+.5)
> cat("Test statistic:",format(stat),"  s.d.:",
+     format(sqrt(var.stat(m1,m2))), "\n")
Test statistic: -0.1388297   s.d.: 0.06441034 
> #Use unbiased prior with variance 1000 (almost flat)
> b <- gbayes(0, 1000, m1, m2, stat, var.stat, 2*m1, 2*m2)
> print(b)
$mean.prior
[1] 0

$var.prior
[1] 1000

$mean.post
[1] -0.1388291

$var.post
[1] 0.004148675

$mean.pred
[1] -0.1388291

$var.pred
[1] 0.006227504

attr(,"class")
[1] "gbayes"
> plot(b)
> #To get posterior Prob[parameter > w] use 
> # 1-pnorm(w, b$mean.post, sqrt(b$var.post))
> 
> 
> #If g(effect, n1, n2) is the power function to
> #detect an effect of 'effect' with samples size for groups 1 and 2
> #of n1,n2, estimate the expected power by getting 1000 random
> #draws from the posterior distribution, computing power for
> #each value of the population effect, and averaging the 1000 powers
> #This code assumes that g will accept vector-valued 'effect'
> #For the 2-sample proportion problem just addressed, 'effect'
> #could be taken approximately as the change in the arcsin of
> #the square root of the probability of the event
> 
> 
> g <- function(effect, n1, n2, alpha=.05) {
+   sd <- sqrt(var.stat(n1,n2))
+   z <- qnorm(1 - alpha/2)
+   effect <- abs(effect)
+   1 - pnorm(z - effect/sd) + pnorm(-z - effect/sd)
+ }
> 
> 
> effects <- rnorm(1000, b$mean.post, sqrt(b$var.post))
> powers <- g(effects, 500, 500)
> hist(powers, nclass=35, xlab='Power')
> describe(powers)
powers 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
    1000        0      997        1   0.8567   0.9476   0.2236   0.1581 
     .10      .25      .50      .75      .90      .95 
  0.4029   0.8509   0.9939   0.9999   1.0000   1.0000 

lowest : 0.0500474 0.0500896 0.0501411 0.0505619 0.0512243
highest: 1         1         1         1         1        
> 
> 
> 
> 
> # gbayes2 examples
> # First consider a study with a binary response where the
> # sample size is n1=500 in the new treatment arm and n2=300
> # in the control arm.  The parameter of interest is the 
> # treated:control log odds ratio, which has variance
> # 1/[n1 p1 (1-p1)] + 1/[n2 p2 (1-p2)].  This is not
> # really constant so we average the variance over plausible
> # values of the probabilities of response p1 and p2.  We
> # think that these are between .4 and .6 and we take a 
> # further short cut
> 
> 
> v <- function(n1, n2, p1, p2) 1/(n1*p1*(1-p1)) + 1/(n2*p2*(1-p2))
> n1 <- 500; n2 <- 300
> ps <- seq(.4, .6, length=100)
> vguess <- quantile(v(n1, n2, ps, ps), .75)
> vguess
       75% 
0.02183459 
> #        75% 
> # 0.02183459
> 
> 
> # The minimally interesting treatment effect is an odds ratio
> # of 1.1.  The prior distribution on the log odds ratio is
> # a 50:50 mixture of a vague Gaussian (mean 0, sd 100) and
> # an informative prior from a previous study (mean 1, sd 1)
> 
> 
> prior <- function(delta) 
+   0.5*dnorm(delta, 0, 100)+0.5*dnorm(delta, 1, 1)
> deltas <- seq(-5, 5, length=150)
> plot(deltas, prior(deltas), type='l')
> 
> 
> # Now compute the power, averaged over this prior
> gbayes2(sqrt(vguess), prior, log(1.1))
[1] 0.6133338
> # [1] 0.6133338
> 
> 
> # See how much power is lost by ignoring the previous
> # study completely
> 
> 
> gbayes2(sqrt(vguess), function(delta)dnorm(delta, 0, 100), log(1.1))
[1] 0.4984588
> # [1] 0.4984588
> 
> 
> # What happens to the power if we really don't believe the treatment
> # is very effective?  Let's use a prior distribution for the log
> # odds ratio that is uniform between log(1.2) and log(1.3).
> # Also check the power against a true null hypothesis
> 
> 
> prior2 <- function(delta) dunif(delta, log(1.2), log(1.3))
> gbayes2(sqrt(vguess), prior2, log(1.1))
[1] 0.1385113
> # [1] 0.1385113
> 
> 
> gbayes2(sqrt(vguess), prior2, 0)
[1] 0.3264065
> # [1] 0.3264065
> 
> 
> # Compare this with the power of a two-sample binomial test to
> # detect an odds ratio of 1.25
> bpower(.5, odds.ratio=1.25, n1=500, n2=300)
    Power 
0.3307486 
> #     Power 
> # 0.3307486
> 
> 
> # For the original prior, consider a new study with equal
> # sample sizes n in the two arms.  Solve for n to get a
> # power of 0.9.  For the variance of the log odds ratio
> # assume a common p in the center of a range of suspected
> # probabilities of response, 0.3.  For this example we
> # use a zero null value and the uniform prior above
> 
> 
> v   <- function(n) 2/(n*.3*.7)
> pow <- function(n) gbayes2(sqrt(v(n)), prior2)
> uniroot(function(n) pow(n)-0.9, c(50,10000))$root
[1] 2119.688
> # [1] 2119.675
> # Check this value
> pow(2119.675)
[1] 0.8999984
> # [1] 0.9
> 
> 
> # Get the posterior density when there is a mixture of two priors,
> # with mixing probability 0.5.  The first prior is almost
> # non-informative (normal with mean 0 and variance 10000) and the
> # second has mean 2 and variance 0.3.  The test statistic has a value
> # of 3 with variance 0.4.
> f <- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3)
> 
> 
> args(f)
function (delta = numeric(0), x = 3, v = 4, mix = 0.5, d0 = 0, 
    v0 = 10000, d1 = 2, v1 = 0.3, dist = function (x, mean = 0, 
        sd = 1, log = FALSE) 
    .Call(C_dnorm, x, mean, sd, log)) 
NULL
> 
> 
> # Plot this density
> delta <- seq(-2, 6, length=150)
> plot(delta, f(delta), type='l')
> 
> 
> # Add to the plot the posterior density that used only
> # the almost non-informative prior
> lines(delta, f(delta, mix=1), lty=2)
> 
> 
> # The same but for an observed statistic of zero
> lines(delta, f(delta, mix=1, x=0), lty=3)
> 
> 
> # Derive the CDF instead of the density
> g <- gbayesMixPost(3, 4, mix=0.5, d0=0, v0=10000, d1=2, v1=0.3,
+                    what='cdf')
> # Had mix=0 or 1, gbayes1PowerNP could have been used instead
> # of gbayesMixPowerNP below
> 
> 
> # Compute the power to detect an effect of delta=1 if the variance
> # of the test statistic is 0.2
> gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12))
Critical value          Power 
     0.3335535      0.9319167 
> 
> 
> # Do the same thing by simulation
> gbayesMixPowerNP(g, 1, 0.2, interval=c(-10,12), nsim=20000)
     Power Lower 0.95 Upper 0.95 
 0.9324500  0.9289717  0.9359283 
> 
> 
> # Compute by what factor the sample size needs to be larger
> # (the variance needs to be smaller) so that the power is 0.9
> ratios <- seq(1, 4, length=50)
> pow <- single(50)
> for(i in 1:50) 
+   pow[i] <- gbayesMixPowerNP(g, 1, 0.2/ratios[i], interval=c(-10,12))[2]
> 
> 
> # Solve for ratio using reverse linear interpolation
> approx(pow, ratios, xout=0.9)$y
[1] NA
> 
> 
> # Check this by computing power
> gbayesMixPowerNP(g, 1, 0.2/2.1, interval=c(-10,12))
Critical value          Power 
     0.3422667      0.9834678 
> # So the study will have to be 2.1 times as large as earlier thought
> 
> 
> 
> cleanEx()
> nameEx("gbayesSeqSim")
> ### * gbayesSeqSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gbayesSeqSim
> ### Title: gbayesSeqSim
> ### Aliases: gbayesSeqSim
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Simulate Bayesian operating characteristics for an unadjusted
> ##D # proportional odds comparison (Wilcoxon test)
> ##D # For 100 simulations, 5 looks, 2 true parameter values, and
> ##D # 2 assertion/prior combinations, compute the posterior probability
> ##D # Use a low-level logistic regression call to speed up simuluations
> ##D # Use data.table to compute various summary measures
> ##D # Total simulation time: 2s
> ##D lfit <- function(x, y) {
> ##D f <- rms::lrm.fit(x, y)
> ##D   k <- length(coef(f))
> ##D   c(coef(f)[k], vcov(f)[k, k])
> ##D }
> ##D gdat <- function(beta, n1, n2) {
> ##D   # Cell probabilities for a 7-category ordinal outcome for the control group
> ##D   p <- c(2, 1, 2, 7, 8, 38, 42) / 100
> ##D 
> ##D   # Compute cell probabilities for the treated group
> ##D   p2 <- pomodm(p=p, odds.ratio=exp(beta))
> ##D   y1 <- sample(1 : 7, n1, p,  replace=TRUE)
> ##D   y2 <- sample(1 : 7, n2, p2, replace=TRUE)
> ##D   list(y1=y1, y2=y2)
> ##D }
> ##D 
> ##D # Assertion 1: log(OR) < 0 under prior with prior mean 0.1 and sigma 1 on log OR scale
> ##D # Assertion 2: OR between 0.9 and 1/0.9 with prior mean 0 and sigma computed so that
> ##D # P(OR > 2) = 0.05
> ##D asserts <- list(list('Efficacy', '<', 0, mu=0.1, sigma=1),
> ##D                 list('Similarity', 'in', log(c(0.9, 1/0.9)),
> ##D                      cutprior=log(2), tailprob=0.05))
> ##D 
> ##D set.seed(1)
> ##D est <- estSeqSim(c(0, log(0.7)), looks=c(50, 75, 95, 100, 200),
> ##D                    gendat=gdat,
> ##D                    fitter=lfit, nsim=100)
> ##D z <- gbayesSeqSim(est, asserts)
> ##D head(z)
> ##D attr(z, 'asserts')
> ##D 
> ##D # Compute the proportion of simulations that hit targets (different target posterior
> ##D # probabilities for efficacy vs. similarity)
> ##D 
> ##D # For the efficacy assessment compute the first look at which the target
> ##D # was hit (set to infinity if never hit)
> ##D require(data.table)
> ##D z <- data.table(z)
> ##D u <- z[, .(first=min(p1 > 0.95)), by=.(parameter, sim)]
> ##D # Compute the proportion of simulations that ever hit the target and
> ##D # that hit it by the 100th subject
> ##D u[, .(ever=mean(first < Inf)),  by=.(parameter)]
> ##D u[, .(by75=mean(first <= 100)), by=.(parameter)]
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getHdata")
> ### * getHdata
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getHdata
> ### Title: Download and Install Datasets for 'Hmisc', 'rms', and
> ###   Statistical Modeling
> ### Aliases: getHdata
> ### Keywords: interface data
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getHdata()          # download list of available datasets
> ##D getHdata(prostate)  # downloads, load( ) or data.restore( )
> ##D                     # runs cleanup.import for S-Plus 6
> ##D getHdata(valung, "contents")   # open browser (options(browser="whatever"))
> ##D                     # after downloading valung.html
> ##D                     # (result of html(contents()))
> ##D getHdata(support, "all")  # download and open one browser window
> ##D datadensity(support)
> ##D attach(support)     # make individual variables available
> ##D getHdata(plasma,  "all")  # download and open two browser windows
> ##D                           # (description file is available for plasma)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getRs")
> ### * getRs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getRs
> ### Title: Interact with github rscripts Project
> ### Aliases: getRs
> ### Keywords: interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D getRs()             # list available scripts
> ##D scripts <- getRs()  # likewise, but store in an object that can easily
> ##D                     # be viewed on demand in RStudio
> ##D getRs('introda.r')  # download introda.r and put in script editor
> ##D getRs(cats=TRUE)    # list available major and minor categories
> ##D categories <- getRs(cats=TRUE)
> ##D # likewise but store results in a list for later viewing
> ##D getRs(cats='reg')   # list all scripts in a major category containing 'reg'
> ##D getRs('importREDCap.r')   # source() to define a function
> ##D # source() a new version of the Hmisc package's cut2 function:
> ##D getRs('cut2.s', grepo='Hmisc', dir='R')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("getZip")
> ### * getZip
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getZip
> ### Title: Open a Zip File From a URL
> ### Aliases: getZip
> ### Keywords: file IO
> 
> ### ** Examples
> 
> ## Not run: 
> ##D read.csv(getZip('http://test.com/z.zip'))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ggMisc")
> ### * ggMisc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: colorFacet
> ### Title: Miscellaneous ggplot2 and grid Helper Functions
> ### Aliases: colorFacet arrGrob print.arrGrob
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D require(ggplot2)
> ##D s <- summaryP(age + sex ~ region + treatment)
> ##D colorFacet(ggplot(s))   # prints directly
> ##D # arrGrob is called by rms::ggplot.Predict and others
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ggfreqScatter")
> ### * ggfreqScatter
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ggfreqScatter
> ### Title: Frequency Scatterplot
> ### Aliases: ggfreqScatter
> ### Keywords: hplot
> 
> ### ** Examples
> 
> require(ggplot2)
Loading required package: ggplot2
> set.seed(1)
> x <- rnorm(1000)
> y <- rnorm(1000)
> count <- sample(1:100, 1000, TRUE)
> x <- rep(x, count)
> y <- rep(y, count)
> # color=alpha=NULL below makes loess smooth over all points
> g <- ggfreqScatter(x, y) +   # might add g=0 if using plotly
+       geom_smooth(aes(color=NULL, alpha=NULL), se=FALSE) +
+       ggtitle("Using Deciles of Frequency Counts, 2500 Bins")
> g
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
Warning: The following aesthetics were dropped during statistical transformation: label.
ℹ This can happen when ggplot fails to infer the correct grouping structure in
  the data.
ℹ Did you forget to specify a `group` aesthetic or to convert a numerical
  variable into a factor?
> # plotly::ggplotly(g, tooltip='label')  # use plotly, hover text = freq. only
> # Plotly makes it somewhat interactive, with hover text tooltips
> 
> # Instead use varying-height sticks to depict frequencies
> ggfreqScatter(x, y, stick=TRUE) +
+  labs(subtitle='Relative height of black lines to gray lines
+ is proportional to cell frequency.
+ Note that points with even tiny frequency are visable
+ (gray line with no visible black line).')
Warning: Removed 1 row containing missing values or values outside the scale range
(`geom_segment()`).
Warning: Removed 1 row containing missing values or values outside the scale range
(`geom_segment()`).
> 
> 
> # Try with x categorical
> x1 <- sample(c('cat', 'dog', 'giraffe'), length(x), TRUE)
> ggfreqScatter(x1, y)
> 
> # Try with y categorical
> y1 <- sample(LETTERS[1:10], length(x), TRUE)
> ggfreqScatter(x, y1)
> 
> # Both categorical, larger point symbols, box instead of circle
> ggfreqScatter(x1, y1, shape=15, size=7)
> # Vary box size instead
> ggfreqScatter(x1, y1, nsize=TRUE, shape=15)
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("hdquantile")
> ### * hdquantile
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hdquantile
> ### Title: Harrell-Davis Distribution-Free Quantile Estimator
> ### Aliases: hdquantile
> ### Keywords: univar
> 
> ### ** Examples
> 
> set.seed(1)
> x <- runif(100)
> hdquantile(x, (1:3)/4, se=TRUE)
     0.25      0.50      0.75 
0.3064350 0.5054821 0.7571213 
attr(,"se")
      0.25       0.50       0.75 
0.03931114 0.04878268 0.02997025 
> 
> ## Not run: 
> ##D # Compare jackknife standard errors with those from the bootstrap
> ##D library(boot)
> ##D boot(x, function(x,i) hdquantile(x[i], probs=(1:3)/4), R=400)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hidingTOC")
> ### * hidingTOC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hidingTOC
> ### Title: Moving and Hiding Table of Contents
> ### Aliases: hidingTOC
> 
> ### ** Examples
> 
> ## Not run: 
> ##D hidingTOC()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hist.data.frame")
> ### * hist.data.frame
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hist.data.frame
> ### Title: Histograms for Variables in a Data Frame
> ### Aliases: hist.data.frame
> ### Keywords: hplot dplot distribution
> 
> ### ** Examples
> 
> d <- data.frame(a=runif(200), b=rnorm(200),
+                 w=factor(sample(c('green','red','blue'), 200, TRUE)))
> hist.data.frame(d)   # in R, just say hist(d)
> 
> 
> 
> cleanEx()
> nameEx("histbackback")
> ### * histbackback
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: histbackback
> ### Title: Back to Back Histograms
> ### Aliases: histbackback
> ### Keywords: dplot hplot distribution
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(1)
> histbackback(rnorm(20), rnorm(30))
> 
> 
> fool <- list(x=rnorm(40), y=rnorm(40))
> histbackback(fool)
> age <- rnorm(1000,50,10)
> sex <- sample(c('female','male'),1000,TRUE)
> histbackback(split(age, sex))
> agef <- age[sex=='female']; agem <- age[sex=='male']
> histbackback(list(Female=agef,Male=agem), probability=TRUE, xlim=c(-.06,.06))
> 
> 
> 
> cleanEx()
> nameEx("histboxp")
> ### * histboxp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: histboxp
> ### Title: Use plotly to Draw Stratified Spike Histogram and Box Plot
> ###   Statistics
> ### Aliases: histboxp histboxpM dhistboxp
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dist <- c(rep(1, 500), rep(2, 250), rep(3, 600))
> ##D Distribution <- factor(dist, 1 : 3, c('Unimodal', 'Bimodal', 'Trimodal'))
> ##D x <- c(rnorm(500, 6, 1),
> ##D        rnorm(200, 3, .7), rnorm(50, 7, .4),
> ##D        rnorm(200, 2, .7), rnorm(300, 5.5, .4), rnorm(100, 8, .4))
> ##D histboxp(x=x, group=Distribution, sd=TRUE)
> ##D X <- data.frame(x, x2=runif(length(x)))
> ##D histboxpM(x=X, group=Distribution, ncols=2)  # separate plots
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("hlab")
> ### * hlab
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hlab
> ### Title: hlab
> ### Aliases: hlab
> 
> ### ** Examples
> 
> d <- data.frame(x=1:10, y=(1:10)/10)
> d <- upData(d, labels=c(x='X', y='Y'), units=c(x='mmHg'), print=FALSE)
> hlab(x)
expression(list(X, scriptstyle(mmHg)))
> hlab(x, html=TRUE)
[1] "X <span style='font-family:Verdana;font-size:75%;'>mmHg</span>"
> hlab(z)
expression(z)
> require(ggplot2)
Loading required package: ggplot2
> ggplot(d, aes(x, y)) + geom_point() + labs(x=hlab(x), y=hlab(y))
> # Can use xlab(hlab(x)) + ylab(hlab(y)) also
> # Store names, labels, units for all variables in d in object
> LabelsUnits <- extractlabs(d)
> # Remove d; labels/units still found
> rm(d)
> hlab(x)
expression(list(X, scriptstyle(mmHg)))
> # Remove LabelsUnits and use a current dataset named
> # d2 instead of the default d
> rm(LabelsUnits)
> options(current_ds='d2')
> 
> 
> 
> cleanEx()

detaching ‘package:ggplot2’

> nameEx("hlabs")
> ### * hlabs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hlabs
> ### Title: hlabs
> ### Aliases: hlabs
> 
> ### ** Examples
> 
> # Name the current dataset d, or specify a name with
> # options(curr_ds='...') or run `extractlabs`, then
> # ggplot(d, aes(x,y)) + geom_point() + hlabs(x,y)
> # to specify only the x-axis label use hlabs(x), or to
> # specify only the y-axis label use hlabs(y=...)
> 
> 
> 
> cleanEx()
> nameEx("hoeffd")
> ### * hoeffd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hoeffd
> ### Title: Matrix of Hoeffding's D Statistics
> ### Aliases: hoeffd print.hoeffd
> ### Keywords: nonparametric htest
> 
> ### ** Examples
> 
> x <- c(-2, -1, 0, 1, 2)
> y <- c(4,   1, 0, 1, 4)
> z <- c(1,   2, 3, 4, NA)
> q <- c(1,   2, 3, 4, 5)
> hoeffd(cbind(x,y,z,q))
D
      x     y     z     q
x 1e+00 0e+00 3e+50 1e+00
y 0e+00 1e+00 3e+50 0e+00
z 3e+50 3e+50 1e+00 3e+50
q 1e+00 0e+00 3e+50 1e+00

avg|F(x,y)-G(x)H(y)|
     x    y z    q
x 0.00 0.04 0 0.16
y 0.04 0.00 0 0.04
z 0.00 0.00 0 0.00
q 0.16 0.04 0 0.00

max|F(x,y)-G(x)H(y)|
     x   y z    q
x 0.00 0.1 0 0.24
y 0.10 0.0 0 0.10
z 0.00 0.0 0 0.00
q 0.24 0.1 0 0.00

n
  x y z q
x 5 5 4 5
y 5 5 4 5
z 4 4 4 4
q 5 5 4 5

P
  x     y     z     q    
x       0.363 0.000 0.000
y 0.363       0.000 0.363
z 0.000 0.000       0.000
q 0.000 0.363 0.000      
> 
> 
> # Hoeffding's test can detect even one-to-many dependency
> set.seed(1)
> x <- seq(-10,10,length=200)
> y <- x*sign(runif(200,-1,1))
> plot(x,y)
> hoeffd(x,y)
D
     x    y
x 1.00 0.06
y 0.06 1.00

avg|F(x,y)-G(x)H(y)|
       x      y
x 0.0000 0.0407
y 0.0407 0.0000

max|F(x,y)-G(x)H(y)|
       x      y
x 0.0000 0.0763
y 0.0763 0.0000

n= 200 

P
  x  y 
x     0
y  0   
> 
> 
> 
> cleanEx()
> nameEx("html")
> ### * html
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: html
> ### Title: Convert an S object to HTML
> ### Aliases: html html.latex html.data.frame html.default htmlVerbatim
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','e')))
> ##D w <- latex(x)
> ##D h <- html(w) # run HeVeA to convert .tex to .html
> ##D h <- html(x) # convert x directly to html
> ##D w <- html(x, link=c('','B'))   # hyperlink first row first col to B
> ##D 
> ##D # Assuming system package tex4ht is installed, easily convert advanced
> ##D # LaTeX tables to html
> ##D getHdata(pbc)
> ##D s <- summaryM(bili + albumin + stage + protime + sex + age + spiders ~ drug,
> ##D               data=pbc, test=TRUE)
> ##D w <- latex(s, npct='slash', file='s.tex')
> ##D z <- html(w)
> ##D browseURL(z$file)
> ##D 
> ##D d <- describe(pbc)
> ##D w <- latex(d, file='d.tex')
> ##D z <- html(w)
> ##D browseURL(z$file)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("impute")
> ### * impute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: impute
> ### Title: Generic Functions and Methods for Imputation
> ### Aliases: impute impute.default print.impute summary.impute [.impute
> ###   is.imputed
> ### Keywords: methods math htest models
> 
> ### ** Examples
> 
> age <- c(1,2,NA,4)
> age.i <- impute(age)
> # Could have used impute(age,2.5), impute(age,mean), impute(age,"random")
> age.i
 1  2  3  4 
 1  2 2*  4 
> summary(age.i)

 1 values imputed to 2 

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    1.75    2.00    2.25    2.50    4.00 
> is.imputed(age.i)
[1] FALSE FALSE  TRUE FALSE
> 
> 
> 
> cleanEx()
> nameEx("knitrSet")
> ### * knitrSet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: knitrSet
> ### Title: knitr Setup and plotly Service Function
> ### Aliases: knitrSet plotlySave
> ### Keywords: interface
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Typical call (without # comment symbols):
> ##D # <<echo=FALSE>>=
> ##D # require(Hmisc)
> ##D # knitrSet()
> ##D # @
> ##D 
> ##D knitrSet()    # use all defaults and don't use a graphics file prefix
> ##D knitrSet('modeling')   # use modeling- prefix for a major section or chapter
> ##D knitrSet(cache=TRUE, echo=FALSE)  # global default to cache and not print code
> ##D knitrSet(w=5,h=3.75)   # override default figure width, height
> ##D 
> ##D # ```{r chunkname}
> ##D # p <- plotly::plot_ly(...)
> ##D # plotlySave(p)   # creates fig.path/chunkname.png
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("labcurve")
> ### * labcurve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: labcurve
> ### Title: Label Curves, Make Keys, and Interactively Draw Points and
> ###   Curves
> ### Aliases: labcurve putKey putKeyEmpty largest.empty drawPlot
> ###   plot.drawPlot bezier
> ### Keywords: hplot aplot dplot iplot
> 
> ### ** Examples
> 
> n <- 2:8
> m <-  length(n)
> type <- c('l','l','l','l','s','l','l')
> # s=step function l=ordinary line (polygon)
> curves <- vector('list', m)
> 
> 
> plot(0,1,xlim=c(0,1),ylim=c(-2.5,4),type='n')
> 
> 
> set.seed(39)
> 
> 
> for(i in 1:m) {
+   x <- sort(runif(n[i]))
+   y <- rnorm(n[i])
+   lines(x, y, lty=i, type=type[i], col=i)
+   curves[[i]] <- list(x=x,y=y)
+ }
> 
> 
> labels <- paste('Label for',letters[1:m])
> labcurve(curves, labels, tilt=TRUE, type=type, col=1:m)
> 
> 
> # Put only single letters on curves at points of 
> # maximum space, and use key() to define the letters,
> # with automatic positioning of the key in the most empty
> # part of the plot
> # Have labcurve do the plotting, leaving extra space for key
> 
> 
> names(curves) <- labels
> labcurve(curves, keys=letters[1:m], type=type, col=1:m,
+          pl=TRUE, ylim=c(-2.5,4))
> 
> 
> # Put plotting symbols at equally-spaced points,
> # with a key for the symbols, ignoring line types
> 
> 
> labcurve(curves, keys=1:m, lty=1, type=type, col=1:m,
+          pl=TRUE, ylim=c(-2.5,4))
> 
> 
> 
> 
> # Plot and label two curves, with line parameters specified with data
> set.seed(191)
> ages.f <- sort(rnorm(50,20,7))
> ages.m <- sort(rnorm(40,19,7))
> height.f <- pmin(ages.f,21)*.2+60
> height.m <- pmin(ages.m,21)*.16+63
> 
> 
> labcurve(list(Female=list(ages.f,height.f,col=2),
+               Male  =list(ages.m,height.m,col=3,lty='dashed')),
+          xlab='Age', ylab='Height', pl=TRUE)
> # add ,keys=c('f','m') to label curves with single letters
> # For S-Plus use lty=2
> 
> 
> # Plot power for testing two proportions vs. n for various odds ratios, 
> # using 0.1 as the probability of the event in the control group.  
> # A separate curve is plotted for each odds ratio, and the curves are
> # labeled at points of maximum separation
> 
> 
> n  <- seq(10, 1000, by=10)
> OR <- seq(.2,.9,by=.1)
> pow <- lapply(OR, function(or,n)list(x=n,y=bpower(p1=.1,odds.ratio=or,n=n)),
+               n=n)
> names(pow) <- format(OR)
> labcurve(pow, pl=TRUE, xlab='n', ylab='Power')
> 
> 
> # Plot some random data and find the largest empty rectangle
> # that is at least .1 wide and .1 tall
> 
> 
> x <- runif(50)
> y <- runif(50)
> plot(x, y)
> z <- largest.empty(x, y, .1, .1)
> z
$x
[1] 0.634

$y
[1] 0.292

$rect
$rect$x
[1] 0.544 0.723 0.723 0.544

$rect$y
[1] -0.0246 -0.0246  0.6080  0.6080


$area
[1] 0.113

> points(z,pch=3)  # mark center of rectangle, or
> polygon(z$rect, col='blue')  # to draw the rectangle, or
> #key(z$x, z$y, \dots stuff for legend)
> 
> 
> 
> 
> # Use the mouse to draw a series of points using one symbol, and
> # two smooth curves or straight lines (if two points are clicked), 
> # none of these being labeled
> 
> 
> # d <- drawPlot(Points(), Curve(), Curve())
> # plot(d)
> 
> 
> ## Not run: 
> ##D # Use the mouse to draw a Gaussian density, two series of points
> ##D # using 2 symbols, one Bezier curve, a step function, and raw data
> ##D # along the x-axis as a 1-d scatter plot (rug plot).  Draw a key.
> ##D # The density function is fit to 3 mouse clicks
> ##D # Abline draws a dotted horizontal reference line
> ##D d <- drawPlot(Curve('Normal',type='gauss'),
> ##D               Points('female'), Points('male'), 
> ##D               Curve('smooth',ask=TRUE,lty=2), Curve('step',type='s',lty=3), 
> ##D               Points(type='r'), Abline(h=.5, lty=2),
> ##D               xlab='X', ylab='y', xlim=c(0,100), key=TRUE)
> ##D plot(d, ylab='Y')
> ##D plot(d, key=FALSE)  # label groups using labcurve
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("label")
> ### * label
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: label
> ### Title: Label Attribute of an Object
> ### Aliases: label label<- label.default label.Surv label<-.default
> ###   labelPlotmath labelLatex [.labelled print.labelled Label
> ###   Label.data.frame llist prList putHcap putHfig plotmathTranslate
> ###   as.data.frame.labelled data.frame.labelled reLabelled
> ###   label.data.frame label<-.data.frame relevel.labelled combineLabels
> ### Keywords: attribute misc utilities
> 
> ### ** Examples
> 
> age <- c(21,65,43)
> y   <- 1:3
> label(age) <- "Age in Years"
> plot(age, y, xlab=label(age))
> 
> data <- data.frame(age=age, y=y)
> label(data)
           age              y 
"Age in Years"             "" 
> 
> label(data, self=TRUE) <- "A data frame"
> label(data, self=TRUE)
[1] "A data frame"
> 
> x1 <- 1:10
> x2 <- 10:1
> label(x2) <- 'Label for x2'
> units(x2) <- 'mmHg'
> x2
Label for x2 [mmHg] 
 [1] 10  9  8  7  6  5  4  3  2  1
> x2[1:5]
Label for x2 [mmHg] 
[1] 10  9  8  7  6
> dframe <- data.frame(x1, x2)
> Label(dframe)
label(x1)	<- ''
label(x2)	<- 'Label for x2'
> 
> labelLatex(x2, hfill=TRUE, bold=TRUE)
[1] "{\\textbf Label for x2}\\hfill {\\smaller[2] mmHg}"
> labelLatex(label='Velocity', units='m/s')
[1] "Velocity {\\smaller[2] m/s}"
> 
> ##In these examples of llist, note that labels are printed after
> ##variable names, because of print.labelled
> a <- 1:3
> b <- 4:6
> label(b) <- 'B Label'
> llist(a,b)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

> llist(a,b,d=0)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

$d
d 
[1] 0

> llist(a,b,0)
$a
a 
[1] 1 2 3

$b
B Label 
[1] 4 5 6

$`0`
0 
[1] 0

> 
> 
> w <- llist(a, b>5, d=101:103)
> sapply(w, function(x){
+   hist(as.numeric(x), xlab=label(x))
+   # locator(1)   ## wait for mouse click
+ })
         a               b > 5           d              
breaks   numeric,5       numeric,3       numeric,5      
counts   integer,4       integer,2       integer,4      
density  numeric,4       numeric,2       numeric,4      
mids     numeric,4       numeric,2       numeric,4      
xname    "as.numeric(x)" "as.numeric(x)" "as.numeric(x)"
equidist TRUE            TRUE            TRUE           
> 
> # Or: for(u in w) {hist(u); title(label(u))}
> 
> 
> 
> cleanEx()
> nameEx("latex")
> ### * latex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latex
> ### Title: Convert an S object to LaTeX, and Related Utilities
> ### Aliases: latex latex.default latex.function latex.list latexTranslate
> ###   htmlTranslate latexSN htmlSN latexVerbatim dvi print.dvi dvi.latex
> ###   dvips dvips.latex dvips.dvi dvigv dvigv.latex dvigv.dvi print.latex
> ###   show.latex show.dvi
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
> ## Not run: 
> ##D latex(x)   # creates x.tex in working directory
> ##D # The result of the above command is an object of class "latex"
> ##D # which here is automatically printed by the latex print method.
> ##D # The latex print method prepends and appends latex headers and
> ##D # calls the latex program in the PATH.  If the latex program is
> ##D # not in the PATH, you will get error messages from the operating
> ##D # system.
> ##D 
> ##D w <- latex(x, file='/tmp/my.tex')
> ##D # Does not call the latex program as the print method was not invoked
> ##D print.default(w)
> ##D # Shows the contents of the w variable without attempting to latex it.
> ##D 
> ##D d <- dvi(w)  # compile LaTeX document, make .dvi
> ##D              # latex assumed to be in path
> ##D d            # or show(d) : run xdvi (assumed in path) to display
> ##D w            # or show(w) : run dvi then xdvi
> ##D dvips(d)     # run dvips to print document
> ##D dvips(w)     # run dvi then dvips
> ##D library(tools)
> ##D texi2dvi('/tmp/my.tex')   # compile and produce pdf file in working dir.
> ## End(Not run)
> latex(x, file="")   # just write out LaTeX code to screen
\begin{table}[!tbp]
\begin{center}
\begin{tabular}{lrrr}
\hline\hline
\multicolumn{1}{l}{x}&\multicolumn{1}{c}{c}&\multicolumn{1}{c}{d}&\multicolumn{1}{c}{this that}\tabularnewline
\hline
a&$1$&$3$&$5$\tabularnewline
b&$2$&$4$&$6$\tabularnewline
\hline
\end{tabular}\end{center}
\end{table}
> 
> ## Not run: 
> ##D # Use paragraph formatting to wrap text to 3 in. wide in a column
> ##D d <- data.frame(x=1:2,
> ##D                 y=c(paste("a",
> ##D                     paste(rep("very",30),collapse=" "),"long string"),
> ##D                 "a short string"))
> ##D latex(d, file="", col.just=c("l", "p{3in}"), table.env=FALSE)
> ## End(Not run)
> 
> ## Not run: 
> ##D # After running latex( ) multiple times with different special styles in
> ##D # effect, make a file that will call for the needed LaTeX packages when
> ##D # latex is run (especially when using Sweave with R)
> ##D if(exists(latexStyles))
> ##D   cat(paste('\usepackage{',latexStyles,'}',sep=''),
> ##D       file='stylesused.tex', sep='\n')
> ##D # Then in the latex job have something like:
> ##D # \documentclass{article}
> ##D # \input{stylesused}
> ##D # \begin{document}
> ##D # ...
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("latexDotchart")
> ### * latexDotchart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexDotchart
> ### Title: Enhanced Dot Chart for LaTeX Picture Environment with epic
> ### Aliases: latexDotchart
> ### Keywords: hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D z <- latexDotchart(c(.1,.2), c('a','bbAAb'), xlab='This Label',
> ##D                    auxdata=c(.1,.2), auxtitle='Zcriteria')
> ##D f <- '/tmp/t.tex'
> ##D cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n', file=f)
> ##D cat(z, sep='\n', file=f, append=TRUE)
> ##D cat('\end{document}\n', file=f, append=TRUE)
> ##D 
> ##D set.seed(135)
> ##D maj <- factor(c(rep('North',13),rep('South',13)))
> ##D g <- paste('Category',rep(letters[1:13],2))
> ##D n <- sample(1:15000, 26, replace=TRUE)
> ##D y1 <- runif(26)
> ##D y2 <- pmax(0, y1 - runif(26, 0, .1))
> ##D z <- latexDotchart(y1, g, groups=maj, auxdata=n, auxtitle='n', xlab='Y',
> ##D                    size.group.labels='large', ttlabels=TRUE)
> ##D f <- '/tmp/t2.tex'
> ##D cat('\documentclass{article}\n\usepackage{epic,color}\n\begin{document}\n\framebox{', file=f)
> ##D cat(z, sep='\n', file=f, append=TRUE)
> ##D cat('}\end{document}\n', file=f, append=TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("latexTabular")
> ### * latexTabular
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexTabular
> ### Title: Convert a Data Frame or Matrix to a LaTeX Tabular
> ### Aliases: latexTabular
> ### Keywords: utilities interface methods file character manip
> 
> ### ** Examples
> 
> x <- matrix(1:6, nrow=2, dimnames=list(c('a','b'),c('c','d','this that')))
> latexTabular(x)   # a character string with LaTeX markup
[1] "{\\fontfamily{phv}\\selectfont \\begin{tabular}{ccc}\nc&d&this that\\\\\n1&3&5\\\\\n2&4&6\\\\\n\\end{tabular}}"
> 
> 
> 
> cleanEx()
> nameEx("latexTherm")
> ### * latexTherm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: latexTherm
> ### Title: Create LaTeX Thermometers and Colored Needles
> ### Aliases: latexTherm latexNeedle pngNeedle
> ### Keywords: utilities interface file character manip
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # The following is in the Hmisc tests directory
> ##D # For a knitr example see latexTherm.Rnw in that directory
> ##D ct <- function(...) cat(..., sep='')
> ##D ct('\documentclass{report}\begin{document}\n')
> ##D latexTherm(c(1, 1, 1, 1), name='lta')
> ##D latexTherm(c(.5, .7, .4, .2), name='ltb')
> ##D latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltc', extra=0)
> ##D latexTherm(c(.5, NA, .75, 0), w=.3, h=1, name='ltcc')
> ##D latexTherm(c(0, 0, 0, 0), name='ltd')
> ##D ct('This is a the first:\lta and the second:\ltb\\ and the third
> ##D without extra:\ltc END\\\nThird with extra:\ltcc END\\ 
> ##D \vspace{2in}\\ 
> ##D All data = zero, frame only:\ltd\\
> ##D \end{document}\n')
> ##D w <- pngNeedle(c(.2, .5, .7))
> ##D cat(tobase64image(w))  # can insert this directly into an html file
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("list.tree")
> ### * list.tree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: list.tree
> ### Title: Pretty-print the Structure of a Data Object
> ### Aliases: list.tree
> ### Keywords: documentation
> 
> ### ** Examples
> 
> X <- list(a=ordered(c(1:30,30:1)),b=c("Rick","John","Allan"),
+           c=diag(300),e=cbind(p=1008:1019,q=4))
> list.tree(X)
 X = list 4 (724376 bytes)
.  a = integer 60= factor (30 levels)( ordered factor )= 1 2 3 4 5 6 7 8 ...
.  b = character 3= Rick John Allan 
.  c = double 90000= array 300 X 300= 1 0 0 0 0 0 0 0 ...
.  e = double 24= named array 12 X 2= 1008 1009 1010 1011 ...
> # In R you can say str(X)
> 
> 
> 
> cleanEx()
> nameEx("mApply")
> ### * mApply
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mApply
> ### Title: Apply a Function to Rows of a Matrix or Vector
> ### Aliases: mApply
> ### Keywords: iteration category
> 
> ### ** Examples
> 
> require(datasets, TRUE)
> a <- mApply(iris[,-5], iris$Species, mean)
> 
> 
> 
> cleanEx()
> nameEx("mChoice")
> ### * mChoice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mChoice
> ### Title: Methods for Storing and Analyzing Multiple Choice Variables
> ### Aliases: mChoice format.mChoice print.mChoice summary.mChoice
> ###   as.character.mChoice as.double.mChoice inmChoice inmChoicelike
> ###   nmChoice match.mChoice [.mChoice print.summary.mChoice is.mChoice
> ###   Math.mChoice Ops.mChoice Summary.mChoice
> ### Keywords: category manip
> 
> ### ** Examples
> 
> options(digits=3)
> set.seed(3)
> n <- 20
> sex <- factor(sample(c("m","f"), n, rep=TRUE))
> age <- rnorm(n, 50, 5)
> treatment <- factor(sample(c("Drug","Placebo"), n, rep=TRUE))
> 
> 
> # Generate a 3-choice variable; each of 3 variables has 5 possible levels
> symp <- c('Headache','Stomach Ache','Hangnail',
+           'Muscle Ache','Depressed')
> symptom1 <- sample(symp, n, TRUE)
> symptom2 <- sample(symp, n, TRUE)
> symptom3 <- sample(symp, n, TRUE)
> cbind(symptom1, symptom2, symptom3)[1:5,]
     symptom1       symptom2      symptom3      
[1,] "Hangnail"     "Headache"    "Headache"    
[2,] "Muscle Ache"  "Depressed"   "Depressed"   
[3,] "Muscle Ache"  "Hangnail"    "Muscle Ache" 
[4,] "Stomach Ache" "Muscle Ache" "Stomach Ache"
[5,] "Headache"     "Muscle Ache" "Depressed"   
> Symptoms <- mChoice(symptom1, symptom2, symptom3, label='Primary Symptoms')
> Symptoms
 [1] Hangnail;Headache                 Muscle Ache;Depressed            
 [3] Hangnail;Muscle Ache              Muscle Ache;Stomach Ache         
 [5] Muscle Ache;Headache;Depressed    Hangnail;Muscle Ache;Headache    
 [7] Stomach Ache;Headache;Depressed   Stomach Ache;Depressed           
 [9] Stomach Ache;Depressed            Muscle Ache;Depressed            
[11] Hangnail;Stomach Ache;Headache    Hangnail;Muscle Ache;Stomach Ache
[13] Hangnail;Stomach Ache;Depressed   Muscle Ache;Headache             
[15] Hangnail;Muscle Ache;Stomach Ache Hangnail;Stomach Ache;Headache   
[17] Depressed                         Hangnail;Muscle Ache;Headache    
[19] Stomach Ache;Headache             Muscle Ache;Stomach Ache;Headache
attr(,"label")
[1] Primary Symptoms
Levels: Hangnail Muscle Ache Stomach Ache Headache Depressed
> print(Symptoms, long=TRUE)
 [1] Hangnail;Headache                 Muscle Ache;Depressed            
 [3] Hangnail;Muscle Ache              Muscle Ache;Stomach Ache         
 [5] Muscle Ache;Headache;Depressed    Hangnail;Muscle Ache;Headache    
 [7] Stomach Ache;Headache;Depressed   Stomach Ache;Depressed           
 [9] Stomach Ache;Depressed            Muscle Ache;Depressed            
[11] Hangnail;Stomach Ache;Headache    Hangnail;Muscle Ache;Stomach Ache
[13] Hangnail;Stomach Ache;Depressed   Muscle Ache;Headache             
[15] Hangnail;Muscle Ache;Stomach Ache Hangnail;Stomach Ache;Headache   
[17] Depressed                         Hangnail;Muscle Ache;Headache    
[19] Stomach Ache;Headache             Muscle Ache;Stomach Ache;Headache
attr(,"label")
[1] Primary Symptoms
Levels: Hangnail Muscle Ache Stomach Ache Headache Depressed
> format(Symptoms[1:5])
[1] "Hangnail;Headache"              "Muscle Ache;Depressed"         
[3] "Hangnail;Muscle Ache"           "Muscle Ache;Stomach Ache"      
[5] "Muscle Ache;Headache;Depressed"
> inmChoice(Symptoms,'Headache')
 [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE
> inmChoicelike(Symptoms, 'head', ignore.case=TRUE)
 [1]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE
> levels(Symptoms)
[1] "Hangnail"     "Muscle Ache"  "Stomach Ache" "Headache"     "Depressed"   
> inmChoice(Symptoms, 3)
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[13]  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE
> # Find all subjects with either of two symptoms
> inmChoice(Symptoms, c('Headache','Hangnail'))
 [1]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> # Note: In this example, some subjects have the same symptom checked
> # multiple times; in practice these redundant selections would be NAs
> # mChoice will ignore these redundant selections
> # Find all subjects with both symptoms
> inmChoice(Symptoms, c('Headache', 'Hangnail'), condition='all')
 [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE
> 
> meanage <- N <- numeric(5)
> for(j in 1:5) {
+  meanage[j] <- mean(age[inmChoice(Symptoms,j)])
+  N[j] <- sum(inmChoice(Symptoms,j))
+ }
> names(meanage) <- names(N) <- levels(Symptoms)
> meanage
    Hangnail  Muscle Ache Stomach Ache     Headache    Depressed 
        47.9         47.8         48.1         47.5         49.9 
> N
    Hangnail  Muscle Ache Stomach Ache     Headache    Depressed 
           9           11           11           10            8 
> 
> # Manually compute mean age for 2 symptoms
> mean(age[symptom1=='Headache' | symptom2=='Headache' | symptom3=='Headache'])
[1] 47.5
> mean(age[symptom1=='Hangnail' | symptom2=='Hangnail' | symptom3=='Hangnail'])
[1] 47.9
> 
> summary(Symptoms)

15 unique combinations

Primary Symptoms

Frequencies of Numbers of Choices Per Observation

 1  2  3 
 1  9 10 

Pairwise Frequencies (Diagonal Contains Marginal Frequencies)

             Hangnail Muscle Ache Stomach Ache Headache Depressed
Hangnail      9        5           5            5        1       
Muscle Ache           11           4            5        3       
Stomach Ache                      11            5        4       
Headache                                       10        2       
Depressed                                                8       

 Frequencies of Top 5 Combinations 

    Hangnail;Muscle Ache;Headache Hangnail;Muscle Ache;Stomach Ache 
                                2                                 2 
   Hangnail;Stomach Ache;Headache             Muscle Ache;Depressed 
                                2                                 2 
           Stomach Ache;Depressed 
                                2 
> 
> #Frequency table sex*treatment, sex*Symptoms
> summary(sex ~ treatment + Symptoms, fun=table)
sex      N= 20  

+---------+------------+--+--+-+
|         |            | N| f|m|
+---------+------------+--+--+-+
|treatment|        Drug|11| 6|5|
|         |     Placebo| 9| 7|2|
+---------+------------+--+--+-+
| Symptoms|    Hangnail| 9| 6|3|
|         | Muscle Ache|11| 8|3|
|         |Stomach Ache|11| 6|5|
|         |    Headache|10| 7|3|
|         |   Depressed| 8| 5|3|
+---------+------------+--+--+-+
|  Overall|            |20|13|7|
+---------+------------+--+--+-+
> # Check:
> ma <- inmChoice(Symptoms, 'Muscle Ache')
> table(sex[ma])

f m 
8 3 
> 
> # could also do:
> # summary(sex ~ treatment + mChoice(symptom1,symptom2,symptom3), fun=table)
> 
> #Compute mean age, separately by 3 variables
> summary(age ~ sex + treatment + Symptoms)
age      N= 20  

+---------+------------+--+----+
|         |            | N| age|
+---------+------------+--+----+
|      sex|           f|13|47.7|
|         |           m| 7|49.8|
+---------+------------+--+----+
|treatment|        Drug|11|47.4|
|         |     Placebo| 9|49.7|
+---------+------------+--+----+
| Symptoms|    Hangnail| 9|47.9|
|         | Muscle Ache|11|47.8|
|         |Stomach Ache|11|48.1|
|         |    Headache|10|47.5|
|         |   Depressed| 8|49.9|
+---------+------------+--+----+
|  Overall|            |20|48.4|
+---------+------------+--+----+
> 
> 
> summary(age ~ sex + treatment + Symptoms, method="cross")

 mean by sex, treatment, Symptoms 

    sex treatment                          Symptoms  N  age
1     f      Drug                         Depressed  0   NA
2     m      Drug                         Depressed  0   NA
3   ALL      Drug                         Depressed  0   NA
4     f   Placebo                         Depressed  1 55.8
5     m   Placebo                         Depressed  0   NA
6   ALL   Placebo                         Depressed  1 55.8
7     f       ALL                         Depressed  1 55.8
8     m       ALL                         Depressed  0   NA
9   ALL       ALL                         Depressed  1 55.8
10    f      Drug                 Hangnail;Headache  0   NA
11    m      Drug                 Hangnail;Headache  1 46.3
12  ALL      Drug                 Hangnail;Headache  1 46.3
13    f   Placebo                 Hangnail;Headache  0   NA
14    m   Placebo                 Hangnail;Headache  0   NA
15  ALL   Placebo                 Hangnail;Headache  0   NA
16    f       ALL                 Hangnail;Headache  0   NA
17    m       ALL                 Hangnail;Headache  1 46.3
18  ALL       ALL                 Hangnail;Headache  1 46.3
19    f      Drug              Hangnail;Muscle Ache  1 46.4
20    m      Drug              Hangnail;Muscle Ache  0   NA
21  ALL      Drug              Hangnail;Muscle Ache  1 46.4
22    f   Placebo              Hangnail;Muscle Ache  0   NA
23    m   Placebo              Hangnail;Muscle Ache  0   NA
24  ALL   Placebo              Hangnail;Muscle Ache  0   NA
25    f       ALL              Hangnail;Muscle Ache  1 46.4
26    m       ALL              Hangnail;Muscle Ache  0   NA
27  ALL       ALL              Hangnail;Muscle Ache  1 46.4
28    f      Drug     Hangnail;Muscle Ache;Headache  1 48.5
29    m      Drug     Hangnail;Muscle Ache;Headache  0   NA
30  ALL      Drug     Hangnail;Muscle Ache;Headache  1 48.5
31    f   Placebo     Hangnail;Muscle Ache;Headache  0   NA
32    m   Placebo     Hangnail;Muscle Ache;Headache  1 55.1
33  ALL   Placebo     Hangnail;Muscle Ache;Headache  1 55.1
34    f       ALL     Hangnail;Muscle Ache;Headache  1 48.5
35    m       ALL     Hangnail;Muscle Ache;Headache  1 55.1
36  ALL       ALL     Hangnail;Muscle Ache;Headache  2 51.8
37    f      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
38    m      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
39  ALL      Drug Hangnail;Muscle Ache;Stomach Ache  0   NA
40    f   Placebo Hangnail;Muscle Ache;Stomach Ache  2 46.4
41    m   Placebo Hangnail;Muscle Ache;Stomach Ache  0   NA
42  ALL   Placebo Hangnail;Muscle Ache;Stomach Ache  2 46.4
43    f       ALL Hangnail;Muscle Ache;Stomach Ache  2 46.4
44    m       ALL Hangnail;Muscle Ache;Stomach Ache  0   NA
45  ALL       ALL Hangnail;Muscle Ache;Stomach Ache  2 46.4
46    f      Drug   Hangnail;Stomach Ache;Depressed  0   NA
47    m      Drug   Hangnail;Stomach Ache;Depressed  1 49.0
48  ALL      Drug   Hangnail;Stomach Ache;Depressed  1 49.0
49    f   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
50    m   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
51  ALL   Placebo   Hangnail;Stomach Ache;Depressed  0   NA
52    f       ALL   Hangnail;Stomach Ache;Depressed  0   NA
53    m       ALL   Hangnail;Stomach Ache;Depressed  1 49.0
54  ALL       ALL   Hangnail;Stomach Ache;Depressed  1 49.0
55    f      Drug    Hangnail;Stomach Ache;Headache  1 46.3
56    m      Drug    Hangnail;Stomach Ache;Headache  0   NA
57  ALL      Drug    Hangnail;Stomach Ache;Headache  1 46.3
58    f   Placebo    Hangnail;Stomach Ache;Headache  1 47.1
59    m   Placebo    Hangnail;Stomach Ache;Headache  0   NA
60  ALL   Placebo    Hangnail;Stomach Ache;Headache  1 47.1
61    f       ALL    Hangnail;Stomach Ache;Headache  2 46.7
62    m       ALL    Hangnail;Stomach Ache;Headache  0   NA
63  ALL       ALL    Hangnail;Stomach Ache;Headache  2 46.7
64    f      Drug             Muscle Ache;Depressed  0   NA
65    m      Drug             Muscle Ache;Depressed  0   NA
66  ALL      Drug             Muscle Ache;Depressed  0   NA
67    f   Placebo             Muscle Ache;Depressed  2 47.7
68    m   Placebo             Muscle Ache;Depressed  0   NA
69  ALL   Placebo             Muscle Ache;Depressed  2 47.7
70    f       ALL             Muscle Ache;Depressed  2 47.7
71    m       ALL             Muscle Ache;Depressed  0   NA
72  ALL       ALL             Muscle Ache;Depressed  2 47.7
73    f      Drug              Muscle Ache;Headache  1 41.7
74    m      Drug              Muscle Ache;Headache  0   NA
75  ALL      Drug              Muscle Ache;Headache  1 41.7
76    f   Placebo              Muscle Ache;Headache  0   NA
77    m   Placebo              Muscle Ache;Headache  0   NA
78  ALL   Placebo              Muscle Ache;Headache  0   NA
79    f       ALL              Muscle Ache;Headache  1 41.7
80    m       ALL              Muscle Ache;Headache  0   NA
81  ALL       ALL              Muscle Ache;Headache  1 41.7
82    f      Drug    Muscle Ache;Headache;Depressed  1 50.8
83    m      Drug    Muscle Ache;Headache;Depressed  0   NA
84  ALL      Drug    Muscle Ache;Headache;Depressed  1 50.8
85    f   Placebo    Muscle Ache;Headache;Depressed  0   NA
86    m   Placebo    Muscle Ache;Headache;Depressed  0   NA
87  ALL   Placebo    Muscle Ache;Headache;Depressed  0   NA
88    f       ALL    Muscle Ache;Headache;Depressed  1 50.8
89    m       ALL    Muscle Ache;Headache;Depressed  0   NA
90  ALL       ALL    Muscle Ache;Headache;Depressed  1 50.8
91    f      Drug          Muscle Ache;Stomach Ache  0   NA
92    m      Drug          Muscle Ache;Stomach Ache  0   NA
93  ALL      Drug          Muscle Ache;Stomach Ache  0   NA
94    f   Placebo          Muscle Ache;Stomach Ache  0   NA
95    m   Placebo          Muscle Ache;Stomach Ache  1 51.3
96  ALL   Placebo          Muscle Ache;Stomach Ache  1 51.3
97    f       ALL          Muscle Ache;Stomach Ache  0   NA
98    m       ALL          Muscle Ache;Stomach Ache  1 51.3
99  ALL       ALL          Muscle Ache;Stomach Ache  1 51.3
100   f      Drug Muscle Ache;Stomach Ache;Headache  0   NA
101   m      Drug Muscle Ache;Stomach Ache;Headache  1 44.3
102 ALL      Drug Muscle Ache;Stomach Ache;Headache  1 44.3
103   f   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
104   m   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
105 ALL   Placebo Muscle Ache;Stomach Ache;Headache  0   NA
106   f       ALL Muscle Ache;Stomach Ache;Headache  0   NA
107   m       ALL Muscle Ache;Stomach Ache;Headache  1 44.3
108 ALL       ALL Muscle Ache;Stomach Ache;Headache  1 44.3
109   f      Drug            Stomach Ache;Depressed  0   NA
110   m      Drug            Stomach Ache;Depressed  2 51.4
111 ALL      Drug            Stomach Ache;Depressed  2 51.4
112   f   Placebo            Stomach Ache;Depressed  0   NA
113   m   Placebo            Stomach Ache;Depressed  0   NA
114 ALL   Placebo            Stomach Ache;Depressed  0   NA
115   f       ALL            Stomach Ache;Depressed  0   NA
116   m       ALL            Stomach Ache;Depressed  2 51.4
117 ALL       ALL            Stomach Ache;Depressed  2 51.4
118   f      Drug             Stomach Ache;Headache  0   NA
119   m      Drug             Stomach Ache;Headache  0   NA
120 ALL      Drug             Stomach Ache;Headache  0   NA
121   f   Placebo             Stomach Ache;Headache  1 49.6
122   m   Placebo             Stomach Ache;Headache  0   NA
123 ALL   Placebo             Stomach Ache;Headache  1 49.6
124   f       ALL             Stomach Ache;Headache  1 49.6
125   m       ALL             Stomach Ache;Headache  0   NA
126 ALL       ALL             Stomach Ache;Headache  1 49.6
127   f      Drug   Stomach Ache;Headache;Depressed  1 45.2
128   m      Drug   Stomach Ache;Headache;Depressed  0   NA
129 ALL      Drug   Stomach Ache;Headache;Depressed  1 45.2
130   f   Placebo   Stomach Ache;Headache;Depressed  0   NA
131   m   Placebo   Stomach Ache;Headache;Depressed  0   NA
132 ALL   Placebo   Stomach Ache;Headache;Depressed  0   NA
133   f       ALL   Stomach Ache;Headache;Depressed  1 45.2
134   m       ALL   Stomach Ache;Headache;Depressed  0   NA
135 ALL       ALL   Stomach Ache;Headache;Depressed  1 45.2
136   f      Drug                               ALL  6 46.5
137   m      Drug                               ALL  5 48.5
138 ALL      Drug                               ALL 11 47.4
139   f   Placebo                               ALL  7 48.7
140   m   Placebo                               ALL  2 53.2
141 ALL   Placebo                               ALL  9 49.7
142   f       ALL                               ALL 13 47.7
143   m       ALL                               ALL  7 49.8
144 ALL       ALL                               ALL 20 48.4
> 
> f <- summary(treatment ~ age + sex + Symptoms, method="reverse", test=TRUE)
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
Warning in chisq.test(tab, correct = FALSE) :
  Chi-squared approximation may be incorrect
> f


Descriptive Statistics by treatment

+---------------------------+----------------------+----------------------+------------------------------+
|                           |Drug                  |Placebo               |  Test                        |
|                           |(N=11)                |(N=9)                 |Statistic                     |
+---------------------------+----------------------+----------------------+------------------------------+
|age                        |        45.8/46.4/48.7|        47.1/49.6/51.3|   F=2.09 d.f.=1,18 P=0.166   |
+---------------------------+----------------------+----------------------+------------------------------+
|sex : m                    |           45%  (5)   |           22%  (2)   |Chi-square=1.17 d.f.=1 P=0.279|
+---------------------------+----------------------+----------------------+------------------------------+
|Primary Symptoms : Hangnail|           45%  (5)   |           44%  (4)   |Chi-square=0.00 d.f.=1 P=0.964|
+---------------------------+----------------------+----------------------+------------------------------+
|    Muscle Ache            |           45%  (5)   |           67%  (6)   |Chi-square=0.90 d.f.=1 P=0.343|
+---------------------------+----------------------+----------------------+------------------------------+
|    Stomach Ache           |           55%  (6)   |           56%  (5)   |Chi-square=0.00 d.f.=1 P=0.964|
+---------------------------+----------------------+----------------------+------------------------------+
|    Headache               |           64%  (7)   |           33%  (3)   |Chi-square=1.82 d.f.=1 P=0.178|
+---------------------------+----------------------+----------------------+------------------------------+
|    Depressed              |           45%  (5)   |           33%  (3)   |Chi-square=0.30 d.f.=1 P=0.582|
+---------------------------+----------------------+----------------------+------------------------------+
> # trio of numbers represent 25th, 50th, 75th percentile
> print(f, long=TRUE)


Descriptive Statistics by treatment

+----------------+----------------------+----------------------+------------------------------+
|                |Drug                  |Placebo               |  Test                        |
|                |(N=11)                |(N=9)                 |Statistic                     |
+----------------+----------------------+----------------------+------------------------------+
|age             |        45.8/46.4/48.7|        47.1/49.6/51.3|   F=2.09 d.f.=1,18 P=0.166   |
+----------------+----------------------+----------------------+------------------------------+
|sex             |                      |                      |Chi-square=1.17 d.f.=1 P=0.279|
+----------------+----------------------+----------------------+------------------------------+
|    m           |           45%  (5)   |           22%  (2)   |                              |
+----------------+----------------------+----------------------+------------------------------+
|Primary Symptoms|                      |                      |                              |
+----------------+----------------------+----------------------+------------------------------+
|    Hangnail    |           45%  (5)   |           44%  (4)   |Chi-square=0.00 d.f.=1 P=0.964|
+----------------+----------------------+----------------------+------------------------------+
|    Muscle Ache |           45%  (5)   |           67%  (6)   |Chi-square=0.90 d.f.=1 P=0.343|
+----------------+----------------------+----------------------+------------------------------+
|    Stomach Ache|           55%  (6)   |           56%  (5)   |Chi-square=0.00 d.f.=1 P=0.964|
+----------------+----------------------+----------------------+------------------------------+
|    Headache    |           64%  (7)   |           33%  (3)   |Chi-square=1.82 d.f.=1 P=0.178|
+----------------+----------------------+----------------------+------------------------------+
|    Depressed   |           45%  (5)   |           33%  (3)   |Chi-square=0.30 d.f.=1 P=0.582|
+----------------+----------------------+----------------------+------------------------------+
> 
> 
> 
> cleanEx()
> nameEx("makeNstr")
> ### * makeNstr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeNstr
> ### Title: creates a string that is a repeat of a substring
> ### Aliases: makeNstr
> ### Keywords: manip character
> 
> ### ** Examples
> 
> makeNstr(" ", 5)
[1] "     "
> 
> ## Don't show: 
> if(makeNstr(" ", 5) != "     ") stop("makeNstr failed test")
> ## End(Don't show)
> 
> 
> 
> cleanEx()
> nameEx("mdb.get")
> ### * mdb.get
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mdb.get
> ### Title: Read Tables in a Microsoft Access Database
> ### Aliases: mdb.get
> ### Keywords: manip IO file
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Read all tables in the Microsoft Access database Nwind.mdb
> ##D d <- mdb.get('Nwind.mdb')
> ##D contents(d)
> ##D for(z in d) print(contents(z))
> ##D # Just print the names of tables in the database
> ##D mdb.get('Nwind.mdb', tables=TRUE)
> ##D # Import one table
> ##D Orders <- mdb.get('Nwind.mdb', tables='Orders')
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("meltData")
> ### * meltData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: meltData
> ### Title: meltData
> ### Aliases: meltData
> 
> ### ** Examples
> 
> d <- data.frame(y1=(1:10)/10, y2=(1:10)/100, x1=1:10, x2=101:110)
> label(d$x1) <- 'X1'
> units(d$x1) <- 'mmHg'
> m=meltData(y1 + y2 ~ x1 + x2, data=d, units=TRUE) # consider also html=TRUE
> print(m)
> m=meltData(y1 + y2 ~ x1 + x2, data=d, tall='left')
> print(m)
> 
> 
> 
> cleanEx()
> nameEx("mgp.axis")
> ### * mgp.axis
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgp.axis
> ### Title: Draw Axes With Side-Specific mgp Parameters
> ### Aliases: mgp.axis mgp.axis.labels
> ### Keywords: iplot dplot environment
> 
> ### ** Examples
> 
> ## Not run: 
> ##D mgp.axis.labels(type='x')  # get default value for x-axis
> ##D mgp.axis.labels(type='y')  # get value for y-axis
> ##D mgp.axis.labels(type='xy') # get 2nd element of both mgps
> ##D mgp.axis.labels(type='x and y')  # get a list with 2 elements
> ##D mgp.axis.labels(c(3,.5,0), type='x')  # set
> ##D options('mgp.axis.labels')            # retrieve
> ##D 
> ##D plot(..., axes=FALSE)
> ##D mgp.axis(1, "X Label")
> ##D mgp.axis(2, "Y Label")
> ##D 
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("minor.tick")
> ### * minor.tick
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: minor.tick
> ### Title: Minor Tick Marks
> ### Aliases: minor.tick
> ### Keywords: aplot hplot
> 
> ### ** Examples
> 
> # Plot with default settings
> plot(runif(20), runif(20))
> minor.tick()
> 
> # Plot with arguments passed to axis()
> plot(c(0,1), c(0,1), type = 'n', axes = FALSE, ann = FALSE)
> # setting up a plot without axes and annotation
> points(runif(20), runif(20))                       # plotting data
> axis(1, pos = 0.5, lwd = 2)                        # showing X-axis at Y = 0.5 with formatting
> axis(2, col = 2)                                   # formatted Y-axis
> minor.tick( nx = 4, ny = 4, tick.ratio = 0.3,
+             x.args = list(pos = 0.5, lwd = 2),     # X-minor tick format argumnets
+             y.args = list(col = 2))                # Y-minor tick format arguments
> 
> 
> 
> cleanEx()
> nameEx("mtitle")
> ### * mtitle
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mtitle
> ### Title: Margin Titles
> ### Aliases: mtitle
> ### Keywords: hplot
> 
> ### ** Examples
> 
> #Set up for 1 plot on figure, give a main title,
> #use date for lr
> plot(runif(20),runif(20))
> mtitle("Main Title")
> 
> 
> #Set up for 2 x 2 matrix of plots with a lower left subtitle and overall title
> par(mfrow=c(2,2), oma=c(3,0,3,0))
> plot(runif(20),runif(20))
> plot(rnorm(20),rnorm(20))
> plot(exp(rnorm(20)),exp(rnorm(20)))
> mtitle("Main Title",ll="n=20")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("multLines")
> ### * multLines
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multLines
> ### Title: Plot Multiple Lines
> ### Aliases: multLines
> ### Keywords: hplot
> 
> ### ** Examples
> 
> if (requireNamespace("plotly")) {
+   x <- 1:4
+   y <- cbind(x, x-3, x-2, x-1, x+1, x+2, x+3)
+   plot(NA, NA, xlim=c(1,4), ylim=c(-2, 7))
+   multLines(x, y, col='blue')
+   multLines(x, y, col='red', pos='right')
+ }
> 
> 
> 
> cleanEx()
> nameEx("nCoincident")
> ### * nCoincident
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nCoincident
> ### Title: nCoincident
> ### Aliases: nCoincident
> 
> ### ** Examples
> 
> nCoincident(c(1:5, 4:5), c(1:5, 4:5)/10)
[1] 2
> 
> 
> 
> cleanEx()
> nameEx("na.delete")
> ### * na.delete
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.delete
> ### Title: Row-wise Deletion na.action
> ### Aliases: na.delete
> ### Keywords: models
> 
> ### ** Examples
> 
> # options(na.action="na.delete")
> # ols(y ~ x)
> 
> 
> 
> cleanEx()
> nameEx("na.detail.response")
> ### * na.detail.response
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.detail.response
> ### Title: Detailed Response Variable Information
> ### Aliases: na.detail.response
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # sex
> # [1] m f f m f f m m m m m m m m f f f m f m
> # age
> # [1] NA 41 23 30 44 22 NA 32 37 34 38 36 36 50 40 43 34 22 42 30
> # y
> # [1] 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0
> # options(na.detail.response=TRUE, na.action="na.delete", digits=3)
> # lrm(y ~ age*sex)
> #
> # Logistic Regression Model
> # 
> # lrm(formula = y ~ age * sex)
> #
> #
> # Frequencies of Responses
> #   0 1 
> #  10 8
> #
> # Frequencies of Missing Values Due to Each Variable
> #  y age sex 
> #  0   2   0
> #
> #
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #     age=NA age!=NA sex!=NA Any NA  No NA 
> #   N    2.0  18.000   20.00    2.0 18.000
> # Mean    0.5   0.444    0.45    0.5  0.444
> #
> # \dots\dots
> # options(na.action="na.keep")
> # describe(y ~ age*sex)
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #      age=NA age!=NA sex!=NA Any NA  No NA 
> #    N    2.0  18.000   20.00    2.0 18.000
> # Mean    0.5   0.444    0.45    0.5  0.444
> #
> # \dots
> # options(na.fun.response="table")  #built-in function table()
> # describe(y ~ age*sex)
> #
> # Statistics on Response by Missing/Non-Missing Status of Predictors
> #
> #   age=NA age!=NA sex!=NA Any NA No NA 
> # 0      1      10      11      1    10
> # 1      1       8       9      1     8
> #
> # \dots
> 
> 
> 
> cleanEx()
> nameEx("na.keep")
> ### * na.keep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: na.keep
> ### Title: Do-nothing na.action
> ### Aliases: na.keep
> ### Keywords: models
> 
> ### ** Examples
> 
> options(na.action="na.keep", na.detail.response=TRUE)
> x1 <- runif(20)
> x2 <- runif(20)
> x2[1:4] <- NA
> y <- rnorm(20)
> describe(y ~ x1*x2)
y ~ x1 * x2 

 3  Variables      20  Observations
--------------------------------------------------------------------------------
y 
        n   missing  distinct      Info      Mean   pMedian       Gmd       .05 
       20         0        20         1 -0.006472  0.009799    0.9847  -1.49668 
      .10       .25       .50       .75       .90       .95 
 -1.38643  -0.39947  -0.05497   0.65566   0.93708   1.11296 

-1.98935169586337 (1, 0.05), -1.47075238389927 (1, 0.05), -1.37705955682861 (1,
0.05), -0.47815005510862 (1, 0.05), -0.41499456329968 (1, 0.05),
-0.394289953710349 (1, 0.05), -0.155795506705329 (1, 0.05), -0.102787727342996
(1, 0.05), -0.0593133967111857 (1, 0.05), -0.0561287395290008 (1, 0.05),
-0.0538050405829051 (1, 0.05), 0.0745649833651906 (1, 0.05), 0.387671611559369
(1, 0.05), 0.417941560199702 (1, 0.05), 0.61982574789471 (1, 0.05),
0.763175748457544 (1, 0.05), 0.782136300731067 (1, 0.05), 0.918977371608218 (1,
0.05), 1.10002537198388 (1, 0.05), 1.35867955152904 (1, 0.05)

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
x1 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
      20        0       20        1   0.5552    0.556   0.3367   0.1708 
     .10      .25      .50      .75      .90      .95 
  0.1992   0.3455   0.6010   0.7717   0.9119   0.9470 

0.0617862704675645 (1, 0.05), 0.176556752528995 (1, 0.05), 0.201681931037456
(1, 0.05), 0.205974574899301 (1, 0.05), 0.2655086631421 (1, 0.05),
0.37212389963679 (1, 0.05), 0.380035179434344 (1, 0.05), 0.384103718213737 (1,
0.05), 0.497699242085218 (1, 0.05), 0.572853363351896 (1, 0.05),
0.62911404389888 (1, 0.05), 0.660797792486846 (1, 0.05), 0.687022846657783 (1,
0.05), 0.717618508264422 (1, 0.05), 0.769841419998556 (1, 0.05),
0.777445221319795 (1, 0.05), 0.898389684967697 (1, 0.05), 0.908207789994776 (1,
0.05), 0.944675268605351 (1, 0.05), 0.991906094830483 (1, 0.05)

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
x2 
       n  missing distinct     Info     Mean  pMedian      Gmd      .05 
      16        4       16        1   0.4721   0.4689   0.3064  0.08431 
     .10      .25      .50      .75      .90      .95 
 0.14708  0.32207  0.44668  0.68228  0.81081  0.83795 
                                                                         
Value      0.0134 0.1079 0.1862 0.2672 0.3403 0.3824 0.3861 0.4113 0.4821
Frequency       1      1      1      1      1      1      1      1      1
Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062
                                                           
Value      0.4935 0.5996 0.6685 0.7237 0.7942 0.8274 0.8697
Frequency       1      1      1      1      1      1      1
Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062

For the frequency table, variable is rounded to the nearest 0
--------------------------------------------------------------------------------
> 
> 
> 
> cleanEx()
> nameEx("nin")
> ### * nin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: %nin%
> ### Title: Find Matching (or Non-Matching) Elements
> ### Aliases: %nin%
> ### Keywords: manip character
> 
> ### ** Examples
> 
> c('a','b','c') %nin% c('a','b')
[1] FALSE FALSE  TRUE
> 
> 
> 
> cleanEx()
> nameEx("nobsY")
> ### * nobsY
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nobsY
> ### Title: Compute Number of Observations for Left Hand Side of Formula
> ### Aliases: nobsY
> ### Keywords: utilities manip
> 
> ### ** Examples
> 
> d <- expand.grid(sex=c('female', 'male', NA),
+                  country=c('US', 'Romania'),
+                  reps=1:2)
> d$subject.id <- c(0, 0, 3:12)
> dm <- addMarginal(d, sex, country)
> dim(dm)
[1] 48  5
> nobsY(sex + country ~ 1, data=d)
$nobs
    sex country 
      8      12 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ 1

> nobsY(sex + country ~ id(subject.id), data=d)
$nobs
    sex country 
      7      11 

$nobsg
NULL

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ 1
<environment: 0x55965de40ce8>

> nobsY(sex + country ~ id(subject.id) + reps, group='reps', data=d)
$nobs
    sex country 
      7      11 

$nobsg
  sex country
1   3       5
2   4       6

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex + country ~ id(subject.id) + reps
<environment: 0x55965d59f0a8>

> nobsY(sex ~ 1, data=d)
$nobs
sex 
  8 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12

$formula
sex ~ 1

> nobsY(sex ~ 1, data=dm)
$nobs
sex 
  8 

$nobsg
NULL

$id
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48

$formula
sex ~ 1

> nobsY(sex ~ id(subject.id), data=dm)
$nobs
sex 
  7 

$nobsg
NULL

$id
 [1]  0  0  3  4  5  6  7  8  9 10 11 12  0  0  3  4  5  6  7  8  9 10 11 12  0
[26]  0  3  4  5  6  7  8  9 10 11 12  0  0  3  4  5  6  7  8  9 10 11 12

$formula
sex ~ id(subject.id)
<environment: 0x55965c4ddd68>

> 
> 
> 
> cleanEx()
> nameEx("nstr")
> ### * nstr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nstr
> ### Title: Creates a string of arbitry length
> ### Aliases: nstr
> ### Keywords: manip character utilities
> 
> ### ** Examples
> 
> nstr(c("a"), c(0,3,4))
[1] ""     "aaa"  "aaaa"
> 
> nstr(c("a", "b", "c"), c(1,2,3))
[1] "a"   "bb"  "ccc"
> 
> nstr(c("a", "b", "c"), 4)
[1] "aaaa" "bbbb" "cccc"
> 
> 
> 
> cleanEx()
> nameEx("pMedian")
> ### * pMedian
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pMedian
> ### Title: pMedian
> ### Aliases: pMedian
> 
> ### ** Examples
> 
> x <- c(1:4, 10000)
> pMedian(x)
[1] 3
> # Compare with brute force calculation and with wilcox.test
> w <- outer(x, x, '+')
> median(w[lower.tri(w, diag=TRUE)]) / 2
[1] 3
> wilcox.test(x, conf.int=TRUE)

	Wilcoxon signed rank exact test

data:  x
V = 15, p-value = 0.06
alternative hypothesis: true location is not equal to 0
95 percent confidence interval:
     1 10000
sample estimates:
(pseudo)median 
             3 

> 
> 
> 
> cleanEx()
> nameEx("pairUpDiff")
> ### * pairUpDiff
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pairUpDiff
> ### Title: pairUpDiff
> ### Aliases: pairUpDiff
> 
> ### ** Examples
> 
> x <- c(1, 4, 7, 2, 5, 3, 6)
> pairUpDiff(x, c(rep('A', 4), rep('B', 3)),
+   c('u','u','v','v','z','z','q'),
+   c('a','b','a','b','a','b','a'), 'a', x-.1, x+.1)
$X
  x major minor group lower upper subscripts
1 1     A     u     a   0.9   1.1          1
2 4     A     u     b   3.9   4.1          2
3 7     A     v     a   6.9   7.1          3
4 2     A     v     b   1.9   2.1          4
7 6     B     q     a   5.9   6.1          7
5 5     B     z     a   4.9   5.1          5
6 3     B     z     b   2.9   3.1          6

$D
    diff major minor     sd lower upper mid lowermid uppermid
A:u    3     A     u 0.0722  2.86  3.14 2.5     2.43     2.57
A:v   -5     A     v 0.0722 -5.14 -4.86 4.5     4.43     4.57
B:q   NA     B     q     NA    NA    NA  NA       NA       NA
B:z   -2     B     z 0.0722 -2.14 -1.86 4.0     3.93     4.07

$dropped
NULL

> 
> 
> 
> cleanEx()
> nameEx("panel.bpplot")
> ### * panel.bpplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: panel.bpplot
> ### Title: Box-Percentile Panel Function for Trellis
> ### Aliases: panel.bpplot bpplotM bpplt bppltp
> ### Keywords: nonparametric hplot distribution
> 
> ### ** Examples
> 
> set.seed(13)
> x <- rnorm(1000)
> g <- sample(1:6, 1000, replace=TRUE)
> x[g==1][1:20] <- rnorm(20)+3   # contaminate 20 x's for group 1
> 
> 
> # default trellis box plot
> require(lattice)
Loading required package: lattice
> bwplot(g ~ x)
> 
> 
> # box-percentile plot with data density (rug plot)
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.49,by=.01), datadensity=TRUE)
> # add ,scat1d.opts=list(tfrac=1) to make all tick marks the same size
> # when a group has > 125 observations
> 
> 
> # small dot for means, show only .05,.125,.25,.375,.625,.75,.875,.95 quantiles
> bwplot(g ~ x, panel=panel.bpplot, cex.means=.3)
> 
> 
> # suppress means and reference lines for lower and upper quartiles
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,.1,.25), means=FALSE, qref=FALSE)
> 
> 
> # continuous plot up until quartiles ("Tootsie Roll plot")
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.01,.25,by=.01))
> 
> 
> # start at quartiles then make it continuous ("coffin plot")
> bwplot(g ~ x, panel=panel.bpplot, probs=seq(.25,.49,by=.01))
> 
> 
> # same as previous but add a spike to give 0.95 interval
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.025,seq(.25,.49,by=.01)))
> 
> 
> # decile plot with reference lines at outer quintiles and median
> bwplot(g ~ x, panel=panel.bpplot, probs=c(.1,.2,.3,.4), qref=c(.5,.2,.8))
> 
> 
> # default plot with tick marks showing all observations outside the outer
> # box (.05 and .95 quantiles), with very small ticks
> bwplot(g ~ x, panel=panel.bpplot, nout=.05, scat1d.opts=list(frac=.01))
> 
> 
> # show 5 smallest and 5 largest observations
> bwplot(g ~ x, panel=panel.bpplot, nout=5)
> 
> 
> # Use a scat1d option (preserve=TRUE) to ensure that the right peak extends 
> # to the same position as the extreme scat1d
> bwplot(~x , panel=panel.bpplot, probs=seq(.00,.5,by=.001), 
+        datadensity=TRUE, scat1d.opt=list(preserve=TRUE))
> 
> # Add an extended box plot to an existing base graphics plot
> plot(x, 1:length(x))
> panel.bpplot(x, 1070, nogrid=TRUE, pch=19, height=15, cex.means=.5)
> 
> # Draw a prototype showing how to interpret the plots
> bpplt()
> 
> # Example for bpplotM
> set.seed(1)
> n <- 800
> d <- data.frame(treatment=sample(c('a','b'), n, TRUE),
+                 sex=sample(c('female','male'), n, TRUE),
+                 age=rnorm(n, 40, 10),
+                 bp =rnorm(n, 120, 12),
+                 wt =rnorm(n, 190, 30))
> label(d$bp) <- 'Systolic Blood Pressure'
> units(d$bp) <- 'mmHg'
> bpplotM(age + bp + wt ~ treatment, data=d)
Error in sRequire("latticeExtra") : 
  package latticeExtra is required but not installed
Calls: bpplotM -> sRequire
Execution halted
