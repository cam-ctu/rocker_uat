
R version 4.3.2 (2023-10-31) -- "Eye Holes"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "lavaan"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('lavaan')
This is lavaan 0.6-17
lavaan is FREE software! Please report any bugs.
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Demo.growth")
> ### * Demo.growth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Demo.growth
> ### Title: Demo dataset for a illustrating a linear growth model.
> ### Aliases: Demo.growth
> 
> ### ** Examples
> 
> head(Demo.growth)
          t1         t2         t3          t4         x1         x2
1  1.7256454  2.1424005  2.7731717  2.51595586 -1.1641026  0.1742293
2 -1.9841595 -4.4006027 -6.0165563 -7.02961801 -1.7454025 -1.5768602
3  0.3195183 -1.2691171  1.5600160  2.86852958  0.9202112 -0.1418180
4  0.7769485  3.5313707  3.1382114  5.36374139  2.3595236  0.7079681
5  0.4489440 -0.7727747 -1.5035150  0.07846742 -1.0887077 -1.0099977
6 -1.7469951 -0.9963400 -0.8242174  0.56692480 -0.5135169 -0.1440428
           c1         c2         c3          c4
1 -0.02767765  0.5549234  0.2544784 -1.00639541
2 -2.03196724  0.1253348 -1.5642323  1.22926875
3  0.05237496 -1.2577408 -1.8033909 -0.32725761
4  0.01911429  0.6473830 -0.4323795 -1.03239779
5  0.65243274  0.7309148 -0.7537816 -0.02745598
6 -0.04452906 -0.4168022 -1.2411670  0.51719763
> 
> 
> 
> cleanEx()
> nameEx("Demo.twolevel")
> ### * Demo.twolevel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Demo.twolevel
> ### Title: Demo dataset for a illustrating a multilevel CFA.
> ### Aliases: Demo.twolevel
> 
> ### ** Examples
> 
> head(Demo.twolevel)
          y1         y2         y3         y4         y5         y6         x1
1  0.2293216  1.3555232 -0.6911702  0.8028079 -0.3011085 -1.7260671  1.1739003
2  0.3085801 -1.8624397 -2.4179783  0.7659289  1.6750617  1.1764210 -1.0039958
3  0.2004934 -1.3400514  0.4376087  1.1974194  1.1951594  1.4988962 -0.4402545
4  1.0447982 -0.9624490 -0.4464898 -0.2027252 -0.4590574  1.1734061 -0.6253657
5  0.6881792 -0.4565633 -0.6422296  0.9900408  1.7662535  0.7944601 -0.8450025
6 -2.0687644 -0.5997856  0.3148418  0.6764432 -0.6519928  1.8405605 -0.7831784
           x2         x3         w1         w2 cluster
1 -0.62315173  0.6470414 -0.2479975 -0.4989800       1
2 -0.56689380  0.0201264 -0.2479975 -0.4989800       1
3 -2.13432572 -0.4591246 -0.2479975 -0.4989800       1
4 -0.33688869  1.2852093 -0.2479975 -0.4989800       1
5 -0.04229954  1.5598970 -0.2479975 -0.4989800       1
6 -0.22441996 -0.3814231 -2.3219338 -0.6910567       2
> 
> model <- '
+     level: 1
+         fw =~ y1 + y2 + y3
+         fw ~ x1 + x2 + x3
+     level: 2
+         fb =~ y1 + y2 + y3
+         fb ~ w1 + w2
+ '
> fit <- sem(model, data = Demo.twolevel, cluster = "cluster")
> summary(fit)
lavaan 0.6.17 ended normally after 36 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        20

  Number of observations                          2500
  Number of clusters [cluster]                     200

Model Test User Model:
                                                      
  Test statistic                                 8.092
  Degrees of freedom                                10
  P-value (Chi-square)                           0.620

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian


Level 1 [within]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  fw =~                                               
    y1                1.000                           
    y2                0.774    0.034   22.671    0.000
    y3                0.734    0.033   22.355    0.000

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  fw ~                                                
    x1                0.510    0.023   22.037    0.000
    x2                0.407    0.022   18.273    0.000
    x3                0.205    0.021    9.740    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .y1                0.986    0.046   21.591    0.000
   .y2                1.066    0.039   27.271    0.000
   .y3                1.011    0.037   27.662    0.000
   .fw                0.546    0.040   13.539    0.000


Level 2 [cluster]:

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  fb =~                                               
    y1                1.000                           
    y2                0.717    0.052   13.824    0.000
    y3                0.587    0.048   12.329    0.000

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  fb ~                                                
    w1                0.165    0.079    2.093    0.036
    w2                0.131    0.076    1.715    0.086

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)
   .y1                0.024    0.075    0.327    0.743
   .y2               -0.016    0.060   -0.269    0.788
   .y3               -0.042    0.054   -0.777    0.437

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .y1                0.058    0.047    1.213    0.225
   .y2                0.120    0.031    3.825    0.000
   .y3                0.149    0.028    5.319    0.000
   .fb                0.899    0.118    7.592    0.000

> 
> 
> 
> cleanEx()
> nameEx("FacialBurns")
> ### * FacialBurns
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FacialBurns
> ### Title: Dataset for illustrating the InformativeTesting function.
> ### Aliases: FacialBurns
> 
> ### ** Examples
> 
> head(FacialBurns)
  Selfesteem HADS Age  TBSA RUM Sex
1         35    6  23  3.50   5   1
2         40    2  61  6.00   4   1
3         38    2  34  8.00   4   1
4         30    4  29  6.00   5   1
5         27   11  46 19.25  13   1
6         35    5  18  6.00   8   1
> 
> 
> 
> cleanEx()
> nameEx("HolzingerSwineford1939")
> ### * HolzingerSwineford1939
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: HolzingerSwineford1939
> ### Title: Holzinger and Swineford Dataset (9 Variables)
> ### Aliases: HolzingerSwineford1939
> 
> ### ** Examples
> 
> head(HolzingerSwineford1939)
  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6
1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143
2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143
3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714
4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714
5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286
6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429
        x7   x8       x9
1 3.391304 5.75 6.361111
2 3.782609 6.25 7.916667
3 3.260870 3.90 4.416667
4 3.000000 5.30 4.861111
5 3.695652 6.30 5.916667
6 4.347826 6.65 7.500000
> 
> 
> 
> cleanEx()
> nameEx("InformativeTesting")
> ### * InformativeTesting
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: InformativeTesting
> ### Title: Testing order/inequality Constrained Hypotheses in SEM
> ### Aliases: InformativeTesting informativetesting
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #########################
> ##D ### real data example ###
> ##D #########################
> ##D # Multiple group path model for facial burns example.
> ##D 
> ##D # model syntax with starting values.
> ##D   burns.model <- 'Selfesteem ~ Age + c(m1, f1)*TBSA + HADS +
> ##D                              start(-.10, -.20)*TBSA  
> ##D                  HADS ~ Age + c(m2, f2)*TBSA + RUM +
> ##D                         start(.10, .20)*TBSA '
> ##D  
> ##D  
> ##D # constraints syntax
> ##D  burns.constraints <- 'f2 > 0  ; m1 < 0
> ##D                        m2 > 0  ; f1 < 0
> ##D                        f2 > m2 ; f1 < m1'
> ##D  
> ##D # we only generate 2 bootstrap samples in this example; in practice
> ##D # you may wish to use a much higher number. 
> ##D # the double bootstrap was switched off; in practice you probably 
> ##D # want to set it to "standard".
> ##D example1 <- InformativeTesting(model = burns.model, data = FacialBurns,
> ##D                                R = 2, constraints = burns.constraints,
> ##D                                double.bootstrap = "no", group = "Sex")
> ##D 
> ##D example1
> ##D 
> ##D ##########################
> ##D ### artificial example ###
> ##D ##########################
> ##D # Simple ANOVA model with 3 groups (N = 20 per group)
> ##D set.seed(1234)
> ##D Y <- cbind(c(rnorm(20,0,1), rnorm(20,0.5,1), rnorm(20,1,1)))
> ##D grp <- c(rep("1", 20), rep("2", 20), rep("3", 20))
> ##D Data <- data.frame(Y, grp)
> ##D 
> ##D #create model matrix
> ##D fit.lm <- lm(Y ~ grp, data = Data)
> ##D mfit <- fit.lm$model
> ##D mm <- model.matrix(mfit)
> ##D 
> ##D Y <- model.response(mfit)
> ##D X <- data.frame(mm[,2:3])
> ##D names(X) <- c("d1", "d2")
> ##D Data.new <- data.frame(Y, X)
> ##D 
> ##D # model
> ##D model <- 'Y ~ 1 + a1*d1 + a2*d2'
> ##D 
> ##D # fit without constraints
> ##D fit <- sem(model, data = Data.new)
> ##D 
> ##D # constraints syntax: mu1 < mu2 < mu3
> ##D constraints <- ' a1 > 0
> ##D                  a1 < a2 '
> ##D 
> ##D # we only generate 10 bootstrap samples in this example; in practice
> ##D # you may wish to use a much higher number, say > 1000. The double 
> ##D # bootstrap is not necessary in case of an univariate ANOVA model.
> ##D example2 <- InformativeTesting(model = model, data = Data.new, 
> ##D                                start = parTable(fit),
> ##D                                R = 10L, double.bootstrap = "no",
> ##D                                constraints = constraints)
> ##D example2
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("PoliticalDemocracy")
> ### * PoliticalDemocracy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: PoliticalDemocracy
> ### Title: Industrialization And Political Democracy Dataset
> ### Aliases: PoliticalDemocracy
> 
> ### ** Examples
> 
> head(PoliticalDemocracy)
     y1       y2       y3       y4       y5       y6       y7       y8       x1
1  2.50 0.000000 3.333333 0.000000 1.250000 0.000000 3.726360 3.333333 4.442651
2  1.25 0.000000 3.333333 0.000000 6.250000 1.100000 6.666666 0.736999 5.384495
3  7.50 8.800000 9.999998 9.199991 8.750000 8.094061 9.999998 8.211809 5.961005
4  8.90 8.800000 9.999998 9.199991 8.907948 8.127979 9.999998 4.615086 6.285998
5 10.00 3.333333 9.999998 6.666666 7.500000 3.333333 9.999998 6.666666 5.863631
6  7.50 3.333333 6.666666 6.666666 6.250000 1.100000 6.666666 0.368500 5.533389
        x2       x3
1 3.637586 2.557615
2 5.062595 3.568079
3 6.255750 5.224433
4 7.567863 6.267495
5 6.818924 4.573679
6 5.135798 3.892270
> 
> 
> 
> cleanEx()
> nameEx("bootstrap")
> ### * bootstrap
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootstrapLavaan
> ### Title: Bootstrapping a Lavaan Model
> ### Aliases: bootstrapLavaan bootstrapLRT
> 
> ### ** Examples
> 
> # fit the Holzinger and Swineford (1939) example
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939, se="none")
> 
> # get the test statistic for the original sample
> T.orig <- fitMeasures(fit, "chisq")
> 
> # bootstrap to get bootstrap test statistics
> # we only generate 10 bootstrap sample in this example; in practice
> # you may wish to use a much higher number
> T.boot <- bootstrapLavaan(fit, R=10, type="bollen.stine",
+                           FUN=fitMeasures, fit.measures="chisq")
> 
> # compute a bootstrap based p-value
> pvalue.boot <- length(which(T.boot > T.orig))/length(T.boot)
> 
> 
> 
> cleanEx()
> nameEx("cfa")
> ### * cfa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cfa
> ### Title: Fit Confirmatory Factor Analysis Models
> ### Aliases: cfa
> 
> ### ** Examples
> 
> ## The famous Holzinger and Swineford (1939) example
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data = HolzingerSwineford1939)
> summary(fit, fit.measures = TRUE)
lavaan 0.6.17 ended normally after 35 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        21

  Number of observations                           301

Model Test User Model:
                                                      
  Test statistic                                85.306
  Degrees of freedom                                24
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               918.852
  Degrees of freedom                                36
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.931
  Tucker-Lewis Index (TLI)                       0.896

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -3737.745
  Loglikelihood unrestricted model (H1)      -3695.092
                                                      
  Akaike (AIC)                                7517.490
  Bayesian (BIC)                              7595.339
  Sample-size adjusted Bayesian (SABIC)       7528.739

Root Mean Square Error of Approximation:

  RMSEA                                          0.092
  90 Percent confidence interval - lower         0.071
  90 Percent confidence interval - upper         0.114
  P-value H_0: RMSEA <= 0.050                    0.001
  P-value H_0: RMSEA >= 0.080                    0.840

Standardized Root Mean Square Residual:

  SRMR                                           0.065

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  visual =~                                           
    x1                1.000                           
    x2                0.554    0.100    5.554    0.000
    x3                0.729    0.109    6.685    0.000
  textual =~                                          
    x4                1.000                           
    x5                1.113    0.065   17.014    0.000
    x6                0.926    0.055   16.703    0.000
  speed =~                                            
    x7                1.000                           
    x8                1.180    0.165    7.152    0.000
    x9                1.082    0.151    7.155    0.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
  visual ~~                                           
    textual           0.408    0.074    5.552    0.000
    speed             0.262    0.056    4.660    0.000
  textual ~~                                          
    speed             0.173    0.049    3.518    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .x1                0.549    0.114    4.833    0.000
   .x2                1.134    0.102   11.146    0.000
   .x3                0.844    0.091    9.317    0.000
   .x4                0.371    0.048    7.779    0.000
   .x5                0.446    0.058    7.642    0.000
   .x6                0.356    0.043    8.277    0.000
   .x7                0.799    0.081    9.823    0.000
   .x8                0.488    0.074    6.573    0.000
   .x9                0.566    0.071    8.003    0.000
    visual            0.809    0.145    5.564    0.000
    textual           0.979    0.112    8.737    0.000
    speed             0.384    0.086    4.451    0.000

> 
> 
> 
> cleanEx()
> nameEx("efa")
> ### * efa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: efa
> ### Title: Exploratory Factor Analysis
> ### Aliases: efa rotation
> 
> ### ** Examples
> 
> ## The famous Holzinger and Swineford (1939) example
> fit <- efa(data = HolzingerSwineford1939, 
+            ov.names = paste("x", 1:9, sep = ""),
+            nfactors = 1:3,
+            rotation = "geomin",
+            rotation.args = list(geomin.epsilon = 0.01, rstarts = 1))
> summary(fit, nd = 3L, cutoff = 0.2, dot.cutoff = 0.05)
This is lavaan 0.6.17 -- running exploratory factor analysis

  Estimator                                         ML
  Rotation method                       GEOMIN OBLIQUE
  Geomin epsilon                                  0.01
  Rotation algorithm (rstarts)                 GPA (1)
  Standardized metric                             TRUE
  Row weights                                     None

  Number of observations                           301

Overview models:
                    aic      bic    sabic   chisq df pvalue   cfi rmsea
  nfactors = 1 7738.448 7805.176 7748.091 312.264 27  0.000 0.677 0.187
  nfactors = 2 7572.491 7668.876 7586.418 130.306 19  0.000 0.874 0.140
  nfactors = 3 7479.081 7601.416 7496.758  22.897 12  0.029 0.988 0.055

Eigenvalues correlation matrix:

    ev1     ev2     ev3     ev4     ev5     ev6     ev7     ev8     ev9 
  3.216   1.639   1.365   0.699   0.584   0.500   0.473   0.286   0.238 

Number of factors:  1 

Standardized loadings: (* = significant at 1% level)

       f1       unique.var   communalities
x1  0.438*           0.808           0.192
x2  0.220*           0.951           0.049
x3  0.223*           0.950           0.050
x4  0.848*           0.281           0.719
x5  0.841*           0.293           0.707
x6  0.838*           0.298           0.702
x7      .*           0.967           0.033
x8  0.201*           0.960           0.040
x9  0.307*           0.906           0.094

                           f1
Sum of squared loadings 2.586
Proportion of total     1.000
Proportion var          0.287
Cumulative var          0.287

Number of factors:  2 

Standardized loadings: (* = significant at 1% level)

       f1      f2       unique.var   communalities
x1  0.261*  0.430*           0.673           0.327
x2      .   0.251*           0.906           0.094
x3          0.455*           0.783           0.217
x4  0.850*                   0.274           0.726
x5  0.867*                   0.264           0.736
x6  0.824*                   0.302           0.698
x7          0.447*           0.802           0.198
x8      .   0.626*           0.630           0.370
x9          0.732*           0.458           0.542

                              f1    f2 total
Sum of sq (obliq) loadings 2.281 1.628 3.909
Proportion of total        0.584 0.416 1.000
Proportion var             0.253 0.181 0.434
Cumulative var             0.253 0.434 0.434

Factor correlations: (* = significant at 1% level)

       f1      f2 
f1  1.000         
f2  0.331*  1.000 

Number of factors:  3 

Standardized loadings: (* = significant at 1% level)

       f1      f2      f3       unique.var   communalities
x1  0.604*      .*                   0.513           0.487
x2  0.507*              .            0.749           0.251
x3  0.691*      .                    0.543           0.457
x4          0.839*                   0.279           0.721
x5      .   0.887*                   0.243           0.757
x6      .   0.806*                   0.305           0.695
x7      .           0.726*           0.502           0.498
x8      .           0.703*           0.469           0.531
x9  0.368*          0.463*           0.543           0.457

                              f2    f1    f3 total
Sum of sq (obliq) loadings 2.226 1.345 1.284 4.855
Proportion of total        0.458 0.277 0.264 1.000
Proportion var             0.247 0.149 0.143 0.539
Cumulative var             0.247 0.397 0.539 0.539

Factor correlations: (* = significant at 1% level)

       f1      f2      f3 
f1  1.000                 
f2  0.327*  1.000         
f3  0.278*  0.230*  1.000 

> fitMeasures(fit, fit.measures = "all")
                         nfct=1    nfct=2    nfct=3
npar                     18.000    26.000    33.000
fmin                      0.519     0.216     0.038
chisq                   312.264   130.306    22.897
df                       27.000    19.000    12.000
pvalue                    0.000     0.000     0.029
baseline.chisq          918.852   918.852   918.852
baseline.df              36.000    36.000    36.000
baseline.pvalue           0.000     0.000     0.000
cfi                       0.677     0.874     0.988
tli                       0.569     0.761     0.963
nnfi                      0.569     0.761     0.963
rfi                       0.547     0.731     0.925
nfi                       0.660     0.858     0.975
pnfi                      0.495     0.453     0.325
ifi                       0.680     0.876     0.988
rni                       0.677     0.874     0.988
logl                  -3851.224 -3760.245 -3706.541
unrestricted.logl     -3695.092 -3695.092 -3695.092
aic                    7738.448  7572.491  7479.081
bic                    7805.176  7668.876  7601.416
ntotal                  301.000   301.000   301.000
bic2                   7748.091  7586.418  7496.758
rmsea                     0.187     0.140     0.055
rmsea.ci.lower            0.169     0.117     0.017
rmsea.ci.upper            0.206     0.163     0.089
rmsea.ci.level            0.900     0.900     0.900
rmsea.pvalue              0.000     0.000     0.365
rmsea.close.h0            0.050     0.050     0.050
rmsea.notclose.pvalue     1.000     1.000     0.120
rmsea.notclose.h0         0.080     0.080     0.080
rmr                       0.169     0.096     0.022
rmr_nomean                0.169     0.096     0.022
srmr                      0.143     0.076     0.017
srmr_bentler              0.143     0.076     0.017
srmr_bentler_nomean       0.143     0.076     0.017
crmr                      0.160     0.085     0.019
crmr_nomean               0.160     0.085     0.019
srmr_mplus                0.143     0.076     0.017
srmr_mplus_nomean         0.143     0.076     0.017
cn_05                    39.666    70.630   277.408
cn_01                    46.269    84.599   345.648
gfi                       0.792     0.900     0.983
agfi                      0.653     0.763     0.938
pgfi                      0.475     0.380     0.262
mfi                       0.623     0.831     0.982
ecvi                      1.157     0.606     0.295
> 
> 
> 
> cleanEx()
> nameEx("fitMeasures")
> ### * fitMeasures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fitMeasures
> ### Title: Fit Measures for a Latent Variable Model
> ### Aliases: fitMeasures fitmeasures fitMeasures,lavaan-method
> ###   fitmeasures,lavaan-method fitindices
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> fitMeasures(fit)
                 npar                  fmin                 chisq 
               21.000                 0.142                85.306 
                   df                pvalue        baseline.chisq 
               24.000                 0.000               918.852 
          baseline.df       baseline.pvalue                   cfi 
               36.000                 0.000                 0.931 
                  tli                  nnfi                   rfi 
                0.896                 0.896                 0.861 
                  nfi                  pnfi                   ifi 
                0.907                 0.605                 0.931 
                  rni                  logl     unrestricted.logl 
                0.931             -3737.745             -3695.092 
                  aic                   bic                ntotal 
             7517.490              7595.339               301.000 
                 bic2                 rmsea        rmsea.ci.lower 
             7528.739                 0.092                 0.071 
       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue 
                0.114                 0.900                 0.001 
       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 
                0.050                 0.840                 0.080 
                  rmr            rmr_nomean                  srmr 
                0.082                 0.082                 0.065 
         srmr_bentler   srmr_bentler_nomean                  crmr 
                0.065                 0.065                 0.073 
          crmr_nomean            srmr_mplus     srmr_mplus_nomean 
                0.073                 0.065                 0.065 
                cn_05                 cn_01                   gfi 
              129.490               152.654                 0.943 
                 agfi                  pgfi                   mfi 
                0.894                 0.503                 0.903 
                 ecvi 
                0.423 
> fitMeasures(fit, "cfi")
  cfi 
0.931 
> fitMeasures(fit, c("chisq", "df", "pvalue", "cfi", "rmsea"))
 chisq     df pvalue    cfi  rmsea 
85.306 24.000  0.000  0.931  0.092 
> fitMeasures(fit, c("chisq", "df", "pvalue", "cfi", "rmsea"), 
+             output = "matrix")
             
chisq  85.306
df     24.000
pvalue  0.000
cfi     0.931
rmsea   0.092
> print(fitMeasures(fit, c("chisq", "df", "pvalue", "cfi", "rmsea"),
+                   output = "text"), add.h0 = TRUE)

Model Test User Model:

  Test statistic                                85.306
  Degrees of freedom                                24
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.931

Root Mean Square Error of Approximation:

  RMSEA                                          0.092
> 
> 
> 
> cleanEx()
> nameEx("getCov")
> ### * getCov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getCov
> ### Title: Utility Functions For Covariance Matrices
> ### Aliases: getCov cor2cov char2num
> 
> ### ** Examples
> 
> # The classic Wheaton et. al. (1977) model 
> # panel data on he stability of alienation
> lower <- '
+  11.834,
+   6.947,    9.364,
+   6.819,    5.091,   12.532,
+   4.783,    5.028,    7.495,    9.986,
+  -3.839,   -3.889,   -3.841,   -3.625,   9.610,
+ -21.899,  -18.831,  -21.748,  -18.775,  35.522,  450.288 '
> 
> # convert to a full symmetric covariance matrix with names
> wheaton.cov <- getCov(lower, names=c("anomia67","powerless67", "anomia71",
+                                      "powerless71","education","sei"))
> 
> # the model
> wheaton.model <- '
+   # measurement model
+     ses     =~ education + sei
+     alien67 =~ anomia67 + powerless67
+     alien71 =~ anomia71 + powerless71
+ 
+   # equations
+     alien71 ~ alien67 + ses
+     alien67 ~ ses
+ 
+   # correlated residuals
+     anomia67 ~~ anomia71
+     powerless67 ~~ powerless71
+ '
> 
> # fitting the model
> fit <- sem(wheaton.model, sample.cov=wheaton.cov, sample.nobs=932)
> 
> # showing the results
> summary(fit, standardized=TRUE)
lavaan 0.6.17 ended normally after 84 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        17

  Number of observations                           932

Model Test User Model:
                                                      
  Test statistic                                 4.735
  Degrees of freedom                                 4
  P-value (Chi-square)                           0.316

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  ses =~                                                                
    education         1.000                               2.607    0.842
    sei               5.219    0.422   12.364    0.000   13.609    0.642
  alien67 =~                                                            
    anomia67          1.000                               2.663    0.774
    powerless67       0.979    0.062   15.895    0.000    2.606    0.852
  alien71 =~                                                            
    anomia71          1.000                               2.850    0.805
    powerless71       0.922    0.059   15.498    0.000    2.628    0.832

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  alien71 ~                                                             
    alien67           0.607    0.051   11.898    0.000    0.567    0.567
    ses              -0.227    0.052   -4.334    0.000   -0.207   -0.207
  alien67 ~                                                             
    ses              -0.575    0.056  -10.195    0.000   -0.563   -0.563

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
 .anomia67 ~~                                                           
   .anomia71          1.623    0.314    5.176    0.000    1.623    0.356
 .powerless67 ~~                                                        
   .powerless71       0.339    0.261    1.298    0.194    0.339    0.121

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .education         2.801    0.507    5.525    0.000    2.801    0.292
   .sei             264.597   18.126   14.597    0.000  264.597    0.588
   .anomia67          4.731    0.453   10.441    0.000    4.731    0.400
   .powerless67       2.563    0.403    6.359    0.000    2.563    0.274
   .anomia71          4.399    0.515    8.542    0.000    4.399    0.351
   .powerless71       3.070    0.434    7.070    0.000    3.070    0.308
    ses               6.798    0.649   10.475    0.000    1.000    1.000
   .alien67           4.841    0.467   10.359    0.000    0.683    0.683
   .alien71           4.083    0.404   10.104    0.000    0.503    0.503

> 
> 
> 
> cleanEx()
> nameEx("growth")
> ### * growth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: growth
> ### Title: Fit Growth Curve Models
> ### Aliases: growth
> 
> ### ** Examples
> 
> ## linear growth model with a time-varying covariate
> model.syntax <- '
+   # intercept and slope with fixed coefficients
+     i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
+     s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
+ 
+   # regressions
+     i ~ x1 + x2
+     s ~ x1 + x2
+ 
+   # time-varying covariates
+     t1 ~ c1
+     t2 ~ c2
+     t3 ~ c3
+     t4 ~ c4
+ '
> 
> fit <- growth(model.syntax, data = Demo.growth)
> summary(fit)
lavaan 0.6.17 ended normally after 31 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        17

  Number of observations                           400

Model Test User Model:
                                                      
  Test statistic                                26.059
  Degrees of freedom                                21
  P-value (Chi-square)                           0.204

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  i =~                                                
    t1                1.000                           
    t2                1.000                           
    t3                1.000                           
    t4                1.000                           
  s =~                                                
    t1                0.000                           
    t2                1.000                           
    t3                2.000                           
    t4                3.000                           

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  i ~                                                 
    x1                0.608    0.060   10.134    0.000
    x2                0.604    0.064    9.412    0.000
  s ~                                                 
    x1                0.262    0.029    9.198    0.000
    x2                0.522    0.031   17.083    0.000
  t1 ~                                                
    c1                0.143    0.050    2.883    0.004
  t2 ~                                                
    c2                0.289    0.046    6.295    0.000
  t3 ~                                                
    c3                0.328    0.044    7.361    0.000
  t4 ~                                                
    c4                0.330    0.058    5.655    0.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
 .i ~~                                                
   .s                 0.075    0.040    1.855    0.064

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)
   .i                 0.580    0.062    9.368    0.000
   .s                 0.958    0.029   32.552    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .t1                0.580    0.080    7.230    0.000
   .t2                0.596    0.054   10.969    0.000
   .t3                0.481    0.055    8.745    0.000
   .t4                0.535    0.098    5.466    0.000
   .i                 1.079    0.112    9.609    0.000
   .s                 0.224    0.027    8.429    0.000

> 
> 
> 
> cleanEx()
> nameEx("lavCor")
> ### * lavCor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavCor
> ### Title: Polychoric, polyserial and Pearson correlations
> ### Aliases: lavCor
> 
> ### ** Examples
> 
> # Holzinger and Swineford (1939) example
> HS9 <- HolzingerSwineford1939[,c("x1","x2","x3","x4","x5",
+                                  "x6","x7","x8","x9")]
> 
> # Pearson correlations
> lavCor(HS9)
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  1.000                                                        
x2  0.297  1.000                                                 
x3  0.441  0.340  1.000                                          
x4  0.373  0.153  0.159  1.000                                   
x5  0.293  0.139  0.077  0.733  1.000                            
x6  0.357  0.193  0.198  0.704  0.720  1.000                     
x7  0.067 -0.076  0.072  0.174  0.102  0.121  1.000              
x8  0.224  0.092  0.186  0.107  0.139  0.150  0.487  1.000       
x9  0.390  0.206  0.329  0.208  0.227  0.214  0.341  0.449  1.000
> 
> # ordinal version, with three categories
> HS9ord <- as.data.frame( lapply(HS9, cut, 3, labels = FALSE) )
> 
> # polychoric correlations, two-stage estimation
> lavCor(HS9ord, ordered=names(HS9ord))
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  1.000                                                        
x2  0.317  1.000                                                 
x3  0.508  0.304  1.000                                          
x4  0.373  0.273  0.136  1.000                                   
x5  0.308  0.226  0.125  0.801  1.000                            
x6  0.319  0.274  0.215  0.683  0.783  1.000                     
x7  0.113 -0.097  0.058  0.169  0.136  0.071  1.000              
x8  0.147  0.094  0.091  0.112  0.160  0.185  0.552  1.000       
x9  0.459  0.204  0.286  0.258  0.265  0.255  0.365  0.454  1.000
> 
> # thresholds only
> lavCor(HS9ord, ordered=names(HS9ord), output = "th")
 x1|t1  x1|t2  x2|t1  x2|t2  x3|t1  x3|t2  x4|t1  x4|t2  x5|t1  x5|t2  x6|t1 
-1.363  0.844 -1.556  0.741 -0.353  0.626 -0.797  0.956 -0.820  0.527  0.188 
 x6|t2  x7|t1  x7|t2  x8|t1  x8|t2  x9|t1  x9|t2 
 1.529 -0.820  1.024 -0.129  1.882 -0.425  1.835 
> 
> # polychoric correlations, with standard errors
> lavCor(HS9ord, ordered=names(HS9ord), se = "standard", output = "est")
   lhs  op rhs est.std    se       z pvalue ci.lower ci.upper
1   x1  ~~  x1   1.000 0.000      NA     NA    1.000    1.000
2   x2  ~~  x2   1.000 0.000      NA     NA    1.000    1.000
3   x3  ~~  x3   1.000 0.000      NA     NA    1.000    1.000
4   x4  ~~  x4   1.000 0.000      NA     NA    1.000    1.000
5   x5  ~~  x5   1.000 0.000      NA     NA    1.000    1.000
6   x6  ~~  x6   1.000 0.000      NA     NA    1.000    1.000
7   x7  ~~  x7   1.000 0.000      NA     NA    1.000    1.000
8   x8  ~~  x8   1.000 0.000      NA     NA    1.000    1.000
9   x9  ~~  x9   1.000 0.000      NA     NA    1.000    1.000
10  x1  ~~  x2   0.317 0.070   4.534  0.000    0.180    0.455
11  x1  ~~  x3   0.508 0.060   8.484  0.000    0.391    0.625
12  x1  ~~  x4   0.373 0.060   6.181  0.000    0.255    0.491
13  x1  ~~  x5   0.308 0.068   4.549  0.000    0.175    0.441
14  x1  ~~  x6   0.319 0.065   4.893  0.000    0.191    0.447
15  x1  ~~  x7   0.113 0.073   1.543  0.123   -0.031    0.257
16  x1  ~~  x8   0.147 0.077   1.921  0.055   -0.003    0.298
17  x1  ~~  x9   0.459 0.073   6.318  0.000    0.317    0.601
18  x2  ~~  x3   0.304 0.066   4.616  0.000    0.175    0.433
19  x2  ~~  x4   0.273 0.069   3.947  0.000    0.138    0.409
20  x2  ~~  x5   0.226 0.069   3.266  0.001    0.090    0.361
21  x2  ~~  x6   0.274 0.070   3.919  0.000    0.137    0.411
22  x2  ~~  x7  -0.097 0.078  -1.241  0.215   -0.250    0.056
23  x2  ~~  x8   0.094 0.085   1.115  0.265   -0.072    0.261
24  x2  ~~  x9   0.204 0.086   2.385  0.017    0.036    0.372
25  x3  ~~  x4   0.136 0.073   1.876  0.061   -0.006    0.279
26  x3  ~~  x5   0.125 0.070   1.786  0.074   -0.012    0.262
27  x3  ~~  x6   0.215 0.079   2.728  0.006    0.060    0.369
28  x3  ~~  x7   0.058 0.074   0.783  0.434   -0.087    0.202
29  x3  ~~  x8   0.091 0.076   1.201  0.230   -0.057    0.239
30  x3  ~~  x9   0.286 0.072   3.954  0.000    0.144    0.428
31  x4  ~~  x5   0.801 0.032  24.928  0.000    0.738    0.864
32  x4  ~~  x6   0.683 0.050  13.794  0.000    0.586    0.781
33  x4  ~~  x7   0.169 0.082   2.058  0.040    0.008    0.330
34  x4  ~~  x8   0.112 0.076   1.474  0.141   -0.037    0.260
35  x4  ~~  x9   0.258 0.066   3.938  0.000    0.130    0.387
36  x5  ~~  x6   0.783 0.042  18.749  0.000    0.702    0.865
37  x5  ~~  x7   0.136 0.069   1.965  0.049    0.000    0.272
38  x5  ~~  x8   0.160 0.074   2.153  0.031    0.014    0.306
39  x5  ~~  x9   0.265 0.065   4.070  0.000    0.137    0.392
40  x6  ~~  x7   0.071 0.078   0.911  0.362   -0.082    0.223
41  x6  ~~  x8   0.185 0.079   2.349  0.019    0.031    0.339
42  x6  ~~  x9   0.255 0.076   3.379  0.001    0.107    0.404
43  x7  ~~  x8   0.552 0.059   9.370  0.000    0.437    0.668
44  x7  ~~  x9   0.365 0.075   4.892  0.000    0.219    0.512
45  x8  ~~  x9   0.454 0.076   5.945  0.000    0.304    0.604
46  x1   |  t1  -1.363 0.103 -13.239  0.000   -1.565   -1.162
47  x1   |  t2   0.844 0.083  10.224  0.000    0.682    1.006
48  x2   |  t1  -1.556 0.115 -13.508  0.000   -1.782   -1.331
49  x2   |  t2   0.741 0.080   9.259  0.000    0.584    0.898
50  x3   |  t1  -0.353 0.074  -4.766  0.000   -0.498   -0.208
51  x3   |  t2   0.626 0.078   8.047  0.000    0.473    0.778
52  x4   |  t1  -0.797 0.081  -9.799  0.000   -0.957   -0.638
53  x4   |  t2   0.956 0.086  11.151  0.000    0.788    1.125
54  x5   |  t1  -0.820 0.082 -10.012  0.000   -0.981   -0.660
55  x5   |  t2   0.527 0.076   6.925  0.000    0.378    0.676
56  x6   |  t1   0.188 0.073   2.588  0.010    0.046    0.331
57  x6   |  t2   1.529 0.113  13.498  0.000    1.307    1.751
58  x7   |  t1  -0.820 0.082 -10.012  0.000   -0.981   -0.660
59  x7   |  t2   1.024 0.088  11.641  0.000    0.852    1.197
60  x8   |  t1  -0.129 0.073  -1.783  0.075   -0.272    0.013
61  x8   |  t2   1.882 0.145  12.989  0.000    1.598    2.166
62  x9   |  t1  -0.425 0.075  -5.678  0.000   -0.571   -0.278
63  x9   |  t2   1.835 0.140  13.131  0.000    1.561    2.109
64  x1  ~1       0.000 0.000      NA     NA    0.000    0.000
65  x2  ~1       0.000 0.000      NA     NA    0.000    0.000
66  x3  ~1       0.000 0.000      NA     NA    0.000    0.000
67  x4  ~1       0.000 0.000      NA     NA    0.000    0.000
68  x5  ~1       0.000 0.000      NA     NA    0.000    0.000
69  x6  ~1       0.000 0.000      NA     NA    0.000    0.000
70  x7  ~1       0.000 0.000      NA     NA    0.000    0.000
71  x8  ~1       0.000 0.000      NA     NA    0.000    0.000
72  x9  ~1       0.000 0.000      NA     NA    0.000    0.000
73  x1 ~*~  x1   1.000 0.000      NA     NA    1.000    1.000
74  x2 ~*~  x2   1.000 0.000      NA     NA    1.000    1.000
75  x3 ~*~  x3   1.000 0.000      NA     NA    1.000    1.000
76  x4 ~*~  x4   1.000 0.000      NA     NA    1.000    1.000
77  x5 ~*~  x5   1.000 0.000      NA     NA    1.000    1.000
78  x6 ~*~  x6   1.000 0.000      NA     NA    1.000    1.000
79  x7 ~*~  x7   1.000 0.000      NA     NA    1.000    1.000
80  x8 ~*~  x8   1.000 0.000      NA     NA    1.000    1.000
81  x9 ~*~  x9   1.000 0.000      NA     NA    1.000    1.000
> 
> # polychoric correlations, full output
> fit.un <- lavCor(HS9ord, ordered=names(HS9ord), se = "standard", 
+                  output = "fit")
> summary(fit.un)
lavaan 0.6.17 ended normally after 1 iteration

  Estimator                                       DWLS
  Optimization method                           NLMINB
  Number of model parameters                        54

  Number of observations                           301


Parameter Estimates:

  Parameterization                               Delta
  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model        Unstructured

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
  x1 ~~                                               
    x2                0.317    0.070    4.534    0.000
    x3                0.508    0.060    8.484    0.000
    x4                0.373    0.060    6.181    0.000
    x5                0.308    0.068    4.549    0.000
    x6                0.319    0.065    4.893    0.000
    x7                0.113    0.073    1.543    0.123
    x8                0.147    0.077    1.921    0.055
    x9                0.459    0.073    6.318    0.000
  x2 ~~                                               
    x3                0.304    0.066    4.616    0.000
    x4                0.273    0.069    3.947    0.000
    x5                0.226    0.069    3.266    0.001
    x6                0.274    0.070    3.919    0.000
    x7               -0.097    0.078   -1.241    0.215
    x8                0.094    0.085    1.115    0.265
    x9                0.204    0.086    2.385    0.017
  x3 ~~                                               
    x4                0.136    0.073    1.876    0.061
    x5                0.125    0.070    1.786    0.074
    x6                0.215    0.079    2.728    0.006
    x7                0.058    0.074    0.783    0.434
    x8                0.091    0.076    1.201    0.230
    x9                0.286    0.072    3.954    0.000
  x4 ~~                                               
    x5                0.801    0.032   24.928    0.000
    x6                0.683    0.050   13.794    0.000
    x7                0.169    0.082    2.058    0.040
    x8                0.112    0.076    1.474    0.141
    x9                0.258    0.066    3.938    0.000
  x5 ~~                                               
    x6                0.783    0.042   18.749    0.000
    x7                0.136    0.069    1.965    0.049
    x8                0.160    0.074    2.153    0.031
    x9                0.265    0.065    4.070    0.000
  x6 ~~                                               
    x7                0.071    0.078    0.911    0.362
    x8                0.185    0.079    2.349    0.019
    x9                0.255    0.076    3.379    0.001
  x7 ~~                                               
    x8                0.552    0.059    9.370    0.000
    x9                0.365    0.075    4.892    0.000
  x8 ~~                                               
    x9                0.454    0.076    5.945    0.000

Intercepts:
                   Estimate  Std.Err  z-value  P(>|z|)
    x1                0.000                           
    x2                0.000                           
    x3                0.000                           
    x4                0.000                           
    x5                0.000                           
    x6                0.000                           
    x7                0.000                           
    x8                0.000                           
    x9                0.000                           

Thresholds:
                   Estimate  Std.Err  z-value  P(>|z|)
    x1|t1            -1.363    0.103  -13.239    0.000
    x1|t2             0.844    0.083   10.224    0.000
    x2|t1            -1.556    0.115  -13.508    0.000
    x2|t2             0.741    0.080    9.259    0.000
    x3|t1            -0.353    0.074   -4.766    0.000
    x3|t2             0.626    0.078    8.047    0.000
    x4|t1            -0.797    0.081   -9.799    0.000
    x4|t2             0.956    0.086   11.151    0.000
    x5|t1            -0.820    0.082  -10.012    0.000
    x5|t2             0.527    0.076    6.925    0.000
    x6|t1             0.188    0.073    2.588    0.010
    x6|t2             1.529    0.113   13.498    0.000
    x7|t1            -0.820    0.082  -10.012    0.000
    x7|t2             1.024    0.088   11.641    0.000
    x8|t1            -0.129    0.073   -1.783    0.075
    x8|t2             1.882    0.145   12.989    0.000
    x9|t1            -0.425    0.075   -5.678    0.000
    x9|t2             1.835    0.140   13.131    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
    x1                1.000                           
    x2                1.000                           
    x3                1.000                           
    x4                1.000                           
    x5                1.000                           
    x6                1.000                           
    x7                1.000                           
    x8                1.000                           
    x9                1.000                           

Scales y*:
                   Estimate  Std.Err  z-value  P(>|z|)
    x1                1.000                           
    x2                1.000                           
    x3                1.000                           
    x4                1.000                           
    x5                1.000                           
    x6                1.000                           
    x7                1.000                           
    x8                1.000                           
    x9                1.000                           

> 
> 
> 
> cleanEx()
> nameEx("lavExport")
> ### * lavExport
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavExport
> ### Title: lavaan Export
> ### Aliases: lavExport
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> out <- lavExport(fit, target = "Mplus", export=FALSE)
> cat(out)
TITLE:
   [This syntax is autogenerated by lavExport]
DATA:
  file is sem.mplus.raw;
  type is individual;
  listwise = on;
VARIABLE:
  names are
     x1 x2 x3 x4 x5 x6
     x7 x8 x9;
  missing are all (-999999);
ANALYSIS:
  type = general;
  estimator = ML;
  information = expected;
  model = nomeanstructure;
MODEL:
  ! this model syntax is autogenerated by lavExport
  visual BY x1@1 (p1);
  visual BY x2* (p2);
  visual BY x3* (p3);
  textual BY x4@1 (p4);
  textual BY x5* (p5);
  textual BY x6* (p6);
  speed BY x7@1 (p7);
  speed BY x8* (p8);
  speed BY x9* (p9);
  x1* (p10);
  x2* (p11);
  x3* (p12);
  x4* (p13);
  x5* (p14);
  x6* (p15);
  x7* (p16);
  x8* (p17);
  x9* (p18);
  visual* (p19);
  textual* (p20);
  speed* (p21);
  visual WITH textual* (p22);
  visual WITH speed* (p23);
  textual WITH speed* (p24);
OUTPUT:
  sampstat standardized tech1;
> 
> 
> 
> cleanEx()
> nameEx("lavInspect")
> ### * lavInspect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavInspect
> ### Title: Inspect or extract information from a fitted lavaan object
> ### Aliases: lavInspect inspect lavTech
> 
> ### ** Examples
> 
> # fit model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> 
> fit <- cfa(HS.model, data = HolzingerSwineford1939, group = "school")
> 
> # extract information
> lavInspect(fit, "sampstat")
$Pasteur
$Pasteur$cov
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  1.395                                                        
x2  0.402  1.504                                                 
x3  0.620  0.477  1.346                                          
x4  0.568  0.089  0.165  1.320                                   
x5  0.472  0.143  0.066  1.080  1.708                            
x6  0.498  0.194  0.222  0.755  0.934  0.974                     
x7  0.046 -0.204 -0.025  0.322  0.171  0.219  1.170              
x8  0.165  0.039  0.152  0.147  0.152  0.206  0.426  0.952       
x9  0.351  0.219  0.332  0.155  0.208  0.186  0.287  0.352  0.978

$Pasteur$mean
   x1    x2    x3    x4    x5    x6    x7    x8    x9 
4.941 5.984 2.487 2.823 3.995 1.922 4.432 5.563 5.418 


$`Grant-White`
$`Grant-White`$cov
      x1    x2    x3    x4    x5    x6    x7    x8    x9
x1 1.319                                                
x2 0.414 1.226                                          
x3 0.534 0.478 1.073                                    
x4 0.440 0.283 0.381 1.257                              
x5 0.411 0.205 0.344 0.933 1.342                        
x6 0.412 0.244 0.407 0.906 0.898 1.280                  
x7 0.123 0.076 0.080 0.241 0.303 0.208 1.062            
x8 0.370 0.195 0.259 0.122 0.240 0.143 0.633 1.094      
x9 0.573 0.281 0.396 0.361 0.422 0.315 0.442 0.567 1.051

$`Grant-White`$mean
   x1    x2    x3    x4    x5    x6    x7    x8    x9 
4.930 6.200 1.996 3.317 4.712 2.469 3.921 5.488 5.327 


> lavTech(fit, "sampstat")
$Pasteur
$Pasteur$cov
            [,1]        [,2]        [,3]      [,4]       [,5]      [,6]
 [1,] 1.39522952  0.40210319  0.62010670 0.5679263 0.47193972 0.4975629
 [2,] 0.40210319  1.50374959  0.47675768 0.0885040 0.14255116 0.1940365
 [3,] 0.62010670  0.47675768  1.34578916 0.1646600 0.06644477 0.2221019
 [4,] 0.56792635  0.08850400  0.16466004 1.3196864 1.07980974 0.7551208
 [5,] 0.47193972  0.14255116  0.06644477 1.0798097 1.70790958 0.9339206
 [6,] 0.49756289  0.19403646  0.22210188 0.7551208 0.93392064 0.9744487
 [7,] 0.04616439 -0.20384455 -0.02469771 0.3215544 0.17090488 0.2191072
 [8,] 0.16519525  0.03851188  0.15161078 0.1469887 0.15150549 0.2056474
 [9,] 0.35114303  0.21867810  0.33207834 0.1551634 0.20842750 0.1861841
             [,7]       [,8]      [,9]
 [1,]  0.04616439 0.16519525 0.3511430
 [2,] -0.20384455 0.03851188 0.2186781
 [3,] -0.02469771 0.15161078 0.3320783
 [4,]  0.32155439 0.14698868 0.1551634
 [5,]  0.17090488 0.15150549 0.2084275
 [6,]  0.21910719 0.20564743 0.1861841
 [7,]  1.16992003 0.42609205 0.2872255
 [8,]  0.42609205 0.95199077 0.3524254
 [9,]  0.28722552 0.35242542 0.9775531

$Pasteur$mean
[1] 4.941239 5.983974 2.487179 2.822650 3.995192 1.922161 4.432274 5.563141
[9] 5.417735


$`Grant-White`
$`Grant-White`$cov
           [,1]       [,2]       [,3]      [,4]      [,5]      [,6]       [,7]
 [1,] 1.3186471 0.41431034 0.53374951 0.4398679 0.4111336 0.4120276 0.12328525
 [2,] 0.4143103 1.22637931 0.47844828 0.2831034 0.2045690 0.2438916 0.07572714
 [3,] 0.5337495 0.47844828 1.07336504 0.3809651 0.3442331 0.4070707 0.07968128
 [4,] 0.4398679 0.28310344 0.38096512 1.2572123 0.9332977 0.9063971 0.24130486
 [5,] 0.4111336 0.20456897 0.34423306 0.9332977 1.3416647 0.8980839 0.30299437
 [6,] 0.4120276 0.24389163 0.40707066 0.9063971 0.8980839 1.2801417 0.20829506
 [7,] 0.1232853 0.07572714 0.07968128 0.2413049 0.3029944 0.2082951 1.06179961
 [8,] 0.3695228 0.19458621 0.25857015 0.1217654 0.2397277 0.1433800 0.63283504
 [9,] 0.5731334 0.28139847 0.39573507 0.3614595 0.4222771 0.3152449 0.44190187
           [,8]      [,9]
 [1,] 0.3695228 0.5731334
 [2,] 0.1945862 0.2813985
 [3,] 0.2585702 0.3957351
 [4,] 0.1217654 0.3614595
 [5,] 0.2397277 0.4222771
 [6,] 0.1433800 0.3152449
 [7,] 0.6328350 0.4419019
 [8,] 1.0943798 0.5666523
 [9,] 0.5666523 1.0510480

$`Grant-White`$mean
[1] 4.929885 6.200000 1.995690 3.317241 4.712069 2.468966 3.920840 5.488276
[9] 5.327203


> 
> 
> 
> cleanEx()
> nameEx("lavListInspect")
> ### * lavListInspect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavListInspect
> ### Title: Inspect or extract information from a lavaanList object
> ### Aliases: lavListInspect lavListTech
> 
> ### ** Examples
> 
> # fit model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> # a data generating function
> generateData <- function() simulateData(HS.model, sample.nobs = 100)
> 
> set.seed(1234)
> fit <- semList(HS.model, dataFunction = generateData, ndat = 5,
+                store.slots = "partable")
> 
> # extract information
> lavListInspect(fit, "free")
$lambda
   visual textul speed
x1      0      0     0
x2      1      0     0
x3      2      0     0
x4      0      0     0
x5      0      3     0
x6      0      4     0
x7      0      0     0
x8      0      0     5
x9      0      0     6

$theta
   x1 x2 x3 x4 x5 x6 x7 x8 x9
x1  7                        
x2  0  8                     
x3  0  0  9                  
x4  0  0  0 10               
x5  0  0  0  0 11            
x6  0  0  0  0  0 12         
x7  0  0  0  0  0  0 13      
x8  0  0  0  0  0  0  0 14   
x9  0  0  0  0  0  0  0  0 15

$psi
        visual textul speed
visual      16             
textual     19     17      
speed       20     21    18

> lavListTech(fit, "free")
$lambda
      [,1] [,2] [,3]
 [1,]    0    0    0
 [2,]    1    0    0
 [3,]    2    0    0
 [4,]    0    0    0
 [5,]    0    3    0
 [6,]    0    4    0
 [7,]    0    0    0
 [8,]    0    0    5
 [9,]    0    0    6

$theta
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
 [1,]    7    0    0    0    0    0    0    0    0
 [2,]    0    8    0    0    0    0    0    0    0
 [3,]    0    0    9    0    0    0    0    0    0
 [4,]    0    0    0   10    0    0    0    0    0
 [5,]    0    0    0    0   11    0    0    0    0
 [6,]    0    0    0    0    0   12    0    0    0
 [7,]    0    0    0    0    0    0   13    0    0
 [8,]    0    0    0    0    0    0    0   14    0
 [9,]    0    0    0    0    0    0    0    0   15

$psi
     [,1] [,2] [,3]
[1,]   16   19   20
[2,]   19   17   21
[3,]   20   21   18

> 
> 
> 
> cleanEx()
> nameEx("lavMatrixRepresentation")
> ### * lavMatrixRepresentation
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavMatrixRepresentation
> ### Title: lavaan matrix representation
> ### Aliases: lavMatrixRepresentation
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> 
> # extract partable
> partable <- parTable(fit)
> 
> # add matrix representation (and show only a few columns)
> lavMatrixRepresentation(partable)[,c("lhs","op","rhs","mat","row","col")]
       lhs op     rhs    mat row col
1   visual =~      x1 lambda   1   1
2   visual =~      x2 lambda   2   1
3   visual =~      x3 lambda   3   1
4  textual =~      x4 lambda   4   2
5  textual =~      x5 lambda   5   2
6  textual =~      x6 lambda   6   2
7    speed =~      x7 lambda   7   3
8    speed =~      x8 lambda   8   3
9    speed =~      x9 lambda   9   3
10      x1 ~~      x1  theta   1   1
11      x2 ~~      x2  theta   2   2
12      x3 ~~      x3  theta   3   3
13      x4 ~~      x4  theta   4   4
14      x5 ~~      x5  theta   5   5
15      x6 ~~      x6  theta   6   6
16      x7 ~~      x7  theta   7   7
17      x8 ~~      x8  theta   8   8
18      x9 ~~      x9  theta   9   9
19  visual ~~  visual    psi   1   1
20 textual ~~ textual    psi   2   2
21   speed ~~   speed    psi   3   3
22  visual ~~ textual    psi   1   2
23  visual ~~   speed    psi   1   3
24 textual ~~   speed    psi   2   3
> 
> 
> 
> cleanEx()
> nameEx("lavNames")
> ### * lavNames
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavNames
> ### Title: lavaan Names
> ### Aliases: lavNames lavaanNames
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> lavNames(fit, "ov")
[1] "x1" "x2" "x3" "x4" "x5" "x6" "x7" "x8" "x9"
> 
> 
> 
> cleanEx()
> nameEx("lavOptions")
> ### * lavOptions
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavOptions
> ### Title: lavaan Options
> ### Aliases: lavOptions lavoptions
> 
> ### ** Examples
> 
> lavOptions()
$model.type
[1] "sem"

$mimic
[1] "lavaan"

$meanstructure
[1] "default"

$int.ov.free
[1] FALSE

$int.lv.free
[1] FALSE

$marker.int.zero
[1] FALSE

$conditional.x
[1] "default"

$fixed.x
[1] "default"

$orthogonal
[1] FALSE

$orthogonal.x
[1] FALSE

$orthogonal.y
[1] FALSE

$std.lv
[1] FALSE

$correlation
[1] FALSE

$effect.coding
[1] FALSE

$ceq.simple
[1] FALSE

$parameterization
[1] "default"

$auto.fix.first
[1] FALSE

$auto.fix.single
[1] FALSE

$auto.var
[1] FALSE

$auto.cov.lv.x
[1] FALSE

$auto.cov.y
[1] FALSE

$auto.th
[1] FALSE

$auto.delta
[1] FALSE

$auto.efa
[1] FALSE

$rotation
[1] "geomin"

$rotation.se
[1] "bordered"

$rotation.args
$rotation.args$orthogonal
[1] FALSE

$rotation.args$row.weights
[1] "default"

$rotation.args$std.ov
[1] TRUE

$rotation.args$geomin.epsilon
[1] 0.001

$rotation.args$orthomax.gamma
[1] 1

$rotation.args$cf.gamma
[1] 0

$rotation.args$oblimin.gamma
[1] 0

$rotation.args$promax.kappa
[1] 4

$rotation.args$target
<0 x 0 matrix>

$rotation.args$target.mask
<0 x 0 matrix>

$rotation.args$rstarts
[1] 30

$rotation.args$algorithm
[1] "gpa"

$rotation.args$reflect
[1] TRUE

$rotation.args$order.lv.by
[1] "index"

$rotation.args$gpa.tol
[1] 1e-05

$rotation.args$tol
[1] 1e-08

$rotation.args$warn
[1] FALSE

$rotation.args$verbose
[1] FALSE

$rotation.args$jac.init.rot
[1] TRUE

$rotation.args$max.iter
[1] 10000


$std.ov
[1] FALSE

$missing
[1] "default"

$sampling.weights.normalization
[1] "total"

$sample.cov.rescale
[1] "default"

$sample.cov.robust
[1] FALSE

$sample.icov
[1] TRUE

$ridge
[1] FALSE

$ridge.constant
[1] "default"

$group.label
NULL

$group.equal
[1] ""

$group.partial
[1] ""

$group.w.free
[1] FALSE

$level.label
NULL

$estimator
[1] "default"

$estimator.args
list()

$likelihood
[1] "default"

$link
[1] "default"

$representation
[1] "default"

$do.fit
[1] TRUE

$bounds
[1] "none"

$se
[1] "default"

$test
[1] "default"

$information
[1] "default" "default"

$h1.information
[1] "structured" "structured"

$observed.information
[1] "hessian" "default"

$information.meat
[1] "default"

$h1.information.meat
[1] "default"

$omega.information
[1] "default"

$omega.h1.information
[1] "default"

$omega.information.meat
[1] "default"

$omega.h1.information.meat
[1] "default"

$scaled.test
[1] "standard"

$ug2.old.approach
[1] FALSE

$bootstrap
[1] 1000

$gamma.n.minus.one
[1] FALSE

$gamma.unbiased
[1] FALSE

$control
list()

$optim.method
[1] "default"

$optim.attempts
[1] 4

$optim.force.converged
[1] FALSE

$optim.gradient
[1] "analytic"

$optim.init_nelder_mead
[1] FALSE

$optim.var.transform
[1] "none"

$optim.parscale
[1] "none"

$optim.partrace
[1] FALSE

$optim.dx.tol
[1] 0.001

$optim.bounds
list()

$em.iter.max
[1] 10000

$em.fx.tol
[1] 1e-08

$em.dx.tol
[1] 1e-04

$em.zerovar.offset
[1] 1e-04

$em.h1.iter.max
[1] 500

$em.h1.tol
[1] 1e-05

$em.h1.warn
[1] TRUE

$optim.gn.iter.max
[1] 200

$optim.gn.stephalf.max
[1] 10

$optim.gn.tol.x
[1] 1e-05

$integration.ngh
[1] 21

$parallel
[1] "no"

$ncpus
[1] 3

$cl
NULL

$iseed
NULL

$zero.add
[1] "default"

$zero.keep.margins
[1] "default"

$zero.cell.warn
[1] FALSE

$start
[1] "default"

$check.start
[1] TRUE

$check.post
[1] TRUE

$check.gradient
[1] TRUE

$check.vcov
[1] TRUE

$check.lv.names
[1] TRUE

$check.lv.interaction
[1] TRUE

$h1
[1] TRUE

$baseline
[1] TRUE

$baseline.conditional.x.free.slopes
[1] TRUE

$implied
[1] TRUE

$loglik
[1] TRUE

$store.vcov
[1] "default"

$parser
[1] "old"

$verbose
[1] FALSE

$warn
[1] TRUE

$debug
[1] FALSE

> lavOptions("std.lv")
$std.lv
[1] FALSE

> lavOptions(c("std.lv", "orthogonal"))
$std.lv
[1] FALSE

$orthogonal
[1] FALSE

> 
> 
> 
> cleanEx()
> nameEx("lavParTable")
> ### * lavParTable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: parTable
> ### Title: Parameter Table
> ### Aliases: parameterTable parametertable parTable partable
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> parTable(fit)
   id     lhs op     rhs user block group free ustart exo label plabel start
1   1  visual =~      x1    1     1     1    0      1   0         .p1. 1.000
2   2  visual =~      x2    1     1     1    1     NA   0         .p2. 0.778
3   3  visual =~      x3    1     1     1    2     NA   0         .p3. 1.107
4   4 textual =~      x4    1     1     1    0      1   0         .p4. 1.000
5   5 textual =~      x5    1     1     1    3     NA   0         .p5. 1.133
6   6 textual =~      x6    1     1     1    4     NA   0         .p6. 0.924
7   7   speed =~      x7    1     1     1    0      1   0         .p7. 1.000
8   8   speed =~      x8    1     1     1    5     NA   0         .p8. 1.225
9   9   speed =~      x9    1     1     1    6     NA   0         .p9. 0.854
10 10      x1 ~~      x1    0     1     1    7     NA   0        .p10. 0.679
11 11      x2 ~~      x2    0     1     1    8     NA   0        .p11. 0.691
12 12      x3 ~~      x3    0     1     1    9     NA   0        .p12. 0.637
13 13      x4 ~~      x4    0     1     1   10     NA   0        .p13. 0.675
14 14      x5 ~~      x5    0     1     1   11     NA   0        .p14. 0.830
15 15      x6 ~~      x6    0     1     1   12     NA   0        .p15. 0.598
16 16      x7 ~~      x7    0     1     1   13     NA   0        .p16. 0.592
17 17      x8 ~~      x8    0     1     1   14     NA   0        .p17. 0.511
18 18      x9 ~~      x9    0     1     1   15     NA   0        .p18. 0.508
19 19  visual ~~  visual    0     1     1   16     NA   0        .p19. 0.050
20 20 textual ~~ textual    0     1     1   17     NA   0        .p20. 0.050
21 21   speed ~~   speed    0     1     1   18     NA   0        .p21. 0.050
22 22  visual ~~ textual    0     1     1   19     NA   0        .p22. 0.000
23 23  visual ~~   speed    0     1     1   20     NA   0        .p23. 0.000
24 24 textual ~~   speed    0     1     1   21     NA   0        .p24. 0.000
     est    se
1  1.000 0.000
2  0.554 0.100
3  0.729 0.109
4  1.000 0.000
5  1.113 0.065
6  0.926 0.055
7  1.000 0.000
8  1.180 0.165
9  1.082 0.151
10 0.549 0.114
11 1.134 0.102
12 0.844 0.091
13 0.371 0.048
14 0.446 0.058
15 0.356 0.043
16 0.799 0.081
17 0.488 0.074
18 0.566 0.071
19 0.809 0.145
20 0.979 0.112
21 0.384 0.086
22 0.408 0.074
23 0.262 0.056
24 0.173 0.049
> 
> 
> 
> cleanEx()
> nameEx("lavPredict")
> ### * lavPredict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavPredict
> ### Title: Predict the values of latent variables (and their indicators).
> ### Aliases: lavPredict lavpredict
> 
> ### ** Examples
> 
> data(HolzingerSwineford1939)
> 
> ## fit model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data = HolzingerSwineford1939)
> head(lavPredict(fit))
          visual     textual       speed
[1,] -0.81767524 -0.13754501  0.06150726
[2,]  0.04951940 -1.01272402  0.62549360
[3,] -0.76139670 -1.87228634 -0.84057276
[4,]  0.41934153  0.01848569 -0.27133710
[5,] -0.41590481 -0.12225009  0.19432951
[6,]  0.02325632 -1.32981727  0.70885348
> head(lavPredict(fit, type = "ov"))
           x1       x2       x3       x4       x5        x6       x7       x8
[1,] 4.118094 5.635456 1.654027 2.923363 4.187433 2.0581851 4.247409 5.599652
[2,] 4.985289 6.115449 2.286533 2.048184 3.213292 1.2476414 4.811396 6.265128
[3,] 4.174373 5.666607 1.695075 1.188622 2.256533 0.4515610 3.345329 4.535242
[4,] 5.355111 6.320146 2.556271 3.079394 4.361108 2.2026924 3.914565 5.206912
[5,] 4.519865 5.857836 1.947067 2.938658 4.204458 2.0723504 4.380232 5.756376
[6,] 4.959026 6.100912 2.267378 1.731091 2.860343 0.9539666 4.894756 6.363489
           x9
[1,] 5.440645
[2,] 6.050613
[3,] 4.465019
[4,] 5.080664
[5,] 5.584297
[6,] 6.140770
> 
> 
> ## ------------------------------------------
> ## merge factor scores to original data.frame
> ## ------------------------------------------
> 
> idx <- lavInspect(fit, "case.idx")
> fscores <- lavPredict(fit)
> ## loop over factors
> for (fs in colnames(fscores)) {
+   HolzingerSwineford1939[idx, fs] <- fscores[ , fs]
+ }
> head(HolzingerSwineford1939)
  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6
1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143
2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143
3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714
4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714
5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286
6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429
        x7   x8       x9      visual     textual       speed
1 3.391304 5.75 6.361111 -0.81767524 -0.13754501  0.06150726
2 3.782609 6.25 7.916667  0.04951940 -1.01272402  0.62549360
3 3.260870 3.90 4.416667 -0.76139670 -1.87228634 -0.84057276
4 3.000000 5.30 4.861111  0.41934153  0.01848569 -0.27133710
5 3.695652 6.30 5.916667 -0.41590481 -0.12225009  0.19432951
6 4.347826 6.65 7.500000  0.02325632 -1.32981727  0.70885348
> 
> 
> ## multigroup models return a list of factor scores (one per group)
> data(HolzingerSwineford1939)
> mgfit <- update(fit, group = "school", group.equal = c("loadings","intercepts"))
> 
> idx <- lavInspect(mgfit, "case.idx") # list: 1 vector per group
> fscores <- lavPredict(mgfit)         # list: 1 matrix per group
> ## loop over groups and factors
> for (g in seq_along(fscores)) {
+   for (fs in colnames(fscores[[g]])) {
+     HolzingerSwineford1939[ idx[[g]], fs] <- fscores[[g]][ , fs]
+   }
+ }
> head(HolzingerSwineford1939)
  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6
1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143
2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143
3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714
4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714
5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286
6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429
        x7   x8       x9      visual      textual       speed
1 3.391304 5.75 6.361111 -0.86916431 -0.003493476  0.01994815
2 3.782609 6.25 7.916667 -0.03259591 -0.713953292  0.48016012
3 3.260870 3.90 4.416667 -0.72425937 -1.594391329 -0.86241184
4 3.000000 5.30 4.861111  0.43542404  0.283596862 -0.30140638
5 3.695652 6.30 5.916667 -0.42312053  0.181227319  0.15179550
6 4.347826 6.65 7.500000 -0.07275056 -1.026971668  0.55486534
> 
> ## -------------------------------------
> ## Use factor scores in susequent models
> ## -------------------------------------
> 
> ## see Examples in semTools package: ?plausibleValues
> 
> 
> 
> cleanEx()
> nameEx("lavPredictY")
> ### * lavPredictY
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavPredictY
> ### Title: Predict the values of y-variables given the values of
> ###   x-variables
> ### Aliases: lavPredictY
> 
> ### ** Examples
> 
> model <- ' 
+   # latent variable definitions
+      ind60 =~ x1 + x2 + x3
+      dem60 =~ y1 + a*y2 + b*y3 + c*y4 
+      dem65 =~ y5 + a*y6 + b*y7 + c*y8
+     
+   # regressions
+     dem60 ~ ind60
+     dem65 ~ ind60 + dem60
+     
+   # residual correlations
+     y1 ~~ y5
+     y2 ~~ y4 + y6
+     y3 ~~ y7
+     y4 ~~ y8
+     y6 ~~ y8
+ '
> fit <- sem(model, data = PoliticalDemocracy)
> 
> lavPredictY(fit, ynames = c("y5", "y6", "y7", "y8"),
+                  xnames = c("x1", "x2", "x3", "y1", "y2", "y3", "y4"))
         y5     y6     y7     y8
 [1,] 2.347 -0.436  2.993  0.528
 [2,] 2.213 -0.173  3.244  0.782
 [3,] 7.663  6.448  9.433  7.583
 [4,] 8.739  7.309 10.298  8.536
 [5,] 8.515  5.164  9.512  7.484
 [6,] 6.555  3.509  7.360  5.624
 [7,] 6.482  3.419  7.271  5.527
 [8,] 6.104  3.229  7.168  4.262
 [9,] 3.422  1.616  4.473  2.495
[10,] 8.563  6.056  9.682  7.892
[11,] 7.153  4.326  8.661  6.531
[12,] 6.803  3.819  7.668  5.956
[13,] 7.619  4.906  9.238  7.154
[14,] 6.967  5.524  8.471  6.233
[15,] 7.147  5.963  7.715  6.802
[16,] 7.275  6.509  8.913  6.808
[17,] 3.937  1.963  5.788  3.858
[18,] 2.500 -0.180  3.718  1.733
[19,] 9.167  7.765 10.500  8.851
[20,] 6.943  3.923  7.283  6.036
[21,] 9.213  7.822 10.557  8.912
[22,] 2.492  0.105  3.032  1.049
[23,] 3.556  0.707  4.608  2.711
[24,] 7.639  5.642  9.412  7.711
[25,] 7.769  6.831  8.592  6.827
[26,] 4.443  0.620  4.851  2.716
[27,] 3.701  0.684  5.080  2.736
[28,] 3.314  1.413  4.765  2.319
[29,] 6.414  2.107  7.011  4.420
[30,] 4.682  0.669  5.092  2.798
[31,] 3.091 -0.011  3.899  1.951
[32,] 1.367 -1.556  1.387 -0.716
[33,] 4.290  0.764  4.683  2.826
[34,] 4.889  1.651  6.543  3.841
[35,] 4.495  4.219  5.676  3.912
[36,] 5.837  5.249  7.493  5.115
[37,] 5.026  3.295  6.122  3.791
[38,] 4.726  2.082  5.886  4.037
[39,] 3.086  1.364  4.077  2.132
[40,] 4.469  4.594  5.857  3.375
[41,] 4.672  4.720  5.748  3.050
[42,] 5.052  3.714  6.436  4.729
[43,] 5.344  3.921  7.261  4.845
[44,] 3.386  1.306  2.730  0.681
[45,] 3.418  1.280  2.736  0.691
[46,] 6.087  5.104  7.050  4.965
[47,] 2.569 -0.454  3.944  1.499
[48,] 7.129  5.960  8.174  5.930
[49,] 5.871  2.347  6.772  3.685
[50,] 6.860  5.938  7.700  5.855
[51,] 3.658  0.904  5.294  2.955
[52,] 3.204  0.224  3.916  1.280
[53,] 4.670  2.433  5.145  3.110
[54,] 5.789  3.461  7.258  4.510
[55,] 2.498  0.183  3.598  1.164
[56,] 2.464  0.089  3.811  1.288
[57,] 2.575  0.349  4.252  1.373
[58,] 7.325  5.971  8.915  6.713
[59,] 3.714  0.854  5.403  3.217
[60,] 8.147  7.351 10.071  8.353
[61,] 1.983 -0.530  2.401  0.368
[62,] 0.999 -1.757  1.182 -0.947
[63,] 1.842 -1.136  1.808 -0.254
[64,] 5.379  2.523  6.181  3.869
[65,] 7.204  6.175  8.903  7.092
[66,] 4.002  0.828  4.672  1.963
[67,] 7.239  6.220  8.947  7.140
[68,] 3.584  1.144  4.862  1.540
[69,] 4.087  0.978  5.129  2.144
[70,] 3.301  1.398  4.749  2.302
[71,] 4.637  4.356  5.630  3.148
[72,] 6.984  5.945  8.481  6.144
[73,] 7.373  5.763  8.974  6.816
[74,] 8.465  5.813  9.599  7.948
[75,] 3.095  1.074  2.974  0.951
> 
> 
> 
> cleanEx()
> nameEx("lavResiduals")
> ### * lavResiduals
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavResiduals
> ### Title: Residuals
> ### Aliases: lavResiduals lavResidual
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data = HolzingerSwineford1939)
> lavResiduals(fit)
$type
[1] "cor.bentler"

$cov
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  0.000                                                        
x2 -0.030  0.000                                                 
x3 -0.008  0.094  0.000                                          
x4  0.071 -0.012 -0.068  0.000                                   
x5 -0.009 -0.027 -0.151  0.005  0.000                            
x6  0.060  0.030 -0.026 -0.009  0.003  0.000                     
x7 -0.140 -0.189 -0.084  0.037 -0.036 -0.014  0.000              
x8 -0.039 -0.052 -0.012 -0.067 -0.036 -0.022  0.075  0.000       
x9  0.149  0.073  0.147  0.048  0.067  0.056 -0.038 -0.032  0.000

$cov.z
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  0.000                                                        
x2 -1.996  0.000                                                 
x3 -0.997  2.689  0.000                                          
x4  2.679 -0.284 -1.899  0.000                                   
x5 -0.359 -0.591 -4.157  1.545  0.000                            
x6  2.155  0.681 -0.711 -2.588  0.942  0.000                     
x7 -3.773 -3.654 -1.858  0.865 -0.842 -0.326  0.000              
x8 -1.380 -1.119 -0.300 -2.021 -1.099 -0.641  4.823  0.000       
x9  4.077  1.606  3.518  1.225  1.701  1.423 -2.325 -4.132  0.000

$summary
                          cov
srmr                    0.065
srmr.se                 0.006
srmr.exactfit.z         6.063
srmr.exactfit.pvalue    0.000
usrmr                   0.058
usrmr.se                0.010
usrmr.ci.lower          0.042
usrmr.ci.upper          0.074
usrmr.closefit.h0.value 0.050
usrmr.closefit.z        0.832
usrmr.closefit.pvalue   0.203

> 
> 
> 
> cleanEx()
> nameEx("lavTables")
> ### * lavTables
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTables
> ### Title: lavaan frequency tables
> ### Aliases: lavTables
> 
> ### ** Examples
> 
> HS9 <- HolzingerSwineford1939[,c("x1","x2","x3","x4","x5",
+                                  "x6","x7","x8","x9")]
> HSbinary <- as.data.frame( lapply(HS9, cut, 2, labels=FALSE) )
> 
> # using the data only
> lavTables(HSbinary, dim = 0L, categorical = names(HSbinary))
      pattern nobs obs.freq obs.prop
1   111111111  301       19    0.063
2   211121111  301       11    0.037
3   111121111  301        7    0.023
4   211111111  301        7    0.023
5   221111111  301        7    0.023
6   221121111  301        7    0.023
7   222221111  301        7    0.023
8   221111211  301        6    0.020
9   221221211  301        6    0.020
10  221222111  301        6    0.020
11  222121111  301        6    0.020
12  111111211  301        5    0.017
13  111221211  301        5    0.017
14  121221211  301        5    0.017
15  122111111  301        5    0.017
16  221221111  301        5    0.017
17  121111111  301        4    0.013
18  211111211  301        4    0.013
19  212111111  301        4    0.013
20  212221111  301        4    0.013
21  222221211  301        4    0.013
22  222222111  301        4    0.013
23  222222222  301        4    0.013
24  111121212  301        3    0.010
25  111221111  301        3    0.010
26  121121111  301        3    0.010
27  211211211  301        3    0.010
28  211221111  301        3    0.010
29  212111211  301        3    0.010
30  212121111  301        3    0.010
31  212221212  301        3    0.010
32  212222111  301        3    0.010
33  221221212  301        3    0.010
34  222111111  301        3    0.010
35  222221112  301        3    0.010
36  222221212  301        3    0.010
37  111111221  301        2    0.007
38  111121211  301        2    0.007
39  112111111  301        2    0.007
40  121111212  301        2    0.007
41  121121112  301        2    0.007
42  121221111  301        2    0.007
43  122111112  301        2    0.007
44  211111122  301        2    0.007
45  211111212  301        2    0.007
46  211121211  301        2    0.007
47  211211111  301        2    0.007
48  211221112  301        2    0.007
49  211221211  301        2    0.007
50  211222112  301        2    0.007
51  211222212  301        2    0.007
52  221121112  301        2    0.007
53  221222112  301        2    0.007
54  222111112  301        2    0.007
55  222111212  301        2    0.007
56  222121211  301        2    0.007
57  222221222  301        2    0.007
58  222222112  301        2    0.007
59  222222211  301        2    0.007
60  111111112  301        1    0.003
61  111111121  301        1    0.003
62  111111212  301        1    0.003
63  111121112  301        1    0.003
64  111121222  301        1    0.003
65  111211111  301        1    0.003
66  111211211  301        1    0.003
67  111211212  301        1    0.003
68  111221212  301        1    0.003
69  111222111  301        1    0.003
70  112111211  301        1    0.003
71  112111212  301        1    0.003
72  112111222  301        1    0.003
73  112121111  301        1    0.003
74  112221111  301        1    0.003
75  121121211  301        1    0.003
76  121121212  301        1    0.003
77  121122211  301        1    0.003
78  121122221  301        1    0.003
79  121211111  301        1    0.003
80  121211211  301        1    0.003
81  121212211  301        1    0.003
82  121221212  301        1    0.003
83  121222111  301        1    0.003
84  122111211  301        1    0.003
85  122111221  301        1    0.003
86  122111222  301        1    0.003
87  122121211  301        1    0.003
88  122121212  301        1    0.003
89  122221121  301        1    0.003
90  122221222  301        1    0.003
91  122222111  301        1    0.003
92  211111112  301        1    0.003
93  211111121  301        1    0.003
94  211111221  301        1    0.003
95  211121122  301        1    0.003
96  211121221  301        1    0.003
97  211122111  301        1    0.003
98  211222111  301        1    0.003
99  211222211  301        1    0.003
100 212111112  301        1    0.003
101 212111221  301        1    0.003
102 212121211  301        1    0.003
103 212121221  301        1    0.003
104 212121222  301        1    0.003
105 212211221  301        1    0.003
106 212221222  301        1    0.003
107 212222112  301        1    0.003
108 212222211  301        1    0.003
109 221111112  301        1    0.003
110 221111221  301        1    0.003
111 221121212  301        1    0.003
112 221122222  301        1    0.003
113 221211111  301        1    0.003
114 221211212  301        1    0.003
115 221221112  301        1    0.003
116 221222211  301        1    0.003
117 221222212  301        1    0.003
118 222111122  301        1    0.003
119 222111221  301        1    0.003
120 222111222  301        1    0.003
121 222121112  301        1    0.003
122 222121222  301        1    0.003
123 222122112  301        1    0.003
124 222211111  301        1    0.003
125 222211112  301        1    0.003
126 222211121  301        1    0.003
127 222211211  301        1    0.003
128 222221121  301        1    0.003
129 222221221  301        1    0.003
130 222222121  301        1    0.003
131 222222122  301        1    0.003
132 222222212  301        1    0.003
133 222222221  301        1    0.003
> lavTables(HSbinary, dim = 1L, categorical = names(HSbinary), stat=c("th.un"))
   id lhs rhs nobs obs.freq obs.prop  th.un
1   1  x1   1  301      105    0.349 -0.388
2   1  x1   2  301      196    0.651    Inf
3   2  x2   1  301      144    0.478 -0.054
4   2  x2   2  301      157    0.522    Inf
5   3  x3   1  301      188    0.625  0.318
6   3  x3   2  301      113    0.375    Inf
7   4  x4   1  301      172    0.571  0.180
8   4  x4   2  301      129    0.429    Inf
9   5  x5   1  301      120    0.399 -0.257
10  5  x5   2  301      181    0.601    Inf
11  6  x6   1  301      255    0.847  1.024
12  6  x6   2  301       46    0.153    Inf
13  7  x7   1  301      178    0.591  0.231
14  7  x7   2  301      123    0.409    Inf
15  8  x8   1  301      262    0.870  1.128
16  8  x8   2  301       39    0.130    Inf
17  9  x9   1  301      221    0.734  0.626
18  9  x9   2  301       80    0.266    Inf
> lavTables(HSbinary, dim = 2L, categorical = names(HSbinary), type = "table")
    lhs rhs nobs
1    x1  x2  301
5    x1  x3  301
9    x1  x4  301
13   x1  x5  301
17   x1  x6  301
21   x1  x7  301
25   x1  x8  301
29   x1  x9  301
33   x2  x3  301
37   x2  x4  301
41   x2  x5  301
45   x2  x6  301
49   x2  x7  301
53   x2  x8  301
57   x2  x9  301
61   x3  x4  301
65   x3  x5  301
69   x3  x6  301
73   x3  x7  301
77   x3  x8  301
81   x3  x9  301
85   x4  x5  301
89   x4  x6  301
93   x4  x7  301
97   x4  x8  301
101  x4  x9  301
105  x5  x6  301
109  x5  x7  301
113  x5  x8  301
117  x5  x9  301
121  x6  x7  301
125  x6  x8  301
129  x6  x9  301
133  x7  x8  301
137  x7  x9  301
141  x8  x9  301
> 
> # fit a model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HSbinary, ordered=names(HSbinary))
> 
> 
> lavTables(fit, 1L)
   id lhs rhs nobs obs.freq obs.prop est.prop X2
1   1  x1   1  301      105    0.349    0.349  0
2   1  x1   2  301      196    0.651    0.651  0
3   2  x2   1  301      144    0.478    0.478  0
4   2  x2   2  301      157    0.522    0.522  0
5   3  x3   1  301      188    0.625    0.625  0
6   3  x3   2  301      113    0.375    0.375  0
7   4  x4   1  301      172    0.571    0.571  0
8   4  x4   2  301      129    0.429    0.429  0
9   5  x5   1  301      120    0.399    0.399  0
10  5  x5   2  301      181    0.601    0.601  0
11  6  x6   1  301      255    0.847    0.847  0
12  6  x6   2  301       46    0.153    0.153  0
13  7  x7   1  301      178    0.591    0.591  0
14  7  x7   2  301      123    0.409    0.409  0
15  8  x8   1  301      262    0.870    0.870  0
16  8  x8   2  301       39    0.130    0.130  0
17  9  x9   1  301      221    0.734    0.734  0
18  9  x9   2  301       80    0.266    0.266  0
> lavTables(fit, 2L, type="cells")
    id lhs rhs nobs row col obs.freq obs.prop est.prop    X2
1    1  x1  x2  301   1   1       63    0.209    0.222 0.228
2    1  x1  x2  301   2   1       81    0.269    0.256 0.198
3    1  x1  x2  301   1   2       42    0.140    0.127 0.400
4    1  x1  x2  301   2   2      115    0.382    0.395 0.128
5    2  x1  x3  301   1   1       83    0.276    0.271 0.022
6    2  x1  x3  301   2   1      105    0.349    0.353 0.017
7    2  x1  x3  301   1   2       22    0.073    0.078 0.078
8    2  x1  x3  301   2   2       91    0.302    0.298 0.020
9    3  x1  x4  301   1   1       76    0.252    0.243 0.101
10   3  x1  x4  301   2   1       96    0.319    0.328 0.075
11   3  x1  x4  301   1   2       29    0.096    0.105 0.233
12   3  x1  x4  301   2   2      100    0.332    0.323 0.076
13   4  x1  x5  301   1   1       56    0.186    0.183 0.020
14   4  x1  x5  301   2   1       64    0.213    0.216 0.017
15   4  x1  x5  301   1   2       49    0.163    0.166 0.022
16   4  x1  x5  301   2   2      132    0.439    0.435 0.009
17   5  x1  x6  301   1   1       99    0.329    0.322 0.043
18   5  x1  x6  301   2   1      156    0.518    0.525 0.027
19   5  x1  x6  301   1   2        6    0.020    0.027 0.522
20   5  x1  x6  301   2   2       40    0.133    0.126 0.111
21   6  x1  x7  301   1   1       60    0.199    0.225 0.893
22   6  x1  x7  301   2   1      118    0.392    0.366 0.549
23   6  x1  x7  301   1   2       45    0.150    0.124 1.627
24   6  x1  x7  301   2   2       78    0.259    0.285 0.706
25   7  x1  x8  301   1   1       95    0.316    0.319 0.011
26   7  x1  x8  301   2   1      167    0.555    0.551 0.006
27   7  x1  x8  301   1   2       10    0.033    0.030 0.118
28   7  x1  x8  301   2   2       29    0.096    0.100 0.035
29   8  x1  x9  301   1   1       83    0.276    0.279 0.010
30   8  x1  x9  301   2   1      138    0.458    0.455 0.006
31   8  x1  x9  301   1   2       22    0.073    0.070 0.042
32   8  x1  x9  301   2   2       58    0.193    0.196 0.015
33   9  x2  x3  301   1   1      108    0.359    0.352 0.043
34   9  x2  x3  301   2   1       80    0.266    0.273 0.056
35   9  x2  x3  301   1   2       36    0.120    0.127 0.120
36   9  x2  x3  301   2   2       77    0.256    0.249 0.061
37  10  x2  x4  301   1   1       98    0.326    0.317 0.078
38  10  x2  x4  301   2   1       74    0.246    0.255 0.097
39  10  x2  x4  301   1   2       46    0.153    0.162 0.152
40  10  x2  x4  301   2   2       83    0.276    0.267 0.093
41  11  x2  x5  301   1   1       70    0.233    0.232 0.000
42  11  x2  x5  301   2   1       50    0.166    0.166 0.000
43  11  x2  x5  301   1   2       74    0.246    0.246 0.000
44  11  x2  x5  301   2   2      107    0.355    0.355 0.000
45  12  x2  x6  301   1   1      131    0.435    0.433 0.004
46  12  x2  x6  301   2   1      124    0.412    0.414 0.005
47  12  x2  x6  301   1   2       13    0.043    0.046 0.042
48  12  x2  x6  301   2   2       33    0.110    0.107 0.018
49  13  x2  x7  301   1   1       88    0.292    0.301 0.080
50  13  x2  x7  301   2   1       90    0.299    0.290 0.083
51  13  x2  x7  301   1   2       56    0.186    0.177 0.136
52  13  x2  x7  301   2   2       67    0.223    0.232 0.104
53  14  x2  x8  301   1   1      128    0.425    0.432 0.032
54  14  x2  x8  301   2   1      134    0.445    0.438 0.031
55  14  x2  x8  301   1   2       16    0.053    0.046 0.293
56  14  x2  x8  301   2   2       23    0.076    0.083 0.164
57  15  x2  x9  301   1   1      114    0.379    0.374 0.020
58  15  x2  x9  301   2   1      107    0.355    0.360 0.021
59  15  x2  x9  301   1   2       30    0.100    0.105 0.072
60  15  x2  x9  301   2   2       50    0.166    0.161 0.047
61  16  x3  x4  301   1   1      118    0.392    0.400 0.050
62  16  x3  x4  301   2   1       54    0.179    0.171 0.118
63  16  x3  x4  301   1   2       70    0.233    0.224 0.090
64  16  x3  x4  301   2   2       59    0.196    0.204 0.099
65  17  x3  x5  301   1   1       81    0.269    0.290 0.440
66  17  x3  x5  301   2   1       39    0.130    0.109 1.170
67  17  x3  x5  301   1   2      107    0.355    0.335 0.381
68  17  x3  x5  301   2   2       74    0.246    0.266 0.479
69  18  x3  x6  301   1   1      165    0.548    0.558 0.052
70  18  x3  x6  301   2   1       90    0.299    0.289 0.101
71  18  x3  x6  301   1   2       23    0.076    0.067 0.440
72  18  x3  x6  301   2   2       23    0.076    0.086 0.340
73  19  x3  x7  301   1   1      113    0.375    0.388 0.118
74  19  x3  x7  301   2   1       65    0.216    0.204 0.225
75  19  x3  x7  301   1   2       75    0.249    0.237 0.193
76  19  x3  x7  301   2   2       48    0.159    0.172 0.266
77  20  x3  x8  301   1   1      175    0.581    0.560 0.252
78  20  x3  x8  301   2   1       87    0.289    0.311 0.454
79  20  x3  x8  301   1   2       13    0.043    0.065 2.174
80  20  x3  x8  301   2   2       26    0.086    0.065 2.176
81  21  x3  x9  301   1   1      148    0.492    0.481 0.066
82  21  x3  x9  301   2   1       73    0.243    0.253 0.126
83  21  x3  x9  301   1   2       40    0.133    0.143 0.222
84  21  x3  x9  301   2   2       40    0.133    0.123 0.260
85  22  x4  x5  301   1   1      102    0.339    0.337 0.002
86  22  x4  x5  301   2   1       18    0.060    0.061 0.012
87  22  x4  x5  301   1   2       70    0.233    0.234 0.003
88  22  x4  x5  301   2   2      111    0.369    0.367 0.002
89  23  x4  x6  301   1   1      167    0.555    0.558 0.005
90  23  x4  x6  301   2   1       88    0.292    0.289 0.009
91  23  x4  x6  301   1   2        5    0.017    0.014 0.193
92  23  x4  x6  301   2   2       41    0.136    0.139 0.019
93  24  x4  x7  301   1   1      111    0.369    0.349 0.321
94  24  x4  x7  301   2   1       67    0.223    0.242 0.464
95  24  x4  x7  301   1   2       61    0.203    0.222 0.506
96  24  x4  x7  301   2   2       62    0.206    0.187 0.601
97  25  x4  x8  301   1   1      149    0.495    0.507 0.090
98  25  x4  x8  301   2   1      113    0.375    0.363 0.125
99  25  x4  x8  301   1   2       23    0.076    0.064 0.711
100 25  x4  x8  301   2   2       16    0.053    0.065 0.696
101 26  x4  x9  301   1   1      132    0.439    0.434 0.016
102 26  x4  x9  301   2   1       89    0.296    0.300 0.023
103 26  x4  x9  301   1   2       40    0.133    0.138 0.050
104 26  x4  x9  301   2   2       40    0.133    0.128 0.054
105 27  x5  x6  301   1   1      119    0.395    0.394 0.001
106 27  x5  x6  301   2   1      136    0.452    0.453 0.001
107 27  x5  x6  301   1   2        1    0.003    0.005 0.100
108 27  x5  x6  301   2   2       45    0.150    0.148 0.003
109 28  x5  x7  301   1   1       72    0.239    0.247 0.070
110 28  x5  x7  301   2   1      106    0.352    0.345 0.050
111 28  x5  x7  301   1   2       48    0.159    0.152 0.114
112 28  x5  x7  301   2   2       75    0.249    0.257 0.068
113 29  x5  x8  301   1   1      103    0.342    0.356 0.168
114 29  x5  x8  301   2   1      159    0.528    0.514 0.116
115 29  x5  x8  301   1   2       17    0.056    0.042 1.408
116 29  x5  x8  301   2   2       22    0.073    0.087 0.685
117 30  x5  x9  301   1   1       95    0.316    0.306 0.088
118 30  x5  x9  301   2   1      126    0.419    0.428 0.063
119 30  x5  x9  301   1   2       25    0.083    0.093 0.292
120 30  x5  x9  301   2   2       55    0.183    0.173 0.156
121 31  x6  x7  301   1   1      150    0.498    0.509 0.061
122 31  x6  x7  301   2   1       28    0.093    0.083 0.377
123 31  x6  x7  301   1   2      105    0.349    0.339 0.092
124 31  x6  x7  301   2   2       18    0.060    0.070 0.446
125 32  x6  x8  301   1   1      225    0.748    0.744 0.004
126 32  x6  x8  301   2   1       37    0.123    0.126 0.025
127 32  x6  x8  301   1   2       30    0.100    0.103 0.031
128 32  x6  x8  301   2   2        9    0.030    0.027 0.120
129 33  x6  x9  301   1   1      193    0.641    0.631 0.045
130 33  x6  x9  301   2   1       28    0.093    0.103 0.276
131 33  x6  x9  301   1   2       62    0.206    0.216 0.131
132 33  x6  x9  301   2   2       18    0.060    0.050 0.566
133 34  x7  x8  301   1   1      167    0.555    0.544 0.059
134 34  x7  x8  301   2   1       95    0.316    0.326 0.098
135 34  x7  x8  301   1   2       11    0.037    0.047 0.684
136 34  x7  x8  301   2   2       28    0.093    0.083 0.388
137 35  x7  x9  301   1   1      144    0.478    0.477 0.002
138 35  x7  x9  301   2   1       77    0.256    0.258 0.004
139 35  x7  x9  301   1   2       34    0.113    0.115 0.008
140 35  x7  x9  301   2   2       46    0.153    0.151 0.006
141 36  x8  x9  301   1   1      202    0.671    0.681 0.040
142 36  x8  x9  301   2   1       19    0.063    0.054 0.512
143 36  x8  x9  301   1   2       60    0.199    0.190 0.144
144 36  x8  x9  301   2   2       20    0.066    0.076 0.361
> lavTables(fit, 2L, type="table", stat=c("cor.un", "G2", "cor"))
    lhs rhs nobs df   cor cor.un    G2
1    x1  x2  301  0 0.367  0.284 0.944
5    x1  x3  301  0 0.383  0.415 0.139
9    x1  x4  301  0 0.303  0.364 0.491
13   x1  x5  301  0 0.296  0.319 0.069
17   x1  x6  301  0 0.327  0.422 0.752
21   x1  x7  301  0 0.132 -0.048 3.718
25   x1  x8  301  0 0.207  0.159 0.167
29   x1  x9  301  0 0.191  0.165 0.073
33   x2  x3  301  0 0.345  0.389 0.283
37   x2  x4  301  0 0.273  0.328 0.422
41   x2  x5  301  0 0.266  0.268 0.001
45   x2  x6  301  0 0.294  0.322 0.069
49   x2  x7  301  0 0.119  0.061 0.403
53   x2  x8  301  0 0.186  0.105 0.511
57   x2  x9  301  0 0.172  0.210 0.160
61   x3  x4  301  0 0.285  0.232 0.355
65   x3  x5  301  0 0.278  0.138 2.418
69   x3  x6  301  0 0.307  0.206 0.926
73   x3  x7  301  0 0.124  0.041 0.802
77   x3  x8  301  0 0.195  0.439 5.147
81   x3  x9  301  0 0.180  0.258 0.674
85   x4  x5  301  0 0.680  0.688 0.019
89   x4  x6  301  0 0.751  0.720 0.214
93   x4  x7  301  0 0.076  0.200 1.894
97   x4  x8  301  0 0.119 -0.029 1.628
101  x4  x9  301  0 0.109  0.146 0.143
105  x5  x6  301  0 0.733  0.761 0.115
109  x5  x7  301  0 0.074  0.023 0.302
113  x5  x8  301  0 0.116 -0.059 2.284
117  x5  x9  301  0 0.107  0.183 0.607
121  x6  x7  301  0 0.081 -0.029 0.985
125  x6  x8  301  0 0.128  0.183 0.177
129  x6  x9  301  0 0.118  0.230 0.995
133  x7  x8  301  0 0.348  0.464 1.271
137  x7  x9  301  0 0.322  0.335 0.020
141  x8  x9  301  0 0.505  0.403 1.043
> lavTables(fit, 2L, type="table", output="table", stat="X2")
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1      .                                                        
x2  0.954      .                                                 
x3  0.138  0.281      .                                          
x4  0.485  0.420  0.356      .                                   
x5  0.069  0.001  2.469  0.019      .                            
x6  0.703  0.069  0.934  0.226  0.105      .                     
x7  3.775  0.404  0.802  1.892  0.302  0.977      .              
x8  0.171  0.520  5.055  1.622  2.377  0.181  1.229      .       
x9  0.073  0.159  0.675  0.143  0.600  1.018  0.020  1.057      .
> 
> 
> 
> cleanEx()
> nameEx("lavTablesFit")
> ### * lavTablesFit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTablesFitCp
> ### Title: Pairwise maximum likelihood fit statistics
> ### Aliases: lavTablesFit lavTablesFitCp lavTablesFitCf lavTablesFitCm
> 
> ### ** Examples
> 
> # Data
> HS9 <- HolzingerSwineford1939[,c("x1","x2","x3","x4","x5",
+                                  "x6","x7","x8","x9")]
> HSbinary <- as.data.frame( lapply(HS9, cut, 2, labels=FALSE) )
> 
> # Single group example with one latent factor
> HS.model <- ' trait =~ x1 + x2 + x3 + x4 '
> fit <- cfa(HS.model, data=HSbinary[,1:4], ordered=names(HSbinary[,1:4]),
+            estimator="PML")
> lavTablesFitCm(fit)
Cm-value, overall:
[1] 3.968427
Degrees of freedom:
[1] 2
> lavTablesFitCp(fit)
CP-values that are significant at a Bonferroni adjusted level of significance
   lhs rhs nobs df    G2 G2.pval alpha.adj
1   x1  x2  301  0 0.678       0     0.008
5   x1  x3  301  0 0.119       0     0.008
9   x1  x4  301  0 0.280       0     0.008
13  x2  x3  301  0 0.215       0     0.008
17  x2  x4  301  0 0.209       0     0.008
21  x3  x4  301  0 0.888       0     0.008
> lavTablesFitCf(fit)
Total response patterns:  16 
Observed response patterns:  16 
Empty response patterns:  0 
Cf results may be biased because of large numbers of empty cells in the multivariate contingency table
Cf-value, overall:
[1] 11.17914
Degrees of freedom
[1] 7
> 
> 
> 
> cleanEx()
> nameEx("lavTest")
> ### * lavTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTest
> ### Title: Test of exact fit
> ### Aliases: lavTest lavtest
> 
> ### ** Examples
> 
> HS.model <- '
+     visual  =~ x1 + x2 + x3
+     textual =~ x4 + x5 + x6
+     speed   =~ x7 + x8 + x9
+ '
> fit <- cfa(HS.model, data = HolzingerSwineford1939)
> lavTest(fit, test = "browne.residual.adf")
$test
[1] "browne.residual.adf"

$stat
[1] 82.40815

$stat.group
[1] 82.40815

$df
[1] 24

$refdistr
[1] "chisq"

$pvalue
[1] 2.503571e-08

$label
[1] "Browne's residual-based (ADF) test"

> 
> 
> 
> cleanEx()
> nameEx("lavTestLRT")
> ### * lavTestLRT
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTestLRT
> ### Title: LRT test
> ### Aliases: lavTestLRT lavtestLRT LRT lavLRTTest lavLRT anova
> 
> ### ** Examples
> 
> HS.model <- '
+     visual  =~ x1 + b1*x2 + x3
+     textual =~ x4 + b2*x5 + x6
+     speed   =~ x7 + b3*x8 + x9
+ '
> fit1 <- cfa(HS.model, data = HolzingerSwineford1939)
> fit0 <- cfa(HS.model, data = HolzingerSwineford1939, 
+             orthogonal = TRUE)
> lavTestLRT(fit1, fit0)

Chi-Squared Difference Test

     Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    
fit1 24 7517.5 7595.3  85.305                                          
fit0 27 7579.7 7646.4 153.527     68.222 0.26875       3  1.026e-14 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
> 
> 
> 
> cleanEx()
> nameEx("lavTestScore")
> ### * lavTestScore
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTestScore
> ### Title: Score test
> ### Aliases: lavTestScore lavtestscore score Score lavScoreTest
> 
> ### ** Examples
> 
> HS.model <- '
+     visual  =~ x1 + b1*x2 + x3
+     textual =~ x4 + b2*x5 + x6
+     speed   =~ x7 + b3*x8 + x9
+ 
+     b1 == b2
+     b2 == b3
+ '
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> 
> # test 1: release both two equality constraints
> lavTestScore(fit, cumulative = TRUE)
$test

total score test:

   test     X2 df p.value
1 score 12.306  2   0.002

$uni

univariate score tests:

  lhs op rhs     X2 df p.value
1  b1 ==  b2 12.060  1   0.001
2  b2 ==  b3  0.947  1   0.330

$cumulative

cumulative score tests:

  lhs op rhs     X2 df p.value
1  b1 ==  b2 12.060  1   0.001
2  b2 ==  b3 12.306  2   0.002

> 
> # test 2: the score test for adding two (currently fixed
> # to zero) cross-loadings
> newpar = '
+     visual =~ x9
+     textual =~ x3
+ '
> lavTestScore(fit, add = newpar)
$test

total score test:

   test     X2 df p.value
1 score 46.061  2       0

$uni

univariate score tests:

          lhs op rhs     X2 df p.value
1  visual=~x9 ==   0 35.117  1   0.000
2 textual=~x3 ==   0 10.938  1   0.001

> 
> # equivalently, "add" can be a parameter table specifying parameters to free,
> # but must include some additional information:
> PT.add <- data.frame(lhs = c("visual","textual"),
+                      op = c("=~","=~"),
+                      rhs = c("x9","x3"),
+                      user = 10L, # needed to identify new parameters
+                      free = 1, # arbitrary numbers > 0
+                      start = 0) # null-hypothesized value
> PT.add
      lhs op rhs user free start
1  visual =~  x9   10    1     0
2 textual =~  x3   10    1     0
> lavTestScore(fit, add = PT.add) # same result as above
$test

total score test:

   test     X2 df p.value
1 score 46.061  2       0

$uni

univariate score tests:

          lhs op rhs     X2 df p.value
1  visual=~x9 ==   0 35.117  1   0.000
2 textual=~x3 ==   0 10.938  1   0.001

> 
> 
> 
> 
> cleanEx()
> nameEx("lavTestWald")
> ### * lavTestWald
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavTestWald
> ### Title: Wald test
> ### Aliases: lavTestWald lavtestwald wald Wald lavWaldTest
> 
> ### ** Examples
> 
> HS.model <- '
+     visual  =~ x1 + b1*x2 + x3
+     textual =~ x4 + b2*x5 + x6
+     speed   =~ x7 + b3*x8 + x9
+ '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> 
> # test 1: test about a single parameter
> # this is the 'chi-square' version of the 
> # z-test from the summary() output
> lavTestWald(fit, constraints = "b1 == 0")
$stat
[1] 30.84248

$df
[1] 1

$p.value
[1] 2.79844e-08

$se
[1] "standard"

> 
> # test 2: several constraints
> con = '
+    2*b1 == b3
+    b2 - b3 == 0
+ '
> lavTestWald(fit, constraints = con)
$stat
[1] 0.147265

$df
[1] 2

$p.value
[1] 0.9290131

$se
[1] "standard"

> 
> 
> 
> cleanEx()
> nameEx("lav_constraints")
> ### * lav_constraints
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_constraints
> ### Title: Utility Functions: Constraints
> ### Aliases: lav_constraints_parse lav_partable_constraints_ceq
> ###   lav_partable_constraints_ciq lav_partable_constraints_def
> 
> ### ** Examples
> 
> myModel <- 'x1 ~ a*x2 + b*x3 + c*x4'
> myParTable <- lavaanify(myModel, as.data.frame. = FALSE)
> con <- ' a == 2*b
+          b - c == 5 '
> conInfo <- lav_constraints_parse(myParTable, constraints = con)
> 
> myModel2 <- 'x1 ~ a*x2 + b*x3 + c*x4
+              a == 2*b
+              b - c == 5 '
> ceq <- lav_partable_constraints_ceq(partable = lavaanify(myModel2))
> ceq( c(2,3,4) )
[1] -4 -6
> 
> 
> 
> cleanEx()
> nameEx("lav_data")
> ### * lav_data
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_data
> ### Title: lavaan data functions
> ### Aliases: lav_data_update
> 
> ### ** Examples
> 
> # generate syntax for an independence model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> 
> # extract data slot and options
> lavdata <- fit@Data
> lavoptions <- lavInspect(fit, "options")
> 
> # create bootstrap sample
> boot.idx <- sample(x = nobs(fit), size = nobs(fit), replace = TRUE)
> newX <- list(lavdata@X[[1]][boot.idx,])
> 
> # generate update lavdata object
> newdata <- lav_data_update(lavdata = lavdata, newX = newX, 
+                            lavoptions = lavoptions)
> 
> 
> 
> cleanEx()
> nameEx("lav_func")
> ### * lav_func
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_func
> ### Title: Utility Functions: Gradient and Jacobian
> ### Aliases: lav_func_gradient_complex lav_func_gradient_simple
> ###   lav_func_jacobian_complex lav_func_jacobian_simple
> 
> ### ** Examples
> 
> # very accurate complex method
> lav_func_gradient_complex(func = exp, x = 1) - exp(1)
[1] 0
> 
> # less accurate forward method
> lav_func_gradient_simple(func = exp, x = 1) - exp(1)
[1] 3.666089e-08
> 
> # very accurate complex method
> diag(lav_func_jacobian_complex(func = exp, x = c(1,2,3))) - exp(c(1,2,3))
[1] 0 0 0
> 
> # less accurate forward method
> diag(lav_func_jacobian_simple(func = exp, x = c(1,2,3))) - exp(c(1,2,3))
[1] 3.666089e-08 1.068189e-07 4.309637e-07
> 
> 
> 
> cleanEx()
> nameEx("lav_matrix")
> ### * lav_matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_matrix
> ### Title: Utility Functions: Matrices and Vectors
> ### Aliases: lav_matrix_vec lav_matrix_vecr lav_matrix_vech
> ###   lav_matrix_vechr lav_matrix_vechu lav_matrix_vechru
> ###   lav_matrix_vech_idx lav_matrix_vech_row_idx lav_matrix_vech_col_idx
> ###   lav_matrix_vechr_idx lav_matrix_vechu_idx lav_matrix_vechru_idx
> ###   lav_matrix_vech_reverse lav_matrix_vechru_reverse
> ###   lav_matrix_upper2full lav_matrix_vechr_reverse
> ###   lav_matrix_vechu_reverse lav_matrix_lower2full lav_matrix_diag_idx
> ###   lav_matrix_diagh_idx lav_matrix_antidiag_idx lav_matrix_duplication
> ###   lav_matrix_duplication_pre lav_matrix_duplication_post
> ###   lav_matrix_duplication_pre_post lav_matrix_duplication_ginv
> ###   lav_matrix_duplication_ginv_pre lav_matrix_duplication_ginv_post
> ###   lav_matrix_duplication_ginv_pre_post lav_matrix_commutation
> ###   lav_matrix_commutation_pre lav_matrix_commutation_post
> ###   lav_matrix_commutation_pre_post lav_matrix_commutation_mn_pre
> ###   lav_matrix_symmetric_sqrt lav_matrix_orthogonal_complement
> ###   lav_matrix_bdiag lav_matrix_trace lav_matrix_cov
> 
> ### ** Examples
> 
> # upper elements of a 3 by 3 symmetric matrix (row by row)
> x <- c(30, 16, 5, 10, 3, 1)
> # construct full symmetric matrix
> S <- lav_matrix_upper2full(x)
> 
> # compute the normal theory `Gamma' matrix given a covariance
> # matrix (S), using the formula: Gamma = 2 * D^{+} (S %x% S) t(D^{+})
> Gamma.NT <- 2 * lav_matrix_duplication_ginv_pre_post(S %x% S)
> Gamma.NT
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] 1800  960  300  512  160   50
[2,]  960  556  170  320   98   30
[3,]  300  170   55   96   31   10
[4,]  512  320   96  200   60   18
[5,]  160   98   31   60   19    6
[6,]   50   30   10   18    6    2
> 
> 
> 
> cleanEx()
> nameEx("lav_model")
> ### * lav_model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_model
> ### Title: lavaan model functions
> ### Aliases: lav_model_get_parameters lav_model_set_parameters
> ###   lav_model_implied lav_model_vcov_se
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> lavmodel <- fit@Model
> 
> est <- lav_model_get_parameters(lavmodel)
> est
 [1] 0.5535003 0.7293702 1.1130766 0.9261462 1.1799508 1.0815302 0.5490540
 [8] 1.1338390 0.8443240 0.3711730 0.4462551 0.3562027 0.7993916 0.4876971
[15] 0.5661313 0.8093160 0.9794914 0.3837476 0.4082324 0.2622246 0.1734947
> 
> 
> 
> cleanEx()
> nameEx("lav_partable")
> ### * lav_partable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_partable
> ### Title: lavaan partable functions
> ### Aliases: lav_partable_independence lav_partable_unrestricted
> ###   lav_partable_df lav_partable_ndat lav_partable_npar
> ###   lav_partable_labels lav_partable_from_lm lav_partable_complete
> ###   lav_partable_attributes lav_partable_merge lav_partable_add
> 
> ### ** Examples
> 
> # generate syntax for an independence model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> lav <- lav_partable_independence(fit)
> as.data.frame(lav, stringsAsFactors = FALSE)
  id lhs op rhs user block group free   ustart exo
1  1  x1 ~~  x1    1     1     1    1 1.358370   0
2  2  x2 ~~  x2    1     1     1    2 1.381784   0
3  3  x3 ~~  x3    1     1     1    3 1.274865   0
4  4  x4 ~~  x4    1     1     1    4 1.350665   0
5  5  x5 ~~  x5    1     1     1    5 1.659786   0
6  6  x6 ~~  x6    1     1     1    6 1.196358   0
7  7  x7 ~~  x7    1     1     1    7 1.183139   0
8  8  x8 ~~  x8    1     1     1    8 1.021983   0
9  9  x9 ~~  x9    1     1     1    9 1.015004   0
> 
> 
> # how many free parameters?
> lav_partable_npar(lav)
[1] 9
> 
> # how many sample statistics?
> lav_partable_ndat(lav)
[1] 45
> 
> 
> 
> cleanEx()
> nameEx("lav_samplestats")
> ### * lav_samplestats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lav_samplestats
> ### Title: lavaan samplestats functions
> ### Aliases: lav_samplestats_from_data
> 
> ### ** Examples
> 
> # generate syntax for an independence model
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> 
> # extract data slot and options
> lavdata <- fit@Data
> lavoptions <- lavInspect(fit, "options")
> 
> # generate sample statistics object
> sampleStats <- lav_samplestats_from_data(lavdata = lavdata, 
+                                          lavoptions = lavoptions)
> 
> 
> 
> cleanEx()
> nameEx("lavaan-class")
> ### * lavaan-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavaan-class
> ### Title: Class For Representing A (Fitted) Latent Variable Model
> ### Aliases: lavaan-class coef,lavaan-method vcov,lavaan-method
> ###   anova,lavaan-method predict,lavaan-method resid,lavaan-method
> ###   residuals,lavaan-method fitted,lavaan-method
> ###   fitted.values,lavaan-method nobs nobs,lavaan-method
> ###   logLik,lavaan-method update,lavaan-method show,lavaan-method
> ###   summary,lavaan-method
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data = HolzingerSwineford1939)
> 
> summary(fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
lavaan 0.6.17 ended normally after 35 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        21

  Number of observations                           301

Model Test User Model:
                                                      
  Test statistic                                85.306
  Degrees of freedom                                24
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               918.852
  Degrees of freedom                                36
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.931
  Tucker-Lewis Index (TLI)                       0.896

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -3737.745
  Loglikelihood unrestricted model (H1)      -3695.092
                                                      
  Akaike (AIC)                                7517.490
  Bayesian (BIC)                              7595.339
  Sample-size adjusted Bayesian (SABIC)       7528.739

Root Mean Square Error of Approximation:

  RMSEA                                          0.092
  90 Percent confidence interval - lower         0.071
  90 Percent confidence interval - upper         0.114
  P-value H_0: RMSEA <= 0.050                    0.001
  P-value H_0: RMSEA >= 0.080                    0.840

Standardized Root Mean Square Residual:

  SRMR                                           0.065

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  visual =~                                                             
    x1                1.000                               0.900    0.772
    x2                0.554    0.100    5.554    0.000    0.498    0.424
    x3                0.729    0.109    6.685    0.000    0.656    0.581
  textual =~                                                            
    x4                1.000                               0.990    0.852
    x5                1.113    0.065   17.014    0.000    1.102    0.855
    x6                0.926    0.055   16.703    0.000    0.917    0.838
  speed =~                                                              
    x7                1.000                               0.619    0.570
    x8                1.180    0.165    7.152    0.000    0.731    0.723
    x9                1.082    0.151    7.155    0.000    0.670    0.665

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  visual ~~                                                             
    textual           0.408    0.074    5.552    0.000    0.459    0.459
    speed             0.262    0.056    4.660    0.000    0.471    0.471
  textual ~~                                                            
    speed             0.173    0.049    3.518    0.000    0.283    0.283

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
   .x1                0.549    0.114    4.833    0.000    0.549    0.404
   .x2                1.134    0.102   11.146    0.000    1.134    0.821
   .x3                0.844    0.091    9.317    0.000    0.844    0.662
   .x4                0.371    0.048    7.779    0.000    0.371    0.275
   .x5                0.446    0.058    7.642    0.000    0.446    0.269
   .x6                0.356    0.043    8.277    0.000    0.356    0.298
   .x7                0.799    0.081    9.823    0.000    0.799    0.676
   .x8                0.488    0.074    6.573    0.000    0.488    0.477
   .x9                0.566    0.071    8.003    0.000    0.566    0.558
    visual            0.809    0.145    5.564    0.000    1.000    1.000
    textual           0.979    0.112    8.737    0.000    1.000    1.000
    speed             0.384    0.086    4.451    0.000    1.000    1.000

R-Square:
                   Estimate
    x1                0.596
    x2                0.179
    x3                0.338
    x4                0.725
    x5                0.731
    x6                0.702
    x7                0.324
    x8                0.523
    x9                0.442

> fitted(fit)
$cov
      x1    x2    x3    x4    x5    x6    x7    x8    x9
x1 1.358                                                
x2 0.448 1.382                                          
x3 0.590 0.327 1.275                                    
x4 0.408 0.226 0.298 1.351                              
x5 0.454 0.252 0.331 1.090 1.660                        
x6 0.378 0.209 0.276 0.907 1.010 1.196                  
x7 0.262 0.145 0.191 0.173 0.193 0.161 1.183            
x8 0.309 0.171 0.226 0.205 0.228 0.190 0.453 1.022      
x9 0.284 0.157 0.207 0.188 0.209 0.174 0.415 0.490 1.015

> coef(fit)
      visual=~x2       visual=~x3      textual=~x5      textual=~x6 
           0.554            0.729            1.113            0.926 
       speed=~x8        speed=~x9           x1~~x1           x2~~x2 
           1.180            1.082            0.549            1.134 
          x3~~x3           x4~~x4           x5~~x5           x6~~x6 
           0.844            0.371            0.446            0.356 
          x7~~x7           x8~~x8           x9~~x9   visual~~visual 
           0.799            0.488            0.566            0.809 
textual~~textual     speed~~speed  visual~~textual    visual~~speed 
           0.979            0.384            0.408            0.262 
  textual~~speed 
           0.173 
> resid(fit, type = "normalized")
$type
[1] "normalized"

$cov
       x1     x2     x3     x4     x5     x6     x7     x8     x9
x1  0.000                                                        
x2 -0.493  0.000                                                 
x3 -0.125  1.539  0.000                                          
x4  1.159 -0.214 -1.170  0.000                                   
x5 -0.153 -0.459 -2.606  0.070  0.000                            
x6  0.983  0.507 -0.436 -0.130  0.048  0.000                     
x7 -2.423 -3.273 -1.450  0.625 -0.617 -0.240  0.000              
x8 -0.655 -0.896 -0.200 -1.162 -0.624 -0.375  1.170  0.000       
x9  2.405  1.249  2.420  0.808  1.126  0.958 -0.625 -0.504  0.000

> 
> 
> 
> cleanEx()
> nameEx("lavaan")
> ### * lavaan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavaan
> ### Title: Fit a Latent Variable Model
> ### Aliases: lavaan
> 
> ### ** Examples
> 
> # The Holzinger and Swineford (1939) example
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- lavaan(HS.model, data=HolzingerSwineford1939,
+               auto.var=TRUE, auto.fix.first=TRUE,
+               auto.cov.lv.x=TRUE)
> summary(fit, fit.measures=TRUE)
lavaan 0.6.17 ended normally after 35 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        21

  Number of observations                           301

Model Test User Model:
                                                      
  Test statistic                                85.306
  Degrees of freedom                                24
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               918.852
  Degrees of freedom                                36
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.931
  Tucker-Lewis Index (TLI)                       0.896

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -3737.745
  Loglikelihood unrestricted model (H1)      -3695.092
                                                      
  Akaike (AIC)                                7517.490
  Bayesian (BIC)                              7595.339
  Sample-size adjusted Bayesian (SABIC)       7528.739

Root Mean Square Error of Approximation:

  RMSEA                                          0.092
  90 Percent confidence interval - lower         0.071
  90 Percent confidence interval - upper         0.114
  P-value H_0: RMSEA <= 0.050                    0.001
  P-value H_0: RMSEA >= 0.080                    0.840

Standardized Root Mean Square Residual:

  SRMR                                           0.065

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  visual =~                                           
    x1                1.000                           
    x2                0.554    0.100    5.554    0.000
    x3                0.729    0.109    6.685    0.000
  textual =~                                          
    x4                1.000                           
    x5                1.113    0.065   17.014    0.000
    x6                0.926    0.055   16.703    0.000
  speed =~                                            
    x7                1.000                           
    x8                1.180    0.165    7.152    0.000
    x9                1.082    0.151    7.155    0.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
  visual ~~                                           
    textual           0.408    0.074    5.552    0.000
    speed             0.262    0.056    4.660    0.000
  textual ~~                                          
    speed             0.173    0.049    3.518    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .x1                0.549    0.114    4.833    0.000
   .x2                1.134    0.102   11.146    0.000
   .x3                0.844    0.091    9.317    0.000
   .x4                0.371    0.048    7.779    0.000
   .x5                0.446    0.058    7.642    0.000
   .x6                0.356    0.043    8.277    0.000
   .x7                0.799    0.081    9.823    0.000
   .x8                0.488    0.074    6.573    0.000
   .x9                0.566    0.071    8.003    0.000
    visual            0.809    0.145    5.564    0.000
    textual           0.979    0.112    8.737    0.000
    speed             0.384    0.086    4.451    0.000

> 
> 
> 
> cleanEx()
> nameEx("lavaanList")
> ### * lavaanList
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lavaanList
> ### Title: Fit List of Latent Variable Models
> ### Aliases: lavaanList semList cfaList
> 
> ### ** Examples
> 
> # The Holzinger and Swineford (1939) example
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> # a data generating function
> generateData <- function() simulateData(HS.model, sample.nobs = 100)
> 
> set.seed(1234)
> fit <- semList(HS.model, dataFunction = generateData, ndat = 5,
+                store.slots = "partable")
> 
> # show parameter estimates, per dataset
> coef(fit)
                        [,1]        [,2]       [,3]        [,4]         [,5]
visual=~x2        0.82300278  1.05771862 0.75448904  0.98124446  0.949981509
visual=~x3        0.92516072  0.90744587 0.91907731  0.89567526  0.863238096
textual=~x5       1.23492389  1.35592445 1.14082464  0.81704440  1.119451993
textual=~x6       1.01870890  1.50656636 0.81347156  0.80062992  1.205761095
speed=~x8         0.92649956  1.31225095 1.14857880  0.95029127  0.916616927
speed=~x9         0.71293786  1.18489498 0.88770018  1.03694462  0.978192011
x1~~x1            0.91852644  1.14154882 0.65472016  1.18106604  0.692187611
x2~~x2            0.68663625  0.95957809 1.09955813  0.87890047  1.088436461
x3~~x3            0.91748376  0.69153510 1.00762407  0.83722940  1.028759505
x4~~x4            0.81744390  1.14629095 0.90973307  0.92065420  1.328687742
x5~~x5            1.07370577  0.88150973 0.73940573  0.87489827  0.730050973
x6~~x6            1.35861446  0.75676787 1.21792077  1.10896316  0.850508059
x7~~x7            0.65777983  1.24425704 0.99961279  0.85763463  1.046511205
x8~~x8            1.04588561  0.56329054 0.77636007  1.36383177  0.802617946
x9~~x9            1.10333846  0.94825732 1.31277608  0.87109024  0.881869999
visual~~visual    1.11588395  0.97485218 1.67574358  0.98384641  1.183373278
textual~~textual  0.88810301  0.47513582 0.91913172  1.22666968  0.899658906
speed~~speed      1.31677025  0.82864127 1.15502138  1.21786196  0.911839066
visual~~textual   0.12699776 -0.11782835 0.08026048 -0.15309895 -0.200896117
visual~~speed     0.14888512 -0.03244788 0.31413134 -0.22086377 -0.007036338
textual~~speed   -0.02220143  0.11592055 0.03965802 -0.07024187 -0.035497170
> 
> 
> 
> cleanEx()
> nameEx("modificationIndices")
> ### * modificationIndices
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: modificationIndices
> ### Title: Modification Indices
> ### Aliases: modificationIndices modificationindices modindices
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> modindices(fit, minimum.value = 10, sort = TRUE)
      lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox
30 visual =~  x9 36.411  0.577   0.519    0.515    0.515
76     x7 ~~  x8 34.145  0.536   0.536    0.859    0.859
28 visual =~  x7 18.631 -0.422  -0.380   -0.349   -0.349
78     x8 ~~  x9 14.946 -0.423  -0.423   -0.805   -0.805
> 
> 
> 
> cleanEx()
> nameEx("mplus2lavaan")
> ### * mplus2lavaan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mplus2lavaan
> ### Title: mplus to lavaan converter
> ### Aliases: mplus2lavaan lavImport
> 
> ### ** Examples
> 
> ## Not run: 
> ##D out <- mplus2lavaan("ex5.1.inp")
> ##D summary(out)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mplus2lavaan.modelSyntax")
> ### * mplus2lavaan.modelSyntax
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mplus2lavaan.modelSyntax
> ### Title: Convert Mplus model syntax to lavaan
> ### Aliases: mplus2lavaan.modelSyntax
> 
> ### ** Examples
> 
> ## Not run: 
> ##D syntax <- '
> ##D     f1 BY x1*1 x2 x3;
> ##D     x1 WITH x2;
> ##D     x3 (1);
> ##D     x2 (1);
> ##D '
> ##D lavSyntax <- mplus2lavaan.modelSyntax(syntax)
> ##D cat(lavSyntax)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("parameterEstimates")
> ### * parameterEstimates
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: parameterEstimates
> ### Title: Parameter Estimates
> ### Aliases: parameterEstimates parameterestimates
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> parameterEstimates(fit)
       lhs op     rhs   est    se      z pvalue ci.lower ci.upper
1   visual =~      x1 1.000 0.000     NA     NA    1.000    1.000
2   visual =~      x2 0.554 0.100  5.554      0    0.358    0.749
3   visual =~      x3 0.729 0.109  6.685      0    0.516    0.943
4  textual =~      x4 1.000 0.000     NA     NA    1.000    1.000
5  textual =~      x5 1.113 0.065 17.014      0    0.985    1.241
6  textual =~      x6 0.926 0.055 16.703      0    0.817    1.035
7    speed =~      x7 1.000 0.000     NA     NA    1.000    1.000
8    speed =~      x8 1.180 0.165  7.152      0    0.857    1.503
9    speed =~      x9 1.082 0.151  7.155      0    0.785    1.378
10      x1 ~~      x1 0.549 0.114  4.833      0    0.326    0.772
11      x2 ~~      x2 1.134 0.102 11.146      0    0.934    1.333
12      x3 ~~      x3 0.844 0.091  9.317      0    0.667    1.022
13      x4 ~~      x4 0.371 0.048  7.779      0    0.278    0.465
14      x5 ~~      x5 0.446 0.058  7.642      0    0.332    0.561
15      x6 ~~      x6 0.356 0.043  8.277      0    0.272    0.441
16      x7 ~~      x7 0.799 0.081  9.823      0    0.640    0.959
17      x8 ~~      x8 0.488 0.074  6.573      0    0.342    0.633
18      x9 ~~      x9 0.566 0.071  8.003      0    0.427    0.705
19  visual ~~  visual 0.809 0.145  5.564      0    0.524    1.094
20 textual ~~ textual 0.979 0.112  8.737      0    0.760    1.199
21   speed ~~   speed 0.384 0.086  4.451      0    0.215    0.553
22  visual ~~ textual 0.408 0.074  5.552      0    0.264    0.552
23  visual ~~   speed 0.262 0.056  4.660      0    0.152    0.373
24 textual ~~   speed 0.173 0.049  3.518      0    0.077    0.270
> parameterEstimates(fit, output = "text")

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|) ci.lower ci.upper
  visual =~                                                             
    x1                1.000                               1.000    1.000
    x2                0.554    0.100    5.554    0.000    0.358    0.749
    x3                0.729    0.109    6.685    0.000    0.516    0.943
  textual =~                                                            
    x4                1.000                               1.000    1.000
    x5                1.113    0.065   17.014    0.000    0.985    1.241
    x6                0.926    0.055   16.703    0.000    0.817    1.035
  speed =~                                                              
    x7                1.000                               1.000    1.000
    x8                1.180    0.165    7.152    0.000    0.857    1.503
    x9                1.082    0.151    7.155    0.000    0.785    1.378

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|) ci.lower ci.upper
  visual ~~                                                             
    textual           0.408    0.074    5.552    0.000    0.264    0.552
    speed             0.262    0.056    4.660    0.000    0.152    0.373
  textual ~~                                                            
    speed             0.173    0.049    3.518    0.000    0.077    0.270

Variances:
                   Estimate  Std.Err  z-value  P(>|z|) ci.lower ci.upper
   .x1                0.549    0.114    4.833    0.000    0.326    0.772
   .x2                1.134    0.102   11.146    0.000    0.934    1.333
   .x3                0.844    0.091    9.317    0.000    0.667    1.022
   .x4                0.371    0.048    7.779    0.000    0.278    0.465
   .x5                0.446    0.058    7.642    0.000    0.332    0.561
   .x6                0.356    0.043    8.277    0.000    0.272    0.441
   .x7                0.799    0.081    9.823    0.000    0.640    0.959
   .x8                0.488    0.074    6.573    0.000    0.342    0.633
   .x9                0.566    0.071    8.003    0.000    0.427    0.705
    visual            0.809    0.145    5.564    0.000    0.524    1.094
    textual           0.979    0.112    8.737    0.000    0.760    1.199
    speed             0.384    0.086    4.451    0.000    0.215    0.553

> 
> 
> 
> cleanEx()
> nameEx("plot.InformativeTesting")
> ### * plot.InformativeTesting
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: InformativeTesting methods
> ### Title: Methods for output InformativeTesting()
> ### Aliases: print.InformativeTesting plot.InformativeTesting
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #########################
> ##D ### real data example ###
> ##D #########################
> ##D # Multiple group path model for facial burns example.
> ##D 
> ##D # model syntax with starting values.
> ##D   burns.model <- 'Selfesteem ~ Age + c(m1, f1)*TBSA + HADS +
> ##D                              start(-.10, -.20)*TBSA  
> ##D                  HADS ~ Age + c(m2, f2)*TBSA + RUM +
> ##D                         start(.10, .20)*TBSA '
> ##D  
> ##D  
> ##D # constraints syntax
> ##D  burns.constraints <- 'f2 > 0  ; m1 < 0
> ##D                        m2 > 0  ; f1 < 0
> ##D                        f2 > m2 ; f1 < m1'
> ##D  
> ##D # we only generate 2 bootstrap samples in this example; in practice
> ##D # you may wish to use a much higher number. 
> ##D # the double bootstrap was switched off; in practice you probably 
> ##D # want to set it to "standard".
> ##D example1 <- InformativeTesting(model = burns.model, data = FacialBurns,
> ##D                                R = 2, constraints = burns.constraints,
> ##D                                double.bootstrap = "no", group = "Sex")
> ##D example1
> ##D plot(example1)
> ##D 
> ##D ##########################
> ##D ### artificial example ###
> ##D ##########################
> ##D # Simple ANOVA model with 3 groups (N = 20 per group)
> ##D set.seed(1234)
> ##D Y <- cbind(c(rnorm(20,0,1), rnorm(20,0.5,1), rnorm(20,1,1)))
> ##D grp <- c(rep("1", 20), rep("2", 20), rep("3", 20))
> ##D Data <- data.frame(Y, grp)
> ##D 
> ##D #create model matrix
> ##D fit.lm <- lm(Y ~ grp, data = Data)
> ##D mfit <- fit.lm$model
> ##D mm <- model.matrix(mfit)
> ##D 
> ##D Y <- model.response(mfit)
> ##D X <- data.frame(mm[,2:3])
> ##D names(X) <- c("d1", "d2")
> ##D Data.new <- data.frame(Y, X)
> ##D 
> ##D # model
> ##D model <- 'Y ~ 1 + a1*d1 + a2*d2'
> ##D 
> ##D # fit without constraints
> ##D fit <- sem(model, data = Data.new)
> ##D 
> ##D # constraints syntax: mu1 < mu2 < mu3
> ##D constraints <- ' a1 > 0
> ##D                  a1 < a2 '
> ##D 
> ##D # we only generate 10 bootstrap samples in this example; in practice
> ##D # you may wish to use a much higher number, say > 1000. The double 
> ##D # bootstrap is not necessary in case of an univariate ANOVA model.
> ##D example2 <- InformativeTesting(model = model, data = Data.new, 
> ##D                                start = parTable(fit),
> ##D                                R = 10L, double.bootstrap = "no",
> ##D                                constraints = constraints)
> ##D example2
> ##D # plot(example2)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("sam")
> ### * sam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sam
> ### Title: Fit Structural Equation Models using the SAM approach
> ### Aliases: sam fsr
> 
> ### ** Examples
> 
> ## The industrialization and Political Democracy Example 
> ## Bollen (1989), page 332
> model <- ' 
+   # latent variable definitions
+      ind60 =~ x1 + x2 + x3
+      dem60 =~ y1 + a*y2 + b*y3 + c*y4
+      dem65 =~ y5 + a*y6 + b*y7 + c*y8
+ 
+   # regressions
+     dem60 ~ ind60
+     dem65 ~ ind60 + dem60
+ 
+   # residual correlations
+     y1 ~~ y5
+     y2 ~~ y4 + y6
+     y3 ~~ y7
+     y4 ~~ y8
+     y6 ~~ y8
+ '
> 
> fit.sam <- sam(model, data = PoliticalDemocracy,
+                mm.list = list(ind = "ind60", dem = c("dem60", "dem65")))
> summary(fit.sam)
This is lavaan 0.6.17 -- using the SAM approach to SEM

  SAM method                                     LOCAL
  Mapping matrix M method                           ML
  Number of measurement blocks                       2
  Estimator measurement part                        ML
  Estimator  structural part                        ML

  Number of observations                            75

Summary Information Measurement + Structural:

  Block      Latent Nind Chisq Df
      1       ind60    3  0.00  0
      2 dem60,dem65    8 15.32 16

  Model-based reliability latent variables:

  ind60 dem60 dem65
  0.966 0.868  0.87

  Summary Information Structural part:

  chisq df cfi rmsea srmr
      0  0   1     0    0

Parameter Estimates:

  Standard errors                              Twostep
  Information                                 Expected
  Information saturated (h1) model          Structured

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  dem60 ~                                             
    ind60             1.454    0.389    3.741    0.000
  dem65 ~                                             
    ind60             0.558    0.225    2.480    0.013
    dem60             0.871    0.076   11.497    0.000

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
    ind60             0.446    0.087    5.135    0.000
   .dem60             3.766    0.848    4.439    0.000
   .dem65             0.189    0.224    0.843    0.399

> 
> 
> 
> cleanEx()
> nameEx("sem")
> ### * sem
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sem
> ### Title: Fit Structural Equation Models
> ### Aliases: sem
> 
> ### ** Examples
> 
> ## The industrialization and Political Democracy Example 
> ## Bollen (1989), page 332
> model <- ' 
+   # latent variable definitions
+      ind60 =~ x1 + x2 + x3
+      dem60 =~ y1 + a*y2 + b*y3 + c*y4
+      dem65 =~ y5 + a*y6 + b*y7 + c*y8
+ 
+   # regressions
+     dem60 ~ ind60
+     dem65 ~ ind60 + dem60
+ 
+   # residual correlations
+     y1 ~~ y5
+     y2 ~~ y4 + y6
+     y3 ~~ y7
+     y4 ~~ y8
+     y6 ~~ y8
+ '
> 
> fit <- sem(model, data = PoliticalDemocracy)
> summary(fit, fit.measures = TRUE)
lavaan 0.6.17 ended normally after 66 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        31
  Number of equality constraints                     3

  Number of observations                            75

Model Test User Model:
                                                      
  Test statistic                                40.179
  Degrees of freedom                                38
  P-value (Chi-square)                           0.374

Model Test Baseline Model:

  Test statistic                               730.654
  Degrees of freedom                                55
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.997
  Tucker-Lewis Index (TLI)                       0.995

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -1548.818
  Loglikelihood unrestricted model (H1)      -1528.728
                                                      
  Akaike (AIC)                                3153.636
  Bayesian (BIC)                              3218.526
  Sample-size adjusted Bayesian (SABIC)       3130.277

Root Mean Square Error of Approximation:

  RMSEA                                          0.028
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.087
  P-value H_0: RMSEA <= 0.050                    0.665
  P-value H_0: RMSEA >= 0.080                    0.083

Standardized Root Mean Square Residual:

  SRMR                                           0.056

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  ind60 =~                                            
    x1                1.000                           
    x2                2.180    0.138   15.751    0.000
    x3                1.818    0.152   11.971    0.000
  dem60 =~                                            
    y1                1.000                           
    y2         (a)    1.191    0.139    8.551    0.000
    y3         (b)    1.175    0.120    9.755    0.000
    y4         (c)    1.251    0.117   10.712    0.000
  dem65 =~                                            
    y5                1.000                           
    y6         (a)    1.191    0.139    8.551    0.000
    y7         (b)    1.175    0.120    9.755    0.000
    y8         (c)    1.251    0.117   10.712    0.000

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  dem60 ~                                             
    ind60             1.471    0.392    3.750    0.000
  dem65 ~                                             
    ind60             0.600    0.226    2.661    0.008
    dem60             0.865    0.075   11.554    0.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
 .y1 ~~                                               
   .y5                0.583    0.356    1.637    0.102
 .y2 ~~                                               
   .y4                1.440    0.689    2.092    0.036
   .y6                2.183    0.737    2.960    0.003
 .y3 ~~                                               
   .y7                0.712    0.611    1.165    0.244
 .y4 ~~                                               
   .y8                0.363    0.444    0.817    0.414
 .y6 ~~                                               
   .y8                1.372    0.577    2.378    0.017

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .x1                0.081    0.019    4.182    0.000
   .x2                0.120    0.070    1.729    0.084
   .x3                0.467    0.090    5.177    0.000
   .y1                1.855    0.433    4.279    0.000
   .y2                7.581    1.366    5.549    0.000
   .y3                4.956    0.956    5.182    0.000
   .y4                3.225    0.723    4.458    0.000
   .y5                2.313    0.479    4.831    0.000
   .y6                4.968    0.921    5.393    0.000
   .y7                3.560    0.710    5.018    0.000
   .y8                3.308    0.704    4.701    0.000
    ind60             0.449    0.087    5.175    0.000
   .dem60             3.875    0.866    4.477    0.000
   .dem65             0.164    0.227    0.725    0.469

> 
> 
> 
> cleanEx()
> nameEx("simulateData")
> ### * simulateData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simulateData
> ### Title: Simulate Data From a Lavaan Model Syntax
> ### Aliases: simulateData
> 
> ### ** Examples
> 
> # specify population model
> population.model <- ' f1 =~ x1 + 0.8*x2 + 1.2*x3
+                       f2 =~ x4 + 0.5*x5 + 1.5*x6
+                       f3 =~ x7 + 0.1*x8 + 0.9*x9
+ 
+                       f3 ~ 0.5*f1 + 0.6*f2
+                     '
> 
> # generate data
> set.seed(1234)
> myData <- simulateData(population.model, sample.nobs=100L)
> 
> # population moments
> fitted(sem(population.model))
$cov
      x1    x2    x3    x4    x5    x6    x7    x8    x9
x1 2.000                                                
x2 0.800 1.640                                          
x3 1.200 0.960 2.440                                    
x4 0.000 0.000 0.000 2.000                              
x5 0.000 0.000 0.000 0.500 1.250                        
x6 0.000 0.000 0.000 1.500 0.750 3.250                  
x7 0.500 0.400 0.600 0.600 0.300 0.900 2.610            
x8 0.050 0.040 0.060 0.060 0.030 0.090 0.161 1.016      
x9 0.450 0.360 0.540 0.540 0.270 0.810 1.449 0.145 2.304

> 
> # sample moments
> round(cov(myData), 3)
       x1     x2     x3     x4     x5     x6    x7     x8    x9
x1  2.181  1.018  1.336 -0.038 -0.005  0.060 0.571  0.243 0.780
x2  1.018  1.616  1.154  0.178 -0.020 -0.137 0.255  0.096 0.237
x3  1.336  1.154  2.305 -0.093 -0.179 -0.310 0.408 -0.053 0.626
x4 -0.038  0.178 -0.093  1.603  0.606  1.173 0.814  0.116 0.537
x5 -0.005 -0.020 -0.179  0.606  1.460  0.936 0.545  0.259 0.290
x6  0.060 -0.137 -0.310  1.173  0.936  3.103 1.130  0.181 0.659
x7  0.571  0.255  0.408  0.814  0.545  1.130 2.839  0.224 1.533
x8  0.243  0.096 -0.053  0.116  0.259  0.181 0.224  1.232 0.298
x9  0.780  0.237  0.626  0.537  0.290  0.659 1.533  0.298 2.213
> round(colMeans(myData), 3)
    x1     x2     x3     x4     x5     x6     x7     x8     x9 
-0.276 -0.186 -0.122 -0.168 -0.138 -0.224 -0.049  0.043 -0.095 
> 
> # fit model
> myModel <- ' f1 =~ x1 + x2 + x3
+              f2 =~ x4 + x5 + x6
+              f3 =~ x7 + x8 + x9
+              f3 ~ f1 + f2 '
> fit <- sem(myModel, data=myData)
> summary(fit)
lavaan 0.6.17 ended normally after 36 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        21

  Number of observations                           100

Model Test User Model:
                                                      
  Test statistic                                28.389
  Degrees of freedom                                24
  P-value (Chi-square)                           0.244

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(>|z|)
  f1 =~                                               
    x1                1.000                           
    x2                0.820    0.131    6.265    0.000
    x3                1.123    0.172    6.531    0.000
  f2 =~                                               
    x4                1.000                           
    x5                0.772    0.170    4.536    0.000
    x6                1.498    0.291    5.147    0.000
  f3 =~                                               
    x7                1.000                           
    x8                0.156    0.091    1.710    0.087
    x9                0.808    0.152    5.323    0.000

Regressions:
                   Estimate  Std.Err  z-value  P(>|z|)
  f3 ~                                                
    f1                0.475    0.150    3.167    0.002
    f2                0.919    0.223    4.122    0.000

Covariances:
                   Estimate  Std.Err  z-value  P(>|z|)
  f1 ~~                                               
    f2               -0.065    0.126   -0.515    0.607

Variances:
                   Estimate  Std.Err  z-value  P(>|z|)
   .x1                0.948    0.198    4.778    0.000
   .x2                0.785    0.149    5.262    0.000
   .x3                0.756    0.211    3.585    0.000
   .x4                0.804    0.173    4.655    0.000
   .x5                0.979    0.164    5.962    0.000
   .x6                1.316    0.340    3.868    0.000
   .x7                0.936    0.331    2.827    0.005
   .x8                1.174    0.168    6.999    0.000
   .x9                0.967    0.241    4.011    0.000
    f1                1.211    0.311    3.896    0.000
    f2                0.783    0.233    3.360    0.001
   .f3                0.997    0.342    2.914    0.004

> 
> 
> 
> cleanEx()
> nameEx("standardizedSolution")
> ### * standardizedSolution
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: standardizedSolution
> ### Title: Standardized Solution
> ### Aliases: standardizedSolution standardizedsolution
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> standardizedSolution(fit)
       lhs op     rhs est.std    se      z pvalue ci.lower ci.upper
1   visual =~      x1   0.772 0.055 14.041      0    0.664    0.880
2   visual =~      x2   0.424 0.060  7.105      0    0.307    0.540
3   visual =~      x3   0.581 0.055 10.539      0    0.473    0.689
4  textual =~      x4   0.852 0.023 37.776      0    0.807    0.896
5  textual =~      x5   0.855 0.022 38.273      0    0.811    0.899
6  textual =~      x6   0.838 0.023 35.881      0    0.792    0.884
7    speed =~      x7   0.570 0.053 10.714      0    0.465    0.674
8    speed =~      x8   0.723 0.051 14.309      0    0.624    0.822
9    speed =~      x9   0.665 0.051 13.015      0    0.565    0.765
10      x1 ~~      x1   0.404 0.085  4.763      0    0.238    0.571
11      x2 ~~      x2   0.821 0.051 16.246      0    0.722    0.920
12      x3 ~~      x3   0.662 0.064 10.334      0    0.537    0.788
13      x4 ~~      x4   0.275 0.038  7.157      0    0.200    0.350
14      x5 ~~      x5   0.269 0.038  7.037      0    0.194    0.344
15      x6 ~~      x6   0.298 0.039  7.606      0    0.221    0.374
16      x7 ~~      x7   0.676 0.061 11.160      0    0.557    0.794
17      x8 ~~      x8   0.477 0.073  6.531      0    0.334    0.620
18      x9 ~~      x9   0.558 0.068  8.208      0    0.425    0.691
19  visual ~~  visual   1.000 0.000     NA     NA    1.000    1.000
20 textual ~~ textual   1.000 0.000     NA     NA    1.000    1.000
21   speed ~~   speed   1.000 0.000     NA     NA    1.000    1.000
22  visual ~~ textual   0.459 0.064  7.189      0    0.334    0.584
23  visual ~~   speed   0.471 0.073  6.461      0    0.328    0.613
24 textual ~~   speed   0.283 0.069  4.117      0    0.148    0.418
> 
> 
> 
> cleanEx()
> nameEx("summary.efaList")
> ### * summary.efaList
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.efaList
> ### Title: Summarizing EFA Fits
> ### Aliases: summary.efaList efaList print.efaList.summary
> 
> ### ** Examples
> 
> ## The famous Holzinger and Swineford (1939) example
> fit <- efa(data = HolzingerSwineford1939, 
+            ov.names = paste("x", 1:9, sep = ""),
+            nfactors = 1:3,
+            rotation = "geomin",
+            rotation.args = list(geomin.epsilon = 0.01, rstarts = 1))
> summary(fit, nd = 3L, cutoff = 0.2, dot.cutoff = 0.05,
+         lambda.structure = TRUE, pvalue = TRUE)
This is lavaan 0.6.17 -- running exploratory factor analysis

  Estimator                                         ML
  Rotation method                       GEOMIN OBLIQUE
  Geomin epsilon                                  0.01
  Rotation algorithm (rstarts)                 GPA (1)
  Standardized metric                             TRUE
  Row weights                                     None

  Number of observations                           301

Overview models:
                    aic      bic    sabic   chisq df pvalue   cfi rmsea
  nfactors = 1 7738.448 7805.176 7748.091 312.264 27  0.000 0.677 0.187
  nfactors = 2 7572.491 7668.876 7586.418 130.306 19  0.000 0.874 0.140
  nfactors = 3 7479.081 7601.416 7496.758  22.897 12  0.029 0.988 0.055

Eigenvalues correlation matrix:

    ev1     ev2     ev3     ev4     ev5     ev6     ev7     ev8     ev9 
  3.216   1.639   1.365   0.699   0.584   0.500   0.473   0.286   0.238 

Number of factors:  1 

Standardized loadings: (* = significant at 1% level)

       f1       unique.var   communalities
x1  0.438*           0.808           0.192
x2  0.220*           0.951           0.049
x3  0.223*           0.950           0.050
x4  0.848*           0.281           0.719
x5  0.841*           0.293           0.707
x6  0.838*           0.298           0.702
x7      .*           0.967           0.033
x8  0.201*           0.960           0.040
x9  0.307*           0.906           0.094

                           f1
Sum of squared loadings 2.586
Proportion of total     1.000
Proportion var          0.287
Cumulative var          0.287

Standardized structure (= LAMBDA %*% PSI):

      f1
x1 0.438
x2 0.220
x3 0.223
x4 0.848
x5 0.841
x6 0.838
x7 0.180
x8 0.201
x9 0.307

P-values standardized loadings:

      f1
x1 0.000
x2 0.000
x3 0.000
x4 0.000
x5 0.000
x6 0.000
x7 0.002
x8 0.001
x9 0.000

P-values unique variances:

x1 x2 x3 x4 x5 x6 x7 x8 x9 
 0  0  0  0  0  0  0  0  0 

P-values factor correlations:

       f1
f1      .

Number of factors:  2 

Standardized loadings: (* = significant at 1% level)

       f1      f2       unique.var   communalities
x1  0.261*  0.430*           0.673           0.327
x2      .   0.251*           0.906           0.094
x3          0.455*           0.783           0.217
x4  0.850*                   0.274           0.726
x5  0.867*                   0.264           0.736
x6  0.824*                   0.302           0.698
x7          0.447*           0.802           0.198
x8      .   0.626*           0.630           0.370
x9          0.732*           0.458           0.542

                              f1    f2 total
Sum of sq (obliq) loadings 2.281 1.628 3.909
Proportion of total        0.584 0.416 1.000
Proportion var             0.253 0.181 0.434
Cumulative var             0.253 0.434 0.434

Factor correlations: (* = significant at 1% level)

       f1      f2 
f1  1.000         
f2  0.331*  1.000 

Standardized structure (= LAMBDA %*% PSI):

      f1    f2
x1 0.403 0.516
x2 0.196 0.288
x3 0.180 0.465
x4 0.852 0.287
x5 0.857 0.258
x6 0.835 0.306
x7 0.140 0.445
x8 0.148 0.606
x9 0.254 0.736

P-values standardized loadings:

      f1    f2
x1 0.000 0.000
x2 0.079 0.000
x3 0.611 0.000
x4 0.000 0.840
x5 0.000 0.354
x6 0.000 0.340
x7 0.890 0.000
x8 0.218 0.000
x9 0.722 0.000

P-values unique variances:

x1 x2 x3 x4 x5 x6 x7 x8 x9 
 0  0  0  0  0  0  0  0  0 

P-values factor correlations:

       f1     f2
f1      .       
f2      0      .

Number of factors:  3 

Standardized loadings: (* = significant at 1% level)

       f1      f2      f3       unique.var   communalities
x1  0.604*      .*                   0.513           0.487
x2  0.507*              .            0.749           0.251
x3  0.691*      .                    0.543           0.457
x4          0.839*                   0.279           0.721
x5      .   0.887*                   0.243           0.757
x6      .   0.806*                   0.305           0.695
x7      .           0.726*           0.502           0.498
x8      .           0.703*           0.469           0.531
x9  0.368*          0.463*           0.543           0.457

                              f2    f1    f3 total
Sum of sq (obliq) loadings 2.226 1.345 1.284 4.855
Proportion of total        0.458 0.277 0.264 1.000
Proportion var             0.247 0.149 0.143 0.539
Cumulative var             0.247 0.397 0.539 0.539

Factor correlations: (* = significant at 1% level)

       f1      f2      f3 
f1  1.000                 
f2  0.327*  1.000         
f3  0.278*  0.230*  1.000 

Standardized structure (= LAMBDA %*% PSI):

      f1    f2    f3
x1 0.674 0.392 0.240
x2 0.488 0.182 0.032
x3 0.673 0.158 0.195
x4 0.301 0.849 0.207
x5 0.228 0.868 0.196
x6 0.341 0.830 0.198
x7 0.062 0.149 0.692
x8 0.286 0.151 0.722
x9 0.505 0.252 0.571

P-values standardized loadings:

      f1    f2    f3
x1 0.000 0.004 0.477
x2 0.000 0.391 0.084
x3 0.000 0.161 0.636
x4 0.370 0.000 0.826
x5 0.067 0.000 0.731
x6 0.071 0.000 0.783
x7 0.135 0.311 0.000
x8 0.345 0.324 0.000
x9 0.001 0.446 0.000

P-values unique variances:

x1 x2 x3 x4 x5 x6 x7 x8 x9 
 0  0  0  0  0  0  0  0  0 

P-values factor correlations:

       f1     f2     f3
f1      .              
f2  0.000      .       
f3  0.008  0.004      .

> 
> 
> 
> cleanEx()
> nameEx("varTable")
> ### * varTable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varTable
> ### Title: Variable Table
> ### Aliases: varTable vartable variableTable variabletable
> 
> ### ** Examples
> 
> HS.model <- ' visual  =~ x1 + x2 + x3
+               textual =~ x4 + x5 + x6
+               speed   =~ x7 + x8 + x9 '
> 
> fit <- cfa(HS.model, data=HolzingerSwineford1939)
> varTable(fit)
  name idx nobs    type exo user  mean   var nlev lnam
1   x1   7  301 numeric   0    0 4.936 1.363    0     
2   x2   8  301 numeric   0    0 6.088 1.386    0     
3   x3   9  301 numeric   0    0 2.250 1.279    0     
4   x4  10  301 numeric   0    0 3.061 1.355    0     
5   x5  11  301 numeric   0    0 4.341 1.665    0     
6   x6  12  301 numeric   0    0 2.186 1.200    0     
7   x7  13  301 numeric   0    0 4.186 1.187    0     
8   x8  14  301 numeric   0    0 5.527 1.025    0     
9   x9  15  301 numeric   0    0 5.374 1.018    0     
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  12.046 19.007 9.022 0.214 0.089 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
