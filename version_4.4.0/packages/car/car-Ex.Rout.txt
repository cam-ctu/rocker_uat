
R version 4.4.0 (2024-04-24) -- "Puppy Cup"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "car"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('car')
Loading required package: carData
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Anova")
> ### * Anova
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Anova
> ### Title: Anova Tables for Various Statistical Models
> ### Aliases: Anova Anova.lm Anova.lme Anova.aov Anova.glm Anova.multinom
> ###   Anova.polr Anova.mer Anova.merMod Anova.mlm Anova.manova Manova
> ###   Manova.mlm print.Anova.mlm summary.Anova.mlm print.summary.Anova.mlm
> ###   print.univaov as.data.frame.univaov Anova.coxph Anova.svyglm
> ###   Anova.svycoxph Anova.rlm Anova.coxme Anova.default
> ### Keywords: htest models regression
> 
> ### ** Examples
> 
> 
> ## Two-Way Anova
> 
> mod <- lm(conformity ~ fcategory*partner.status, data=Moore,
+   contrasts=list(fcategory=contr.sum, partner.status=contr.sum))
> Anova(mod)
Anova Table (Type II tests)

Response: conformity
                         Sum Sq Df F value   Pr(>F)   
fcategory                 11.61  2  0.2770 0.759564   
partner.status           212.21  1 10.1207 0.002874 **
fcategory:partner.status 175.49  2  4.1846 0.022572 * 
Residuals                817.76 39                    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> Anova(mod, type=3)  # note use of contr.sum in call to lm()
Anova Table (Type III tests)

Response: conformity
                         Sum Sq Df  F value    Pr(>F)    
(Intercept)              5752.8  1 274.3592 < 2.2e-16 ***
fcategory                  36.0  2   0.8589  0.431492    
partner.status            239.6  1  11.4250  0.001657 ** 
fcategory:partner.status  175.5  2   4.1846  0.022572 *  
Residuals                 817.8 39                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## One-Way MANOVA
> ## See ?Pottery for a description of the data set used in this example.
> 
> summary(Anova(lm(cbind(Al, Fe, Mg, Ca, Na) ~ Site, data=Pottery)))

Type II MANOVA Tests:

Sum of squares and products for error:
           Al          Fe          Mg          Ca         Na
Al 48.2881429  7.08007143  0.60801429  0.10647143 0.58895714
Fe  7.0800714 10.95084571  0.52705714 -0.15519429 0.06675857
Mg  0.6080143  0.52705714 15.42961143  0.43537714 0.02761571
Ca  0.1064714 -0.15519429  0.43537714  0.05148571 0.01007857
Na  0.5889571  0.06675857  0.02761571  0.01007857 0.19929286

------------------------------------------
 
Term: Site 

Sum of squares and products for the hypothesis:
            Al          Fe          Mg         Ca         Na
Al  175.610319 -149.295533 -130.809707 -5.8891637 -5.3722648
Fe -149.295533  134.221616  117.745035  4.8217866  5.3259491
Mg -130.809707  117.745035  103.350527  4.2091613  4.7105458
Ca   -5.889164    4.821787    4.209161  0.2047027  0.1547830
Na   -5.372265    5.325949    4.710546  0.1547830  0.2582456

Multivariate Tests: Site
                 Df test stat  approx F num Df   den Df     Pr(>F)    
Pillai            3   1.55394   4.29839     15 60.00000 2.4129e-05 ***
Wilks             3   0.01230  13.08854     15 50.09147 1.8404e-12 ***
Hotelling-Lawley  3  35.43875  39.37639     15 50.00000 < 2.22e-16 ***
Roy               3  34.16111 136.64446      5 20.00000 9.4435e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## MANOVA for a randomized block design (example courtesy of Michael Friendly:
> ##  See ?Soils for description of the data set)
> 
> soils.mod <- lm(cbind(pH,N,Dens,P,Ca,Mg,K,Na,Conduc) ~ Block + Contour*Depth,
+     data=Soils)
> Manova(soils.mod)

Type II MANOVA Tests: Pillai test statistic
              Df test stat approx F num Df den Df    Pr(>F)    
Block          3    1.6758   3.7965     27     81 1.777e-06 ***
Contour        2    1.3386   5.8468     18     52 2.730e-07 ***
Depth          3    1.7951   4.4697     27     81 8.777e-08 ***
Contour:Depth  6    1.2351   0.8640     54    180    0.7311    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> summary(Anova(soils.mod), univariate=TRUE, multivariate=FALSE,
+     p.adjust.method=TRUE)

 Type II Sums of Squares
              df       pH         N     Dens        P       Ca      Mg        K
Block          3  1.23247 0.0038257 0.111250   6591.2  14.6009 13.8776 0.323442
Contour        2  0.26066 0.0054845 0.047279  12224.5  25.3941  6.9637 0.473288
Depth          3 14.95897 0.1637141 1.589817 224703.5 378.3826 34.8509 1.130825
Contour:Depth  6  0.51587 0.0030885 0.088371  10260.6   8.4497  4.5386 0.020262
residuals     33  4.24730 0.0358700 0.433050  55810.8  71.4284 27.7522 0.409308
                    Na   Conduc
Block          15.4013   7.3116
Contour        13.3148  12.8727
Depth         446.6692 674.4830
Contour:Depth   3.0929   5.8732
residuals      29.9400  46.7515

 F-tests
                 pH     N  Dens     P    Ca   Mg     K     Na Conduc
Block          3.19  1.76  2.83  0.65  2.25 8.25  8.69   2.83   1.72
Contour        1.01  1.68  0.60  2.41  5.87 2.76  6.36   4.89   4.54
Depth         38.74 25.10 40.38 66.43 58.27 6.91 30.39 246.16 158.70
Contour:Depth  0.67  0.95  3.37  2.02  0.65 1.80  0.82   1.14   0.69

 p-values
              pH         N          Dens       P          Ca         Mg        
Block         0.03622480 0.18784329 0.05368061 0.69011331 0.10102309 0.00124231
Contour       0.37427327 0.18986452 0.72782049 0.08459444 0.00661088 0.05764573
Depth         6.4142e-11 5.6100e-11 3.7698e-11 2.6890e-12 2.8152e-13 8.1354e-05
Contour:Depth 0.67593329 0.42910155 0.04669490 0.12985165 0.68927638 0.16658584
              K          Na         Conduc    
Block         0.00021642 0.02469800 0.18188132
Contour       0.00016011 0.00637205 0.01807839
Depth         1.2952e-09 < 2.22e-16 < 2.22e-16
Contour:Depth 0.45056838 0.34871568 0.65840514

 p-values adjusted (by term) for simultaneous inference by holm method
              pH         N          Dens       P          Ca         Mg        
Block         0.2173488  0.5456440  0.2684030  0.6901133  0.4040923  0.0099385 
Contour       0.7485465  0.5695936  0.7485465  0.3383777  0.0509764  0.2882286 
Depth         2.2440e-10 2.2440e-10 1.8849e-10 1.6134e-11 1.9707e-12 8.1354e-05
Contour:Depth 1.0000000  1.0000000  0.4202541  1.0000000  1.0000000  1.0000000 
              K          Na         Conduc    
Block         0.0019478  0.1728860  0.5456440 
Contour       0.0014410  0.0509764  0.1084704 
Depth         2.5904e-09 < 2.22e-16 < 2.22e-16
Contour:Depth 1.0000000  1.0000000  1.0000000 
> 
> ## a multivariate linear model for repeated-measures data
> ## See ?OBrienKaiser for a description of the data set used in this example.
> 
> phase <- factor(rep(c("pretest", "posttest", "followup"), c(5, 5, 5)),
+     levels=c("pretest", "posttest", "followup"))
> hour <- ordered(rep(1:5, 3))
> idata <- data.frame(phase, hour)
> idata
      phase hour
1   pretest    1
2   pretest    2
3   pretest    3
4   pretest    4
5   pretest    5
6  posttest    1
7  posttest    2
8  posttest    3
9  posttest    4
10 posttest    5
11 followup    1
12 followup    2
13 followup    3
14 followup    4
15 followup    5
> 
> mod.ok <- lm(cbind(pre.1, pre.2, pre.3, pre.4, pre.5,
+                      post.1, post.2, post.3, post.4, post.5,
+                      fup.1, fup.2, fup.3, fup.4, fup.5) ~  treatment*gender,
+                 data=OBrienKaiser)
> (av.ok <- Anova(mod.ok, idata=idata, idesign=~phase*hour))

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                            Df test stat approx F num Df den Df    Pr(>F)    
(Intercept)                  1   0.96954   318.34      1     10 6.532e-09 ***
treatment                    2   0.48092     4.63      2     10 0.0376868 *  
gender                       1   0.20356     2.56      1     10 0.1409735    
treatment:gender             2   0.36350     2.86      2     10 0.1044692    
phase                        1   0.85052    25.61      2      9 0.0001930 ***
treatment:phase              2   0.68518     2.61      4     20 0.0667354 .  
gender:phase                 1   0.04314     0.20      2      9 0.8199968    
treatment:gender:phase       2   0.31060     0.92      4     20 0.4721498    
hour                         1   0.93468    25.04      4      7 0.0003043 ***
treatment:hour               2   0.30144     0.35      8     16 0.9295212    
gender:hour                  1   0.29274     0.72      4      7 0.6023742    
treatment:gender:hour        2   0.57022     0.80      8     16 0.6131884    
phase:hour                   1   0.54958     0.46      8      3 0.8324517    
treatment:phase:hour         2   0.66367     0.25     16      8 0.9914415    
gender:phase:hour            1   0.69505     0.85      8      3 0.6202076    
treatment:gender:phase:hour  2   0.79277     0.33     16      8 0.9723693    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> summary(av.ok, multivariate=FALSE)

Univariate Type II Repeated-Measures ANOVA Assuming Sphericity

                            Sum Sq num Df Error SS den Df  F value    Pr(>F)
(Intercept)                 7260.0      1  228.056     10 318.3435 6.532e-09
treatment                    211.3      2  228.056     10   4.6323  0.037687
gender                        58.3      1  228.056     10   2.5558  0.140974
treatment:gender             130.2      2  228.056     10   2.8555  0.104469
phase                        167.5      2   80.278     20  20.8651 1.274e-05
treatment:phase               78.7      4   80.278     20   4.8997  0.006426
gender:phase                   1.7      2   80.278     20   0.2078  0.814130
treatment:gender:phase        10.2      4   80.278     20   0.6366  0.642369
hour                         106.3      4   62.500     40  17.0067 3.191e-08
treatment:hour                 1.2      8   62.500     40   0.0929  0.999257
gender:hour                    2.6      4   62.500     40   0.4094  0.800772
treatment:gender:hour          7.8      8   62.500     40   0.6204  0.755484
phase:hour                    11.1      8   96.167     80   1.1525  0.338317
treatment:phase:hour           6.3     16   96.167     80   0.3256  0.992814
gender:phase:hour              6.6      8   96.167     80   0.6900  0.699124
treatment:gender:phase:hour   14.2     16   96.167     80   0.7359  0.749562
                               
(Intercept)                 ***
treatment                   *  
gender                         
treatment:gender               
phase                       ***
treatment:phase             ** 
gender:phase                   
treatment:gender:phase         
hour                        ***
treatment:hour                 
gender:hour                    
treatment:gender:hour          
phase:hour                     
treatment:phase:hour           
gender:phase:hour              
treatment:gender:phase:hour    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Mauchly Tests for Sphericity

                            Test statistic p-value
phase                              0.74927 0.27282
treatment:phase                    0.74927 0.27282
gender:phase                       0.74927 0.27282
treatment:gender:phase             0.74927 0.27282
hour                               0.06607 0.00760
treatment:hour                     0.06607 0.00760
gender:hour                        0.06607 0.00760
treatment:gender:hour              0.06607 0.00760
phase:hour                         0.00478 0.44939
treatment:phase:hour               0.00478 0.44939
gender:phase:hour                  0.00478 0.44939
treatment:gender:phase:hour        0.00478 0.44939


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

                             GG eps Pr(>F[GG])    
phase                       0.79953  7.323e-05 ***
treatment:phase             0.79953    0.01223 *  
gender:phase                0.79953    0.76616    
treatment:gender:phase      0.79953    0.61162    
hour                        0.46028  8.741e-05 ***
treatment:hour              0.46028    0.97879    
gender:hour                 0.46028    0.65346    
treatment:gender:hour       0.46028    0.64136    
phase:hour                  0.44950    0.34573    
treatment:phase:hour        0.44950    0.94019    
gender:phase:hour           0.44950    0.58903    
treatment:gender:phase:hour 0.44950    0.64634    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

                               HF eps   Pr(>F[HF])
phase                       0.9278594 2.387543e-05
treatment:phase             0.9278594 8.089765e-03
gender:phase                0.9278594 7.984495e-01
treatment:gender:phase      0.9278594 6.319975e-01
hour                        0.5592802 2.014357e-05
treatment:hour              0.5592802 9.887716e-01
gender:hour                 0.5592802 6.911521e-01
treatment:gender:hour       0.5592802 6.692976e-01
phase:hour                  0.7330608 3.440460e-01
treatment:phase:hour        0.7330608 9.804731e-01
gender:phase:hour           0.7330608 6.552382e-01
treatment:gender:phase:hour 0.7330608 7.080122e-01
> 
> ## A "doubly multivariate" design with two  distinct repeated-measures variables
> ## (example courtesy of Michael Friendly)
> ## See ?WeightLoss for a description of the dataset.
> 
> imatrix <- matrix(c(
+ 	1,0,-1, 1, 0, 0,
+ 	1,0, 0,-2, 0, 0,
+ 	1,0, 1, 1, 0, 0,
+ 	0,1, 0, 0,-1, 1,
+ 	0,1, 0, 0, 0,-2,
+ 	0,1, 0, 0, 1, 1), 6, 6, byrow=TRUE)
> colnames(imatrix) <- c("WL", "SE", "WL.L", "WL.Q", "SE.L", "SE.Q")
> rownames(imatrix) <- colnames(WeightLoss)[-1]
> (imatrix <- list(measure=imatrix[,1:2], month=imatrix[,3:6]))
$measure
    WL SE
wl1  1  0
wl2  1  0
wl3  1  0
se1  0  1
se2  0  1
se3  0  1

$month
    WL.L WL.Q SE.L SE.Q
wl1   -1    1    0    0
wl2    0   -2    0    0
wl3    1    1    0    0
se1    0    0   -1    1
se2    0    0    0   -2
se3    0    0    1    1

> contrasts(WeightLoss$group) <- matrix(c(-2,1,1, 0,-1,1), ncol=2)
> (wl.mod<-lm(cbind(wl1, wl2, wl3, se1, se2, se3)~group, data=WeightLoss))

Call:
lm(formula = cbind(wl1, wl2, wl3, se1, se2, se3) ~ group, data = WeightLoss)

Coefficients:
             wl1       wl2       wl3       se1       se2       se3     
(Intercept)   5.34444   4.45000   2.17778  14.92778  13.79444  16.28333
group1        0.42222   0.55833   0.04722   0.08889  -0.26944   0.60000
group2        0.43333   1.09167  -0.02500   0.18333  -0.22500   0.71667

> Anova(wl.mod, imatrix=imatrix, test="Roy")

Type II Repeated Measures MANOVA Tests: Roy test statistic
              Df test stat approx F num Df den Df    Pr(>F)    
measure        1    86.203  1293.04      2     30 < 2.2e-16 ***
group:measure  2     0.356     5.52      2     31  0.008906 ** 
month          1     9.407    65.85      4     28 7.807e-14 ***
group:month    2     1.772    12.84      4     29 3.909e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## mixed-effects models examples:
> 
> ## Not run: 
> ##D  # loads nlme package
> ##D 	library(nlme)
> ##D 	example(lme)
> ##D 	Anova(fm2)
> ## End(Not run)
> 
> ## Not run: 
> ##D  # loads lme4 package
> ##D 	library(lme4)
> ##D 	example(glmer)
> ##D 	Anova(gm1)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("Boot")
> ### * Boot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Boot
> ### Title: Bootstrapping for regression models
> ### Aliases: Boot Boot.default Boot.lm Boot.glm Boot.nls
> ### Keywords: regression
> 
> ### ** Examples
> 
> m1 <- lm(Fertility ~ ., swiss)
> betahat.boot <- Boot(m1, R=199) # 199 bootstrap samples--too small to be useful
Loading required namespace: boot
> summary(betahat.boot)  # default summary

Number of bootstrap replications R = 199 
                 original    bootBias    bootSE  bootMed
(Intercept)      66.91518 -1.02882995 10.944359 66.56737
Agriculture      -0.17211  0.00090838  0.060318 -0.16913
Examination      -0.25801  0.00229418  0.247031 -0.27161
Education        -0.87094 -0.01195505  0.201078 -0.89334
Catholic          0.10412 -0.00022590  0.030387  0.10450
Infant.Mortality  1.07705  0.05052016  0.475142  1.13569
> confint(betahat.boot)
Bootstrap bca confidence intervals

                       2.5 %      97.5 %
(Intercept)      44.25372647 92.63368913
Agriculture      -0.28415560 -0.05517064
Examination      -0.66167616  0.27418932
Education        -1.23127702 -0.46509770
Catholic          0.04094191  0.16205988
Infant.Mortality -0.28238152  1.88103788
> hist(betahat.boot)
> # Bootstrap for the estimated residual standard deviation:
> sigmahat.boot <- Boot(m1, R=199, f=sigmaHat, labels="sigmaHat")
> summary(sigmahat.boot)
           R original bootBias  bootSE bootMed
sigmaHat 199   7.1654 -0.53505 0.76112  6.6352
> confint(sigmahat.boot)
Warning in norm.inter(t, adj.alpha) :
  extreme order statistics used as endpoints
Bootstrap bca confidence intervals

            2.5 %   97.5 %
sigmaHat 6.374564 8.500058
> 
> 
> 
> cleanEx()
> nameEx("Boxplot")
> ### * Boxplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Boxplot
> ### Title: Boxplots With Point Identification
> ### Aliases: Boxplot Boxplot.default Boxplot.formula Boxplot.list
> ###   Boxplot.data.frame Boxplot.matrix
> ### Keywords: hplot
> 
> ### ** Examples
> 
> Boxplot(~income, data=Prestige, id=list(n=Inf)) # identify all outliers
[1] "general.managers"         "lawyers"                 
[3] "physicians"               "veterinarians"           
[5] "osteopaths.chiropractors"
> Boxplot(income ~ type, data=Prestige)
[1] "general.managers" "physicians"      
> Boxplot(income ~ type, data=Prestige, at=c(1, 3, 2))
[1] "general.managers" "physicians"      
> Boxplot(k5 + k618 ~ lfp*wc, data=Mroz)
[1] "746" "53" 
> with(Prestige, Boxplot(income, id=list(labels=rownames(Prestige))))
[1] "general.managers"         "lawyers"                 
[3] "physicians"               "veterinarians"           
[5] "osteopaths.chiropractors"
> with(Prestige, Boxplot(income, type, id=list(labels=rownames(Prestige))))
[1] "general.managers" "physicians"      
> Boxplot(scale(Prestige[, 1:4]))
[1] "general.managers"         "lawyers"                 
[3] "physicians"               "veterinarians"           
[5] "osteopaths.chiropractors"
> 
> 
> 
> cleanEx()
> nameEx("Contrasts")
> ### * Contrasts
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Contrasts
> ### Title: Functions to Construct Contrasts
> ### Aliases: Contrasts contr.Treatment contr.Sum contr.Helmert
> ### Keywords: models regression
> 
> ### ** Examples
> 
> # contr.Treatment vs. contr.treatment in the base package:
> 
> lm(prestige ~ (income + education)*type, data=Prestige, 
+     contrasts=list(type="contr.Treatment"))

Call:
lm(formula = prestige ~ (income + education) * type, data = Prestige, 
    contrasts = list(type = "contr.Treatment"))

Coefficients:
           (Intercept)                  income               education  
              2.275753                0.003522                1.713275  
          type[T.prof]              type[T.wc]     income:type[T.prof]  
             15.351896              -33.536652               -0.002903  
     income:type[T.wc]  education:type[T.prof]    education:type[T.wc]  
             -0.002072                1.387809                4.290875  

> 
> ##  Call:
> ##  lm(formula = prestige ~ (income + education) * type, data = Prestige,
> ##      contrasts = list(type = "contr.Treatment"))
> ##  
> ##  Coefficients:
> ##          (Intercept)                  income               education  
> ##              2.275753                0.003522                1.713275  
> ##          type[T.prof]              type[T.wc]     income:type[T.prof]  
> ##              15.351896              -33.536652               -0.002903  
> ##      income:type[T.wc]  education:type[T.prof]    education:type[T.wc]  
> ##              -0.002072                1.387809                4.290875  
> 
> lm(prestige ~ (income + education)*type, data=Prestige, 
+     contrasts=list(type="contr.treatment"))    

Call:
lm(formula = prestige ~ (income + education) * type, data = Prestige, 
    contrasts = list(type = "contr.treatment"))

Coefficients:
       (Intercept)              income           education            typeprof  
          2.275753            0.003522            1.713275           15.351896  
            typewc     income:typeprof       income:typewc  education:typeprof  
        -33.536652           -0.002903           -0.002072            1.387809  
  education:typewc  
          4.290875  

> 
> ##  Call:
> ##  lm(formula = prestige ~ (income + education) * type, data = Prestige,
> ##      contrasts = list(type = "contr.treatment"))
> ##  
> ##  Coefficients:
> ##      (Intercept)              income           education  
> ##          2.275753            0.003522            1.713275  
> ##          typeprof              typewc     income:typeprof  
> ##          15.351896          -33.536652           -0.002903  
> ##      income:typewc  education:typeprof    education:typewc  
> ##          -0.002072            1.387809            4.290875      
> 
> 
> 
> cleanEx()
> nameEx("Ellipses")
> ### * Ellipses
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Ellipses
> ### Title: Ellipses, Data Ellipses, and Confidence Ellipses
> ### Aliases: ellipse dataEllipse confidenceEllipse
> ###   confidenceEllipse.default confidenceEllipse.lm confidenceEllipse.glm
> ###   confidenceEllipse.mlm confidenceEllipses confidenceEllipses.default
> ###   confidenceEllipses.mlm
> ### Keywords: hplot aplot
> 
> ### ** Examples
> 
> dataEllipse(Duncan$income, Duncan$education, levels=0.1*1:9, 
+     ellipse.label=0.1*1:9, lty=2, fill=TRUE, fill.alpha=0.1)
>     
> confidenceEllipse(lm(prestige ~ income + education, data=Duncan), Scheffe=TRUE)
> 
> confidenceEllipse(lm(prestige ~ income + education, data=Duncan), vcov.=hccm)
> 
> confidenceEllipse(lm(prestige ~ income + education, data=Duncan), 
+ 	L=c("income + education", "income - education"))
> 	
> confidenceEllipses(lm(prestige ~ income + education + type, data=Duncan),
+   fill=TRUE)
> cov2cor(vcov(lm(prestige ~ income + education + type, 
+   data=Duncan))) # correlations among coefficients
            (Intercept)     income  education   typeprof     typewc
(Intercept)   1.0000000 -0.3484661 -0.6102747  0.5427211  0.3483203
income       -0.3484661  1.0000000 -0.2880828 -0.2016613 -0.1997770
education    -0.6102747 -0.2880828  1.0000000 -0.7761809 -0.5592417
typeprof      0.5427211 -0.2016613 -0.7761809  1.0000000  0.7072750
typewc        0.3483203 -0.1997770 -0.5592417  0.7072750  1.0000000
> 
> wts <- rep(1, nrow(Duncan))
> wts[c(6, 16)] <- 0 # delete Minister, Conductor
> with(Duncan, {
+ 	dataEllipse(income, prestige, levels=0.68)
+ 	dataEllipse(income, prestige, levels=0.68, robust=TRUE, 
+ 	    plot.points=FALSE, col="green3")
+ 	dataEllipse(income, prestige, weights=wts, levels=0.68, 
+ 	    plot.points=FALSE, col="brown")
+ 	dataEllipse(income, prestige, weights=wts, robust=TRUE, levels=0.68, 
+ 		plot.points=FALSE, col="blue")
+ 	})
>     
> with(Prestige, dataEllipse(income, education, type, 
+     id=list(n=2, labels=rownames(Prestige)), pch=15:17,
+     xlim=c(0, 25000), center.pch="+",
+     group.labels=c("Blue Collar", "Professional", "White Collar"),
+     ylim=c(5, 20), level=.95, fill=TRUE, fill.alpha=0.1))
> 
> 
> 
> cleanEx()
> nameEx("Export")
> ### * Export
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Export
> ### Title: Export a data frame to disk in one of many formats
> ### Aliases: Export
> ### Keywords: utilities connections
> 
> ### ** Examples
> 
> if(require("rio")) {
+ 
+ Export(Duncan, "Duncan.csv", keep.row.names="occupation")
+ Duncan2 <- Import("Duncan.csv") # Automatically restores row.names
+ identical(Duncan, Duncan2)
+ # cleanup
+ unlink("Duncan.csv")
+ 
+ }
Loading required package: rio
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘rio’
> 
> 
> 
> cleanEx()
> nameEx("Import")
> ### * Import
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Import
> ### Title: Import data from many file formats
> ### Aliases: Import
> ### Keywords: utilities connections
> 
> ### ** Examples
> 
> if(require("rio")) {
+ 
+ head(Duncan, 3) # first three rows
+ Export(Duncan, "Duncan.csv", keep.row.names="occupation")
+ Duncan2 <- Import("Duncan.csv") # Automatically restores row.names and factors
+ brief(Duncan2) 
+ identical(Duncan, Duncan2) # FALSE because type is of a different class
+ Duncan3 <- Import("Duncan.csv", stringsAsFactors=TRUE) 
+ brief(Duncan3) 
+ identical(Duncan, Duncan3) # TRUE type is of same class
+ # cleanup
+ unlink("Duncan.csv")
+ 
+ }
Loading required package: rio
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘rio’
> 
> 
> 
> cleanEx()
> nameEx("Predict")
> ### * Predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict
> ### Title: Model Predictions
> ### Aliases: Predict Predict.lm
> ### Keywords: models
> 
> ### ** Examples
> 
> mod <- lm(interlocks ~ log(assets), data=Ornstein)
> newd <- data.frame(assets=exp(4:12))
> (p1 <- predict(mod, newd, interval="prediction"))
         fit        lwr      upr
1 -11.546844 -35.632612 12.53892
2  -4.213580 -28.167353 19.74019
3   3.119684 -20.746798 26.98617
4  10.452948 -13.371440 34.27734
5  17.786212  -6.041518 41.61394
6  25.119476   1.242988 48.99596
7  32.452740   8.482354 56.42313
8  39.786004  15.677109 63.89490
9  47.119268  22.828014 71.41052
> p2 <- Predict(mod, newd, interval="prediction", vcov.=vcov)
> all.equal(p1, p2) # the same
[1] TRUE
> 
> (predict(mod, newd, se=TRUE))
$fit
         1          2          3          4          5          6          7 
-11.546844  -4.213580   3.119684  10.452948  17.786212  25.119476  32.452740 
         8          9 
 39.786004  47.119268 

$se.fit
        1         2         3         4         5         6         7         8 
1.9662292 1.4938511 1.0750019 0.7988559 0.8241457 1.1308251 1.5610312 2.0379832 
        9 
2.5354361 

$df
[1] 246

$residual.scale
[1] 12.06931

> (p3 <- Predict(mod, newd, se=TRUE, vcov.=hccm)) # larger SEs
$fit
         1          2          3          4          5          6          7 
-11.546844  -4.213580   3.119684  10.452948  17.786212  25.119476  32.452740 
         8          9 
 39.786004  47.119268 

$se.fit
        1         2         3         4         5         6         7         8 
2.8225421 1.9026552 1.0414745 0.6003835 1.2031087 2.0846342 3.0091773 3.9466208 
        9 
4.8895503 

$df
[1] 246

$residual.scale
[1] 12.06931

> p4 <- Predict(mod, newd, se=TRUE, vcov.=hccm(mod, type="hc3"))
> all.equal(p3, p4) # the same
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("S")
> ### * S
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: S
> ### Title: Modified Functions for Summarizing Linear, Generalized Linear,
> ###   and Some Other Models
> ### Aliases: S Confint S.lm S.glm S.default S.multinom S.polr S.lme
> ###   S.lmerMod S.glmerMod S.data.frame print.S.lm print.S.glm
> ###   print.S.multinom print.S.polr print.S.lme print.S.lmerMod
> ###   print.S.glmerMod Confint.lm Confint.glm Confint.multinom Confint.polr
> ###   Confint.lme Confint.lmerMod Confint.glmerMod Confint.default
> ### Keywords: models regression
> 
> ### ** Examples
> 
> mod.prestige <- lm(prestige ~ education + income + type, Prestige)
> S(mod.prestige, vcov.=hccm)
Call: lm(formula = prestige ~ education + income + type, data = Prestige)
Standard errors computed by hccm 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.6229292  5.2381025  -0.119 0.905593    
education    3.6731661  0.6982758   5.260 9.16e-07 ***
income       0.0010132  0.0002672   3.793 0.000265 ***
typeprof     6.0389707  3.7951209   1.591 0.114948    
typewc      -2.7372307  2.4384681  -1.123 0.264531    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard deviation: 7.095 on 93 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared: 0.8349
F-statistic: 120.2 on 4 and 93 DF,  p-value: < 2.2e-16 
   AIC    BIC 
669.02 684.52 

> S(mod.prestige, brief=TRUE)
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.6229292  5.2275255  -0.119    0.905    
education    3.6731661  0.6405016   5.735 1.21e-07 ***
income       0.0010132  0.0002209   4.586 1.40e-05 ***
typeprof     6.0389707  3.8668551   1.562    0.122    
typewc      -2.7372307  2.5139324  -1.089    0.279    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard deviation: 7.095 on 93 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared: 0.8349
F-statistic: 117.5 on 4 and 93 DF,  p-value: < 2.2e-16 
   AIC    BIC 
669.02 684.52 

> Confint(mod.prestige, vcov.=hccm)
Standard errors computed by hccm 
                Estimate         2.5 %       97.5 %
(Intercept) -0.622929165 -1.102476e+01  9.778904256
education    3.673166052  2.286529e+00  5.059803411
income       0.001013193  4.826801e-04  0.001543706
typeprof     6.038970651 -1.497387e+00 13.575328718
typewc      -2.737230718 -7.579545e+00  2.105083723
> 
> # A logit model
> mod.mroz <- glm(lfp ~ ., data=Mroz, family=binomial)
> S(mod.mroz)
Call: glm(formula = lfp ~ ., family = binomial, data = Mroz)

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.182140   0.644375   4.938 7.88e-07 ***
k5          -1.462913   0.197001  -7.426 1.12e-13 ***
k618        -0.064571   0.068001  -0.950 0.342337    
age         -0.062871   0.012783  -4.918 8.73e-07 ***
wcyes        0.807274   0.229980   3.510 0.000448 ***
hcyes        0.111734   0.206040   0.542 0.587618    
lwg          0.604693   0.150818   4.009 6.09e-05 ***
inc         -0.034446   0.008208  -4.196 2.71e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1029.75  on 752  degrees of freedom
Residual deviance:  905.27  on 745  degrees of freedom

 logLik      df     AIC     BIC 
-452.63       8  921.27  958.26 

Number of Fisher Scoring iterations: 4

Exponentiated Coefficients and Confidence Bounds
              Estimate     2.5 %     97.5 %
(Intercept) 24.0982799 6.9377228 87.0347916
k5           0.2315607 0.1555331  0.3370675
k618         0.9374698 0.8200446  1.0710837
age          0.9390650 0.9154832  0.9625829
wcyes        2.2417880 1.4347543  3.5387571
hcyes        1.1182149 0.7467654  1.6766380
lwg          1.8306903 1.3689201  2.4768235
inc          0.9661401 0.9502809  0.9814042

> 
> # use for data frames vs. summary()
> Duncan.1 <-Duncan
> Duncan.1$type <- as.character(Duncan$type)
> summary(Duncan.1)
     type               income        education         prestige    
 Length:45          Min.   : 7.00   Min.   :  7.00   Min.   : 3.00  
 Class :character   1st Qu.:21.00   1st Qu.: 26.00   1st Qu.:16.00  
 Mode  :character   Median :42.00   Median : 45.00   Median :41.00  
                    Mean   :41.87   Mean   : 52.56   Mean   :47.69  
                    3rd Qu.:64.00   3rd Qu.: 84.00   3rd Qu.:81.00  
                    Max.   :81.00   Max.   :100.00   Max.   :97.00  
> S(Duncan.1)
   type        income        education         prestige    
 bc  :21   Min.   : 7.00   Min.   :  7.00   Min.   : 3.00  
 prof:18   1st Qu.:21.00   1st Qu.: 26.00   1st Qu.:16.00  
 wc  : 6   Median :42.00   Median : 45.00   Median :41.00  
           Mean   :41.87   Mean   : 52.56   Mean   :47.69  
           3rd Qu.:64.00   3rd Qu.: 84.00   3rd Qu.:81.00  
           Max.   :81.00   Max.   :100.00   Max.   :97.00  
> 
> ## Not run: 
> ##D  # generates an error, which can then be corrected to run example
> ##D # Using the bootstrap for standard errors
> ##D b1 <- Boot(mod.prestige)
> ##D S(mod.prestige, vcov.= vcov(b1))
> ##D Confint(b1) # run with the boot object to get corrected confidence intervals
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ScatterplotSmoothers")
> ### * ScatterplotSmoothers
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ScatterplotSmoothers
> ### Title: Smoothers to Draw Lines on Scatterplots
> ### Aliases: ScatterplotSmoothers gamLine quantregLine loessLine
> ### Keywords: hplot
> 
> ### ** Examples
> 
> scatterplot(prestige ~ income, data=Prestige)
> scatterplot(prestige ~ income, data=Prestige, smooth=list(smoother=gamLine))
> scatterplot(prestige ~ income, data=Prestige,
+     smooth=list(smoother=quantregLine))
> 
> scatterplot(prestige ~ income | type, data=Prestige)
> scatterplot(prestige ~ income | type, data=Prestige,
+     smooth=list(smoother=gamLine))
> scatterplot(prestige ~ income | type, data=Prestige,
+     smooth=list(smoother=quantregLine))
> scatterplot(prestige ~ income | type, data=Prestige, smooth=FALSE)
> 
> scatterplot(prestige ~ income | type, data=Prestige, 
+     smooth=list(spread=TRUE))
> scatterplot(prestige ~ income | type, data=Prestige,
+     smooth=list(smoother=gamLine, spread=TRUE))
> scatterplot(prestige ~ income | type, data=Prestige,
+     smooth=list(smoother=quantregLine, spread=TRUE))
> 
> scatterplot(weight ~ repwt | sex, data=Davis,
+     smooth=list(smoother=loessLine, spread=TRUE, style="lines"))
> scatterplot(weight ~ repwt | sex, data=Davis,
+     smooth=list(smoother=gamLine, spread=TRUE, style="lines")) # messes up
> scatterplot(weight ~ repwt | sex, data=Davis,
+     smooth=list(smoother=quantregLine, spread=TRUE, style="lines")) #  robust
> 
> set.seed(12345)
> w <- 1 + rpois(100, 5)
> x <- rnorm(100)
> p <- 1/(1 + exp(-(x + 0.5*x^2)))
> y <- rbinom(100, w, p)
> scatterplot(y/w ~ x, smooth=list(smoother=gamLine, family="binomial",
+     weights=w))
> scatterplot(y/w ~ x, smooth=list(smoother=gamLine, family=binomial,
+     link="probit", weights=w))
> scatterplot(y/w ~ x, smooth=list(smoother=loessLine), reg=FALSE)
> 
> y <- rbinom(100, 1, p)
> scatterplot(y ~ x, smooth=list(smoother=gamLine, family=binomial))
> 
> 
> 
> cleanEx()
> nameEx("Tapply")
> ### * Tapply
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tapply
> ### Title: Apply a Function to a Variable Within Factor Levels
> ### Aliases: Tapply
> ### Keywords: misc manip
> 
> ### ** Examples
> 
> Tapply(conformity ~ partner.status + fcategory, mean, data=Moore)
              fcategory
partner.status     high  low   medium
          high 11.85714 17.4 14.27273
          low  12.62500  8.9  7.25000
> Tapply(conformity ~ partner.status + fcategory, mean, data=Moore, 
+     trim=0.2)
              fcategory
partner.status     high       low medium
          high 11.40000 17.333333  15.00
          low  12.16667  8.666667   7.25
> 
> Moore[1, 2] <- NA
> Tapply(conformity ~ partner.status + fcategory, mean, data=Moore)
              fcategory
partner.status     high  low   medium
          high 11.85714 17.4 14.27273
          low  12.62500   NA  7.25000
> Tapply(conformity ~ partner.status + fcategory, mean, data=Moore, 
+   na.rm=TRUE)
              fcategory
partner.status     high  low   medium
          high 11.85714 17.4 14.27273
          low  12.62500  9.0  7.25000
> Tapply(conformity ~ partner.status + fcategory, mean, data=Moore, 
+   na.action=na.omit)  # equivalent
              fcategory
partner.status     high  low   medium
          high 11.85714 17.4 14.27273
          low  12.62500  9.0  7.25000
> remove("Moore")
> 
> 
> 
> cleanEx()
> nameEx("TransformationAxes")
> ### * TransformationAxes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: TransformationAxes
> ### Title: Axes for Transformed Variables
> ### Aliases: basicPowerAxis bcPowerAxis bcnPowerAxis yjPowerAxis
> ###   probabilityAxis
> ### Keywords: aplot
> 
> ### ** Examples
> 
> UN <- na.omit(UN)
> par(mar=c(5, 4, 4, 4) + 0.1) # leave space on right
> 
> with(UN, plot(log(ppgdp, 10), log(infantMortality, 10)))
> basicPowerAxis(0, base=10, side="above", 
+   at=c(50, 200, 500, 2000, 5000, 20000), grid=TRUE, 
+   axis.title="GDP per capita")
> basicPowerAxis(0, base=10, side="right",
+   at=c(5, 10, 20, 50, 100), grid=TRUE, 
+   axis.title="infant mortality rate per 1000")
> 
> with(UN, plot(bcPower(ppgdp, 0), bcPower(infantMortality, 0)))
> bcPowerAxis(0, side="above", 
+   grid=TRUE, axis.title="GDP per capita")
> bcPowerAxis(0, side="right",
+   grid=TRUE, axis.title="infant mortality rate per 1000")
> 
> with(UN, qqPlot(logit(infantMortality/1000)))
[1] 155  75
> probabilityAxis()
> 
> with(UN, qqPlot(qnorm(infantMortality/1000)))
[1]  1 33
> probabilityAxis(at=c(.005, .01, .02, .04, .08, .16), scale="probit")
> 
> qqPlot(bcnPower(Ornstein$interlocks, lambda=1/3, gamma=0.1))
[1] 2 3
> bcnPowerAxis(1/3, 0.1, at=c(o=0, 5, 10, 20, 40, 80))
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("avPlots")
> ### * avPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: avPlots
> ### Title: Added-Variable Plots
> ### Aliases: avPlots avPlots.default avp avPlot avPlot.lm avPlot.glm
> ###   avPlot3d avPlot3d.lm avPlot3d.glm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> avPlots(lm(prestige ~ income + education + type, data=Duncan))
> 
> avPlots(glm(partic != "not.work" ~ hincome + children, 
+   data=Womenlf, family=binomial), id=FALSE, pt.wts=TRUE)
>   
> m1 <- lm(partic ~ tfr + menwage + womwage + debt + parttime, Bfox)
> par(mfrow=c(1,3))
> # marginal plot, ignoring other predictors:
> with(Bfox, dataEllipse(womwage, partic, levels=0.5)) 
> abline(lm(partic ~ womwage, Bfox), col="red", lwd=2)
> # AV plot, adjusting for others:
> avPlots(m1, ~ womwage, ellipse=list(levels=0.5)) 
> # AV plot, adjusting and scaling as in marginal plot
> avPlots(m1, ~ womwage, marginal.scale=TRUE, ellipse=list(levels=0.5)) 
> 
> # 3D AV plot, requires the rgl package
> if (interactive() && require("rgl")){
+ avPlot3d(lm(prestige ~ income + education + type, data=Duncan), 
+   "income", "education")
+ }
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("bcPower")
> ### * bcPower
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bcPower
> ### Title: Box-Cox, Box-Cox with Negatives Allowed, Yeo-Johnson and Basic
> ###   Power Transformations
> ### Aliases: bcPower bcnPower bcnPowerInverse yjPower basicPower
> ### Keywords: regression
> 
> ### ** Examples
> 
> U <- c(NA, (-3:3))
> ## Not run: bcPower(U, 0)  # produces an error as U has negative values
> bcPower(U, 0, gamma=4)
          Z1^0
[1,]        NA
[2,] 0.0000000
[3,] 0.6931472
[4,] 1.0986123
[5,] 1.3862944
[6,] 1.6094379
[7,] 1.7917595
[8,] 1.9459101
> bcPower(U, .5, jacobian.adjusted=TRUE, gamma=4)
       Z1^0.5
[1,]       NA
[2,] 0.000000
[3,] 1.523048
[4,] 2.691724
[5,] 3.676964
[6,] 4.544977
[7,] 5.329721
[8,] 6.051368
> bcnPower(U, 0, gamma=2)
[1]         NA -1.1947632 -0.8813736 -0.4812118  0.0000000  0.4812118  0.8813736
[8]  1.1947632
> basicPower(U, lambda = 0, gamma=4)
       log(Z1)
[1,]        NA
[2,] 0.0000000
[3,] 0.6931472
[4,] 1.0986123
[5,] 1.3862944
[6,] 1.6094379
[7,] 1.7917595
[8,] 1.9459101
> yjPower(U, 0)
[1]         NA -7.5000000 -4.0000000 -1.5000000  0.0000000  0.6931472  1.0986123
[8]  1.3862944
> V <- matrix(1:10, ncol=2)
> bcPower(V, c(0, 2))
          Z1^0 Z2^2
[1,] 0.0000000 17.5
[2,] 0.6931472 24.0
[3,] 1.0986123 31.5
[4,] 1.3862944 40.0
[5,] 1.6094379 49.5
> basicPower(V, c(0,1))
       log(Z1) Z2^1
[1,] 0.0000000    6
[2,] 0.6931472    7
[3,] 1.0986123    8
[4,] 1.3862944    9
[5,] 1.6094379   10
> 
> 
> 
> cleanEx()
> nameEx("boxCox")
> ### * boxCox
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: boxCox
> ### Title: Graph the profile log-likelihood for Box-Cox transformations in
> ###   1D, or in 2D with the bcnPower family.
> ### Aliases: boxCox boxCox2d boxCox.lm boxCox.default boxCox.formula
> ###   boxCox.bcnPowerTransform
> ### Keywords: regression
> 
> ### ** Examples
> 
>   with(trees, boxCox(Volume ~ log(Height) + log(Girth), data = trees,
+          lambda = seq(-0.25, 0.25, length = 10)))
Warning in plot.window(...) : "data" is not a graphical parameter
Warning in plot.xy(xy, type, ...) : "data" is not a graphical parameter
Warning in axis(side = side, at = at, labels = labels, ...) :
  "data" is not a graphical parameter
Warning in axis(side = side, at = at, labels = labels, ...) :
  "data" is not a graphical parameter
Warning in box(...) : "data" is not a graphical parameter
Warning in title(...) : "data" is not a graphical parameter
> 
>   data("quine", package = "MASS")
>   with(quine, boxCox(Days ~ Eth*Sex*Age*Lrn, 
+          lambda = seq(-0.05, 0.45, len = 20), family="yjPower"))
> 
> 
> 
> cleanEx()
> nameEx("boxCoxVariable")
> ### * boxCoxVariable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: boxCoxVariable
> ### Title: Constructed Variable for Box-Cox Transformation
> ### Aliases: boxCoxVariable
> ### Keywords: manip regression
> 
> ### ** Examples
> 
> mod <- lm(interlocks + 1 ~ assets, data=Ornstein)
> mod.aux <- update(mod, . ~ . + boxCoxVariable(interlocks + 1))
> summary(mod.aux)

Call:
lm(formula = interlocks + 1 ~ assets + boxCoxVariable(interlocks + 
    1), data = Ornstein)

Residuals:
     Min       1Q   Median       3Q      Max 
-23.1895  -6.7012   0.5411   6.7728  12.0506 

Coefficients:
                                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)                     1.461e+01  5.426e-01  26.920   <2e-16 ***
assets                         -7.142e-05  5.119e-05  -1.395    0.164    
boxCoxVariable(interlocks + 1)  7.427e-01  4.136e-02  17.956   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.247 on 245 degrees of freedom
Multiple R-squared:  0.7986,	Adjusted R-squared:  0.797 
F-statistic: 485.7 on 2 and 245 DF,  p-value: < 2.2e-16

> # avPlots(mod.aux, "boxCoxVariable(interlocks + 1)")
> 
> 
> 
> cleanEx()
> nameEx("boxTidwell")
> ### * boxTidwell
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: boxTidwell
> ### Title: Box-Tidwell Transformations
> ### Aliases: boxTidwell boxTidwell.formula boxTidwell.default
> ###   print.boxTidwell
> ### Keywords: regression
> 
> ### ** Examples
> 
> boxTidwell(prestige ~ income + education, ~ type + poly(women, 2), data=Prestige)
          MLE of lambda Score Statistic (t) Pr(>|t|)    
income         -0.34763             -4.4824 2.19e-05 ***
education       1.25383              0.2170   0.8287    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

iterations =  8 

Score test for null hypothesis that all lambdas = 1:
F = 10.068, df = 2 and 89, Pr(>F) = 0.0001144

> 
> 
> 
> cleanEx()
> nameEx("brief")
> ### * brief
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: brief
> ### Title: Print Abbreviated Ouput
> ### Aliases: brief brief.data.frame brief.tbl brief.matrix brief.numeric
> ###   brief.integer brief.character brief.factor brief.function brief.list
> ###   brief.lm brief.glm brief.multinom brief.polr brief.default
> ### Keywords: manip
> 
> ### ** Examples
> 
> brief(rnorm(100))
100 element numeric vector
   [1] -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772
   [6] -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387

. . . (17 lines omitted)

  [96]  0.558486426 -1.276592208 -0.573265414 -1.224612615 -0.473400636 
> brief(Duncan)
45 x 4 data.frame (40 rows omitted)
           type income education prestige
            [f]    [i]       [i]      [i]
accountant prof     62        86       82
pilot      prof     72        76       83
architect  prof     75        92       90
. . .                                         
policeman  bc       34        47       41
waiter     bc        8        32       10
> brief(OBrienKaiser, elided=TRUE)
16 x 17 data.frame (11 rows and 7 columns omitted)
   treatment gender pre.1 pre.2 pre.3 pre.4 pre.5 post.1 . . . fup.4 fup.5
         [f]    [f]   [n]   [n]   [n]   [n]   [n]    [n]         [n]   [n]
1    control    M       1     2     4     2     1      3           4     4
2    control    M       4     4     5     3     4      2           4     1
3    control    M       5     6     5     7     7      4           7     6
. . .                                                                          
15   B          F       2     2     3     4     4      6           6     7
16   B          F       4     5     7     5     4      7           8     7
> brief(matrix(1:500, 10, 50))
10 x 50 matrix (38 columns omitted)
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] . . . [,49] [,50]
 [1,]    1   11   21   31   41   51   61   71   81    91         481   491
 [2,]    2   12   22   32   42   52   62   72   82    92         482   492
 [3,]    3   13   23   33   43   53   63   73   83    93         483   493
 [4,]    4   14   24   34   44   54   64   74   84    94         484   494
 [5,]    5   15   25   35   45   55   65   75   85    95         485   495
 [6,]    6   16   26   36   46   56   66   76   86    96         486   496
 [7,]    7   17   27   37   47   57   67   77   87    97         487   497
 [8,]    8   18   28   38   48   58   68   78   88    98         488   498
 [9,]    9   19   29   39   49   59   69   79   89    99         489   499
[10,]   10   20   30   40   50   60   70   80   90   100         490   500
> brief(lm)
lm <- function (formula, data, subset, weights, na.action, method = "qr", 
    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
    contrasts = NULL, offset, ...) 
{
    ret.x <- x

. . . (64 lines omitted)

        z$qr <- NULL
    z
}
> 
> mod.prestige <- lm(prestige ~ education + income + type, Prestige)
> brief(mod.prestige, pvalues=TRUE)
           (Intercept) education   income typeprof typewc
Estimate        -0.623  3.67e+00 0.001013    6.039 -2.737
Std. Error       5.228  6.41e-01 0.000221    3.867  2.514
Pr(>|t|)         0.905  1.21e-07 0.000014    0.122  0.279

 Residual SD = 7.09 on 93 df, R-squared = 0.835 
> brief(mod.prestige, ~ type)
           typeprof typewc
Estimate       6.04  -2.74
Std. Error     3.87   2.51
> mod.mroz <- glm(lfp ~ ., data=Mroz, family=binomial)
> brief(mod.mroz)
              (Intercept)     k5    k618     age wcyes hcyes   lwg      inc
Estimate            3.182 -1.463 -0.0646 -0.0629 0.807 0.112 0.605 -0.03445
Std. Error          0.644  0.197  0.0680  0.0128 0.230 0.206 0.151  0.00821
exp(Estimate)      24.098  0.232  0.9375  0.9391 2.242 1.118 1.831  0.96614

 Residual deviance = 905 on 745 df
> 
> 
> 
> cleanEx()
> nameEx("carHexsticker")
> ### * carHexsticker
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: carHexsticker
> ### Title: View the Official Hex Sticker for the car Package
> ### Aliases: carHexsticker
> ### Keywords: misc
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  # intended for interactive use
> ##D carHexsticker()
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("carPalette")
> ### * carPalette
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: carPalette
> ### Title: Set or Retrieve 'car' Package Color Palette
> ### Aliases: carPalette
> ### Keywords: color
> 
> ### ** Examples
> 
> # Standard color palette
> palette()
[1] "black"   "#DF536B" "#61D04F" "#2297E6" "#28E2E5" "#CD0BBC" "#F5C710"
[8] "gray62" 
> # car standard color palette
> carPalette()
[1] "black"   "blue"    "magenta" "cyan"    "orange"  "gray"    "green3" 
[8] "red"    
> # set colors to all black
> carPalette(rep("black", 8))
> # Use a custom color palette with 12 distinct colors
> carPalette(sample(colors(distinct=TRUE), 12))
> # restore default
> carPalette("default")
> 
> 
> 
> cleanEx()
> nameEx("carWeb")
> ### * carWeb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: carWeb
> ### Title: Access to the R Companion to Applied Regression Website
> ### Aliases: carWeb
> ### Keywords: interface
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D  # meant for interactive use
> ##D carWeb()
> ##D carWeb(setup=TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ceresPlots")
> ### * ceresPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ceresPlots
> ### Title: Ceres Plots
> ### Aliases: ceresPlots ceresPlots.default ceresPlot ceresPlot.lm
> ###   ceresPlot.glm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> ceresPlots(lm(prestige~income+education+type, data=Prestige), terms= ~ . - type)
> 
> 
> 
> cleanEx()
> nameEx("compareCoefs")
> ### * compareCoefs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: compareCoefs
> ### Title: Print estimated coefficients and their standard errors in a
> ###   table for several regression models.
> ### Aliases: compareCoefs
> ### Keywords: print
> 
> ### ** Examples
> 
> mod1 <- lm(prestige ~ income + education, data=Duncan)
> mod2 <- update(mod1, subset=-c(6,16))
> mod3 <- update(mod1, . ~ . + type)
> mod4 <- update(mod1, . ~ . + I(income + education)) # aliased coef.
> compareCoefs(mod1)
Calls:
lm(formula = prestige ~ income + education, data = Duncan)

            Model 1
(Intercept)   -6.06
SE             4.27
                   
income        0.599
SE            0.120
                   
education    0.5458
SE           0.0983
                   
> compareCoefs(mod1, mod2, mod4)
Calls:
1: lm(formula = prestige ~ income + education, data = Duncan)
2: lm(formula = prestige ~ income + education, data = Duncan, subset = -c(6,
   16))
3: lm(formula = prestige ~ income + education + I(income + education), data 
  = Duncan)

                      Model 1 Model 2 Model 3
(Intercept)             -6.06   -6.41   -6.06
SE                       4.27    3.65    4.27
                                             
income                  0.599   0.867   0.599
SE                      0.120   0.122   0.120
                                             
education              0.5458  0.3322  0.5458
SE                     0.0983  0.0987  0.0983
                                             
I(income + education)                 aliased
SE                                           
                                             
> compareCoefs(mod1, mod2, mod3, zvals=TRUE, pvals=TRUE)
Calls:
1: lm(formula = prestige ~ income + education, data = Duncan)
2: lm(formula = prestige ~ income + education, data = Duncan, subset = -c(6,
   16))
3: lm(formula = prestige ~ income + education + type, data = Duncan)

            Model 1 Model 2 Model 3
(Intercept)  -6.065  -6.409  -0.185
SE            4.272   3.653   3.714
z             -1.42   -1.75   -0.05
Pr(>|z|)    0.15571 0.07932 0.96026
                                   
income       0.5987  0.8674  0.5975
SE           0.1197  0.1220  0.0894
z              5.00    7.11    6.69
Pr(>|z|)    5.6e-07 1.1e-12 2.3e-11
                                   
education    0.5458  0.3322  0.3453
SE           0.0983  0.0987  0.1136
z              5.56    3.36    3.04
Pr(>|z|)    2.8e-08 0.00077 0.00237
                                   
typeprof                      16.66
SE                             6.99
z                              2.38
Pr(>|z|)                    0.01722
                                   
typewc                       -14.66
SE                             6.11
z                             -2.40
Pr(>|z|)                    0.01639
                                   
> compareCoefs(mod1, mod2, se=FALSE)
Calls:
1: lm(formula = prestige ~ income + education, data = Duncan)
2: lm(formula = prestige ~ income + education, data = Duncan, subset = -c(6,
   16))

            Model 1 Model 2
(Intercept)   -6.06   -6.41
income        0.599   0.867
education     0.546   0.332
> compareCoefs(mod1, mod1, vcov.=list(vcov, hccm))
Standard errors computed by list(vcov, hccm)Calls:
1: lm(formula = prestige ~ income + education, data = Duncan)
2: lm(formula = prestige ~ income + education, data = Duncan)

            Model 1 Model 2
(Intercept)   -6.06   -6.06
SE             4.27    3.15
                           
income        0.599   0.599
SE            0.120   0.183
                           
education    0.5458  0.5458
SE           0.0983  0.1495
                           
> 
> 
> 
> cleanEx()
> nameEx("crPlots")
> ### * crPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: crPlots
> ### Title: Component+Residual (Partial Residual) Plots
> ### Aliases: crPlots.default crPlots crp crPlot crPlot.lm crPlot3d
> ###   crPlot3d.lm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> crPlots(m<-lm(prestige ~ income + education, data=Prestige)) 
> 
> crPlots(m, terms=~ . - education) # get only one plot
> 
> crPlots(lm(prestige ~ log2(income) + education + poly(women,2), data=Prestige))
> 
> crPlots(glm(partic != "not.work" ~ hincome + children, 
+   data=Womenlf, family=binomial), smooth=list(span=0.75))
> 
> # 3D C+R plot, requires the rgl, effects, and mgcv packages
> if (interactive() && require(rgl) && require(effects) && require(mgcv)){
+ crPlot3d(lm(prestige ~ income*education + women, data=Prestige), 
+     "income", "education") 
+ }
> 
> 
> 
> cleanEx()
> nameEx("deltaMethod")
> ### * deltaMethod
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deltaMethod
> ### Title: Estimate and Standard Error of a Nonlinear Function of Estimated
> ###   Regression Coefficients
> ### Aliases: deltaMethod deltaMethod.default deltaMethod.lm deltaMethod.nls
> ###   deltaMethod.multinom deltaMethod.polr deltaMethod.survreg
> ###   deltaMethod.coxph deltaMethod.mer deltaMethod.merMod deltaMethod.lme
> ###   deltaMethod.lmList
> ### Keywords: models regression
> 
> ### ** Examples
> 
> m1 <- lm(time ~ t1 + t2, data = Transact) 
> deltaMethod(m1, "b1/b2", parameterNames= paste("b", 0:2, sep="")) 
      Estimate      SE   2.5 % 97.5 %
b1/b2  2.68465 0.31899 2.05945 3.3099
> deltaMethod(m1, "t1/t2", rhs=1) # use names of preds. rather than coefs.
      Estimate      SE   2.5 %  97.5 % Hypothesis z value  Pr(>|z|)    
t1/t2  2.68465 0.31899 2.05945 3.30985    1.00000  5.2813 1.283e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> deltaMethod(m1, "t1/t2", vcov=hccm) # use hccm function to est. vars.
      Estimate      SE   2.5 % 97.5 %
t1/t2  2.68465 0.55836 1.59030  3.779
> deltaMethod(m1, "1/(Intercept)")
              Estimate         SE      2.5 % 97.5 %
1/Intercept  0.0069267  0.0081825 -0.0091107  0.023
> # The next example invokes the default method by extracting the
> # vector of estimates and covariance matrix explicitly
> deltaMethod(coef(m1), "t1/t2", vcov.=vcov(m1))
      Estimate      SE   2.5 % 97.5 %
t1/t2  2.68465 0.31899 2.05945 3.3099
> 
> 
> 
> cleanEx()
> nameEx("densityPlot")
> ### * densityPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: densityPlot
> ### Title: Nonparametric Density Estimates
> ### Aliases: densityPlot densityPlot.default densityPlot.formula
> ###   adaptiveKernel depan dbiwt
> ### Keywords: hplot
> 
> ### ** Examples
> 
> densityPlot(~ income, show.bw=TRUE, method="kernel", data=Prestige)
> densityPlot(~ income, show.bw=TRUE, data=Prestige)
> densityPlot(~ income, from=0, normalize=TRUE, show.bw=TRUE, data=Prestige)
> 
> densityPlot(income ~ type, data=Prestige)
> densityPlot(~ income, show.bw=TRUE, method="kernel", data=Prestige)
> densityPlot(~ income, show.bw=TRUE, data=Prestige)
> densityPlot(~ income, from=0, normalize=TRUE, show.bw=TRUE, data=Prestige)
> 
> densityPlot(income ~ type, kernel=depan, data=Prestige)
> densityPlot(income ~ type, kernel=depan, legend=list(location="top"), data=Prestige)
> 
> plot(adaptiveKernel(UN$infantMortality, from=0, adjust=0.75), col="magenta")
> lines(density(na.omit(UN$infantMortality), from=0, adjust=0.75), col="blue")
> rug(UN$infantMortality, col="cyan")
> legend("topright", col=c("magenta", "blue"), lty=1,
+        legend=c("adaptive kernel", "kernel"), inset=0.02)
> 
> 
> plot(adaptiveKernel(UN$infantMortality, from=0, adjust=0.75), col="magenta")
> lines(density(na.omit(UN$infantMortality), from=0, adjust=0.75), col="blue")
> rug(UN$infantMortality, col="cyan")
> legend("topright", col=c("magenta", "blue"), lty=1,
+        legend=c("adaptive kernel", "kernel"), inset=0.02)
> 
> 
> 
> 
> cleanEx()
> nameEx("dfbetaPlots")
> ### * dfbetaPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dfbetaPlots
> ### Title: dfbeta and dfbetas Index Plots
> ### Aliases: dfbetaPlots dfbetasPlots dfbetaPlots.lm dfbetasPlots.lm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> dfbetaPlots(lm(prestige ~ income + education + type, data=Duncan))
> 
> dfbetasPlots(glm(partic != "not.work" ~ hincome + children, 
+   data=Womenlf, family=binomial))
> 
> 
> 
> cleanEx()
> nameEx("durbinWatsonTest")
> ### * durbinWatsonTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: durbinWatsonTest
> ### Title: Durbin-Watson Test for Autocorrelated Errors
> ### Aliases: durbinWatsonTest dwt durbinWatsonTest.lm
> ###   durbinWatsonTest.default print.durbinWatsonTest
> ### Keywords: regression ts
> 
> ### ** Examples
> 
> durbinWatsonTest(lm(fconvict ~ tfr + partic + degrees + mconvict, data=Hartnagel))
 lag Autocorrelation D-W Statistic p-value
   1        0.688345     0.6168636       0
 Alternative hypothesis: rho != 0
> 
> 
> 
> cleanEx()
> nameEx("hccm")
> ### * hccm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hccm
> ### Title: Heteroscedasticity-Corrected Covariance Matrices
> ### Aliases: hccm hccm.lm hccm.default
> ### Keywords: regression
> 
> ### ** Examples
> 
> mod <- lm(interlocks ~ assets + nation, data=Ornstein)
> print(vcov(mod), digits=4)
            (Intercept)     assets  nationOTH   nationUK   nationUS
(Intercept)   1.079e+00 -1.588e-05 -1.037e+00 -1.057e+00 -1.032e+00
assets       -1.588e-05  1.642e-09  1.155e-05  1.362e-05  1.109e-05
nationOTH    -1.037e+00  1.155e-05  7.019e+00  1.021e+00  1.003e+00
nationUK     -1.057e+00  1.362e-05  1.021e+00  7.405e+00  1.017e+00
nationUS     -1.032e+00  1.109e-05  1.003e+00  1.017e+00  2.128e+00
> ##             (Intercept)     assets  nationOTH   nationUK   nationUS
> ## (Intercept)   1.079e+00 -1.588e-05 -1.037e+00 -1.057e+00 -1.032e+00
> ## assets       -1.588e-05  1.642e-09  1.155e-05  1.362e-05  1.109e-05
> ## nationOTH    -1.037e+00  1.155e-05  7.019e+00  1.021e+00  1.003e+00
> ## nationUK     -1.057e+00  1.362e-05  1.021e+00  7.405e+00  1.017e+00
> ## nationUS     -1.032e+00  1.109e-05  1.003e+00  1.017e+00  2.128e+00
> 
> print(hccm(mod), digits=4)             
            (Intercept)     assets  nationOTH   nationUK   nationUS
(Intercept)   1.664e+00 -3.957e-05 -1.569e+00 -1.611e+00 -1.572e+00
assets       -3.957e-05  6.752e-09  2.275e-05  3.051e-05  2.231e-05
nationOTH    -1.569e+00  2.275e-05  8.209e+00  1.539e+00  1.520e+00
nationUK     -1.611e+00  3.051e-05  1.539e+00  4.476e+00  1.543e+00
nationUS     -1.572e+00  2.231e-05  1.520e+00  1.543e+00  1.946e+00
> ##             (Intercept)     assets  nationOTH   nationUK   nationUS
> ## (Intercept)   1.664e+00 -3.957e-05 -1.569e+00 -1.611e+00 -1.572e+00
> ## assets       -3.957e-05  6.752e-09  2.275e-05  3.051e-05  2.231e-05
> ## nationOTH    -1.569e+00  2.275e-05  8.209e+00  1.539e+00  1.520e+00
> ## nationUK     -1.611e+00  3.051e-05  1.539e+00  4.476e+00  1.543e+00
> ## nationUS     -1.572e+00  2.231e-05  1.520e+00  1.543e+00  1.946e+00
> 
> 
> 
> cleanEx()
> nameEx("hist.boot")
> ### * hist.boot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hist.boot
> ### Title: Methods Functions to Support 'boot' Objects
> ### Aliases: hist.boot summary.boot confint.boot vcov.boot Confint.boot
> ### Keywords: regression
> 
> ### ** Examples
> 
> m1 <- lm(Fertility ~ ., swiss)
> betahat.boot <- Boot(m1, R=99) # 99 bootstrap samples--too small to be useful
> summary(betahat.boot)  # default summary

Number of bootstrap replications R = 99 
                 original   bootBias    bootSE  bootMed
(Intercept)      66.91518 -0.9970650 10.741052 65.46346
Agriculture      -0.17211  0.0093437  0.068390 -0.15951
Examination      -0.25801  0.0112816  0.270882 -0.24834
Education        -0.87094  0.0208045  0.250042 -0.87156
Catholic          0.10412 -0.0010184  0.032095  0.10109
Infant.Mortality  1.07705  0.0074252  0.376525  1.07665
> confint(betahat.boot)
Warning in norm.inter(t, adj.alpha) :
  extreme order statistics used as endpoints
Bootstrap bca confidence intervals

                       2.5 %      97.5 %
(Intercept)      49.06729634 89.11115569
Agriculture      -0.33421161 -0.05305446
Examination      -0.85317867  0.24850573
Education        -1.32762803 -0.25747939
Catholic          0.04583415  0.17213601
Infant.Mortality  0.22978250  1.72526336
> hist(betahat.boot)
Warning in norm.inter(t, adj.alpha) :
  extreme order statistics used as endpoints
> 
> 
> 
> cleanEx()
> nameEx("infIndexPlot")
> ### * infIndexPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: infIndexPlot
> ### Title: Influence Index Plot
> ### Aliases: infIndexPlot influenceIndexPlot infIndexPlot.lm
> ###   infIndexPlot.lmerMod infIndexPlot.influence.merMod
> ###   infIndexPlot.influence.lme
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> influenceIndexPlot(lm(prestige ~ income + education + type, Duncan))
> 
> ## Not run: 
> ##D  # a little slow
> ##D   if (require(lme4)){
> ##D       print(fm1 <- lmer(Reaction ~ Days + (Days | Subject),
> ##D           sleepstudy)) # from ?lmer
> ##D       infIndexPlot(influence(fm1, "Subject"))
> ##D       infIndexPlot(influence(fm1))
> ##D       }
> ##D       
> ##D   if (require(lme4)){
> ##D       gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
> ##D           data = cbpp, family = binomial) # from ?glmer
> ##D       infIndexPlot(influence(gm1, "herd", maxfun=100))
> ##D       infIndexPlot(influence(gm1, maxfun=100))
> ##D       gm1.11 <- update(gm1, subset = herd != 11) # check deleting herd 11
> ##D       compareCoefs(gm1, gm1.11)
> ##D       }
> ##D     
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("influence-mixed-models")
> ### * influence-mixed-models
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: influence.mixed.models
> ### Title: Influence Diagnostics for Mixed-Effects Models
> ### Aliases: influence.mixed.models influence.lme
> ###   cooks.distance.influence.lme dfbeta.influence.lme
> ###   dfbetas.influence.lme
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> if (require(nlme)){
+     print(fm1 <- lme(distance ~ age, data = Orthodont))
+     infIndexPlot(influence(fm1, "Subject"))
+     infIndexPlot(influence(fm1))
+     }
Loading required package: nlme
Linear mixed-effects model fit by REML
  Data: Orthodont 
  Log-restricted-likelihood: -221.3183
  Fixed: distance ~ age 
(Intercept)         age 
 16.7611111   0.6601852 

Random effects:
 Formula: ~age | Subject
 Structure: General positive-definite
            StdDev    Corr  
(Intercept) 2.3270339 (Intr)
age         0.2264276 -0.609
Residual    1.3100399       

Number of Observations: 108
Number of Groups: 27 
> 
> 
> 
> cleanEx()

detaching ‘package:nlme’

> nameEx("influencePlot")
> ### * influencePlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: influencePlot
> ### Title: Regression Influence Plot
> ### Aliases: influencePlot influencePlot.lm influencePlot.lmerMod
> ###   influence.plot
> ### Keywords: regression
> 
> ### ** Examples
> 
> influencePlot(lm(prestige ~ income + education, data=Duncan))
               StudRes        Hat      CookD
minister     3.1345186 0.17305816 0.56637974
reporter    -2.3970224 0.05439356 0.09898456
conductor   -1.7040324 0.19454165 0.22364122
RR.engineer  0.8089221 0.26908963 0.08096807
> ## Not run: 
> ##D  # requires user interaction to identify points
> ##D influencePlot(lm(prestige ~ income + education, data=Duncan), 
> ##D     id=list(method="identify"))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("invResPlot")
> ### * invResPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: invResPlot
> ### Title: Inverse Response Plots to Transform the Response
> ### Aliases: invResPlot inverseResponsePlot inverseResponsePlot.lm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> m2 <- lm(rate ~ log(len) + log(adt) + slim + shld + log(sigs1), Highway1)
> invResPlot(m2)
      lambda      RSS
1  0.1350783 31.57739
2 -1.0000000 35.45785
3  0.0000000 31.63514
4  1.0000000 33.68958
> 
> 
> 
> cleanEx()
> nameEx("invTranPlot")
> ### * invTranPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: invTranPlot
> ### Title: Choose a Predictor Transformation Visually or Numerically
> ### Aliases: invTranPlot invTranPlot.default invTranPlot.formula
> ###   invTranEstimate
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> with(UN, invTranPlot(ppgdp, infantMortality))
      lambda       RSS
1 -0.3208097  54816.14
2 -1.0000000  83395.51
3  0.0000000  62851.31
4  1.0000000 120583.35
> with(UN, invTranEstimate(ppgdp, infantMortality))
$lambda
[1] -0.3208097

$lowerCI
[1] -0.4034811

$upperCI
[1] -0.2386709

> 
> 
> 
> cleanEx()
> nameEx("leveneTest")
> ### * leveneTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: leveneTest
> ### Title: Levene's Test
> ### Aliases: leveneTest leveneTest.formula leveneTest.lm leveneTest.default
> ### Keywords: htest
> 
> ### ** Examples
> 
> with(Moore, leveneTest(conformity, fcategory))
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  2   0.046 0.9551
      42               
> with(Moore, leveneTest(conformity, interaction(fcategory, partner.status)))
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  5  1.4694 0.2219
      39               
> leveneTest(conformity ~ fcategory*partner.status, data=Moore)
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  5  1.4694 0.2219
      39               
> leveneTest(lm(conformity ~ fcategory*partner.status, data=Moore))
Levene's Test for Homogeneity of Variance (center = median)
      Df F value Pr(>F)
group  5  1.4694 0.2219
      39               
> leveneTest(conformity ~ fcategory*partner.status, data=Moore, center=mean)
Levene's Test for Homogeneity of Variance (center = mean)
      Df F value Pr(>F)
group  5  1.7915 0.1373
      39               
> leveneTest(conformity ~ fcategory*partner.status, data=Moore, center=mean, trim=0.1)
Levene's Test for Homogeneity of Variance (center = mean: 0.1)
      Df F value Pr(>F)
group  5  1.7962 0.1363
      39               
> 
> 
> 
> cleanEx()
> nameEx("leveragePlots")
> ### * leveragePlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: leveragePlots
> ### Title: Regression Leverage Plots
> ### Aliases: leveragePlots leveragePlot leveragePlot.lm leveragePlot.glm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> leveragePlots(lm(prestige~(income+education)*type, data=Duncan))
> 
> 
> 
> cleanEx()
> nameEx("linearHypothesis")
> ### * linearHypothesis
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: linearHypothesis
> ### Title: Test Linear Hypothesis
> ### Aliases: linearHypothesis lht linearHypothesis.lm linearHypothesis.glm
> ###   linearHypothesis.mlm linearHypothesis.polr linearHypothesis.default
> ###   linearHypothesis.mer linearHypothesis.merMod linearHypothesis.lme
> ###   linearHypothesis.svyglm linearHypothesis.rlm linearHypothesis.survreg
> ###   linearHypothesis.lmList linearHypothesis.nlsList matchCoefs
> ###   matchCoefs.default matchCoefs.mer matchCoefs.merMod matchCoefs.lme
> ###   matchCoefs.mlm matchCoefs.lmList makeHypothesis printHypothesis
> ###   print.linearHypothesis.mlm
> ### Keywords: htest models regression
> 
> ### ** Examples
> 
> mod.davis <- lm(weight ~ repwt, data=Davis)
> 
> ## the following are equivalent:
> linearHypothesis(mod.davis, diag(2), c(0,1))
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

  Res.Df   RSS Df Sum of Sq      F Pr(>F)
1    183 13074                           
2    181 12828  2    245.97 1.7353 0.1793
> linearHypothesis(mod.davis, c("(Intercept) = 0", "repwt = 1"))
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

  Res.Df   RSS Df Sum of Sq      F Pr(>F)
1    183 13074                           
2    181 12828  2    245.97 1.7353 0.1793
> linearHypothesis(mod.davis, c("(Intercept)", "repwt"), c(0,1))
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

  Res.Df   RSS Df Sum of Sq      F Pr(>F)
1    183 13074                           
2    181 12828  2    245.97 1.7353 0.1793
> linearHypothesis(mod.davis, c("(Intercept)", "repwt = 1"))
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

  Res.Df   RSS Df Sum of Sq      F Pr(>F)
1    183 13074                           
2    181 12828  2    245.97 1.7353 0.1793
> 
> ## use asymptotic Chi-squared statistic
> linearHypothesis(mod.davis, c("(Intercept) = 0", "repwt = 1"), test = "Chisq")
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

  Res.Df   RSS Df Sum of Sq  Chisq Pr(>Chisq)
1    183 13074                               
2    181 12828  2    245.97 3.4706     0.1763
> 
> 
> ## the following are equivalent:
>   ## use HC3 standard errors via white.adjust option
> linearHypothesis(mod.davis, c("(Intercept) = 0", "repwt = 1"), 
+     white.adjust = TRUE)
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

Note: Coefficient covariance matrix supplied.

  Res.Df Df      F  Pr(>F)  
1    183                    
2    181  2 3.3896 0.03588 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
>   ## covariance matrix *function*
> linearHypothesis(mod.davis, c("(Intercept) = 0", "repwt = 1"), vcov = hccm)
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

Note: Coefficient covariance matrix supplied.

  Res.Df Df      F  Pr(>F)  
1    183                    
2    181  2 3.3896 0.03588 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
>   ## covariance matrix *estimate*
> linearHypothesis(mod.davis, c("(Intercept) = 0", "repwt = 1"), 
+     vcov = hccm(mod.davis, type = "hc3"))
Linear hypothesis test

Hypothesis:
(Intercept) = 0
repwt = 1

Model 1: restricted model
Model 2: weight ~ repwt

Note: Coefficient covariance matrix supplied.

  Res.Df Df      F  Pr(>F)  
1    183                    
2    181  2 3.3896 0.03588 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> mod.duncan <- lm(prestige ~ income + education, data=Duncan)
> 
> ## the following are all equivalent:
> linearHypothesis(mod.duncan, "1*income - 1*education = 0")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "income = education")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "income - education")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "1income - 1education = 0")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "0 = 1*income - 1*education")
Linear hypothesis test

Hypothesis:
- income  + education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "income-education=0")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "1*income - 1*education + 1 = 1")
Linear hypothesis test

Hypothesis:
income - education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> linearHypothesis(mod.duncan, "2income = 2*education")
Linear hypothesis test

Hypothesis:
2 income - 2 education = 0

Model 1: restricted model
Model 2: prestige ~ income + education

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7518.9                           
2     42 7506.7  1    12.195 0.0682 0.7952
> 
> mod.duncan.2 <- lm(prestige ~ type*(income + education), data=Duncan)
> coefs <- names(coef(mod.duncan.2))
> 
> ## test against the null model (i.e., only the intercept is not set to 0)
> linearHypothesis(mod.duncan.2, coefs[-1]) 
Linear hypothesis test

Hypothesis:
typeprof = 0
typewc = 0
income = 0
education = 0
typeprof:income = 0
typewc:income = 0
typeprof:education = 0
typewc:education = 0

Model 1: restricted model
Model 2: prestige ~ type * (income + education)

  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    
1     44 43688                                  
2     36  3351  8     40337 54.174 < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## test all interaction coefficients equal to 0
> linearHypothesis(mod.duncan.2, coefs[grep(":", coefs)], verbose=TRUE)

Hypothesis matrix:
                   (Intercept) typeprof typewc income education typeprof:income
typeprof:income              0        0      0      0         0               1
typewc:income                0        0      0      0         0               0
typeprof:education           0        0      0      0         0               0
typewc:education             0        0      0      0         0               0
                   typewc:income typeprof:education typewc:education
typeprof:income                0                  0                0
typewc:income                  1                  0                0
typeprof:education             0                  1                0
typewc:education               0                  0                1

Right-hand-side vector:
[1] 0 0 0 0

Estimated linear function (hypothesis.matrix %*% coef - rhs)
   typeprof:income      typewc:income typeprof:education   typewc:education 
       -0.36914256        -0.36030837         0.01859107         0.10677092 


Estimated variance/covariance matrix for linear function
                   typeprof:income typewc:income typeprof:education
typeprof:income         0.04156710   0.017091995        -0.02462562
typewc:income           0.01709200   0.067378054        -0.01508772
typeprof:education     -0.02462562  -0.015087722         0.10135862
typewc:education       -0.01508772  -0.009442361         0.07828368
                   typewc:education
typeprof:income        -0.015087722
typewc:income          -0.009442361
typeprof:education      0.078283679
typewc:education        0.131161899

Linear hypothesis test

Hypothesis:
typeprof:income = 0
typewc:income = 0
typeprof:education = 0
typewc:education = 0

Model 1: restricted model
Model 2: prestige ~ type * (income + education)

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     40 3798.0                           
2     36 3350.6  4    447.31 1.2015  0.327
> linearHypothesis(mod.duncan.2, matchCoefs(mod.duncan.2, ":"), verbose=TRUE) # equivalent

Hypothesis matrix:
                   (Intercept) typeprof typewc income education typeprof:income
typeprof:income              0        0      0      0         0               1
typewc:income                0        0      0      0         0               0
typeprof:education           0        0      0      0         0               0
typewc:education             0        0      0      0         0               0
                   typewc:income typeprof:education typewc:education
typeprof:income                0                  0                0
typewc:income                  1                  0                0
typeprof:education             0                  1                0
typewc:education               0                  0                1

Right-hand-side vector:
[1] 0 0 0 0

Estimated linear function (hypothesis.matrix %*% coef - rhs)
   typeprof:income      typewc:income typeprof:education   typewc:education 
       -0.36914256        -0.36030837         0.01859107         0.10677092 


Estimated variance/covariance matrix for linear function
                   typeprof:income typewc:income typeprof:education
typeprof:income         0.04156710   0.017091995        -0.02462562
typewc:income           0.01709200   0.067378054        -0.01508772
typeprof:education     -0.02462562  -0.015087722         0.10135862
typewc:education       -0.01508772  -0.009442361         0.07828368
                   typewc:education
typeprof:income        -0.015087722
typewc:income          -0.009442361
typeprof:education      0.078283679
typewc:education        0.131161899

Linear hypothesis test

Hypothesis:
typeprof:income = 0
typewc:income = 0
typeprof:education = 0
typewc:education = 0

Model 1: restricted model
Model 2: prestige ~ type * (income + education)

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     40 3798.0                           
2     36 3350.6  4    447.31 1.2015  0.327
> lh <- linearHypothesis(mod.duncan.2, coefs[grep(":", coefs)])
> attr(lh, "value") # value of linear function
                          [,1]
typeprof:income    -0.36914256
typewc:income      -0.36030837
typeprof:education  0.01859107
typewc:education    0.10677092
> attr(lh, "vcov")  # covariance matrix of linear function
                   typeprof:income typewc:income typeprof:education
typeprof:income         0.04156710   0.017091995        -0.02462562
typewc:income           0.01709200   0.067378054        -0.01508772
typeprof:education     -0.02462562  -0.015087722         0.10135862
typewc:education       -0.01508772  -0.009442361         0.07828368
                   typewc:education
typeprof:income        -0.015087722
typewc:income          -0.009442361
typeprof:education      0.078283679
typewc:education        0.131161899
> 
> ## a multivariate linear model for repeated-measures data
> ## see ?OBrienKaiser for a description of the data set used in this example.
> 
> mod.ok <- lm(cbind(pre.1, pre.2, pre.3, pre.4, pre.5, 
+                      post.1, post.2, post.3, post.4, post.5, 
+                      fup.1, fup.2, fup.3, fup.4, fup.5) ~  treatment*gender, 
+                 data=OBrienKaiser)
> coef(mod.ok)
                        pre.1      pre.2       pre.3      pre.4      pre.5
(Intercept)         3.9027778  4.2777778  5.43055556  4.6111111  4.1388889
treatment1          0.1180556  0.1388889 -0.07638889  0.1805556  0.1944444
treatment2         -0.2291667 -0.3333333 -0.14583333 -0.7083333 -0.6666667
gender1            -0.6527778 -0.7777778 -0.18055556 -0.1111111 -0.6388889
treatment1:gender1 -0.4930556 -0.3888889 -0.54861111 -0.1805556 -0.1944444
treatment2:gender1  0.6041667  0.5833333  0.27083333  0.7083333  1.1666667
                       post.1     post.2     post.3      post.4        post.5
(Intercept)         5.0277778  5.5416667  6.9166667  6.36111111  4.833333e+00
treatment1          0.7638889  0.8958333  0.8333333  0.72222222  9.166667e-01
treatment2          0.2916667  0.1875000 -0.2500000  0.08333333 -1.652689e-16
gender1            -0.8611111 -0.4583333 -0.4166667 -0.52777778 -1.000000e+00
treatment1:gender1 -0.6805556 -0.6041667 -0.3333333 -0.55555556 -5.000000e-01
treatment2:gender1  0.9583333  0.6875000  0.2500000  0.91666667  1.250000e+00
                        fup.1      fup.2      fup.3       fup.4      fup.5
(Intercept)         6.0138889  6.1527778  7.7777778  6.16666667  5.3472222
treatment1          0.9236111  1.0347222  1.0972222  0.95833333  0.8819444
treatment2         -0.0625000 -0.0625000 -0.1250000  0.12500000  0.2291667
gender1            -0.5972222 -0.9027778 -0.7777778 -0.83333333 -0.4305556
treatment1:gender1 -0.2152778 -0.1597222 -0.3472222 -0.04166667 -0.1736111
treatment2:gender1  0.6875000  1.1875000  0.8750000  1.12500000  0.3958333
> 
> ## specify the model for the repeated measures:
> phase <- factor(rep(c("pretest", "posttest", "followup"), c(5, 5, 5)),
+     levels=c("pretest", "posttest", "followup"))
> hour <- ordered(rep(1:5, 3))
> idata <- data.frame(phase, hour)
> idata
      phase hour
1   pretest    1
2   pretest    2
3   pretest    3
4   pretest    4
5   pretest    5
6  posttest    1
7  posttest    2
8  posttest    3
9  posttest    4
10 posttest    5
11 followup    1
12 followup    2
13 followup    3
14 followup    4
15 followup    5
>  
> ## test the four-way interaction among the between-subject factors 
> ## treatment and gender, and the intra-subject factors 
> ## phase and hour              
>     
> linearHypothesis(mod.ok, c("treatment1:gender1", "treatment2:gender1"),
+     title="treatment:gender:phase:hour", idata=idata, idesign=~phase*hour, 
+     iterms="phase:hour")

 Response transformation matrix:
       phase1:hour.L phase2:hour.L phase1:hour.Q phase2:hour.Q phase1:hour.C
pre.1  -6.324555e-01  0.000000e+00     0.5345225     0.0000000 -3.162278e-01
pre.2  -3.162278e-01  0.000000e+00    -0.2672612     0.0000000  6.324555e-01
pre.3  -3.510833e-17  0.000000e+00    -0.5345225     0.0000000  1.755417e-16
pre.4   3.162278e-01  0.000000e+00    -0.2672612     0.0000000 -6.324555e-01
pre.5   6.324555e-01  0.000000e+00     0.5345225     0.0000000  3.162278e-01
post.1  0.000000e+00 -6.324555e-01     0.0000000     0.5345225  0.000000e+00
post.2  0.000000e+00 -3.162278e-01     0.0000000    -0.2672612  0.000000e+00
post.3  0.000000e+00 -3.510833e-17     0.0000000    -0.5345225  0.000000e+00
post.4  0.000000e+00  3.162278e-01     0.0000000    -0.2672612  0.000000e+00
post.5  0.000000e+00  6.324555e-01     0.0000000     0.5345225  0.000000e+00
fup.1   6.324555e-01  6.324555e-01    -0.5345225    -0.5345225  3.162278e-01
fup.2   3.162278e-01  3.162278e-01     0.2672612     0.2672612 -6.324555e-01
fup.3   3.510833e-17  3.510833e-17     0.5345225     0.5345225 -1.755417e-16
fup.4  -3.162278e-01 -3.162278e-01     0.2672612     0.2672612  6.324555e-01
fup.5  -6.324555e-01 -6.324555e-01    -0.5345225    -0.5345225 -3.162278e-01
       phase2:hour.C phase1:hour^4 phase2:hour^4
pre.1   0.000000e+00     0.1195229     0.0000000
pre.2   0.000000e+00    -0.4780914     0.0000000
pre.3   0.000000e+00     0.7171372     0.0000000
pre.4   0.000000e+00    -0.4780914     0.0000000
pre.5   0.000000e+00     0.1195229     0.0000000
post.1 -3.162278e-01     0.0000000     0.1195229
post.2  6.324555e-01     0.0000000    -0.4780914
post.3  1.755417e-16     0.0000000     0.7171372
post.4 -6.324555e-01     0.0000000    -0.4780914
post.5  3.162278e-01     0.0000000     0.1195229
fup.1   3.162278e-01    -0.1195229    -0.1195229
fup.2  -6.324555e-01     0.4780914     0.4780914
fup.3  -1.755417e-16    -0.7171372    -0.7171372
fup.4   6.324555e-01     0.4780914     0.4780914
fup.5  -3.162278e-01    -0.1195229    -0.1195229

Sum of squares and products for the hypothesis:
              phase1:hour.L phase2:hour.L phase1:hour.Q phase2:hour.Q
phase1:hour.L     5.4102798     3.5699513     6.7909995     6.2606189
phase2:hour.L     3.5699513     2.4878345     4.9059045     4.9999819
phase1:hour.Q     6.7909995     4.9059045     9.8895116    10.6507647
phase2:hour.Q     6.2606189     4.9999819    10.6507647    12.9553354
phase1:hour.C     1.1913017     0.8254258     1.6217811     1.6371522
phase2:hour.C     0.6411192     0.2798054     0.3444363    -0.1994646
phase1:hour^4     0.8870900     0.6632771     1.3639276     1.5387054
phase2:hour^4     2.8928996     1.3127598     1.7155235    -0.5700924
              phase1:hour.C phase2:hour.C phase1:hour^4 phase2:hour^4
phase1:hour.L    1.19130170    0.64111922    0.88708998     2.8928996
phase2:hour.L    0.82542579    0.27980535    0.66327707     1.3127598
phase1:hour.Q    1.62178110    0.34443634    1.36392763     1.7155235
phase2:hour.Q    1.63715221   -0.19946463    1.53870545    -0.5700924
phase1:hour.C    0.27402676    0.09854015    0.21852508     0.4595809
phase2:hour.C    0.09854015    0.23114355    0.02069149     0.9885932
phase1:hour^4    0.21852508    0.02069149    0.19138860     0.1229579
phase2:hour^4    0.45958089    0.98859321    0.12295794     4.2344456

Sum of squares and products for error:
              phase1:hour.L phase2:hour.L phase1:hour.Q phase2:hour.Q
phase1:hour.L    24.1583333     11.183333     20.516120     9.7615316
phase2:hour.L    11.1833333     45.133333      9.423470    11.8321596
phase1:hour.Q    20.5161195      9.423470     29.255952    14.3690476
phase2:hour.Q     9.7615316     11.832160     14.369048    17.9523810
phase1:hour.C     0.8083333     -5.866667      5.486460     0.1972027
phase2:hour.C     2.8833333    -16.516667      2.352346    -1.3381609
phase1:hour^4   -13.5909725      4.598568     -9.436739     0.2129589
phase2:hour^4    -7.0553368    -13.008277     -7.985957    -4.0728381
              phase1:hour.C phase2:hour.C phase1:hour^4 phase2:hour^4
phase1:hour.L     0.8083333      2.883333   -13.5909725    -7.0553368
phase2:hour.L    -5.8666667    -16.516667     4.5985678   -13.0082773
phase1:hour.Q     5.4864597      2.352346    -9.4367393    -7.9859571
phase2:hour.Q     0.1972027     -1.338161     0.2129589    -4.0728381
phase1:hour.C     6.5916667      6.650000    -1.7165886     0.8819171
phase2:hour.C     6.6500000     18.866667   -10.2995319     3.3386862
phase1:hour^4    -1.7165886    -10.299532    22.8273810     9.8809524
phase2:hour^4     0.8819171      3.338686     9.8809524    21.5476190

Multivariate Tests: treatment:gender:phase:hour
                 Df test stat  approx F num Df den Df  Pr(>F)
Pillai            2 0.7927708 0.3283431     16      8 0.97237
Wilks             2 0.3621700 0.2481248     16      6 0.98808
Hotelling-Lawley  2 1.3333221 0.1666653     16      4 0.99620
Roy               2 0.7955979 0.3977990      8      4 0.87560
> 
> ## mixed-effects models examples:
> 
> ## Not run: 
> ##D  # loads nlme package
> ##D 	library(nlme)
> ##D 	example(lme)
> ##D 	linearHypothesis(fm2, "age = 0")
> ## End(Not run)
> 
> ## Not run: 
> ##D  # loads lme4 package
> ##D 	library(lme4)
> ##D 	example(glmer)
> ##D 	linearHypothesis(gm1, matchCoefs(gm1, "period"))
> ## End(Not run)
> 
> if (require(nnet)){
+   print(m <- multinom(partic ~ hincome + children, data=Womenlf))
+   print(coefs <- as.vector(outer(c("not.work.", "parttime."), 
+                             c("hincome", "childrenpresent"),
+                             paste0)))
+   linearHypothesis(m, coefs) # ominbus Wald test
+ }
Loading required package: nnet
# weights:  12 (6 variable)
initial  value 288.935032 
iter  10 value 211.441198
final  value 211.440963 
converged
Call:
multinom(formula = partic ~ hincome + children, data = Womenlf)

Coefficients:
         (Intercept)    hincome childrenpresent
not.work   -1.982826 0.09723134        2.558598
parttime   -3.415105 0.10411973        2.580127

Residual Deviance: 422.8819 
AIC: 434.8819 
[1] "not.work.hincome"         "parttime.hincome"        
[3] "not.work.childrenpresent" "parttime.childrenpresent"
Linear hypothesis test

Hypothesis:
not.work.hincome = 0
parttime.hincome = 0
not.work.childrenpresent = 0
parttime.childrenpresent = 0

Model 1: restricted model
Model 2: partic ~ hincome + children

  Df  Chisq Pr(>Chisq)    
1                         
2  4 58.435  6.183e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> 
> 
> 
> cleanEx()

detaching ‘package:nnet’

> nameEx("logit")
> ### * logit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logit
> ### Title: Logit Transformation
> ### Aliases: logit
> ### Keywords: manip
> 
> ### ** Examples
> 
> save.opt <- options(digits=4)
> logit(.1*0:10)
Warning in logit(0.1 * 0:10) : proportions remapped to (0.025, 0.975)
 [1] -3.6636 -1.9924 -1.2950 -0.8001 -0.3847  0.0000  0.3847  0.8001  1.2950
[10]  1.9924  3.6636
> logit(.1*0:10, adjust=0)
 [1]    -Inf -2.1972 -1.3863 -0.8473 -0.4055  0.0000  0.4055  0.8473  1.3863
[10]  2.1972     Inf
> options(save.opt)
> 
> 
> 
> cleanEx()
> nameEx("marginalModelPlot")
> ### * marginalModelPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mmps
> ### Title: Marginal Model Plotting
> ### Aliases: mmps mmp mmp.lm mmp.glm mmp.default marginalModelPlot
> ###   marginalModelPlots
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> c1 <- lm(infantMortality ~ ppgdp, UN)
> mmps(c1)
> c2 <- update(c1, ~ log(ppgdp))
> mmps(c2)
> # include SD lines
> p1 <- lm(prestige ~ income + education, Prestige)
> mmps(p1, sd=TRUE)
> # condition on type:
> mmps(p1, ~. | type)
> # logisitic regression example
> # smoothers return warning messages.
> # fit a separate smoother and color for each type of occupation.
> m1 <- glm(lfp ~ ., family=binomial, data=Mroz)
> mmps(m1)
Warning in mmps(m1) : Interactions and/or factors skipped
> 
> 
> 
> cleanEx()
> nameEx("mcPlots")
> ### * mcPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mcPlots
> ### Title: Draw Linear Model Marginal and Conditional Plots in Parallel or
> ###   Overlaid
> ### Aliases: mcPlots mcPlots.default mcPlot mcPlot.lm mcPlot.glm
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> m1 <- lm(partic ~ tfr + menwage + womwage + debt + parttime, data = Bfox)
> mcPlot(m1, "womwage")
> mcPlot(m1, "womwage", overlaid=FALSE, ellipse=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("ncvTest")
> ### * ncvTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ncvTest
> ### Title: Score Test for Non-Constant Error Variance
> ### Aliases: ncvTest ncvTest.lm ncvTest.glm
> ### Keywords: htest regression
> 
> ### ** Examples
> 
> ncvTest(lm(interlocks ~ assets + sector + nation, data=Ornstein))
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 46.98537, Df = 1, p = 7.1518e-12
> 
> ncvTest(lm(interlocks ~ assets + sector + nation, data=Ornstein), 
+     ~ assets + sector + nation, data=Ornstein)
Non-constant Variance Score Test 
Variance formula: ~ assets + sector + nation 
Chisquare = 74.73535, Df = 13, p = 1.0663e-10
> 
> 
> 
> cleanEx()
> nameEx("outlierTest")
> ### * outlierTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: outlierTest
> ### Title: Bonferroni Outlier Test
> ### Aliases: outlierTest outlierTest.lm outlierTest.glm outlierTest.lmerMod
> ###   print.outlierTest
> ### Keywords: regression htest
> 
> ### ** Examples
> 
> outlierTest(lm(prestige ~ income + education, data=Duncan))
No Studentized residuals with Bonferroni p < 0.05
Largest |rstudent|:
         rstudent unadjusted p-value Bonferroni p
minister 3.134519          0.0031772      0.14297
> 
> 
> 
> cleanEx()
> nameEx("panel.car")
> ### * panel.car
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: panel.car
> ### Title: Panel Function for Coplots
> ### Aliases: panel.car
> ### Keywords: aplot
> 
> ### ** Examples
> 
> coplot(prestige ~ income|education, panel=panel.car, 
+   col="red", data=Prestige)
> 
> 
> 
> cleanEx()
> nameEx("poTest")
> ### * poTest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: poTest
> ### Title: Test for Proportional Odds in the Proportional-Odds
> ###   Logistic-Regression Model
> ### Aliases: poTest poTest.polr print.poTest
> ### Keywords: models htest
> 
> ### ** Examples
> 
> if (require("MASS")){
+     .W <- Womenlf
+     .W$partic <- factor(.W$partic, levels=c("not.work", "parttime", "fulltime"))
+     poTest(polr(partic ~ hincome + children + region, data=.W))
+ }
Loading required package: MASS

Tests for Proportional Odds
polr(formula = partic ~ hincome + children + region, data = .W)

                b[polr] b[>not.work] b[>parttime] Chisquare df Pr(>Chisq)    
Overall                                               26.47  6    0.00018 ***
hincome         -0.0569      -0.0453      -0.1011      5.73  1    0.01666 *  
childrenpresent -2.0099      -1.6043      -2.7266     17.90  1    2.3e-05 ***
regionBC         0.1530       0.3420      -0.7228      2.69  1    0.10114    
regionOntario    0.2687       0.1878       0.0620      0.07  1    0.78563    
regionPrairie    0.4797       0.4719       0.3501      0.05  1    0.82760    
regionQuebec    -0.0711      -0.1731      -0.2974      0.07  1    0.79252    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("pointLabel")
> ### * pointLabel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pointLabel
> ### Title: Label placement for points to avoid overlaps
> ### Aliases: pointLabel
> ### Keywords: aplot
> 
> ### ** Examples
> 
> n <- 50
> x <- rnorm(n)*10
> y <- rnorm(n)*10
> plot(x, y, col = "red", pch = 20)
> pointLabel(x, y, as.character(round(x,5)), offset = 0, cex = .7)
> 
> plot(x, y, col = "red", pch = 20)
> pointLabel(x, y, expression(over(alpha, beta[123])), offset = 0, cex = .8)
> 
> 
> 
> 
> cleanEx()
> nameEx("powerTransform")
> ### * powerTransform
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: powerTransform
> ### Title: Finding Univariate or Multivariate Power Transformations
> ### Aliases: powerTransform powerTransform.default powerTransform.lm
> ###   powerTransform.formula powerTransform.lmerMod
> ### Keywords: regression
> 
> ### ** Examples
> 
> # Box Cox Method, univariate
> summary(p1 <- powerTransform(cycles ~ len + amp + load, Wool))
bcPower Transformation to Normality 
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1   -0.0592           0      -0.1789       0.0606

Likelihood ratio test that transformation parameter is equal to 0
 (log transformation)
                            LRT df    pval
LR test, lambda = (0) 0.9213384  1 0.33712

Likelihood ratio test that no transformation is needed
                           LRT df       pval
LR test, lambda = (1) 84.07566  1 < 2.22e-16
> # fit linear model with transformed response:
> coef(p1, round=TRUE)
Y1 
 0 
> summary(m1 <- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))

Call:
lm(formula = bcPower(cycles, p1$roundlam) ~ len + amp + load, 
    data = Wool)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.43592 -0.11250  0.00802  0.11635  0.26790 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 10.551813   0.616683  17.111 1.41e-14 ***
len          0.016648   0.000875  19.025 1.43e-15 ***
amp         -0.630866   0.043752 -14.419 5.22e-13 ***
load        -0.078524   0.008750  -8.974 5.66e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1856 on 23 degrees of freedom
Multiple R-squared:  0.9658,	Adjusted R-squared:  0.9614 
F-statistic: 216.8 on 3 and 23 DF,  p-value: < 2.2e-16

> 
> # Multivariate Box Cox uses Highway1 data
> summary(powerTransform(cbind(len, adt, trks, sigs1) ~ 1, Highway1))
bcPower Transformations to Multinormality 
      Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
len      0.1439           0      -0.2728       0.5606
adt      0.0876           0      -0.1712       0.3464
trks    -0.6954           0      -1.9046       0.5139
sigs1   -0.2654           0      -0.5575       0.0267

Likelihood ratio test that transformation parameters are equal to 0
 (all log transformations)
                                 LRT df    pval
LR test, lambda = (0 0 0 0) 6.014218  4 0.19809

Likelihood ratio test that no transformations are needed
                                 LRT df       pval
LR test, lambda = (1 1 1 1) 127.7221  4 < 2.22e-16
> 
> # Multivariate transformation to normality within levels of 'htype'
> summary(a3 <- powerTransform(cbind(len, adt, trks, sigs1) ~ htype, Highway1))
bcPower Transformations to Multinormality 
      Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
len      0.1451        0.00      -0.2733       0.5636
adt      0.2396        0.33       0.0255       0.4536
trks    -0.7336        0.00      -1.9408       0.4735
sigs1   -0.2959       -0.50      -0.5511      -0.0408

Likelihood ratio test that transformation parameters are equal to 0
 (all log transformations)
                                LRT df    pval
LR test, lambda = (0 0 0 0) 13.1339  4 0.01064

Likelihood ratio test that no transformations are needed
                                 LRT df       pval
LR test, lambda = (1 1 1 1) 140.5853  4 < 2.22e-16
> 
> # test lambda = (0 0 0 -1)
> testTransform(a3, c(0, 0, 0, -1))
                                  LRT df       pval
LR test, lambda = (0 0 0 -1) 31.12644  4 2.8849e-06
> 
> # save the rounded transformed values, plot them with a separate
> # color for each highway type
> transformedY <- bcPower(with(Highway1, cbind(len, adt, trks, sigs1)),
+                         coef(a3, round=TRUE))
> ## Not run: 
> ##D  # generates a smoother warning
> ##D scatterplotMatrix( ~ transformedY|htype, Highway1) 
> ## End(Not run)
> 
> # With negative responses, use the bcnPower family
> m2 <- lm(I1L1 ~ pool, LoBD)
> summary(p2 <- powerTransform(m2, family="bcnPower"))
bcnPower transformation to Normality 

Estimated power, lambda
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1    0.5411         0.5        0.254       0.8282

Estimated location, gamma
   Est gamma Std Err. Wald Lower Bound Wald Upper Bound
Y1   16.5676  12.4057                0          40.8829

Likelihood ratio tests about transformation parameters
                           LRT df         pval
LR test, lambda = (0) 56.69014  1 5.107026e-14
LR test, lambda = (1) 46.95805  1 7.252310e-12
> testTransform(p2, .5)
                              LRT df      pval
LR test, lambda = (0.5) 0.3364919  1 0.5618612
> summary(powerTransform(update(m2, cbind(LoBD$I1L2, LoBD$I1L1) ~ .), family="bcnPower"))
bcnPower transformation to Multinormality 

Estimated power, lambda
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1     0.554         0.5       0.3121       0.7959
Y2     0.516         0.5       0.2204       0.8115

Estimated location, gamma
   Est gamma Std Err. Wald Lower Bound Wald Upper Bound
Y1   13.5844   9.4546                0          32.1154
Y2   17.6867  12.5295                0          42.2444

Likelihood ratio tests about transformation parameters
                              LRT df pval
LR test, lambda = (0 0) 119.24792  2    0
LR test, lambda = (1 1)  99.35734  2    0
> 
> ## Not run: 
> ##D  # takes a few seconds:
> ##D   # multivariate bcnPower, with 8 responses
> ##D   summary(powerTransform(update(m2, as.matrix(LoBD[, -1]) ~ .), family="bcnPower"))
> ##D   # multivariate bcnPower, fit with one iteration using starting values as estimates
> ##D   summary(powerTransform(update(m2, as.matrix(LoBD[, -1]) ~ .), family="bcnPower", itmax=1))
> ## End(Not run)
> 
> # mixed effects model
> ## Not run: 
> ##D  # uses the lme4 package
> ##D   data <- reshape(LoBD[1:20, ], varying=names(LoBD)[-1], direction="long", v.names="y")
> ##D   names(data) <- c("pool", "assay", "y", "id")
> ##D   data$assay <- factor(data$assay)
> ##D   require(lme4)
> ##D   m2 <- lmer(y ~ pool + (1|assay), data)
> ##D   summary(l2 <- powerTransform(m2, family="bcnPower", verbose=TRUE))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("qqPlot")
> ### * qqPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qqPlot
> ### Title: Quantile-Comparison Plot
> ### Aliases: qqPlot qqp qqPlot.default qqPlot.formula qqPlot.lm qqPlot.glm
> ### Keywords: distribution univar regression
> 
> ### ** Examples
> 
> x<-rchisq(100, df=2)
> qqPlot(x)
[1] 82 34
> qqPlot(x, dist="chisq", df=2, envelope=list(style="lines"))
[1] 82 34
> 
> qqPlot(~ income, data=Prestige, subset = type == "prof")
general.managers       physicians 
               2               24 
> qqPlot(income ~ type, data=Prestige, layout=c(1, 3))
> 
> qqPlot(lm(prestige ~ income + education + type, data=Duncan),
+ 	envelope=.99)
 minister machinist 
        6        28 
> 
> 
> 
> cleanEx()
> nameEx("recode")
> ### * recode
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: recode
> ### Title: Recode a Variable
> ### Aliases: recode Recode
> ### Keywords: manip
> 
> ### ** Examples
> 
> x <- rep(1:3, 3)
> x
[1] 1 2 3 1 2 3 1 2 3
> recode(x, "c(1, 2) = 'A'; 
+            else = 'B'")
[1] "A" "A" "B" "A" "A" "B" "A" "A" "B"
> Recode(x, "1~2 -> ':=1' // 3 -> ';=2'", to.value="->", 
+        interval="~", separator="//")
[1] ":=1" ":=1" ";=2" ":=1" ":=1" ";=2" ":=1" ":=1" ";=2"
> 
> 
> 
> cleanEx()
> nameEx("regLine")
> ### * regLine
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: regLine
> ### Title: Plot Regression Line
> ### Aliases: regLine
> ### Keywords: aplot
> 
> ### ** Examples
> 
> plot(repwt ~ weight, pch=c(1,2)[sex], data=Davis)
> regLine(lm(repwt~weight, subset=sex=="M", data=Davis))
> regLine(lm(repwt~weight, subset=sex=="F", data=Davis), lty=2)
> 
> 
> 
> cleanEx()
> nameEx("residualPlots")
> ### * residualPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residualPlots
> ### Title: Residual Plots for Linear and Generalized Linear Models
> ### Aliases: residualPlots residualPlots.default residualPlots.lm
> ###   residualPlots.glm residualPlot residualPlot.default residualPlot.lm
> ###   residualPlot.glm residCurvTest residCurvTest.lm residCurvTest.glm
> ###   tukeyNonaddTest
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> m1 <- lm(prestige ~ income + type, data=Prestige)
> residualPlots(m1)
           Test stat Pr(>|Test stat|)    
income       -3.9175        0.0001707 ***
type                                     
Tukey test   -4.9317        8.153e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> residualPlots(m1, terms= ~ 1 | type) # plot vs. yhat grouping by type
> 
> 
> 
> cleanEx()
> nameEx("scatter3d")
> ### * scatter3d
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scatter3d
> ### Title: Three-Dimensional Scatterplots and Point Identification
> ### Aliases: scatter3d scatter3d.formula scatter3d.default Identify3d
> ### Keywords: hplot
> 
> ### ** Examples
> 
>     if(interactive() && require(rgl) && require(mgcv)){
+ scatter3d(prestige ~ income + education, data=Duncan, id=list(n=3))
+ Sys.sleep(5) # wait 5 seconds
+ scatter3d(prestige ~ income + education | type, data=Duncan)
+ Sys.sleep(5)
+ scatter3d(prestige ~ income + education | type, surface=FALSE,
+ 	ellipsoid=TRUE, revolutions=3, data=Duncan)
+ scatter3d(prestige ~ income + education, fit=c("linear", "additive"),
+ 	data=Prestige)
+ Sys.sleep(5)
+ scatter3d(prestige ~ income + education | type,
+     radius=(1 + women)^(1/3), data=Prestige)
+ Sys.sleep(5)
+ if (require(mvtnorm)){
+   local({
+     set.seed(123)
+     Sigma <- matrix(c(
+       1, 0.5,
+       0.5, 1),
+       2, 2
+     )
+     X <- rmvnorm(200, sigma=Sigma)
+     D <- data.frame(
+       x1 = X[, 1],
+       x2 = X[, 2]
+     )
+     D$y <- with(D, 10 + 1*x1 + 2*x2 + 3*x1*x2 + rnorm(200, sd=3))
+     # plot true regression function
+     scatter3d(y ~ x1 + x2, D, 
+               reg.function=10 + 1*x + 2*z + 3*x*z,
+               fit="quadratic", revolutions=2)
+   })
+ }
+ 	}
> 	## Not run: 
> ##D  # requires user interaction to identify points
> ##D # drag right mouse button to identify points, click right button in open area to exit
> ##D scatter3d(prestige ~ income + education, data=Duncan, id=list(method="identify"))
> ##D scatter3d(prestige ~ income + education | type, data=Duncan, id=list(method="identify"))
> ##D     
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("scatterplot")
> ### * scatterplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scatterplot
> ### Title: Enhanced Scatterplots with Marginal Boxplots, Point Marking,
> ###   Smoothers, and More
> ### Aliases: scatterplot scatterplot.formula scatterplot.default sp
> ### Keywords: hplot
> 
> ### ** Examples
> 
> scatterplot(prestige ~ income, data=Prestige, ellipse=TRUE, 
+   smooth=list(style="lines"))
> 
> scatterplot(prestige ~ income, data=Prestige, 
+   smooth=list(smoother=quantregLine))
>   
> scatterplot(prestige ~ income, data=Prestige, 
+   smooth=list(smoother=quantregLine, border="FALSE"))
> 
> # use quantile regression for median and quartile fits
> scatterplot(prestige ~ income | type, data=Prestige,
+   smooth=list(smoother=quantregLine, var=TRUE, span=1, lwd=4, lwd.var=2))
> 
> scatterplot(prestige ~ income | type, data=Prestige, 
+   legend=list(coords="topleft"))
> 
> scatterplot(vocabulary ~ education, jitter=list(x=1, y=1),
+             data=Vocab, smooth=FALSE, lwd=3)
> 
> scatterplot(infantMortality ~ ppgdp, log="xy", data=UN, id=list(n=5))
           Angola Equatorial Guinea             Gabon             Qatar 
                4                54                62               143 
          Somalia 
              159 
> 
> scatterplot(income ~ type, data=Prestige)
[1] "2"  "24"
> 
> ## Not run: 
> ##D  # interactive point identification
> ##D     # remember to exit from point-identification mode
> ##D     scatterplot(infantMortality ~ ppgdp, id=list(method="identify"), data=UN)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("scatterplotMatrix")
> ### * scatterplotMatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scatterplotMatrix
> ### Title: Scatterplot Matrices
> ### Aliases: scatterplotMatrix scatterplotMatrix.formula
> ###   scatterplotMatrix.default spm
> ### Keywords: hplot
> 
> ### ** Examples
> 
> scatterplotMatrix(~ income + education + prestige | type, data=Duncan)
Warning in smoother(x[subs], y[subs], col = smoother.args$col[i], log.x = FALSE,  :
  could not fit smooth
> scatterplotMatrix(~ income + education + prestige | type, data=Duncan,
+     regLine=FALSE, smooth=list(span=1))
> scatterplotMatrix(~ income + education + prestige,
+     data=Duncan, id=TRUE, smooth=list(method=gamLine))
> 
> 
> 
> cleanEx()
> nameEx("showLabels")
> ### * showLabels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: showLabels
> ### Title: Functions to Identify and Mark Extreme Points in a 2D Plot.
> ### Aliases: showLabels
> ### Keywords: utilities
> 
> ### ** Examples
> 
> plot(income ~ education, Prestige)
> with(Prestige, showLabels(education, income,
+      labels = rownames(Prestige), method=list("x", "y"), n=3))
university.teachers          physicians       veterinarians    general.managers 
                 21                  24                  25                   2 
         physicians             lawyers 
                 24                  17 
> m <- lm(income ~ education, Prestige)
> plot(income ~ education, Prestige)
> abline(m)
> with(Prestige, showLabels(education, income,
+      labels=rownames(Prestige), method=abs(residuals(m)), n=4))
        general.managers               physicians                  lawyers 
                       2                       24                       17 
osteopaths.chiropractors 
                      26 
> 
> 
> 
> cleanEx()
> nameEx("sigmaHat")
> ### * sigmaHat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sigmaHat
> ### Title: Return the scale estimate for a regression model
> ### Aliases: sigmaHat sigmaHat.default sigmaHat.glm sigmaHat.lm
> ### Keywords: regression
> 
> ### ** Examples
> 
> m1 <- lm(prestige ~ income + education, data=Duncan)
> sigmaHat(m1)
[1] 13.36903
> 
> 
> 
> cleanEx()
> nameEx("some")
> ### * some
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: some
> ### Title: Sample a Few Elements of an Object
> ### Aliases: some some.data.frame some.matrix some.default
> ### Keywords: utilities
> 
> ### ** Examples
> 
> some(Duncan)
                   type income education prestige
accountant         prof     62        86       82
author             prof     55        90       76
welfare.worker     prof     41        84       59
factory.owner      prof     60        56       81
bookkeeper           wc     29        72       39
insurance.agent      wc     55        71       41
streetcar.motorman   bc     42        26       19
taxi.driver          bc      9        19       10
barber               bc     16        26       20
shoe.shiner          bc      9        17        3
> some(Duncan, cols=names(Duncan)[1:3])
            type income education
professor   prof     64        93
reporter      wc     67        87
engineer    prof     72        86
teacher     prof     48        91
bookkeeper    wc     29        72
carpenter     bc     21        23
taxi.driver   bc      9        19
barber        bc     16        26
shoe.shiner   bc      9        17
watchman      bc     17        25
> 
> 
> 
> cleanEx()
> nameEx("spreadLevelPlot")
> ### * spreadLevelPlot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spreadLevelPlot
> ### Title: Spread-Level Plots
> ### Aliases: spreadLevelPlot slp spreadLevelPlot.formula
> ###   spreadLevelPlot.default spreadLevelPlot.lm print.spreadLevelPlot
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> spreadLevelPlot(interlocks + 1 ~ nation, data=Ornstein)
    LowerHinge Median UpperHinge Hinge-Spread
US           2    6.0         13           11
UK           4    9.0         14           10
CAN          6   13.0         30           24
OTH          4   15.5         24           20

Suggested power transformation:  0.1534487 
> slp(lm(interlocks + 1 ~ assets + sector + nation, data=Ornstein))
Warning in spreadLevelPlot.lm(...) : 
1 negative fitted value removed

Suggested power transformation:  0.4096738 
> 
> 
> 
> cleanEx()
> nameEx("strings2factors")
> ### * strings2factors
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: strings2factors
> ### Title: Convert Character-String Variables in a Data Frame to Factors
> ### Aliases: strings2factors strings2factors.data.frame
> ### Keywords: manip
> 
> ### ** Examples
> 
> M <- Moore # from the carData package
> M$partner <- as.character(Moore$partner.status)
> M$fcat <- as.character(Moore$fcategory)
> M$names <- rownames(M) # values are unique
> str(M)
'data.frame':	45 obs. of  7 variables:
 $ partner.status: Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ conformity    : int  8 4 8 7 10 6 12 4 13 12 ...
 $ fcategory     : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ fscore        : int  37 57 65 20 36 18 51 44 31 36 ...
 $ partner       : chr  "low" "low" "low" "low" ...
 $ fcat          : chr  "low" "high" "high" "low" ...
 $ names         : chr  "1" "2" "3" "4" ...
> str(strings2factors(M))

The following character variables were converted to factors
 partner fcat 
'data.frame':	45 obs. of  7 variables:
 $ partner.status: Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ conformity    : int  8 4 8 7 10 6 12 4 13 12 ...
 $ fcategory     : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ fscore        : int  37 57 65 20 36 18 51 44 31 36 ...
 $ partner       : Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ fcat          : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ names         : chr  "1" "2" "3" "4" ...
> str(strings2factors(M,
+   levels=list(partner=c("low", "high"), fcat=c("low", "medium", "high"))))

The following character variables were converted to factors
 partner fcat 
'data.frame':	45 obs. of  7 variables:
 $ partner.status: Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ conformity    : int  8 4 8 7 10 6 12 4 13 12 ...
 $ fcategory     : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ fscore        : int  37 57 65 20 36 18 51 44 31 36 ...
 $ partner       : Factor w/ 2 levels "low","high": 1 1 1 1 1 1 1 1 1 1 ...
 $ fcat          : Factor w/ 3 levels "low","medium",..: 1 3 3 1 1 1 2 2 1 1 ...
 $ names         : chr  "1" "2" "3" "4" ...
> str(strings2factors(M, which="partner", levels=list(partner=c("low", "high"))))

 partner was converted to a factor'data.frame':	45 obs. of  7 variables:
 $ partner.status: Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ conformity    : int  8 4 8 7 10 6 12 4 13 12 ...
 $ fcategory     : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ fscore        : int  37 57 65 20 36 18 51 44 31 36 ...
 $ partner       : Factor w/ 2 levels "low","high": 1 1 1 1 1 1 1 1 1 1 ...
 $ fcat          : chr  "low" "high" "high" "low" ...
 $ names         : chr  "1" "2" "3" "4" ...
> str(strings2factors(M, not="partner", exclude.unique=FALSE))
Warning in strings2factors.data.frame(M, not = "partner", exclude.unique = FALSE) :
  all values of names are unique

The following character variables were converted to factors
 fcat names 
'data.frame':	45 obs. of  7 variables:
 $ partner.status: Factor w/ 2 levels "high","low": 2 2 2 2 2 2 2 2 2 2 ...
 $ conformity    : int  8 4 8 7 10 6 12 4 13 12 ...
 $ fcategory     : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ fscore        : int  37 57 65 20 36 18 51 44 31 36 ...
 $ partner       : chr  "low" "low" "low" "low" ...
 $ fcat          : Factor w/ 3 levels "high","low","medium": 2 1 1 2 2 2 3 3 2 2 ...
 $ names         : Factor w/ 45 levels "1","10","11",..: 1 12 23 34 41 42 43 44 45 2 ...
> 
> 
> 
> cleanEx()
> nameEx("subsets")
> ### * subsets
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: subsets
> ### Title: Plot Output from regsubsets Function in leaps package
> ### Aliases: subsets subsets.regsubsets
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> if (require(leaps)){
+     subsets(regsubsets(undercount ~ ., data=Ericksen),
+             legend=c(3.5, -37))
+ }
Loading required package: leaps
Warning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘leaps’
> 
> 
> 
> cleanEx()
> nameEx("symbox")
> ### * symbox
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: symbox
> ### Title: Boxplots for transformations to symmetry
> ### Aliases: symbox symbox.formula symbox.default symbox.lm
> ### Keywords: hplot
> 
> ### ** Examples
> 
> symbox(~ income, data=Prestige)
> symbox(lm(wages ~ education + poly(age, 2) + sex, data=SLID))
> 
> 
> 
> cleanEx()
> nameEx("testTransform")
> ### * testTransform
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: testTransform
> ### Title: Likelihood-Ratio Tests for Univariate or Multivariate Power
> ###   Transformations to Normality
> ### Aliases: testTransform testTransform.powerTransform
> ###   testTransform.lmerModpowerTransform
> ###   testTransform.bcnPowerTransformlmer
> ### Keywords: regression
> 
> ### ** Examples
> 
> summary(a3 <- powerTransform(cbind(len, adt, trks, sigs1) ~ htype, Highway1))
bcPower Transformations to Multinormality 
      Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
len      0.1451        0.00      -0.2733       0.5636
adt      0.2396        0.33       0.0255       0.4536
trks    -0.7336        0.00      -1.9408       0.4735
sigs1   -0.2959       -0.50      -0.5511      -0.0408

Likelihood ratio test that transformation parameters are equal to 0
 (all log transformations)
                                LRT df    pval
LR test, lambda = (0 0 0 0) 13.1339  4 0.01064

Likelihood ratio test that no transformations are needed
                                 LRT df       pval
LR test, lambda = (1 1 1 1) 140.5853  4 < 2.22e-16
> # test lambda = (0 0 0 -1)
> testTransform(a3, c(0, 0, 0, -1))
                                  LRT df       pval
LR test, lambda = (0 0 0 -1) 31.12644  4 2.8849e-06
> summary(q1 <- powerTransform(lm(cbind(LoBD$I1L2, LoBD$I1L1) ~ pool, LoBD), family="bcnPower"))
bcnPower transformation to Multinormality 

Estimated power, lambda
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1     0.554         0.5       0.3121       0.7959
Y2     0.516         0.5       0.2204       0.8115

Estimated location, gamma
   Est gamma Std Err. Wald Lower Bound Wald Upper Bound
Y1   13.5844   9.4546                0          32.1154
Y2   17.6867  12.5295                0          42.2444

Likelihood ratio tests about transformation parameters
                              LRT df pval
LR test, lambda = (0 0) 119.24792  2    0
LR test, lambda = (1 1)  99.35734  2    0
> testTransform(q1, c(.3, .8))
                                 LRT df        pval
LR test, lambda = (0.3 0.8) 36.59772  2 1.12955e-08
> 
> 
> 
> 
> cleanEx()
> nameEx("vif")
> ### * vif
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vif
> ### Title: Variance Inflation Factors
> ### Aliases: vif vif.default vif.lm vif.merMod vif.polr vif.svyolr
> ### Keywords: regression
> 
> ### ** Examples
> 
> vif(lm(prestige ~ income + education, data=Duncan))
   income education 
   2.1049    2.1049 
> vif(lm(prestige ~ income + education + type, data=Duncan))
              GVIF Df GVIF^(1/(2*Df))
income    2.209178  1        1.486330
education 5.297584  1        2.301648
type      5.098592  2        1.502666
> vif(lm(prestige ~ (income + education)*type, data=Duncan),
+     type="terms") # not recommended
there are higher-order terms (interactions) in this model
consider setting type = 'predictor'; see ?vif
                      GVIF Df GVIF^(1/(2*Df))
income            4.824438  1        2.196460
education        32.778424  1        5.725244
type            480.931237  2        4.682963
income:type     176.244403  2        3.643584
education:type 1233.746316  2        5.926612
> vif(lm(prestige ~ (income + education)*type, data=Duncan),
+     type="predictor")
GVIFs computed for predictors
              GVIF Df GVIF^(1/(2*Df))    Interacts With Other Predictors
income    375.2745  5        1.808985              type        education
education 119.2534  5        1.613047              type           income
type        1.0000  8        1.000000 income, education             --  
> 
> 
> 
> cleanEx()
> nameEx("wcrossprod")
> ### * wcrossprod
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wcrossprod
> ### Title: Weighted Matrix Crossproduct
> ### Aliases: wcrossprod
> ### Keywords: array algebra
> 
> ### ** Examples
> 
> set.seed(12345)
> n <- 24
> drop <- 4
> sex <- sample(c("M", "F"), n, replace=TRUE)
> x1 <- 1:n
> x2 <- sample(1:n)
> extra <- c( rep(0, n - drop), floor(15 + 10 * rnorm(drop)) )
> y1 <- x1 + 3*x2 + 6*(sex=="M") + floor(10 * rnorm(n)) + extra
> y2 <- x1 - 2*x2 - 8*(sex=="M") + floor(10 * rnorm(n)) + extra
> # assign non-zero weights to 'dropped' obs
> wt <- c(rep(1, n-drop), rep(.2,drop))
> 
> X <- cbind(x1, x2)
> Y <- cbind(y1, y2)
> wcrossprod(X)
     x1   x2
x1 4900 4145
x2 4145 4900
> wcrossprod(X, w=wt)
       x1     x2
x1 3276.0 2970.6
x2 2970.6 3935.2
> 
> wcrossprod(X, Y)
      y1    y2
x1 20553  -748
x2 20955 -4169
> wcrossprod(X, Y, w=wt)
        y1      y2
x1 13263.4 -2058.4
x2 15180.6 -5167.4
> 
> wcrossprod(x1, y1)
      [,1]
[1,] 20553
> wcrossprod(x1, y1, w=wt)
        [,1]
[1,] 13263.4
> 
> 
> 
> 
> cleanEx()
> nameEx("which.names")
> ### * which.names
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: whichNames
> ### Title: Position of Row Names
> ### Aliases: which.names whichNames whichNames.data.frame
> ###   whichNames.default
> ### Keywords: utilities
> 
> ### ** Examples
> 
> whichNames(c('minister', 'conductor'), Duncan)
 minister conductor 
        6        16 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  16.252 21.402 12.503 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
